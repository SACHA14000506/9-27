project,parent_hashes,commit_hash,author_name,author_email,author_date,author_date_unix_timestamp,commit_message,la,ld,fileschanged,nf,ns,nd,entropy,ndev,lt,nuc,age,exp,rexp,sexp,classification,fix,is_buggy_commit
pytorch,c6d7e1e6bfd52a8f93debd44aaaecde4daf759ac,0025e1c776938cc824974a374ba3156c98941510,Kongsea,kongsea@gmail.com,Mon Jul 10 03:20:07 2017 -0500,1499656807.0,"Fix typos in the docstrings of Conv3d, AvgPool3d and MaxPool3d (#2030)

* Fix a typo of the docstring of Conv3d

* Fix typos in docstrings of 3D operations.",3.0,3.0,"torch/nn/modules/conv.py,torch/nn/modules/pooling.py",2.0,3,1,0.918295834,31.0,1511.0,1.0,455539.0,1105.0,16360.4346,0.0,Corrective,1.0,1
pytorch,e24eee04f071123c0dca80fa5b354e3bb76fe16a,00a5980cdf5ce6a181345eb7983e0b1f87ac643a,Adam Paszke,adam.paszke@gmail.com,Sun Jan 29 00:22:41 2017 +0100,1485649361.0,Improve RNN doc formatting,45.0,44.0,torch/nn/modules/rnn.py,1.0,3,1,0,19.0,508.0,1.0,97712.0,403.0,3972.074258,0.0,Perfective,0.0,1
pytorch,81d40aaf96e84adde9f4fa2fe26761f1a54bc0b6,00b8ebe60c5dfa3c14a76e71e0166d0bb8cda4e3,James Reed,jamesreed@fb.com,Thu Oct 08 04:32:51 2020 -0700,1602131571.0,"[FX] Preserve type annotations on generated code in Graph (#45880)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45880

Test Plan: Imported from OSS

Reviewed By: dzhulgakov

Differential Revision: D24127303

Pulled By: jamesr66a

fbshipit-source-id: 3a042bcfb0bf9f58ac318cc814dfc3cca683c7f8",137.0,37.0,"test/test_fx.py,torch/fx/graph.py,torch/fx/node.py,torch/fx/proxy.py,torch/fx/symbolic_trace.py",5.0,3,2,1.746911112,1.0,1817.0,3.0,498990.6,5811.0,13526.5,0.0,,0.0,1
pytorch,6fde0cb507d59d2a9168f3051feba6865e9d1048,00d2befba11a1e9c85146a4470721eb75596d5b7,Richard Zou,zou3519@users.noreply.github.com,Tue Nov 07 18:47:20 2017 -0500,1510080440.0,THTensor_varOuterDim numeric stability (#3533),31.0,16.0,"aten/src/THC/THCTensorMathReduce.cuh,aten/src/THC/generic/THCTensorMathReduce.cu,test/test_cuda.py",3.0,5,2,1.054058945,37.0,2282.0,1.0,78411.0,2084.0,24024.85823,0.0,,0.0,1
pytorch,62aa4e096b8414ad22796d0dbffa12988a9f3793,00f3e0d8c9be00973d93fb2a9a7c6ec381b6b5a2,Catherine Lee,csl@fb.com,Fri Jan 27 17:52:33 2023 +0000,1674841953.0,"[ci] Set step level timeout (#93084)

Not super important, but it is nice for the logs because the logs now say ""the action timed out"" instead of ""the action was cancelled"".  It also makes the job status ""failure"" instead of ""cancelled""

also adds timeout minutes as an input for rocm and mac tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/93084
Approved by: https://github.com/huydhn",41.0,4.0,".github/workflows/_linux-test.yml,.github/workflows/_mac-test.yml,.github/workflows/_rocm-test.yml",3.0,2,1,1.521928095,1.0,749.0,1.0,167322.0,11754.0,27314.0,0.0,Feature Addition,0.0,1
pytorch,e86d8ebfb6722a252261c4c82e5d7d2e9bba8ea8,01164988b3233e6178b9efe487d54a3e4578464f,Richard Zou,zou3519@gmail.com,Thu Apr 28 22:14:31 2022 -0700,1651184071.0,[functorch] Some more test annotations,21.0,23.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1590.0,1.0,0.0,1020.0,1403.5,0.0,,0.0,1
pytorch,64657930116e9b40a52fc7efeba07940367e7a7f,0118dec2e3085f195d0c5e922a490a072a5e50fb,Jacob Szwejbka,jakeszwe@fb.com,Tue Feb 02 18:25:56 2021 -0800,1612290356.0,"[Pytorch] Expanded Bundled Inputs To Any Public Function (#51153)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51153

Enabled bundled inputs for all public functions that the user wants in a torchscript module. An important caveat here is that you cant add bundled inputs to functions that were in the nn.module but weren't caught in the scripting/tracing process that brought the model to torchscript.

Old Api is exactly the same. Still only works on forward, return types the same, etc.

-----------New API-------------

Attachment of inputs:

***augment_model_with_bundled_inputs*** : works the same as before but added the option to specify an info dictionary.

***augment_many_model_functions_with_bundled_inputs*** : Similar to the above function but allows the user to specify a Dict[Callable, List[<inputs>]] (mapping function references to the bundled inputs for that function) to attach bundled inputs to many functions

Consumption of inputs:

***get_all_bundled_inputs_for_<function_name>()*** : Works exactly like get_all_bundled_inputs does, but can be used for functions other then forward if you know ahead of time what they are called, and if they have bundled inputs.

***get_bundled_inputs_functions_and_info()*** : This is easily the hackiest function. Returns a Dict['str', 'str'] mapping function_names to get_all_bundled_inputs_for_<function_name>. A user can then execute the functions specified in the values with something like
    all_info = model.get_bundled_inputs_functions_and_info()
    for func_name in all_info.keys():
        input_func_name = all_info[func_name]['get_inputs_function_name'][0]
        func_to_run = getattr(loaded, input_func_name)
The reason its done this way is because torchscript doesn't support 'Any' type yet meaning I can't return the bundled inputs directly because they could be different types for each function. Torchscript also doesn't support callable so I can't return a function reference directly either.
ghstack-source-id: 120768561

Test Plan:
Got a model into torchscript using the available methods that I'm aware of (tracing, scripting, old scripting method). Not really sure how tracing brings in functions that arent in the forward call path though. Attached bundled inputs and info to them successfully. Changes to TorchTest.py on all but the last version of this diff (where it will be/is removed for land) illustrate what I did to test.

Created and ran unit test

Reviewed By: dreiss

Differential Revision: D25931961

fbshipit-source-id: 36e87c9a585554a83a932e4dcf07d1f91a32f046",290.0,80.0,"test/test_bundled_inputs.py,torch/utils/bundled_inputs.py,torch/utils/mobile_optimizer.py",3.0,3,2,0.957248601,1.0,415.0,2.0,2864099.6666666665,8562.0,19295.5,0.0,Feature Addition,0.0,1
pytorch,92fde6cf067450e921b95731aaecd9b024e493d9,014372e707a07ff211c8d0d7dfecea559ea6de76,Gregory Chanan,gchanan@fb.com,Wed May 10 18:33:24 2017 -0700,1494441204.0,"Support ""fused"" ops: addcmul/addcdiv.",284.0,123.0,"test/test_torch.py,tools/cwrap/plugins/Broadcast.py,torch/csrc/generic/methods/TensorMath.cwrap",3.0,8,3,1.162309139,31.0,5607.0,3.0,0.0,883.0,11434.44394,0.0,Feature Addition,0.0,1
pytorch,602aec325d2e4baa9dbf5c74f4dc205dd7c10031,0150f40ddea0aa65336e55e4ac549d400c8e180f,Francis Charette Migneault,francis.charette.migneault@gmail.com,Fri Feb 14 06:13:38 2020 -0800,1581660818.0,"dont force msvc /Ox flag which can conflict with /RTC1 in debug config (#33164)

Summary:
Relates to https://github.com/pytorch/pytorch/issues/33132

This fix doesn't add full multi-configuration support described in https://github.com/pytorch/pytorch/issues/33132 but at least avoid the error presented in the issue when `CMAKE_BUILD_TYPE=Debug` is used with MSVC.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33164

Differential Revision: D19899727

Pulled By: ezyang

fbshipit-source-id: 28a364d920c4a3fb577c6b484ccd69a133fbcf5d",15.0,11.0,"cmake/Codegen.cmake,cmake/public/utils.cmake",2.0,2,1,0.961236605,3.0,511.0,2.0,8764773.5,14752.0,39654.33333,0.0,Corrective,1.0,1
pytorch,34643a6c87d56aa3a9d60d499aa74673171c518f,01562488779eab32298047f852153ce9cdaef607,Richard Zou,zou3519@gmail.com,Thu Mar 10 16:25:34 2022 -0800,1646929534.0,[functorch] unsupported random errors,45.0,1.0,"functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/test/test_vmap.py",2.0,4,1,0.95033767,1.0,3930.0,1.0,0.0,876.0,1226.0,0.0,,0.0,1
pytorch,b6672b10e153b63748874ca9008fd3160f38c3dd,017b0ae9431ae3780a4eb9bf6d8865dfcd02cd92,Kulin Seth,kulinseth@gmail.com,Tue May 31 02:09:03 2022 +0000,1653962943.0,"MPS: Fix crashes in view tensors due to buffer size mismatch (#78496)

Fixes #78247, #77886

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78496
Approved by: https://github.com/albanD, https://github.com/malfet",35.0,27.0,"aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/LinearAlgebra.mm,aten/src/ATen/native/mps/operations/LossOps.mm,aten/src/ATen/native/mps/operations/UnaryOps.mm,test/test_mps.py",7.0,7,2,2.37637577,1.0,7506.0,5.0,593291.0,3736.0,8817.0,0.0,Corrective,1.0,1
pytorch,afeeb81e790602f274c1543ae395c918019f2a73,01a35dcace904876c636005e725611079c375bf3,Adam Paszke,adam.paszke@gmail.com,Tue Apr 11 13:33:22 2017 -0700,1491917602.0,Fix coalesced CUDA collectives for nonhomogeneous lists,58.0,2.0,"test/test_cuda.py,torch/cuda/comm.py",2.0,3,2,0.566509507,28.0,1067.0,1.0,4020.0,616.0,4396.553486,0.0,Corrective,1.0,1
pytorch,33e9a0b5f6674bd429bda689534e1c987b38cf6e,01a88625820642f8580b0434f805b75fc0930748,Nikita Vedeneev,nik@quansight.com,Tue Nov 16 15:25:37 2021 -0800,1637076337.0,"OpInfo tests for `nn.functional.max_pool{n}d`. (#68075)

Summary:
As per title.

It is planned to use these tests for fixing issues with the max_unpools' backward methods reported in https://github.com/pytorch/pytorch/issues/67658 and https://github.com/pytorch/pytorch/issues/67657.
max_unpool.backward methods are not tested and implemented with custom kernels. We can replace these kernels with advanced indexing operations (i.e. `gather`) which are efficient and well tested.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68075

Reviewed By: malfet

Differential Revision: D32308317

Pulled By: mruberry

fbshipit-source-id: 9f91c6e6a9d78c19230e93fc0a3164f4eb7b8ec5",106.0,28.0,"torch/csrc/jit/runtime/symbolic_shape_registry.cpp,torch/testing/_internal/common_methods_invocations.py",2.0,6,1,0.295832178,2.0,13997.0,2.0,811819.5,17108.0,40233.5,0.0,Corrective,1.0,1
pytorch,54e03cdda9fca7fcd8b29e40812213b6ebc8c091,01dbbeeeb5ab7ede28e333982e98713282a0e4b8,Sherlock Huang,bahuang@fb.com,Tue Sep 27 04:15:56 2022 +0000,1664252156.0,"Expose cpp_backtrace to python binding (#84896)

We can now get cpp stack trace by calling torch.utils.get_cpp_backtrace()

Sample output when calling from a torch_dispatch stack:
```
<omitting python frames>
frame #23: torch::handle_torch_function_no_python_arg_parser(c10::ArrayRef<pybind11::handle>, _object*, _object*, char const*, _object*, char const*, torch::TorchFunctionName) (0x7f69330bab90 in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/utils/python_arg_parser.cpp:323)
frame #24: <unknown function> (0x7f6932a09e79 in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/autograd/python_variable.cpp:2252)
frame #25: <unknown function> (0x7f69261aee33 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/PythonFallbackKernel.cpp:56)
frame #26: <unknown function> (0x7f69261afef9 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/BoxedKernel_impl.h:19)
frame #27: c10::BoxedKernel::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const (0x7f6932aadced in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/BoxedKernel_impl.h:41)
frame #28: <unknown function> (0x7f6926fae9b9 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/impl/boxing.h:227)
frame #29: at::Tensor c10::Dispatcher::redispatch<at::Tensor, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&)> const&, c10::DispatchKeySet, at::Tensor const&) const (0x7f6926e821f5 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/KernelFunction_impl.h:106)
frame #30: at::_ops::alias::redispatch(c10::DispatchKeySet, at::Tensor const&) (0x7f6927142c31 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/dispatch/Dispatcher.h:438)
frame #31: <unknown function> (0x7f692ae4f8be in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:1361)
frame #32: <unknown function> (0x7f692ae4f9b1 in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:1362)
frame #33: <unknown function> (0x7f692aef77e9 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13)
frame #34: <unknown function> (0x7f6926fae7d8 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/KernelFunction_impl.h:50)
frame #35: at::Tensor c10::Dispatcher::redispatch<at::Tensor, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&)> const&, c10::DispatchKeySet, at::Tensor const&) const (0x7f6926e821c9 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/KernelFunction_impl.h:97)
frame #36: at::_ops::alias::redispatch(c10::DispatchKeySet, at::Tensor const&) (0x7f6927142c31 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/dispatch/Dispatcher.h:438)
frame #37: <unknown function> (0x7f6929ec654a in /fsx/users/bahuang/repos/pytorch_fsx/build/aten/src/ATen/RedispatchFunctions.h:10697)
frame #38: <unknown function> (0x7f6929d9edae in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/autograd/generated/VariableType_1.cpp:2837)
frame #39: <unknown function> (0x7f6929d9f043 in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/autograd/generated/VariableType_1.cpp:2838)
frame #40: <unknown function> (0x7f6929e7d2f9 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/impl/WrapFunctionIntoFunctor.h:13)
frame #41: <unknown function> (0x7f6929eb1344 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:478)
frame #42: <unknown function> (0x7f6929ea7b99 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:490)
frame #43: <unknown function> (0x7f6929e7d370 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:563)
frame #44: <unknown function> (0x7f6929e7d43a in /fsx/users/bahuang/repos/pytorch_fsx/c10/util/C++17.h:239)
frame #45: <unknown function> (0x7f6929e7d48c in /fsx/users/bahuang/repos/pytorch_fsx/c10/util/C++17.h:364)
frame #46: <unknown function> (0x7f6929e7d50a in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:554)
frame #47: c10::BoxedKernel::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const (0x7f6932aadced in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/BoxedKernel_impl.h:41)
frame #48: c10::KernelFunction::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const (0x7f6932aadd26 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/KernelFunction_impl.h:43)
frame #49: c10::Dispatcher::redispatchBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const (0x7f692603890a in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/dispatch/Dispatcher.h:652)
frame #50: <unknown function> (0x7f69260387f9 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/dispatch/Dispatcher.h:388)
frame #51: <unknown function> (0x7f69261af0ef in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/PythonFallbackKernel.cpp:96)
frame #52: <unknown function> (0x7f69261aff2b in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/BoxedKernel_impl.h:25)
frame #53: c10::BoxedKernel::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const (0x7f6932aadced in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/BoxedKernel_impl.h:41)
frame #54: c10::KernelFunction::callBoxed(c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const (0x7f6932aadd26 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/boxing/KernelFunction_impl.h:43)
frame #55: c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const (0x7f6925fd6ab2 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/dispatch/Dispatcher.h:628)
frame #56: <unknown function> (0x7f6925fd6690 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/dispatch/Dispatcher.h:376)
frame #57: <unknown function> (0x7f692bf5b525 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/dispatch/Dispatcher.h:380)
frame #58: <unknown function> (0x7f692bf59fac in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/jit/runtime/register_c10_ops.cpp:15)
frame #59: <unknown function> (0x7f692bf5af41 in /usr/include/c++/7/bits/std_function.h:316)
frame #60: std::function<void (std::vector<c10::IValue, std::allocator<c10::IValue> >&)>::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >&) const (0x7f6932ab9a0f in /usr/include/c++/7/bits/std_function.h:706)
frame #61: <unknown function> (0x7f6932aad541 in /fsx/users/bahuang/repos/pytorch_fsx/aten/src/ATen/core/stack.h:41)
frame #62: <unknown function> (0x7f6932ab3102 in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/jit/python/pybind_utils.h:1206 (discriminator 1))
frame #63: <unknown function> (0x7f6932ab3943 in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/jit/python/pybind_utils.h:1272)
frame #64: <unknown function> (0x7f6932a46120 in /fsx/users/bahuang/repos/pytorch_fsx/torch/csrc/jit/python/init.cpp:1767)
frame #65: <unknown function> (0x7f6932a997be in /fsx/users/bahuang/repos/pytorch_fsx/third_party/pybind11/include/pybind11/cast.h:1441)
frame #66: <unknown function> (0x7f6932a8a985 in /fsx/users/bahuang/repos/pytorch_fsx/third_party/pybind11/include/pybind11/cast.h:1410)
frame #67: <unknown function> (0x7f6932a66e1e in /fsx/users/bahuang/repos/pytorch_fsx/third_party/pybind11/include/pybind11/pybind11.h:249)
frame #68: <unknown function> (0x7f6932a66ec2 in /fsx/users/bahuang/repos/pytorch_fsx/third_party/pybind11/include/pybind11/pybind11.h:224)
frame #69: <unknown function> (0x7f6932473111 in /fsx/users/bahuang/repos/pytorch_fsx/third_party/pybind11/include/pybind11/pybind11.h:929)
frame #104: __libc_start_main (0x7f693485dc87 in /build/glibc-uZu3wS/glibc-2.27/csu/../csu/libc-start.c:310)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84896
Approved by: https://github.com/ezyang",27.0,0.0,"torch/_C/__init__.pyi.in,torch/csrc/Module.cpp,torch/utils/__init__.py,torch/utils/cpp_backtrace.py",4.0,4,1,1.371305402,43.0,2847.0,3.0,312555.75,7704.0,18102.0,0.0,Feature Addition,1.0,1
pytorch,cffad597ea620600472c509b642a9ad03a56fdbe,01ddd5dde6cf6238bb5e0aed99ffce9d2f4aa8a8,kshitij12345,kshitijkalambarkar@gmail.com,Sun Nov 28 00:06:03 2021 -0800,1638057963.0,"[opinfo] use dtypes instead of dtypesIfCPU (#68732)

Summary:
Reland https://github.com/pytorch/pytorch/issues/67619

Replace usage of dtypesIfCPU with dtypes in OpInfo class and also make it a mandatory argument.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68732

Reviewed By: jbschlosser

Differential Revision: D32594344

Pulled By: mruberry

fbshipit-source-id: 660b38aef97752ba064228e8989041ed1d5777fe",153.0,194.0,"torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo_helper.py",2.0,3,1,0.090756651,2.0,14084.0,2.0,274145.0,17321.0,40698.0,0.0,,0.0,1
pytorch,38609cc47d061c6ff3e98cb693e1d5e275b03eda,02179827cb7cbc805277af2312309b81108cb914,Shen Li,cs.shenli@gmail.com,Sun Apr 02 03:59:58 2023 +0000,1680407998.0,"[Easy] Include SPMD and DTensor files in UFMT checks (#98148)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98148
Approved by: https://github.com/fegin",285.0,386.0,".lintrunner.toml,test/distributed/_spmd/test_tracing.py,test/distributed/_tensor/test_api.py,test/distributed/_tensor/test_common_rules.py,test/distributed/_tensor/test_device_mesh.py,test/distributed/_tensor/test_dtensor.py,test/distributed/_tensor/test_dtensor_ops.py,test/distributed/_tensor/test_init.py,test/distributed/_tensor/test_pointwise_ops.py,test/distributed/_tensor/test_utils.py,torch/distributed/_spmd/aot_function_patch.py,torch/distributed/_spmd/api.py,torch/distributed/_spmd/comm_tensor.py,torch/distributed/_spmd/distribute.py,torch/distributed/_spmd/experimental_ops.py,torch/distributed/_spmd/graph_utils.py,torch/distributed/_spmd/iter_graph_module.py,torch/distributed/_tensor/__init__.py,torch/distributed/_tensor/_utils.py,torch/distributed/_tensor/api.py,torch/distributed/_tensor/device_mesh.py,torch/distributed/_tensor/dispatch.py,torch/distributed/_tensor/examples/checkpoint_example.py,torch/distributed/_tensor/op_schema.py,torch/distributed/_tensor/ops/common_rules.py,torch/distributed/_tensor/ops/tensor_ops.py,torch/distributed/_tensor/ops/view_ops.py,torch/distributed/_tensor/placement_types.py,torch/distributed/_tensor/redistribute.py,torch/distributed/_tensor/sharding_prop.py",30.0,10,2,4.170661652,3.0,11632.0,17.0,1263957.0333333334,14047.0,32221.0,0.0,,0.0,1
pytorch,c8c967fa431920cfb232aeabe85ba6b178fa5a78,024333860331e33cbba251f62c50f55e04c97f21,Sam Gross,sgross@fb.com,Mon Oct 09 22:38:58 2017 -0700,1507588738.0,"Generate PyTorch-style NN bindings

This generates NN bindings with a similar interface to PyTorch's
torch.nn.functional package. The file nn.yaml specifies function
signatures and THNN implementations.

Each NN operation generates three functions. For example:

  - conv2d
  - conv2d_forward
  - conv2d_backward

The conv2d and conv2d_forward functions differ in how they handle
buffers that need to be passed to the backward function. conv2d_forward
takes the buffers as parameters. conv2d creates the buffers internally
and discards them.",510.0,71.0,"aten/CMakeLists.txt,aten/src/ATen/Utils.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/gen.py,aten/src/ATen/nn.yaml,aten/src/ATen/nn_parse.py,aten/src/ATen/test/scalar_test.cpp",7.0,4,1,1.607422473,1.0,1299.0,4.0,0.0,259.0,1428.5,0.0,,0.0,1
pytorch,9f71997380ca85685c007c73879c4f1669d66155,0251ba61089795a7a27c0473e4bb022805432c1f,Yael Dekel,yaeld@microsoft.com,Tue Jun 09 03:34:09 2020 -0700,1591673649.0,"Fix ONNX export of RNNs with no bias (#36894)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/34800 .

Currently, the LSTM/RNN/GRU export to ONNX can't handle models without a bias term.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36894

Reviewed By: hl475

Differential Revision: D21134794

Pulled By: houseroad

fbshipit-source-id: e71e089025a3dc7e8c883ff99cd788c5f302492e",123.0,13.0,".jenkins/caffe2/test.sh,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",3.0,6,3,0.904048388,8.0,6176.0,2.0,631308.6666666666,2673.0,6556.0,0.0,Corrective,1.0,1
pytorch,f69fb3829a35e4b46181c33f2cbfae5eb7bbe11a,025e43c2634023de4c62fe4f1afeda6a9f7565da,Calvin Lee,calvinleenyc@gmail.com,Tue Mar 13 08:38:04 2018 -0400,1520930284.0,"Attempt to fix #5718. (#5726)

* Attempt to fix #5718.

* markdown fix for LPPool1d

* It's sum pooling, not average pooling.  (I both tested this and considered the math.)",4.0,4.0,torch/nn/modules/pooling.py,1.0,3,1,0,36.0,1112.0,1.0,272690.0,2456.0,24785.35823,0.0,Corrective,1.0,1
pytorch,723a600ebdbb999392a30b5169e5073e637da52d,0262fd0f915cf44a843afafed074d20a98c5b704,Edward Yang,ezyang@fb.com,Wed Jul 25 05:24:18 2018 -0700,1532496258.0,"Delete Tensor::typeString() (#9764)

Summary:
The primary use-site of typeString was checked_cast_tensor.
I did a little more than I needed in this patch, to set
the stage for actually deleting the tensor type.

Specifically, I modified checked_cast_tensor to explicitly
take Backend and ScalarType, the idea being that once we
remove the tensor subclasses, we will delete the T template
parameter.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9764

Differential Revision: D8969196

Pulled By: ezyang

fbshipit-source-id: 9de92b974b2c28f12ddad13429917515810f24c6",54.0,28.0,"aten/src/ATen/UndefinedTensor.cpp,aten/src/ATen/UndefinedTensor.h,aten/src/ATen/Utils.h,aten/src/ATen/copy_wrapper.py,aten/src/ATen/function_wrapper.py,aten/src/ATen/templates/TensorDerived.cpp,aten/src/ATen/templates/TensorDerived.h,test/test_torch.py",8.0,5,2,2.160803303,40.0,10424.0,4.0,120955.75,3094.0,7536.333333,0.0,,1.0,1
pytorch,4a599f47fbe0595c165e395856e877148988e279,027d7f7ba55ce2617aace11330c5345b0d58b2f0,Hong Xu,hong@topbug.net,Fri Mar 13 19:23:07 2020 -0700,1584127387.0,"Delete AT_WARN and replace all AT_WARN with TORCH_WARN (#34623)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34623

The bandaid of ""AT_WARN"" keeps introducing new warnings. Let's get rid
of it entirely.

Close #34502

Test Plan: Imported from OSS

Differential Revision: D20420112

Pulled By: albanD

fbshipit-source-id: 7160c113cb4deb2d2f50a375356f423fe5e86f50",27.0,43.0,"aten/src/ATen/native/DispatchStub.cpp,aten/src/ATen/native/IndexingUtils.h,aten/src/ATen/native/LegacyDefinitions.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/cuda/LegacyDefinitions.cpp,aten/src/ATen/native/cudnn/RNN.cpp,c10/util/Exception.h,torch/csrc/autograd/functions/comm.cpp,torch/csrc/autograd/python_anomaly_mode.cpp,torch/csrc/jit/api/compilation_unit.h,torch/csrc/jit/api/function_impl.h,torch/csrc/jit/api/module.h,torch/csrc/jit/frontend/tracer.cpp,torch/csrc/jit/mobile/interpreter.cpp,torch/csrc/jit/runtime/interpreter.cpp,torch/csrc/jit/runtime/register_prim_ops.cpp,torch/csrc/jit/runtime/register_special_ops.cpp,torch/csrc/jit/runtime/vararg_functions.cpp,torch/csrc/jit/serialization/export.cpp",19.0,18,3,3.87214016,7.0,11408.0,15.0,2365674.4210526315,81.0,298.5,0.0,,0.0,1
pytorch,1d3f3a1a0cc0f66b51d8e8f4006fb200e18e2818,0282c5ae6996584b908946cb694b02443dfd3c9a,Pieter Noordhuis,pietern@fb.com,Wed Nov 27 14:40:24 2019 -0800,1574865624.0,"Add helper to aggregate multiple process groups (#25768)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25768

The round robin process group can be constructed from multiple other
process groups. Every collective call against this new process group
is delegated to the specified process groups in a round robin fashion.

Doing so may benefit performance when calling into multiple NCCL
process groups. Instead of adding support for round-robin usage of
NCCL communicators, we achieve the same without changing the NCCL
process group and adding this wrapper class.

The API to create this round robin process group is a bit harsh. If we
find it adds significant benefit we can revisit and make this a first
class citizen in the torch.distributed module.
ghstack-source-id: 94578376

Test Plan: The newly added test passes.

Reviewed By: chenyangyu1988

Differential Revision: D17226323

fbshipit-source-id: ec9f754b66f33b983fee30bfb86a1c4c5d74767d",263.0,0.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/CMakeLists.txt,torch/lib/c10d/ProcessGroupRoundRobin.cpp,torch/lib/c10d/ProcessGroupRoundRobin.hpp",5.0,7,2,1.728840849,6.0,4190.0,2.0,133057.33333333334,13522.0,36973.33333,0.0,Feature Addition,0.0,1
pytorch,a3298c2f64b1534c93197aaf96ed84cf8db7cd95,02d89f9f1d7f32ebf7ec509d5c14b2f39690997a,Rohan Varma,rvarm1@fb.com,Sat Dec 05 02:40:24 2020 -0800,1607136024.0,"scatter_object_list API for c10d (#43930)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/43930

Closes #23232. As part of addressing #23232, this PR adds support for scatter_object_list which is an API to scatter arbitrary picklable objects to all the other ranks.

The implementation approach follows a similar approach as https://github.com/pytorch/pytorch/pull/42189. The result of the `scatter` is stored as the first element of `scatter_object_output_list`, and the src rank is expected to provide an input list `scatter_object_input_list` which contains the objects to scatter.

Note that this API requires 1 broadcast and 2 scatters. This is because we must communicate the maximum object size to be scattered, which only the src rank knows about. After that, we also need to communicate the objects themselves as well as the true sizes of the object.

Note that the API is designed to match the tensor-based collectives other than supporting async_op. For now, it is a blocking call. If we see demand to support async_op, we will have to make more progress on merging work/future to support this.

It only works for Gloo because NCCL doesn't support scatter.
ghstack-source-id: 117904065

Reviewed By: mrshenli

Differential Revision: D23430686

fbshipit-source-id: f033b89cd82dadd194f2b036312a98423449c26b",113.0,0.0,"torch/distributed/distributed_c10d.py,torch/testing/_internal/distributed/distributed_test.py",2.0,5,1,0.821629295,2.0,6555.0,2.0,4763.0,7230.0,16334.5,0.0,Feature Addition,0.0,1
pytorch,98c02c20b161ff8e4181e24336de4936039e9f5f,02e7eba309f42c65c77945c5c7ca793859c50bc5,Alican Bozkurt,alicanb@gmail.com,Tue Jan 02 00:41:18 2018 +0300,1514853678.0,"Implement Chi2 distribution (#4425)

* add chi2

* add tests for chi2

* add randomized test comments",122.0,1.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/chi2.py",4.0,5,3,1.32774556,8.0,1058.0,2.0,308362.6666666667,2234.0,24315.35823,0.0,Feature Addition,0.0,1
pytorch,334e78b1ce1d0ab658baa4621a7b4efb3bda2798,03007b3dda7c4b5bc339106bbf0dd9f7a409c7c9,Lingyi Liu,lingyiliu@fb.com,Wed Sep 25 20:40:48 2019 -0700,1569444048.0,"Quantized Interpolate Kernel(upsample_bilinear2d) (#26631)

Summary:
We implement the quantized upsample_bilinear2d case for interpolate kernel in this PR.

For nhwc performance improvement:
import torch, time

for dtype in [torch.qint8, torch.quint8, torch.qint32]:
    print('****', str(dtype), '*****')
    x = torch.rand(1, 56, 56, 256)

    q_x = torch.quantize_per_tensor(x, 0.5, 1, dtype)
    q_x = q_x.permute([0, 3, 1, 2])

    x = x.permute([0, 3, 1, 2])

    NITER = 100

    s = time.time()
    for i in range(NITER):
        float_out = torch.nn.functional.interpolate(x, size=5, scale_factor=None, mode=""bilinear"", align_corners=True)
    time_per_iter_float = (time.time() - s) / NITER

    s = time.time()
    for i in range(NITER):
        quant_out = torch.nn.quantized.functional.interpolate(q_x, size=5, scale_factor=None, mode=""bilinear"", align_corners=True)
    time_per_iter_quant = (time.time() - s) / NITER

    ref_quantized = torch.quantize_per_tensor(float_out, 0.5, 1, dtype)
    #  torch.testing.assert_allclose(ref_quantized.dequantize(), quant_out.dequantize())

    print('time/iter ms (float)', 'time/iter ms (quant)', 'quant/float', sep='\t')
    print(time_per_iter_float * 1000, time_per_iter_quant * 1000, time_per_iter_quant / time_per_iter_float, sep='\t')

    bytes_float = (x.numel() + float_out.numel()) * x.element_size()
    bytes_quant = (q_x.numel() + quant_out.numel()) * q_x.element_size()

    float_bw_gbps = bytes_float / time_per_iter_float / 1e9
    quant_bw_gbps = bytes_quant / time_per_iter_quant / 1e9

    print('GB/s float', 'GB/s quant', sep='\t')
    print(float_bw_gbps, quant_bw_gbps, sep='\t')

===========without nhwc handling===========
**** torch.qint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
1.999044418334961       2.5860953330993652      1.2936657681940702
GB/s float      GB/s quant
1.6192056416115257      0.3129103516188541
**** torch.quint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
2.02730655670166        2.6061582565307617      1.2855274639721328
GB/s float      GB/s quant
1.596632728927902       0.3105014816242217
**** torch.qint32 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
2.0180463790893555      2.4047350883483887      1.1916153728010588
GB/s float      GB/s quant
1.603959172365819       1.3460376636426636

===========with nhwc handling===========

**** torch.qint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
2.0913314819335938      0.09696483612060547     0.04636512047863123
GB/s float      GB/s quant
1.5477527249803915      8.345458337015
**** torch.quint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
2.1065664291381836      0.09959936141967773     0.04728042754408879
GB/s float      GB/s quant
1.5365591871338384      8.124710725706763
**** torch.qint32 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
2.044203281402588       0.6003522872924805      0.29368521846837126
GB/s float      GB/s quant
1.5834354779917448      5.391607675216635
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26631

Differential Revision: D17521498

Pulled By: llyfacebook

fbshipit-source-id: 385ae0f77777cd8bee385cafb80e492127b7d103",383.0,55.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp,test/test_quantized.py,torch/nn/quantized/functional.py",6.0,11,3,1.684392402,10.0,9690.0,4.0,115925.8,11744.0,32969.33333,0.0,Feature Addition,0.0,1
pytorch,221edddd18e2c434a67e97689f882e42862e7ada,03132c1f56df2124866382f1b3376c09ede0c534,Ailing Zhang,ailzhang@fb.com,Thu Feb 28 05:36:37 2019 -0800,1551332197.0,"convolution/matmul/dropout (#17523)

Summary:
* Add AD formula for _convolution & matmul & dropout
* add prim::range, fixes #17483
Example:
```
dim = 3
x = range(dim)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17523

Differential Revision: D14254002

Pulled By: ailzhang

fbshipit-source-id: ba60d77b047db347929b72beca2623fb26aec957",112.0,41.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/Dropout.cpp,torch/csrc/jit/autodiff.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/script/compiler.cpp,torch/csrc/jit/symbolic_script.cpp",6.0,9,2,1.185032679,8.0,7178.0,5.0,1331039.0,7256.0,22173.33333,0.0,Corrective,1.0,1
pytorch,22ddddfb809249d0e9bf06a80349b927e3984e54,03617574d38aec868a4e477b94b2289c9568940e,Iurii Zdebskyi,iuriiz@fb.com,Wed Jun 05 17:07:12 2019 -0700,1559754432.0,"Ð¡hange type of a tensor with bools (#19097)

Summary:
**This is **bc-breaking** change**
Change dtype of a tensor which was created from bool data.
Old behavior: torch.tensor([True, False]) -> uint8 tensor
Now: torch.tensor([True, False]) -> bool tensor

Tested via tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19097

Reviewed By: ezyang

Differential Revision: D15632553

Pulled By: izdeby

fbshipit-source-id: b019150844c561a6845710a3c62b12f06b68bbe3",14.0,11.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/core/jit_type.h,test/test_dataloader.py,test/test_jit.py,test/test_torch.py,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/jit/register_special_ops.cpp,torch/csrc/utils/tensor_new.cpp",8.0,10,3,2.774693952,42.0,35375.0,8.0,455492.875,9166.0,26780.83333,0.0,,0.0,1
pytorch,98581b9f7eab4d05613c39bb0f0573b9455e2bb8,036c3f93af1b94bb133fe5db7b50f48672b9171c,Sam Gross,colesbury@gmail.com,Thu May 25 04:35:19 2017 -0400,1495686919.0,"Check for released variables in SavedVariable::unpack() (#1648)

Fixes #1288",26.0,10.0,"test/test_nn.py,torch/csrc/autograd/functions/convolution.cpp,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h",5.0,5,2,2.155221529,28.0,4907.0,5.0,521452.2,783.0,12860.56049,0.0,Corrective,1.0,1
pytorch,2a2007e5aca575eb495e42dbf4404451a351eeca,0397d7c0c8501aaa8c4fe2a4857f43213adbfd5d,Richard Zou,zou3519@gmail.com,Wed Apr 10 01:08:59 2019 -0700,1554858539.0,"EmbeddingBag w/ per_sample_weights CPU backward (#18799)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18799
ghimport-source-id: 58a6f629e890449013f24a9b6282664ca2a1e3ba

Reviewed By: cpuhrsch

Differential Revision: D14851417

Pulled By: zou3519

fbshipit-source-id: c36b9d469989354bf6cef1c2c3dc4f13e7cb1a25",98.0,46.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,test/test_nn.py",4.0,6,2,1.164045869,43.0,14616.0,1.0,4.0,8011.0,24164.33333,0.0,,0.0,1
pytorch,43b303bfc009269a05a2785e4a04aae4b3fc9970,03bfd7a8730c4c224c0c3a0765790c7d48ed0c48,Junjie Bai,jbai@fb.com,Thu Oct 19 02:55:27 2017 -0700,1508381727.0,"In Predictor interface allow real model inputs to be fed in run* functions

Summary: https://github.com/caffe2/caffe2/issues/1294

Reviewed By: jerryzh168

Differential Revision: D6086990

fbshipit-source-id: 7d21269055d91cc223a72f6352cdb45584f5b56b",13.0,13.0,"caffe2/core/predictor.cc,caffe2/core/predictor_test.cc",2.0,2,1,1,3.0,245.0,1.0,1060357.0,2139.0,5324.833333,0.0,,0.0,1
pytorch,dd8bfe2b933bfc4766786372f255113c99fcac99,03cc9fabd4d31e8165b391311f896374fe81fe69,Prabhat Roy,prabhatr@usc.edu,Fri Apr 16 17:45:27 2021 -0700,1618595127.0,"Added complex datatype support to sigmoid on cuda (#55975)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/55359

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55975

Reviewed By: ezyang

Differential Revision: D27770438

Pulled By: prabhat00155

fbshipit-source-id: 730193950805ce28d8672104fe446a647194e8cb",11.0,6.0,"aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,torch/testing/_internal/common_methods_invocations.py",4.0,9,2,1.75748399,8.0,6510.0,3.0,51086.5,10922.0,24094.5,0.0,Corrective,1.0,1
pytorch,bd0e564d40dcc8f87b8c8d11ff8a7ad904da9ecc,03d4198a67881f8bab8e4be58ed349bafa41cd01,James Reed,jamesreed@fb.com,Sat Sep 07 02:25:52 2019 -0700,1567823152.0,"Use more efficient specialized Quantize routine (#25731)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25731

I didn't notice this before, but the QuantizeAvx2 routine was requantizing only a single vector of 8 floats into 1/4 of a 256-bit int8 register. This switches it to use a specialization that goes from 4 float vectors into a whole int8 vector, borrowed from C2

Test Plan: Imported from OSS

Differential Revision: D17214413

Pulled By: jamesr66a

fbshipit-source-id: 1d6fc556e43739e9a4b0dba5df2332beb1b3795b",71.0,53.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,test/test_quantized.py",2.0,6,2,0.067669825,1.0,2314.0,2.0,58626.0,11221.0,31593.83333,0.0,,0.0,1
pytorch,6ab07febcea936e75bc95d3ebdbb087b2033ba11,03d8ab4decdd9a7391ea6c026d0b095708288ca7,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Tue Oct 11 13:03:20 2022 +0000,1665493400.0,"Skip forward AD tests for torch.native_batch_norm (#86206)

`test_forward_mode_AD` has problems with `torch.native_batch_norm` when computing Jacobian using finite-differences. Weirdly this test unexpectedly passed on periodic CI. Let's skip this test instead of xfailing.
Fixes https://github.com/pytorch/pytorch/issues/86175
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86206
Approved by: https://github.com/soulitzer",2.0,1.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,7.0,18236.0,1.0,72587.0,8223.0,19527.5,0.0,Corrective,1.0,1
pytorch,665c148e423bf36a06b45a1b52b3ec68403d665c,03f3a0331ba6aaa0d65ce99e6e7a84161d9fa42c,Brian Hirsh,hirsheybar@fb.com,Thu Oct 28 17:43:11 2021 -0700,1635442991.0,"add slice/select/diagonal_scatter variants as primitive ops (#64430)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64430

The functionalization pass needs `{view}_scatter` versions of the slice/select/diagonal ops in order to correctly propagate mutations from a view to its base. On top of that, the implementations need to be primitive w.r.t. autograd, because they look something like `...slice().copy_()`, and the functionalization pass can't use views + mutations inside of it's own alias-removal machinery!

I added some basic tests that I tried to base off of existing tests for views (particularly around testing the derivative formulas), but I'm wondering if I should add something more comprehensive.

Also, as_strided fits into this category - the functionalization pass will need an `as_strided_scatter` op that's primitive w.r.t. autograd. I didn't add it for now, because it'll involve duplicating a bunch of logic from the current `as_strided_backward()` function, and also writing a derivative formula that I wasn't sure how to write :)

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D31942092

Pulled By: bdhirsh

fbshipit-source-id: c702a57c2748a7c771c14e4bcc3e996b48fcc4c8",312.0,10.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",7.0,9,3,2.180564136,33.0,45391.0,5.0,335462.28571428574,16666.0,39066.0,0.0,Corrective,0.0,1
pytorch,24461a756a90209df45073193ebb2d310f0a830b,0427afadd1a2a9b0c12906664d010920b6f8c6fb,Edward Z. Yang,ezyang@mit.edu,Tue May 01 14:28:31 2018 -0400,1525184911.0,"Make AT_ASSERT/AT_ERROR non-printf based, other tweaks (#7104)

* Make AT_ASSERT/AT_ERROR non-printf based, other tweaks

- AT_ASSERT/AT_ERROR don't take printf strings anymore; instead,
  they take a comma-separated list of things you wanted to print
  (bringing it inline with Caffe2's conventions).

  Instead of AT_ASSERT(x == 0, ""%d is not zero"", x)
  you write AT_ASSERT(x == 0, x, "" is not zero"")

  This is done by way of a new variadic template at::str(), which
  takes a list of arguments and cats their string reps (as per
  operator<<) together.

- A bunch of the demangling logic that was in Error.h is now
  moved to Error.cpp (better header hygiene.)  Also, demangle
  has been moved out to its own helper function, and also
  a new helper demangle_type (from Caffe2) added.

- A bunch of AT_ASSERT converted into AT_CHECK, to more properly
  convey which checks can be caused by user error, and which are
  due to logic error in ATen.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* CR

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* Fix test failure.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* buildfix

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* More fixes.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* One more fix

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* Try harder

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",418.0,374.0,"aten/src/ATen/ArrayRef.h,aten/src/ATen/CheckGenerator.h,aten/src/ATen/Context.h,aten/src/ATen/Dispatch.h,aten/src/ATen/Error.cpp,aten/src/ATen/Error.h,aten/src/ATen/ExpandUtils.h,aten/src/ATen/MatrixRef.h,aten/src/ATen/Scalar.h,aten/src/ATen/TensorGeometry.h,aten/src/ATen/TensorOperators.h,aten/src/ATen/Utils.h,aten/src/ATen/WrapDimUtilsMulti.h,aten/src/ATen/copy_wrapper.py,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/ConvolutionTBC.cpp,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/Memory.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/RoiPooling.cpp,aten/src/ATen/native/SparseMM.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cpu/CapabilityDispatch.h,aten/src/ATen/native/cuda/RoiPooling.cu,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/templates/Functions.h,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/TensorDense.cpp,aten/src/ATen/templates/TensorMethods.h,aten/src/ATen/templates/TensorSparse.cpp,aten/src/ATen/templates/Type.cpp,test/cpp_extensions/cuda_extension.cpp,tools/autograd/templates/Functions.cpp,tools/autograd/templates/VariableType.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/tensor_new.cpp",41.0,18,4,3.322732444,12.0,10095.0,18.0,1906183.075,1022.0,2654.305292,0.0,Corrective,1.0,1
pytorch,4bbff920148f282555f52677d9325b683ba2d1b2,0436ea125b3557e6855fb2f36de0aeb8623555b2,Peter Bell,peterbell10@live.co.uk,Fri Jan 22 17:30:18 2021 -0800,1611336618.0,"OpInfo: Remove promotes_integers_to_float and infer it instead (#50279)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50279

This allows different sample inputs to have different behavior for the same
operator. For example, `div(..., rounding_mode='true')` will promote but other
rounding modes don't. The current boolean flag is too restrictive to allow this.

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D25950011

Pulled By: mruberry

fbshipit-source-id: 7e82b82bedc626b2b6970d92d5b25676183ec384",60.0,75.0,"test/test_ops.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",4.0,4,2,1.214102173,2.0,7178.0,4.0,51557.0,8278.0,18704.0,0.0,,0.0,1
pytorch,a0d08b2199660cd595121926a231ebb85aaabc5f,04526a49d3c2a691d1a85565c4a96cf3fd6525ad,Supriya Rao,supriyar@fb.com,Fri Oct 02 06:51:12 2020 -0700,1601621472.0,"[quant] creating quint4x2 dtype for quantized tensors (#44678)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/44678

This is a prototype PR that introduces 4 bit qtensors. The new dtype added for this is c10::quint4x2
The underlying storage for this is still uint8_t, so we pack 2 4-bit values in a byte while quantizing it.

This change uses most of the existing scaffolding for qtensor storage. We allocate storage
based on the dtype before creating a new qtensor.

It also adds a dispatch mechanism for this dtype so we can use this to get the bitwidth, qmin and qmax info
while quantizing and packing the qtensor (when we add 2-bit qtensor)

Kernels that use this dtype should be aware of the packing format.

Test Plan:
Locally tested
```
x = torch.ones((100, 100), dtype=torch.float)
qx_8bit = torch.quantize_per_tensor(x, scale=1.0, zero_point=2, dtype=torch.quint8)
qx = torch.quantize_per_tensor(x, scale=1.0, zero_point=2, dtype=torch.quint4x2)

torch.save(x, ""temp.p"")
print('Size float (B):', os.path.getsize(""temp.p""))
os.remove('temp.p')

torch.save(qx_8bit, ""temp.p"")
print('Size quantized 8bit(B):', os.path.getsize(""temp.p""))
os.remove('temp.p')

torch.save(qx, ""temp.p"")
print('Size quantized 4bit(B):', os.path.getsize(""temp.p""))
os.remove('temp.p')
```

Size float (B): 40760
Size quantized 8bit(B): 10808
Size quantized 4bit(B): 5816

Imported from OSS

Reviewed By: raghuramank100

Differential Revision: D23993134

fbshipit-source-id: 073bf262f9680416150ba78ed2d932032275946d",274.0,39.0,"aten/src/ATen/DLConvertor.cpp,aten/src/ATen/Dispatch.h,aten/src/ATen/native/quantized/affine_quantizer.cpp,aten/src/ATen/native/quantized/affine_quantizer.h,aten/src/ATen/native/quantized/cpu/int_repr_quant.cpp,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/quantized/Quantizer.cpp,aten/src/TH/CMakeLists.txt,aten/src/TH/THGenerateQTypes.h,aten/src/TH/THGenerateQUInt4x2Type.h,aten/src/TH/generic/THStorage.h,c10/core/ScalarType.h,c10/util/quint4x2.h,c10/util/typeid.cpp,c10/util/typeid.h,test/quantization/test_quantized_tensor.py,tools/pyi/gen_pyi.py,torch/__init__.py,torch/csrc/Module.cpp,torch/csrc/Storage.h,torch/csrc/utils.h,torch/csrc/utils/tensor_dtypes.cpp",22.0,20,5,3.618783918,42.0,9130.0,16.0,6877532.7,5668.0,13293.5,0.0,Feature Addition,1.0,1
pytorch,d635d0f86eaf4e8d2e8db6f717585846c85297a2,04c5d978b9785f373450157082f35679841fa486,BowenBao,bowbao@microsoft.com,Fri Feb 11 18:31:25 2022 -0800,1644604285.0,"[ONNX] Refactor _run_symbolic_function (#67573) (#68491)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68491

* Allows implementing symbolic functions for domains other than `aten`, for example `prim`, in symbolic_opset#.py.
* Allows symbolic function to access extra context if needed, through `SymbolicFunctionState`.
  * Particularly, the `prim::PythonOp` special case can access node without the need of passing node through inputs. Updates will be made downstreams, and in a follow-up PR we will remove the previous workaround in exporter.
* `prim::Loop`, `prim::If`, etc are now moved outside of `_run_symbolic_function` from utils.py, and to symbolic_opset9.py.

Motivation for this change:
- Better maintainability and reducing complexity. Easier to add symbolic for operators, both simple and complex ones (that need additional context), without the former needing to know the existence of the latter.
- The design idea was long outdated. prim ops are no longer rare special cases, and they shouldn't all be handled inside `_run_symbolic_function`. As a result this function becomes too clumsy. There were also prim ops symbolic added in symbolic_opset#.py with signature `prim_[opname]`, creating separation and confusion.

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D32483782

Pulled By: malfet

fbshipit-source-id: f9affc31b1570af30ffa6668da9375da111fd54a

Co-authored-by: BowenBao <bowbao@microsoft.com>
(cherry picked from commit 1e04ffd2fd1511d7d144f23afc997bf16fa1d2cc)",410.0,282.0,"docs/source/onnx.rst,test/onnx/test_custom_ops.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/_C/__init__.pyi.in,torch/csrc/jit/python/python_ir.cpp,torch/onnx/__init__.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",11.0,10,3,2.317009992,15.0,20484.0,8.0,1537205.0909090908,656.0,1550.0,0.0,Feature Addition,0.0,1
pytorch,7d2a17876fc0b7b251f7e206d84aea13ae08ce3f,050a2588b567439ac0aea370649722cf4757f3a3,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Tue Jul 17 17:54:03 2018 -0700,1531850043.0,"change stft to have consistent signature with librosa (#9497)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9497

Fixes #7883 by using `rfft`.

It's worth noting that this is BC breaking. And it's impossible to detect the change because the two signatures before and after this change supports a common subset of calling patterns, e.g., `stft(Tensor, int, int)`. (some other calling patterns will raise error).

soumith and I plan to change the current `stft` interface because it is a bit messy and non-standard. rafaelvalle suggested us that `librosa` is a good reference API to align with. After discussing with soumith and ezyang , and given that `stft` is only out for 1 release, I decide to go with directly changing the signature. Also, my understanding is that most researchers in this field will welcome this change as `librosa` seems to be the golden-standard here. (it doesn't yet support all `pad_mode` but those will become available if added to `F.pad`.)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9308

Reviewed By: ezyang

Differential Revision: D8806148

Pulled By: SsnL

fbshipit-source-id: f6e8777d0c34d4a4d7024e638dc9c63242e8bb58",248.0,231.0,"aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/native_functions.yaml,test/common.py,test/test_cuda.py,test/test_dataloader.py,test/test_torch.py,torch/_torch_docs.py,torch/functional.py,torch/tensor.py",9.0,6,3,2.706220529,42.0,20154.0,7.0,573240.8888888889,2966.0,6974.833333,0.0,Corrective,1.0,1
pytorch,349dc2f31b1a8459de327538850d93cf0ec04549,050bf897691a41bbdae42412753baf3c86c9cef9,Horace He,horacehe2007@yahoo.com,Thu Sep 16 05:04:41 2021 -0400,1631768681.0,"[functorch] Added Index.Tensor batching rule (pytorch/functorch#123)

* handled some cases of index.Tensor

* fixed merge errors

* Added batching rules for index, both cases are batched

* fix some issues

* handled some cases of index.Tensor

* fixed merge errors

* Added batching rules for index, both cases are batched

* fix some issues

* fix tests

* handled some cases of index.Tensor

* fixed merge errors

* fixed tests",101.0,142.0,"functorch/functorch/csrc/BatchRulesScatterOps.cpp,functorch/test/common_utils.py,functorch/test/functorch_additional_op_db.py,functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",7.0,4,1,2.174817297,1.0,5289.0,6.0,1.4285714285714286,350.0,524.0,0.0,Corrective,1.0,1
pytorch,2f50c119545c654088bb2796fa23e747099ca053,051132f11969a2b877102dc6ef594717159dcf1a,Nick Gibson,nickg@fb.com,Thu Apr 02 07:08:37 2020 -0700,1585811317.0,"[TensorExpr] simplification of round + mod pattern. (#35683)

Summary:
Adds capabilities to the TensorExpr IR Simplifier to simplify down Round + Mod patterns (e.g. `(x/y)*y + x%y => x`) via means of lifting integer rounding into a temporary `RoundOff` node.

This integrates with existing simplification mechanisms (folding, factorization, reordering, etc) to allow simplification of compound expressions: e.g. `20 * (x  / (16 / 2)) * 2 + (11 % 6) * (x % (7+1)) => 5 * x.`.

Tests: ran tensorexpr cpp and python tests, ran a hpc benchmark and verified results and time didn't regress.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35683

Differential Revision: D20811316

Pulled By: nickgg

fbshipit-source-id: 0cd6a517fb9548b3bc689768304b97375df5ac58",558.0,133.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_mutator.h,torch/csrc/jit/tensorexpr/ir_printer.cpp,torch/csrc/jit/tensorexpr/ir_printer.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/ir_visitor.cpp,torch/csrc/jit/tensorexpr/ir_visitor.h,torch/csrc/jit/tensorexpr/types.cpp",12.0,7,2,2.006539157,2.0,4768.0,2.0,455265.3333333333,710.0,2014.0,0.0,Feature Addition,0.0,1
pytorch,3411ec6e326439039007d8b7080346245a22a0ff,0549e1f3840d15a1b60a24437bd061294fd431a2,Hongyi Jia,jiayisuse@fb.com,Tue May 05 12:43:25 2020 -0700,1588682605.0,"[Tensorpipe/RPC] tensorpipe RPC agent (#35483)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35483

Implement the initial version of TensorPipe RPC agent, and register to RPC registry to expose to Python interface. As a starter, it utilizes all available TensorPipe transports (shm, uv) and channels (basic, cma).

Test Plan:
https://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/experimental/jiayisuse/tensorpipe_rpc
  export MASTER_ADDR=127.0.0.1
  export MASTER_PORT=28500
  buck build mode/dev-nosan mode/no-gpu //experimental/jiayisuse/tensorpipe_rpc:main
  ./buck-out/gen/experimental/jiayisuse/tensorpipe_rpc/main.par
  buck build mode/dev-nosan mode/no-gpu //experimental/jiayisuse/tensorpipe_rpc:benchmark
  ./buck-out/gen/experimental/jiayisuse/tensorpipe_rpc/benchmark.par

Multiple connections with async echo
  ./buck-out/gen/experimental/jiayisuse/tensorpipe_rpc/async_echo.par

Reviewed By: lw

Differential Revision: D20088366

fbshipit-source-id: 980f641af3321ca93583c62753e1c9174b7d4afc",629.0,0.0,"tools/build_variables.bzl,torch/CMakeLists.txt,torch/csrc/distributed/rpc/init.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h,torch/distributed/rpc/backend_registry.py",6.0,7,2,1.437318491,5.0,1656.0,4.0,251110.75,1676.0,4374.5,0.0,Feature Addition,0.0,1
pytorch,92314c83fa8bd27638ece087a95f0505e960d326,05523268469c48725aed180aa01cb0cdf8260237,Jane Wang,janewang@fb.com,Wed Dec 12 00:13:31 2018 -0800,1544573611.0,"add gloo scatter support on GPU (#14917)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14917

as titled

Reviewed By: pietern

Differential Revision: D13271560

fbshipit-source-id: 0187a3390f8ebd72a2c074e7a651432159d427c0",161.0,7.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.924133542,3.0,2956.0,1.0,95899.0,5996.0,18626.83333,0.0,Feature Addition,0.0,1
pytorch,543919cfc8d7014dfa365b55c459fa2b3f746891,05624bcf7b831ed18b857701757da2a4e076c493,George Qi,georgeqi94@gmail.com,Mon Jun 13 18:07:07 2022 +0000,1655143627.0,"add sizes to slowpath

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79295

Approved by: https://github.com/ezyang",134.0,14.0,"c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,c10/core/impl/PyInterpreter.cpp,c10/core/impl/PyInterpreter.h,test/test_python_dispatch.py,third_party/ideep,torch/csrc/autograd/python_variable.cpp,torch/overrides.py",8.0,8,4,2.314853421,31.0,9832.0,6.0,228610.625,4301.0,10334.0,0.0,Feature Addition,0.0,1
pytorch,a1dd608260d00df959eccddd43d5873a476a45a8,058c1284be2f4274f291476b7b7a8be33dd34ab0,Lu Fang,lufang@fb.com,Tue Oct 16 02:51:38 2018 -0700,1539658298.0,"Fix the symbolic for pixel shuffle (#12192)

Summary:
Using Transpose + Reshape, not using DepthToSpace, since they are not available in C2 yet.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12192

Reviewed By: BIT-silence

Differential Revision: D10129913

Pulled By: houseroad

fbshipit-source-id: b60ee6d53b8ee95fd22f12e628709b951a83fab6",22.0,0.0,"test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic.py",2.0,4,2,0.945660305,12.0,2322.0,2.0,18993.5,4641.0,13717.83333,0.0,Corrective,1.0,1
pytorch,9b6441ecbc86dbffc14a52277d7b37018a1a0bb2,05908e824348be1ecb6ea01035c8abecbd1d40b7,David Pollack,david@da3.net,Sat Jan 13 11:19:31 2018 +0100,1515842371.0,"current code works with dim = 3, so I added it to dim checks",14.0,5.0,"test/common_nn.py,test/test_nn.py,torch/nn/functional.py",3.0,3,2,1.377963039,37.0,8318.0,3.0,208479.0,913.0,6717.172317,0.0,Feature Addition,0.0,1
pytorch,034e87171087b472c2fe6a900512e10964438e5d,0597eb56c29b3986aa412161b1d3e82b6b36e8d5,Michael Lazos,mlazos@fb.com,Sat Dec 16 00:42:39 2023 +0000,1702687359.0,"Generate exhaustive compiled optimizer tests (#115906)

Generates tests for all permutations of arguments using the existing optimizer infos.
Covers capturable, cpu/gpu, single/multitensor and optimizer specific constants like rho/etas, etc.

[new test list](https://gist.github.com/mlazos/d3404383e7c3d490cbb51b7d6c750629)
[old test list](https://gist.github.com/mlazos/e0043aee1b6a0962d2f3ac8193aa62f8)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/115906
Approved by: https://github.com/janeyx99",131.0,35.0,test/inductor/test_compiled_optimizers.py,1.0,2,1,0,1.0,231.0,1.0,2414206.0,23164.0,52464.5,0.0,Perfective,0.0,1
pytorch,635bb5ec9d99c53aeded558c54dd4c093fa273bb,05c2bafc9d20b39f348194453e0625c41bf9fc1a,Luca Antiga,luca.antiga@orobix.com,Wed Jun 07 00:21:20 2017 +0200,1496794880.0,Have median reduce over all dims and return just the value when dim is not provided,68.0,5.0,"test/test_torch.py,torch/_torch_docs.py,torch/autograd/_functions/reduce.py,torch/csrc/generic/methods/TensorCompare.cwrap,torch/lib/THD/master_worker/common/Functions.hpp,torch/lib/THD/master_worker/master/generic/THDTensor.cpp,torch/lib/THD/master_worker/master/generic/THDTensor.h,torch/lib/THD/master_worker/worker/Dispatch.cpp,torch/lib/THD/master_worker/worker/dispatch/Tensor.cpp,torch/lib/THPP/Tensor.hpp,torch/lib/THPP/tensors/THCSTensor.hpp,torch/lib/THPP/tensors/THCTensor.hpp,torch/lib/THPP/tensors/THSTensor.hpp,torch/lib/THPP/tensors/THTensor.hpp,torch/lib/THPP/tensors/generic/THCSTensor.cpp,torch/lib/THPP/tensors/generic/THCTensor.cpp,torch/lib/THPP/tensors/generic/THSTensor.cpp,torch/lib/THPP/tensors/generic/THTensor.cpp",18.0,18,2,3.58570715,32.0,20126.0,1.0,65649.0,1086.0,15190.88961,0.0,,0.0,1
pytorch,92a85ecbabe5d9f434a33b1400c0265d734179b3,05d1dcc14ce59561f3d8fcf993061df98d366230,Richard Zou,zou3519@gmail.com,Wed Nov 03 14:34:15 2021 -0700,1635950055.0,"Split channels_last test cases for tensor conversion OpInfos (#67368)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67368

This PR adds an addition test variant for the tensor conversion
functions (bfloat16, char, long, ...) that tests channels_last. This is
because some backends (mostly just functorch right now) don't have
channels last handling and may want to test that separately from the
more general case of these operations.

Test Plan: - wait for tests

Reviewed By: mruberry

Differential Revision: D31972959

Pulled By: zou3519

fbshipit-source-id: 68fea46908b2cdfeb0607908898bb8f9ef25b264",132.0,4.0,"test/test_jit_fuser_te.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.351490192,2.0,14551.0,2.0,130443.0,16797.0,39429.5,0.0,Feature Addition,0.0,1
pytorch,"96cd92a0a90f326553f6dac28534e69dddcadcd3,73d15cf64320b4b77e7393efa1bf1e913404cfd6",05fb544f23f9e9801009c07c6bb037ef197d3e90,soumith,soumith@fb.com,Tue Sep 13 18:16:09 2016 -0700,1473790569.0,Merge commit '73d15cf64320b4b77e7393efa1bf1e913404cfd6',1078.0,602.0,"torch/lib/THCUNN/BCECriterion.cu,torch/lib/THCUNN/CMakeLists.txt,torch/lib/THCUNN/HardTanh.cu,torch/lib/THCUNN/SpatialConvolutionMM.cu,torch/lib/THCUNN/SpatialDilatedMaxPooling.cu,torch/lib/THCUNN/SpatialMaxPooling.cu,torch/lib/THCUNN/THCUNN.h,torch/lib/THCUNN/VolumetricDilatedMaxPooling.cu,torch/lib/THCUNN/VolumetricMaxPooling.cu,torch/lib/THCUNN/cmake/select_compute_arch.cmake",10.0,4,1,2.683883205,6.0,2151.0,1.0,1899263.0,104.0,1292.563528,0.0,,0.0,1
pytorch,e4faebca0db24af1281cced882a612c7ece9bd49,062e70590c143bfc79e7edf82b6335cf62d5e3e5,anjali411,chourdiaanjali123@gmail.com,Tue Apr 20 17:20:36 2021 -0700,1618939236.0,"Add OpInfo tests for torch.{dot, vdot, bmm, mv} (#56409)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56409

Reviewed By: nikithamalgifb

Differential Revision: D27870769

Pulled By: anjali411

fbshipit-source-id: a1a0e89856529a4739c7612c5b1e3c5ed2569126",77.0,10.0,"test/test_autograd.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.362051252,42.0,14264.0,2.0,12734.5,11030.0,24348.0,0.0,Feature Addition,0.0,1
pytorch,3a772b798a952017537124926ee389c9cad14115,064c47845380715e290eb335919a18fe3821ee83,Jiakai Liu,liujiakai@fb.com,Fri Mar 20 23:16:00 2020 -0700,1584746160.0,"[pytorch] register c10 ops for static dispatch to unblock c10 boxing

Summary:
PR #32521 broke static dispatch because some ops are no longer
registered in register_aten_ops_*.cpp - it expects the c10 registers in
TypeDefault.cpp / CPUType.cpp / etc to register these ops. However, all
c10 registers are inside `#ifndef USE_STATIC_DISPATCH` section.

To measure the OSS mobile build size impact of this PR:
```
 # default build: SELECTED_OP_LIST=MobileNetV2.yaml scripts/build_pytorch_android.sh armeabi-v7a
 # mobilenetv2 custom build: scripts/build_pytorch_android.sh armeabi-v7a
```

- Before this PR, Android AAR size for arm-v7:
* default build: 5.5M;
* mobilenetv2 custom build: 3.2M;

- After this PR:
* default build: 6.4M;
* mobilenetv2 custom build: 3.3M;

It regressed default build size by ~1M because more root ops are
registered by c10 registers, e.g. backward ops which are filtered out by
gen_jit_dispatch.py for inference-only mobile build.

mobilenetv2 custom build size regressed by ~100k presumably because
the op whitelist is not yet applied to things like BackendSelectRegister.

Differential Revision: D20266240

Test Plan: Imported from OSS

Pulled By: ljk53

fbshipit-source-id: 97a9a06779f8c62fe3ff5cce089aa7fa9dee3c4a",77.0,82.0,"aten/src/ATen/templates/BackendSelectRegister.cpp,aten/src/ATen/templates/PerOpRegistration.cpp,aten/src/ATen/templates/SparseTypeDerived.cpp,aten/src/ATen/templates/TypeDefault.cpp,aten/src/ATen/templates/TypeDerived.cpp,cmake/Codegen.cmake,tools/code_analyzer/gen_op_registration_whitelist.py,tools/code_analyzer/gen_transitive_deps.py",8.0,7,3,1.783903914,8.0,482.0,4.0,1859976.857142857,304.0,862.5,0.0,,1.0,1
pytorch,d9d5d9a913df229638aca20a3981926f18472aff,064d156511068035ed486d09342c4258e8479ece,Jie,jiej@nvidia.com,Thu Aug 15 04:01:50 2019 -0700,1565841710.0,"(#23574)

Summary:
Assert that there's no multiple written-to to a single memory location, which
caused corrupted output.
Fixed batched matrix trlu logic, which relies on the previous copy behavior to
support tensors with stride 0 at leading dimension.

This fixes the issue proposed at: https://github.com/pytorch/pytorch/issues/23063
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23574

Differential Revision: D16600717

Pulled By: ezyang

fbshipit-source-id: e41e14f03eccf97398b64ba43647110beb1529e6",44.0,10.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,test/test_torch.py",4.0,5,2,1.777689151,40.0,14514.0,4.0,368622.5,10659.0,30256.33333,0.0,Corrective,1.0,1
pytorch,cb6c3526c6f21ad59aa1ee9cbe0d141c5f98c589,0651887eb4fd85ebbf65ab29ff2b634226871fee,xueht-fnst,xueht.fnst@cn.fujitsu.com,Fri Jul 10 22:21:08 2020 -0700,1594419668.0,"Improve repr for torch.iinfo & torch.finfo (#40488)

Summary:
- fix https://github.com/pytorch/pytorch/issues/39991
- Include directly `min`/`max`/`eps`/`tiny` values in repr of `torch.iinfo` & `torch.finfo` for inspection
- Use `torch.float16` / `torch.int16` instead of uncorrespond names `Half` / `Short`
- The improved repr is shown just like:
```
>>> torch.iinfo(torch.int8)
iinfo(type=torch.int8, max=127, min=-128)
>>> torch.iinfo(torch.int16)
iinfo(type=torch.int16, max=32767, min=-32768)
>>> torch.iinfo(torch.int32)
iinfo(type=torch.int32, max=2.14748e+09, min=-2.14748e+09)
>>> torch.iinfo(torch.int64)
iinfo(type=torch.int64, max=9.22337e+18, min=-9.22337e+18)
>>> torch.finfo(torch.float16)
finfo(type=torch.float16, eps=0.000976563, max=65504, min=-65504, tiny=6.10352e-05)
>>> torch.finfo(torch.float32)
finfo(type=torch.float32, eps=1.19209e-07, max=3.40282e+38, min=-3.40282e+38, tiny=1.17549e-38)
>>> torch.finfo(torch.float64)
finfo(type=torch.float64, eps=2.22045e-16, max=1.79769e+308, min=-1.79769e+308, tiny=2.22507e-308)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/40488

Differential Revision: D22445301

Pulled By: mruberry

fbshipit-source-id: 552af9904c423006084b45d6c4adfb4b5689db54",91.0,27.0,"docs/source/type_info.rst,test/test_type_info.py,torch/_C/__init__.pyi.in,torch/csrc/TypeInfo.cpp,torch/csrc/utils/tensor_dtypes.cpp,torch/csrc/utils/tensor_dtypes.h",6.0,7,3,1.274881584,5.0,796.0,6.0,27684461.166666668,3527.0,8368.5,0.0,Corrective,1.0,1
pytorch,6eca9e052dde33f368debf456e3cbb5e2ff15a18,066f26c7fa7dcfb85beab2ced7aa7487b78c5bf0,Gregory Chanan,gchanan@fb.com,Wed Nov 15 17:12:35 2017 -0800,1510765955.0,[ATen] Introduce templatized native functions and implement is_signed.,68.0,14.0,"aten/src/ATen/function_wrapper.py,aten/src/ATen/native/NativeFunctions-inl.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/ATen/templates/NativeFunctions.h",5.0,5,1,1.135210255,5.0,1183.0,2.0,80559.0,43.0,86.0,0.0,Feature Addition,0.0,1
pytorch,c4b0db5079ccf8d0c92c2950933d78926f501431,067f799e9ff3cc24010492d512ad610caf4143ea,gchanan,gregchanan@gmail.com,Fri Nov 17 20:57:56 2017 -0500,1510952276.0,"Implement remaining Variable fallthrough methods via ATen (#3744)

* Use aten version of is_signed.

* Define is_cuda native function and use it for variable.

* Use ATen dim for Variable dim/ndimension.

* Get rid of dim, ndimension fallthroughs in variable.py.

* Move size/stride Variable methods to use ATen.

* Implement shape property on Variable via ATen.

* Remove the _getattr__ function from Variable.

* Get rid of dispatch functions and avoid cast.

* Add THPUtils_packInt64Array.

* Throw python errors.

* Use fallthrough and fix fallthrough generation for native functions.

* is_cuda is a property, not a method.",129.0,25.0,"aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/python_variable_methods.cpp,torch/autograd/variable.py,torch/csrc/Size.cpp,torch/csrc/Size.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/utils/python_tuples.h",11.0,12,3,2.646025377,37.0,4062.0,6.0,211087.7,341.0,954.9058694,0.0,Corrective,1.0,1
pytorch,1deb89507454c7e57ccd453f75e4396082d73980,0684d074256bac5fe2ed0c4180625d5d78c65a75,Shen Li,cs.shenli@gmail.com,Fri Jan 08 03:43:44 2021 -0800,1610077424.0,"Remove FutureMessage from sender TensorPipeAgent (#50024)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50024

Test Plan: Imported from OSS

Reviewed By: lw

Differential Revision: D25753386

Pulled By: mrshenli

fbshipit-source-id: fdca051b805762a2c88f965ceb3edf1c25d40a56",22.0,20.0,"torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h",2.0,4,1,0.892623013,1.0,1527.0,1.0,206.0,7942.0,17960.5,0.0,,0.0,1
pytorch,9f9dd4f072dd60cc1c46b7e619de28c84f2eeef1,06a0cfc0ea0cc703a1ebc8148181ac3e3cb80ab5,Catherine Lee,csl@fb.com,Tue Jul 19 19:50:57 2022 +0000,1658260257.0,"pytest to run test_ops, test_ops_gradients, test_ops_jit in non linux cuda environments (#79898)

This PR uses pytest to run test_ops, test_ops_gradients, and test_ops_jit in parallel in non linux cuda environments to decrease TTS.  I am excluding linux cuda because running in parallel results in errors due to running out of memory

Notes:
* update hypothesis version for compatability with pytest
* use rerun-failures to rerun tests (similar to flaky tests, although these test files generally don't have flaky tests)
  * reruns are denoted by a rerun tag in the xml.  Failed reruns also have the failure tag.  Successes (meaning that the test is flaky) do not have the failure tag.
* see https://docs.google.com/spreadsheets/d/1aO0Rbg3y3ch7ghipt63PG2KNEUppl9a5b18Hmv2CZ4E/edit#gid=602543594 for info on speedup (or slowdown in the case of slow tests)
  * expecting windows tests to decrease by 60 minutes total
* slow test infra is expected to stay the same - verified by running pytest and unittest on the same job and check the number of skipped/run tests
* test reports to s3 changed - add entirely new table to keep track of invoking_file times
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79898
Approved by: https://github.com/malfet, https://github.com/janeyx99",304.0,37.0,".circleci/docker/requirements-ci.txt,.jenkins/pytorch/macos-test.sh,.jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat,pytest.ini,test/conftest.py,test/test_ops.py,tools/stats/upload_test_stats.py,tools/test/test_upload_test_stats.py,torch/testing/_internal/common_utils.py",9.0,12,5,2.024984819,5.0,5872.0,7.0,7295785.5,5493.0,12896.0,0.0,Feature Addition,0.0,1
pytorch,5c66662e58c5b87b3f39913bde056d5ecfd4e58e,06a7cb59019ee57c679ba2cf7d51e36bd3710ad4,Shen Li,shenli@fb.com,Thu Dec 20 18:21:02 2018 -0800,1545330062.0,"Implementing cuda kernel for tril_indices and triu_indices (#15203)

Summary:
Followup PR of #14904, and the stretch goal of #12653.

Directly calculate coordinates in the original tensor using column index in the result tensor. Every GPU thread takes care of a column (two numbers) in the output tensor.

The implementation detects and handles precision loss during calculating the square root of a `int64_t` variable, and supports tensors with up to `row * column = 2 ^ 59` numbers.

Algorithm details are describe in [comments of TensorFactories.cu](https://github.com/pytorch/pytorch/blob/23ddb6f58a1c8a7a660a793f174cf014230176c6/aten/src/ATen/native/cuda/TensorFactories.cu#L109-L255).

zou3519
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15203

Reviewed By: zou3519

Differential Revision: D13517695

Pulled By: mrshenli

fbshipit-source-id: 86b305d22cac08c8962a3b0cf8e9e620b7ec33ea",569.0,171.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorFactories.h,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/native_functions.yaml,test/common_methods_invocations.py,test/test_cuda.py,test/test_torch.py,torch/_torch_docs.py",8.0,7,3,2.371344826,42.0,23699.0,7.0,153623.7142857143,6164.0,19169.83333,0.0,Non Functional,0.0,1
pytorch,8fdec15a55527001c2692675126146529e158fcc,06ab3f962fd2c8a3042126fa5a711e84d973603a,Adam Paszke,adam.paszke@gmail.com,Wed Sep 21 02:37:20 2016 -0700,1474425440.0,Refactor _C extension to export some utilities,730.0,768.0,"setup.py,test/test_autograd.py,test/test_cuda.py,tools/cwrap/plugins/StandaloneExtension.py,tools/cwrap/plugins/THPPlugin.py,tools/cwrap/plugins/templates/module_head.cpp,tools/cwrap/plugins/templates/module_tail.cpp,tools/nnwrap/generate_wrappers.py,torch/__init__.py,torch/csrc/Exceptions.h,torch/csrc/Generator.cpp,torch/csrc/Generator.h,torch/csrc/Module.cpp,torch/csrc/Module.h,torch/csrc/Storage.h,torch/csrc/THP.h,torch/csrc/THP_API.h,torch/csrc/Tensor.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Module.h,torch/csrc/cuda/Storage.h,torch/csrc/cuda/THCP.h,torch/csrc/cuda/Tensor.h,torch/csrc/cuda/override_macros.h,torch/csrc/cuda/undef_macros.h,torch/csrc/cuda/utils.h,torch/csrc/generic/Storage.cpp,torch/csrc/generic/Storage.h,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/Tensor.h,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/utils.cpp,torch/csrc/generic/utils.h,torch/csrc/utils.cpp,torch/csrc/utils.h,torch/tensor.py",37.0,10,3,4.486877981,7.0,8944.0,4.0,398185.0,182.0,3839.532937,0.0,Perfective,0.0,1
pytorch,255d14947db934bd1134a77b11d97982887f2aa4,06bdd491fb431a7c7cc58c2402e485fcb8406ef9,Richard Zou,zou3519@gmail.com,Tue Dec 27 02:44:07 2022 -0800,1672109047.0,"[vmap] fix reduction boxed batching rules (#91109)

Fixes https://github.com/pytorch/pytorch/issues/91041

There's a bug in our boxed reduction batching rules for a very specific
case: vmap over a Tensor of shape [1] for an operation where the
output rank is supposed to be less than the input rank, e.g.

```
x = torch.tensor([10.], device=device)
y = vmap(lambda x: x.sum(0))(x)
```

The boxed reduction batching rule handles three types of ""reduction""
operations:
- reduction operations with an optional keepdim argument, which
specifies if the output should have the same or smaller rank than the
input
- reduction operations without a keepdim arg that morally have keepdim=True (like cumsum --
which never actually modifies the rank of the tensor but is still a
""reduction"" since it sums a bunch of things together)
- reduction operations without a keepdim arg that morally have
keepdim=False. (just torch.count_nonzero).

Furthermore, PyTorch has special handling for scalar tensors (e.g.
tensors of shape []). It is valid to do
`torch.sum(torch.tensor(10.), dim=0)`.

This PR updates the `boxed_reduction_batch_rule` to handle the
interaction between the three kinds of reduction and the scalar tensor
cases correctly. Concretely, it:
- introduces additional templates to `boxed_reduction_batch_rule` for
what type of ""keepdim"" reduction this is.
- splits the old REDUCTION_BOXED macro (which was a good default) into
REDUCTION_NO_KEEPDIM_ARG and REDUCTION_WITH_KEEPDIM_ARG (which are also
opionated defaults) and uses them.

Test Plan:
- Given an input of shape [], our vmap OpInfo test suite only produces
a Tensor of shape [B] with B = 2. At first glance this doesn't look
sufficient to test this case (vmap over Tensor[1]), but the claim is
that it is because the boxed_reduction_batch_rule is agnostic to the shape
of the dimension being vmapped over. Previously it was not due to
the semantics of `squeeze`; this PR adds internal asserts to make it agnostic.
- there is a light test for vmap over the Tensor of shape [1] for
torch.sum as a sanity check.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91109
Approved by: https://github.com/samdow",133.0,47.0,"aten/src/ATen/functorch/BatchRulesReduceOps.cpp,test/functorch/test_vmap.py",2.0,6,2,0.331839916,1.0,5279.0,2.0,1000099.0,10879.0,24779.5,0.0,Corrective,1.0,1
pytorch,78f06e0690d5b174628f8cfb5d2d82e967881e8e,06d0536dad28a20fffbd06f5772e969401fa47dd,Natalia Gimelshein,ngimel@fb.com,Sun Dec 19 19:55:22 2021 -0800,1639943722.0,"Low precision support for jiterator (#70157)

Summary:
This adds support for bfloat16 and fp16 types for jiterator by adding at::Half and at::BFloat16 classes to the jiterator code template. The only methods defined in those classes are construction from float and implicit conversion to float. Mathematical operations on them never need to be defined, because jiterator is written in a way to implicitly upcast the inputs to the functor, so all math has to be performed on float only (e.g. compute part of the kernel would always be written as
```
        out[j] = i0<float>(arg0[j]);
```
It also adds support for casting to complex outputs, by adding a similar templated class c10::complex<T>. Originally I planned to only support float -> complex complex for it, but to compile fetch_and_cast function we also need complex -> float conversion. We can avoid it by compiling fetch_and_cast for a different subset of types, but I'm not doing it in this PR. Thus, technically, we can compile a kernel that would accept complex inputs and produce wrong results, but we are guarding against it by static asserting that none of the functor datatype are complex, and runtime-checking that none of the inputs are complex.
Adding bfloat16, half and complex support allows us to remove special handling for type promotion tests for gcd.
i0 (that supports half and bfloat16 inputs) is moved to use jiterator.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70157

Reviewed By: mruberry

Differential Revision: D33221645

Pulled By: ngimel

fbshipit-source-id: 9cfe8aba3498a0604c4ea62c217292ea06c826b1",161.0,42.0,"aten/src/ATen/native/cuda/Loops.cuh,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,aten/src/ATen/native/cuda/jit_utils.cu,test/test_binary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,1.455260182,2.0,20653.0,3.0,418752.6,17874.0,42291.0,0.0,Feature Addition,0.0,1
pytorch,01dfa7620d715fb22e703a36db0930077252408c,06d74e6b24a6f7dfd0b8d05e2171d74def166140,Bin Bao,binbao@meta.com,Thu Aug 31 16:26:02 2023 +0000,1693499162.0,"Revert ""[AOTInductor] Include constants in AOTInductor .so file. (#10â¦ (#108349)

This reverts commit c3239442a3dd1040b251ff33bef40589cba40e1c due to internal test failures.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108349
Approved by: https://github.com/aakhundov, https://github.com/zhxchen17",71.0,397.0,"benchmarks/dynamo/common.py,test/cpp/aot_inductor/test.cpp,test/cpp/aot_inductor/test.py,test/inductor/test_aot_inductor.py,test/inductor/test_inductor_freezing.py,torch/_export/__init__.py,torch/_inductor/codecache.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/compile_fx.py,torch/_inductor/config.py,torch/_inductor/freezing.py,torch/_inductor/graph.py,torch/_inductor/virtualized.py,torch/csrc/inductor/aot_inductor_model.h,torch/csrc/inductor/aot_inductor_model_container.h",15.0,12,3,3.297248277,1.0,12301.0,6.0,95975.06666666668,19236.0,43685.5,0.0,,0.0,1
pytorch,8e58135a263dfacfb44ce1032251df3c12eb6929,0748ea56ebaddc44efcf5ff734bede6c14b7fb89,vfdev,vfdev.5@gmail.com,Sun Oct 22 13:59:57 2017 +0200,1508680797.0,"Change size by kernel_size in __repr__

Probably, __repr__ should return `MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))` -> `MaxPool2d (kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1)))`",8.0,8.0,torch/nn/modules/pooling.py,1.0,3,1,0,35.0,1035.0,1.0,319727.0,769.0,6469.172317,0.0,,0.0,1
pytorch,e25e501bea8056f58e854c023ef6b494b8e81f54,07513cfd1d284a058810efca9107f0697bba55ab,Thomas Viehmann,tv.github@beamnet.de,Thu May 03 01:50:29 2018 +0200,1525312229.0,implement sum over multiple dimensions (fixes #2006) (#6152),110.0,25.0,"aten/src/ATen/WrapDimUtilsMulti.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_torch_docs.py",8.0,9,4,1.930743643,40.0,19577.0,5.0,125701.5,626.0,3562.5,0.0,Corrective,1.0,1
pytorch,236e1dff8155fb943d7557462852b8121dfdfcc0,07865422666473033b369751aad88ee9d3c19f55,Richard Zou,zou3519@users.noreply.github.com,Thu Mar 31 13:37:17 2022 -0400,1648733837.0,"[functorch] jvp x vjp testing (pytorch/functorch#343)

Failures are either:
- lack of PyTorch forward-mode AD support (mostly)
- efficient zero tensors errors
- CUDA asserts (really need to be investigated).

All of the problems should be reproducible on the pytorch/pytorch side.",115.0,0.0,"functorch/test/test_ops.py,functorch/test/xfail_suggester.py",2.0,2,1,0.072016472,1.0,1467.0,2.0,5.0,921.0,1281.5,0.0,,0.0,1
pytorch,177b4509ceeb7a8ae2e176f52f4b41d9e71dc993,07be53b57f00c58c73bf8f3563213ad77f0c6e15,cpuhrsch,cpuhrsch@googlemail.com,Mon Feb 12 19:20:32 2018 -0500,1518463232.0,"Move EmbeddingBag into ATen (#4856)

This diff creates code related to EmbeddingBag in ATen. It also allows sparse gradients.",623.0,27.0,"aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,tools/autograd/derivatives.yaml,torch/nn/functional.py,torch/nn/modules/sparse.py",8.0,11,4,1.900393346,37.0,9727.0,5.0,939669.5,176.0,12869.22461,0.0,,0.0,1
pytorch,95d2318510371523b3406ae1d4818f8f0607bbc6,07d315fce876dc4379bbf99bc756aa4b544a1e47,Edward Yang,ezyang@fb.com,Thu Mar 11 15:15:16 2021 -0800,1615475716.0,"Revert D26676150: Simplify index expressions constructed in loop flattening - #51173

Test Plan: revert-hammer

Differential Revision:
D26676150 (https://github.com/pytorch/pytorch/commit/1f01899e4a2c04063fd8305ee304f063c04edc78)

Original commit changeset: e202e0c8610e

fbshipit-source-id: 9611dda6897b67e16e44c731994bc9e5fccab0b9",43.0,534.0,"test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",2.0,7,2,0.999633802,1.0,7026.0,1.0,45596.0,9659.0,21407.0,0.0,,0.0,1
pytorch,045ebc771d5070696f839e586285ace9c06f1339,07d398fb269eebe314ae898287494a2bfdc7f278,kshitij12345,kshitijkalambarkar@gmail.com,Wed Sep 07 09:33:37 2022 +0000,1662543217.0,"[composite compliance] linalg_householder_product (#84180)

Ref: #69991
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84180
Approved by: https://github.com/zou3519",84.0,33.0,"functorch/test/test_ops.py,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/opinfo/definitions/linalg.py",3.0,9,2,0.810498996,3.0,10425.0,3.0,674068.6666666666,7120.0,16642.0,0.0,,0.0,1
pytorch,8e49afa908688d2cc1ea6f762b6cccadf6808d3e,07dbf0db4617aa9a5df1f6bf6d208a102b017f1d,xiaobingsuper,xiaobing.zhang@intel.com,Tue Mar 31 21:04:00 2020 -0700,1585688640.0,"bfloat16: vectorized clamp, clamp_min and clmap_max (#35082)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/35082

Test Plan: Imported from OSS

Differential Revision: D20721148

Pulled By: ngimel

fbshipit-source-id: 949b66e28bfc6049a891fced9ea308131b3675c6",184.0,143.0,"aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,test/test_torch.py,torch/testing/_internal/common_device_type.py",4.0,11,3,0.667526543,41.0,18213.0,3.0,635768.5,649.0,1850.5,0.0,,0.0,1
pytorch,2aad28a539162d443a22b5d750f91910e1775e3a,07e45334038e6ddb5dfb9ba2c9da10da4e8e1e8b,Brian Hirsh,hirsheybar@fb.com,Tue May 24 20:27:45 2022 -0700,1653424065.0,"reland of as_strided support for functionalization; introduce as_strided_scatter

This reverts commit a95f1edd8549b6a249ffa448df073ac4c8b81382.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78199

Approved by: https://github.com/ezyang",147.0,3.0,"aten/src/ATen/FunctionalInverses.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/test_functionalization.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",12.0,12,4,2.827295516,37.0,66705.0,4.0,6878.416666666667,3613.0,8577.5,0.0,Feature Addition,0.0,1
pytorch,236d3afd8245c727788767055ff2872be8b4aa66,08020220f39e5572144cb0e4e4c1baecfaf16716,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Thu Jun 24 02:21:17 2021 -0700,1624501277.0,"[Testing] Adding reference tests to `OpInfo` class (#59369)

Summary:
This PR will ideally add `ref` argument to `OpInfo` base class. The idea is to add reference checks for all the ops _eligible_. For more discussion, please check https://github.com/pytorch/pytorch/issues/58294

* [x] Migrate (but not removing yet) and modify helper functions from `UnaryUfuncOpInfo` class to `OpInfo` base class.
* [x] Test the reference checks for multiple ops. (also decide a list of different and eligible ops for this)
* [x] Handle possible edge cases (for example: `uint64` isn't implemented in PyTorch but is there in NumPy, and this needs to be handled -- more on this later) -- _Update_: We decided that these reference tests should only test for values and not types.
* [x] Create a sample PR for a single (of all different categories?) on adding reference functions to the eligible ops. -- _Update_: This is being done in this PR only.
* [x] ~Remove reference tests from `test_unary_ufuncs.py` and test to make sure that nothing breaks.~ (*Update*: We won't be touching Unary Ufunc reference tests in this PR)
* [x] Add comments, remove unnecessary prints/comments (added for debugging).

Note: To keep the PR description short, examples of edge cases encountered have been mentioned in the comments below.

cc: mruberry pmeier kshitij12345

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59369

Reviewed By: ngimel

Differential Revision: D29347252

Pulled By: mruberry

fbshipit-source-id: 69719deddb1d23c53db45287a7e66c1bfe7e65bb",72.0,28.0,"test/test_ops.py,test/test_shape_ops.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",4.0,4,2,1.739835756,2.0,11816.0,4.0,494390.5,13300.0,30067.5,0.0,Corrective,1.0,1
pytorch,4c35c630eca9a7a3fbfc8f4bc72ea2fd5ba0dd05,08648061f7690f6fad817677006b7159978db08d,Trevor Killeen,killeent@users.noreply.github.com,Wed Jun 28 14:01:45 2017 -0400,1498658505.0,Advanced Indexing 2A - Colons + Adjacent Adv Indexers (#1890),635.0,131.0,"test/test_autograd.py,test/test_torch.py,torch/csrc/generic/Tensor.cpp",3.0,4,2,1.171829599,33.0,7463.0,3.0,126688.0,1055.0,12578.54911,0.0,,0.0,1
pytorch,bcbb36e99a0fc0a89ccf0be913f4c0e050871ec7,0876bab8b782ef5e91b4830001a87ed8065ba43c,gchanan,gregchanan@gmail.com,Mon Dec 18 20:45:01 2017 -0500,1513629901.0,"Support CPU Apply in ATen and implement standard_gamma using it (#4161)

* Support CPU Apply directly in ATen and implement standard_gamma using it.

Main changes in this PR:
1) Added a TH_APPLY-style templatized function for CPU apply calls (currently only 2 and 3 tensor argument
versions are supported, but more are easy to add).  In fact, this is basically identical to TH_APPLY, except
it uses ATen functions and the API is a template instead of a macro.  The template takes an operation that
is performed on the data (and an indicator to signal early termination); i.e. you don't need to know that
x_data is a pointer to the current data location of x.

2) Refactors the ATen dispatch code to easily generate dispatch code for different subsets of the scalar types.
This is in preference to the template_scalar path, which requires valid specialization of each scalar type.  Valid
specializations are  particularly annoying with CUDA because you most likely can't put the specializations
in a header so need to write some sort of for-all-scalar-type macro to get the correct specializations.
Currently, we only generate dispatch_all (all scalar types, the equivalent existed already), and
dispatch_cpu_floating_types (which is used by standard_gamma).

3) Implements standard_gamma using the above changes (this is an arbitrary choice, it was the latest
apply macro to be committed).  The forward is bound via Declarations.yaml,
the backward via the Apply template, and then they are hooked together in derivatives.yaml.  This eliminates
needing to change TH at all going forward, which means one can write idiomatic C++ instead of the TH-style macros
(e.g. TH_MATH_NAME).

* Generate Dispatch code with nicer spacing.

* Small cleanups.

* Fix typo.

* Add TODOs for changing macros, remove dead code.

* Use a lambda function.

* Get rid of early exit.

* Rename Scalar,ScalarType template parameters to CScalar.

* Reorder _standard_gamma_grad parameters.

* Add comments explaining calling convention.

* Don't generate Dispatch.h anymore.

* Get rid of backend specific checks in dispatch.

* Fix empty/scalar check.",449.0,144.0,"aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Declarations.cwrap,aten/src/ATen/Dispatch.h,aten/src/ATen/dispatch_macros.py,aten/src/ATen/gen.py,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/scalar_test.cpp,aten/src/TH/generic/THTensorMath.c,aten/src/TH/generic/THTensorMath.h,tools/autograd/derivatives.yaml,torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorRandom.cwrap,torch/distributions/gamma.py",15.0,14,3,2.819812225,39.0,11651.0,8.0,777038.6153846154,375.0,1148.405869,0.0,Corrective,1.0,1
pytorch,ed0f629fe92db3b6516db233799154c9a8090d5e,08891b0a4e08e2c642deac2042a02238a4d34c67,Tongzhou Wang,SsnL@users.noreply.github.com,Sat Mar 24 16:16:18 2018 -0400,1521908178.0,"Group Normalization (#5968)

* Group Normalization

* move to ATen",361.0,95.0,"aten/src/ATen/cudnn/README.md,aten/src/ATen/native/BatchNorm.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/batchnorm.py,torch/nn/modules/instancenorm.py,torch/nn/modules/normalization.py",10.0,9,3,2.550724257,37.0,10365.0,7.0,1157948.888888889,523.0,2442.0,0.0,,0.0,1
pytorch,5f8c69d414833a7b57e630585c9238caaa20687f,090b2260e44e2281e6019a5cd28f9bdd432c8764,Horace He,chilli@fb.com,Fri May 06 02:43:57 2022 -0700,1651805037.0,"[functorch] Try adding log_sigmoid_forward decomp (pytorch/functorch#778)

* try adding log_sigmoid_forward decomp

* add elias' fix for multiple outputs

* fix stuff

* test

* modify tests to have special case for jvp decompositions

Co-authored-by: samdow <samdow@fb.com>",41.0,5.0,"functorch/functorch/_src/decompositions.py,functorch/functorch/_src/eager_transforms.py,functorch/functorch/csrc/BatchRulesHelper.cpp,functorch/functorch/csrc/BatchRulesHelper.h,functorch/functorch/csrc/DynamicLayer.cpp,functorch/test/test_ops.py",6.0,5,1,2.278818132,1.0,3700.0,4.0,0.3333333333333333,1048.0,1435.5,0.0,Corrective,1.0,1
pytorch,259d7299dbc2c24b6e9107bbd1fb6baf4a8e9e74,09296c34a4b582316bfd40547e367bee0c883044,Lingyi Liu,lingyiliu@fb.com,Tue Mar 10 22:27:14 2020 -0700,1583879234.0,"Add the build for runtime dispatch for AVX, AVX2 instruction set (#26125)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26125

We already had some optimization implementation using AVX2 for improve the quantized kernel performance. In this diff, we want to enable the runtime dispatch.

Test Plan:
Sandcastle build and test

Also test with a python binary calling into vectorized op.

torch.__config__.show()
PyTorch built with:
  - GCC 4.2
  - clang 8.0.20181009
  - Intel(R) Math Kernel Library Version 2017.0.3 Product Build 20170413 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.18.1 (Git Hash N/A)
  - OpenMP 1
  - **CPU capability usage: AVX2**
  - Build settings:

Reviewed By: jamesr66a

Differential Revision: D17337251

fbshipit-source-id: 8e22d10011a12a4eaf54cea3485353eb1811d828",78.0,0.0,"aten/src/ATen/Version.cpp,test/cpp/api/dispatch.cpp",2.0,6,2,0.918295834,1.0,174.0,1.0,3126054.0,15349.0,41089.83333,0.0,Feature Addition,1.0,1
pytorch,e08303c740be7069fd207c4cbe99ab4eabc7705f,095c328d9fb86c348fbd100c3ee995a0698ac9a9,Rong Rong (AI Infra),rongr@fb.com,Fri Apr 30 15:23:56 2021 -0700,1619796236.0,"Add supported backward_dtype to OpInfo (#56156)

Summary:
Related to https://github.com/pytorch/pytorch/issues/55601.

- [x] removed complex autograd checker in `test_supported_backward`
- [x] created `backward_dtype[If<Device>]` that inherits from normal `dtype[If<Device>]` by default
- [x] removed all skip for backward test, instead added backward dtype
- [x] change complex autograd to a function call: `support_complex_autograd(device_type)` that depends on `backward_dtype*` since they essentially mean the same thing for complex types

TODO for next PR
- add `test_unsupported_backward` to verify they are actually unsupported.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56156

Reviewed By: mruberry

Differential Revision: D27926717

Pulled By: walterddr

fbshipit-source-id: 9a4af8612278ca44a97b6f1510b6b175852c893b",89.0,61.0,"test/test_ops.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,1.017083712,2.0,7914.0,3.0,219949.3333333333,11494.0,25971.5,0.0,Feature Addition,0.0,1
pytorch,5f310c5e27e40e990e1f871209bb33a62378a888,0973c5a1cc183cab2fd016ee3029539080b2e539,Philip Meier,github.pmeier@posteo.de,Fri Feb 25 05:47:38 2022 -0800,1645768058.0,"align signature of make_tensor with other creation ops (#72702)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72702

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D34457729

Pulled By: mruberry

fbshipit-source-id: 83d580c4201eef946dc9cf4b9e28a3d36be55609
(cherry picked from commit aa4cf20fbeb4b795595729b8ac2e6ba7707d8283)",525.0,511.0,"test/test_binary_ufuncs.py,test/test_expanded_weights.py,test/test_jit.py,test/test_linalg.py,test/test_reductions.py,test/test_shape_ops.py,test/test_sort_and_select.py,test/test_sparse.py,test/test_tensor_creation_ops.py,test/test_testing.py,test/test_torch.py,test/test_unary_ufuncs.py,test/test_view_ops.py,torch/testing/__init__.py,torch/testing/_creation.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_modules.py",17.0,4,2,2.77016667,47.0,71499.0,15.0,2599908.1764705884,995.0,2477.0,0.0,,0.0,1
pytorch,b4f3a989da375675785754e4b12d2ce757a1ffaa,09a1b1cf87fb8724e644693f2c00318ed2637095,albanD,desmaison.alban@gmail.com,Tue May 25 14:28:06 2021 -0700,1621952886.0,"Forward AD formulas batch 1 (#57768)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57768

Note that this PR implements formulas only for ops that are supported by OpInfo.

Test Plan: Imported from OSS

Reviewed By: zou3519, malfet

Differential Revision: D28387766

Pulled By: albanD

fbshipit-source-id: b4ba1cf1ac1dfd46cdd889385c9c2d5df3cf7a71",48.0,8.0,"tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,2,1.935634581,14.0,10408.0,3.0,247402.25,12431.0,28126.5,0.0,,0.0,1
pytorch,a432b9a7c60d391e5308c6f8676896d3b744f6e9,09c417ae6504dc9fbb56945c015799f655d5cb9e,Mikayla Gawarecki,mikaylagawarecki@gmail.com,Thu Jan 27 17:32:33 2022 -0800,1643304753.0,"Add new reduce options and autograd support for scatter_reduce (#71788)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71788

Test Plan: Imported from OSS

Reviewed By: mikaylagawarecki

Differential Revision: D33778525

Pulled By: cpuhrsch

fbshipit-source-id: 47b8544e29df3075bc6ede894c59499a7ffec876
(cherry picked from commit ddcddac7262dd78a4002aaaea08daa3c50526028)",185.0,38.0,"aten/src/ATen/native/TensorAdvancedIndexing.cpp,test/test_torch.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",6.0,12,4,2.231257512,45.0,34020.0,4.0,51427.333333333336,246.0,496.0,0.0,Feature Addition,0.0,1
pytorch,9fdf7ec6a21f1bf9fabccb27ff2b54d94a82c3f3,09d10c43296b7ba2341b32be5e2f0eca2538debe,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Fri Jul 30 13:54:04 2021 -0700,1627653244.0,"OpInfo for nn.functional.softmax (#62077)

Summary:
This PR:

* Adds OpInfo for `softmax` and `nn.functional.softmax` (alias).
* Skip removal for `test_jit_alias_remapping` test of `log_softmax`.

Please see https://github.com/facebookresearch/functorch/issues/78 and https://github.com/pytorch/pytorch/issues/54261.

cc: mruberry zou3519 pmeier

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62077

Reviewed By: heitorschueroff

Differential Revision: D29990019

Pulled By: zou3519

fbshipit-source-id: 67476990b54a5dd824eed9d10236e118564f2501",37.0,28.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8268.0,1.0,87218.0,14282.0,32671.5,0.0,Feature Addition,0.0,1
pytorch,d9bde84b84522ae24610ed43a679697fe86b8786,09f40ae06f9a238b0aee6ae8e0e1d8096947c92b,li-roy,8813817+li-roy@users.noreply.github.com,Wed Apr 25 03:49:12 2018 -0700,1524628152.0,silence compiler warnings (#6915),2.0,2.0,aten/src/ATen/native/Linear.cpp,1.0,4,1,0,5.0,350.0,1.0,576465.0,608.0,3519.5,0.0,,0.0,1
pytorch,c71b12851d53bbcff21854c44782ef0769fcd068,0a00858095977e5c5563297322ce4a43e658d1a8,Sean Ross-Ross,srossross@gmail.com,Fri Dec 09 05:58:07 2022 +0000,1670565487.0,"Implement checks for vmap escaped errors (#89585)

Follow on to https://github.com/pytorch/pytorch/pull/89077
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89585
Approved by: https://github.com/zou3519",35.0,19.0,"aten/src/ATen/functorch/BatchRulesConvolution.cpp,aten/src/ATen/functorch/BatchRulesHelper.h,aten/src/ATen/functorch/BatchRulesLoss.cpp,aten/src/ATen/functorch/BatchRulesNorm.cpp,aten/src/ATen/functorch/BatchRulesReduceOps.cpp,aten/src/ATen/functorch/BatchRulesScatterOps.cpp,aten/src/ATen/functorch/BatchRulesViews.cpp,test/functorch/test_vmap.py",8.0,6,2,2.525968633,1.0,8922.0,7.0,1852476.125,10364.0,23710.0,0.0,,0.0,1
pytorch,72803dbcfddd5df3dffd8c3f238ecd4ac19a761a,0a07488ed2c47765e337e290bd138c0e6e459cbd,Richard Barnes,rbarnes@fb.com,Tue Oct 19 10:25:14 2021 -0700,1634639114.0,"use irange for loops 1 (#66741)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66741

Modified loops in files under fbsource/fbcode/caffe2/ from the format

`for(TYPE var=x0;var<x_max;x++)`

to the format

`for(const auto var: irange(xmax))`

This was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.

Test Plan: Sandcastle

Reviewed By: ngimel

Differential Revision: D31705360

fbshipit-source-id: 7115f76e381ad2d98584eb534961c3cbb957ebaa",513.0,428.0,"android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp,android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp,aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Context.h,aten/src/ATen/ExpandUtils.cpp,aten/src/ATen/ExpandUtils.h,aten/src/ATen/MemoryOverlap.cpp,aten/src/ATen/NamedTensorUtils.cpp,aten/src/ATen/ParallelNative.cpp,aten/src/ATen/SparseTensorImpl.h,aten/src/ATen/SparseTensorUtils.cpp,aten/src/ATen/TensorIndexing.cpp,aten/src/ATen/TensorIndexing.h,aten/src/ATen/TensorIterator.cpp,aten/src/ATen/TensorIterator.h,aten/src/ATen/TensorIteratorInternal.h,aten/src/ATen/TensorNames.cpp,aten/src/ATen/TensorUtils.cpp,aten/src/ATen/VmapTransforms.cpp,aten/src/ATen/WrapDimUtils.h,aten/src/ATen/WrapDimUtilsMulti.h,aten/src/ATen/benchmarks/stateful_conv1d.cpp,aten/src/ATen/core/Array.h,aten/src/ATen/core/Formatting.cpp,aten/src/ATen/core/MT19937RNGEngine.h,aten/src/ATen/core/TensorAccessor.h,aten/src/ATen/core/boxing/impl/test_helpers.h,aten/src/ATen/core/dispatch/DispatchKeyExtractor.h,aten/src/ATen/core/dispatch/backend_fallback_test.cpp,aten/src/ATen/core/function_schema.h,aten/src/ATen/core/function_schema_inl.h,aten/src/ATen/core/ivalue_inl.h,aten/src/ATen/core/op_registration/infer_schema.cpp,aten/src/ATen/core/qualified_name.h,aten/src/ATen/core/stack.h,aten/src/ATen/cpu/vec/functional_base.h,aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec/vec256/vec256_double.h,aten/src/ATen/cpu/vec/vec256/vec256_float.h,aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vec/vec256/vec256_int.h,aten/src/ATen/cpu/vec/vec256/vec256_qint.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_quint8_vsx.h,aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h,aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h,aten/src/ATen/cpu/vec/vec512/vec512_double.h,aten/src/ATen/cpu/vec/vec512/vec512_float.h,aten/src/ATen/cpu/vec/vec512/vec512_int.h,aten/src/ATen/cpu/vec/vec512/vec512_qint.h,aten/src/ATen/cpu/vec/vec_base.h,aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cudnn/Descriptors.cpp,aten/src/ATen/miopen/Descriptors.cpp,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/AdaptiveAveragePooling.cpp,aten/src/ATen/native/AdaptiveAveragePooling3d.cpp,aten/src/ATen/native/AdaptiveMaxPooling2d.cpp,aten/src/ATen/native/AdaptiveMaxPooling3d.cpp,aten/src/ATen/native/AveragePool3d.cpp,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/BlasKernel.cpp,aten/src/ATen/native/Bucketization.cpp,aten/src/ATen/native/Col2Im.cpp,aten/src/ATen/native/ComplexHelper.h,aten/src/ATen/native/ConstantPadNd.cpp,aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/ConvolutionMM2d.cpp,aten/src/ATen/native/ConvolutionMM3d.cpp,aten/src/ATen/native/ConvolutionTBC.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Cross.cpp,aten/src/ATen/native/DilatedConvolutionUtils.h,aten/src/ATen/native/DilatedMaxPool3d.cpp,aten/src/ATen/native/Dropout.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/Fill.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/Im2Col.cpp,aten/src/ATen/native/IndexingUtils.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/LossMultiLabelMargin.cpp,aten/src/ATen/native/LossMultiMargin.cpp,aten/src/ATen/native/LossNLL.cpp,aten/src/ATen/native/LossNLL2d.cpp,aten/src/ATen/native/NNPACK.cpp,aten/src/ATen/native/NamedTensor.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/PackedSequence.cpp,aten/src/ATen/native/Pool.h,aten/src/ATen/native/QuantizedLinear.cpp",100.0,23,2,6.058020051,9.0,48764.0,3.0,295151.7,16359.0,38429.5,0.0,Feature Addition,0.0,1
pytorch,b647804a557cee584ad4c98bb08847e164c824ed,0a3fb45d3d2cfacbd0469bbdba0e6cb1a2cd1bbe,Brennan Vincent,btv@fb.com,Thu Jun 06 20:13:49 2019 -0700,1559852029.0,"allow passing Python built-in types as dtypes (#21215)

Summary:
Another simple bit of syntax that NumPy supports and we don't.

Support int, float, and bool.

```python
>>> torch.randn((2,3), dtype=float)
tensor([[-0.1752, -0.3240, -0.6148],
        [ 0.1861,  1.6472,  0.1687]], dtype=torch.float64)
```

A bit confusingly, Python's ""float"" actually means double, but nothing we can do about that.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21215

Differential Revision: D15697012

Pulled By: umanwizard

fbshipit-source-id: 9a38d960a610b8e67023486b0c9265edd3c22246",38.0,2.0,"test/test_torch.py,torch/csrc/Dtype.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h",4.0,4,2,1.756052061,40.0,13036.0,4.0,4267360.5,9212.0,26853.83333,0.0,,0.0,1
pytorch,201d02ef77f5ec2cfb15386fceeb956dfa669b43,0a580da5824f0f9d9f8e9a46012dda1ca0d2886f,Guilherme Leobas,guilhermeleobas@gmail.com,Fri Oct 06 16:46:20 2023 -0300,1696610780.0,"Add batch decomposition for torch.linalg.eigh (#110640)

Closes https://github.com/pytorch/pytorch/issues/108481

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110640
Approved by: https://github.com/kshitij12345, https://github.com/zou3519",3.0,3.0,"aten/src/ATen/functorch/BatchRulesDecompositions.cpp,test/functorch/test_vmap.py,test/functorch/test_vmap_registrations.py",3.0,6,2,1.251629167,2.0,6070.0,1.0,261801.0,20572.0,47015.0,0.0,Feature Addition,0.0,1
pytorch,b27fc0ff855fd19759be5b707dc2b4387eb40599,0a6828a3063240ba5ec426e236d195a372fa51b4,BowenBao,bowbao@microsoft.com,Thu May 27 19:03:59 2021 -0700,1622142239.0,"[ONNX] use consistent quoting for string literals (#57757) (#58695)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58695

As PEP8 says: ""Pick a rule and stick to it."" [1]

[1] https://www.python.org/dev/peps/pep-0008/#string-quotes

Test Plan: Imported from OSS

Reviewed By: driazati

Differential Revision: D28714811

Pulled By: SplitInfinity

fbshipit-source-id: c95103aceb1725c17c034dc6fc8216627f189548

Co-authored-by: Gary Miguel <garymiguel@microsoft.com>",1156.0,1156.0,"test/onnx/debug_embed_params.py,test/onnx/export_onnx_tests_filter.py,test/onnx/export_onnx_tests_generator.py,test/onnx/pytorch_helper.py,test/onnx/test_caffe2_common.py,test/onnx/test_custom_ops.py,test/onnx/test_models.py,test/onnx/test_models_onnxruntime.py,test/onnx/test_onnx_opset.py,test/onnx/test_operators.py,test/onnx/test_pytorch_common.py,test/onnx/test_pytorch_helper.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_caffe2_quantized.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_pytorch_onnx_onnxruntime_cuda.py,test/onnx/test_pytorch_onnx_shape_inference.py,test/onnx/test_utility_funs.py,test/onnx/test_verify.py,test/onnx/verify.py,torch/onnx/__init__.py,torch/onnx/symbolic_caffe2.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset7.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",32.0,4,2,3.423727588,13.0,24217.0,21.0,7564785.65625,12532.0,28430.5,0.0,,0.0,1
pytorch,43fd6b234dba08a67d6bb5a3eb18969b0cc41ea1,0a8c8c1dbead2f845e524ae32c19167d80363148,Edward Yang,ezyang@fb.com,Sun Sep 02 22:24:01 2018 -0700,1535927041.0,"Rename real to scalar_t. (#11163)

Summary:
This is necessary to allow us to use the complex header
which defines real (and is very sad if real is macro'ed).

We should also fix accreal, ureal, Real and REAL, but
only 'real' is the real blocker.

```
codemod -d aten/src/TH --extensions c,cc,cpp,cu,cuh,h,TARGETS,py,hpp '\breal\b' scalar_t
codemod -d aten/src/THC --extensions c,cc,cpp,cu,cuh,h,TARGETS,py,hpp '\breal\b' scalar_t
codemod -d aten/src/THNN --extensions c,cc,cpp,cu,cuh,h,TARGETS,py,hpp '\breal\b' scalar_t
codemod -d aten/src/THCUNN --extensions c,cc,cpp,cu,cuh,h,TARGETS,py,hpp '\breal\b' scalar_t
```

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11163

Reviewed By: SsnL

Differential Revision: D9619906

Pulled By: ezyang

fbshipit-source-id: 922cb3a763c0bffecbd81200c1cefc6b8ea70942",3684.0,3684.0,"aten/src/TH/THGenerateByteType.h,aten/src/TH/THGenerateCharType.h,aten/src/TH/THGenerateDoubleType.h,aten/src/TH/THGenerateFloatType.h,aten/src/TH/THGenerateHalfType.h,aten/src/TH/THGenerateIntType.h,aten/src/TH/THGenerateLongType.h,aten/src/TH/THGenerateShortType.h,aten/src/TH/generic/THBlas.cpp,aten/src/TH/generic/THBlas.h,aten/src/TH/generic/THLapack.cpp,aten/src/TH/generic/THLapack.h,aten/src/TH/generic/THStorage.cpp,aten/src/TH/generic/THStorage.h,aten/src/TH/generic/THStorageCopy.cpp,aten/src/TH/generic/THStorageCopy.h,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensor.h,aten/src/TH/generic/THTensorApply.hpp,aten/src/TH/generic/THTensorConv.cpp,aten/src/TH/generic/THTensorConv.h,aten/src/TH/generic/THTensorCopy.cpp,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorFastGetSet.hpp,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorLapack.h,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/TH/generic/THTensorRandom.cpp,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.cpp,aten/src/TH/generic/THVectorDispatch.cpp,aten/src/THC/THCGenerateByteType.h,aten/src/THC/THCGenerateCharType.h,aten/src/THC/THCGenerateDoubleType.h,aten/src/THC/THCGenerateFloatType.h,aten/src/THC/THCGenerateHalfType.h,aten/src/THC/THCGenerateIntType.h,aten/src/THC/THCGenerateLongType.h,aten/src/THC/THCGenerateShortType.h,aten/src/THC/THCNumerics.cuh,aten/src/THC/THCTensorIndex.cu,aten/src/THC/THCTensorMathPointwise.cuh,aten/src/THC/THCTensorRandom.cu,aten/src/THC/generic/THCStorage.cpp,aten/src/THC/generic/THCStorage.cu,aten/src/THC/generic/THCStorage.h,aten/src/THC/generic/THCStorageCopy.cpp,aten/src/THC/generic/THCStorageCopy.cu,aten/src/THC/generic/THCStorageCopy.h,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensor.h,aten/src/THC/generic/THCTensorCopy.cpp,aten/src/THC/generic/THCTensorCopy.cu,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorIndex.h,aten/src/THC/generic/THCTensorMasked.cu,aten/src/THC/generic/THCTensorMasked.h,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMath.h,aten/src/THC/generic/THCTensorMathBlas.cu,aten/src/THC/generic/THCTensorMathBlas.h,aten/src/THC/generic/THCTensorMathCompare.cu,aten/src/THC/generic/THCTensorMathCompare.h,aten/src/THC/generic/THCTensorMathCompareT.cu,aten/src/THC/generic/THCTensorMathMagma.cu,aten/src/THC/generic/THCTensorMathPairwise.cu,aten/src/THC/generic/THCTensorMathPairwise.h,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathPointwise.h,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorMathReduce.h,aten/src/THC/generic/THCTensorMathScan.cu,aten/src/THC/generic/THCTensorMode.cu,aten/src/THC/generic/THCTensorRandom.cu,aten/src/THC/generic/THCTensorScatterGather.cu,aten/src/THC/generic/THCTensorScatterGather.h,aten/src/THC/generic/THCTensorSort.cu,aten/src/THC/generic/THCTensorTopK.cu,aten/src/THCUNN/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/Abs.cu,aten/src/THCUNN/generic/AbsCriterion.cu,aten/src/THCUNN/generic/BCECriterion.cu,aten/src/THCUNN/generic/BatchNormalization.cu,aten/src/THCUNN/generic/ClassNLLCriterion.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/DistKLDivCriterion.cu,aten/src/THCUNN/generic/ELU.cu,aten/src/THCUNN/generic/FeatureLPPooling.cu,aten/src/THCUNN/generic/GatedLinearUnit.cu,aten/src/THCUNN/generic/HardTanh.cu,aten/src/THCUNN/generic/IndexLinear.cu,aten/src/THCUNN/generic/L1Cost.cu,aten/src/THCUNN/generic/LeakyReLU.cu,aten/src/THCUNN/generic/LogSigmoid.cu,aten/src/THCUNN/generic/LookupTable.cu,aten/src/THCUNN/generic/LookupTableBag.cu,aten/src/THCUNN/generic/MSECriterion.cu,aten/src/THCUNN/generic/MarginCriterion.cu,aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/MultiMarginCriterion.cu,aten/src/THCUNN/generic/PReLU.cu,aten/src/THCUNN/generic/RReLU.cu,aten/src/THCUNN/generic/Sigmoid.cu,aten/src/THCUNN/generic/SmoothL1Criterion.cu,aten/src/THCUNN/generic/SoftMarginCriterion.cu,aten/src/THCUNN/generic/SoftPlus.cu,aten/src/THCUNN/generic/SoftShrink.cu,aten/src/THCUNN/generic/SparseLinear.cu,aten/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/SpatialAveragePooling.cu,aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu,aten/src/THCUNN/generic/SpatialConvolutionLocal.cu,aten/src/THCUNN/generic/SpatialConvolutionMM.cu,aten/src/THCUNN/generic/SpatialCrossMapLRN.cu,aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu,aten/src/THCUNN/generic/SpatialFractionalMaxPooling.cu,aten/src/THCUNN/generic/SpatialFullDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialReflectionPadding.cu,aten/src/THCUNN/generic/SpatialReplicationPadding.cu,aten/src/THCUNN/generic/SpatialSubSampling.cu,aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu,aten/src/THCUNN/generic/SpatialUpSamplingNearest.cu,aten/src/THCUNN/generic/Sqrt.cu,aten/src/THCUNN/generic/Square.cu,aten/src/THCUNN/generic/Tanh.cu,aten/src/THCUNN/generic/TemporalConvolution.cu,aten/src/THCUNN/generic/TemporalMaxPooling.cu,aten/src/THCUNN/generic/TemporalReflectionPadding.cu,aten/src/THCUNN/generic/TemporalReplicationPadding.cu,aten/src/THCUNN/generic/TemporalRowConvolution.cu,aten/src/THCUNN/generic/TemporalUpSamplingLinear.cu,aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu,aten/src/THCUNN/generic/Threshold.cu,aten/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFullDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricReplicationPadding.cu,aten/src/THCUNN/generic/VolumetricUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricUpSamplingTrilinear.cu,aten/src/THNN/generic/Abs.c,aten/src/THNN/generic/AbsCriterion.c,aten/src/THNN/generic/BCECriterion.c,aten/src/THNN/generic/BatchNormalization.c,aten/src/THNN/generic/ClassNLLCriterion.c,aten/src/THNN/generic/Col2Im.c,aten/src/THNN/generic/DistKLDivCriterion.c,aten/src/THNN/generic/ELU.c,aten/src/THNN/generic/FeatureLPPooling.c,aten/src/THNN/generic/GatedLinearUnit.c,aten/src/THNN/generic/HardShrink.c,aten/src/THNN/generic/HardTanh.c,aten/src/THNN/generic/Im2Col.c,aten/src/THNN/generic/IndexLinear.c,aten/src/THNN/generic/L1Cost.c,aten/src/THNN/generic/LeakyReLU.c,aten/src/THNN/generic/Linear.c,aten/src/THNN/generic/LogSigmoid.c,aten/src/THNN/generic/LookupTable.c,aten/src/THNN/generic/MSECriterion.c,aten/src/THNN/generic/MarginCriterion.c,aten/src/THNN/generic/MultiLabelMarginCriterion.c,aten/src/THNN/generic/MultiMarginCriterion.c,aten/src/THNN/generic/PReLU.c,aten/src/THNN/generic/RReLU.c,aten/src/THNN/generic/Sigmoid.c,aten/src/THNN/generic/SmoothL1Criterion.c,aten/src/THNN/generic/SoftMarginCriterion.c,aten/src/THNN/generic/SoftPlus.c,aten/src/THNN/generic/SoftShrink.c,aten/src/THNN/generic/SparseLinear.c,aten/src/THNN/generic/SpatialAdaptiveAveragePooling.c,aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c,aten/src/THNN/generic/SpatialAveragePooling.c,aten/src/THNN/generic/SpatialClassNLLCriterion.c,aten/src/THNN/generic/SpatialConvolutionLocal.c,aten/src/THNN/generic/SpatialConvolutionMM.c,aten/src/THNN/generic/SpatialConvolutionMap.c,aten/src/THNN/generic/SpatialDilatedConvolution.c,aten/src/THNN/generic/SpatialDilatedMaxPooling.c,aten/src/THNN/generic/SpatialFractionalMaxPooling.c,aten/src/THNN/generic/SpatialFullConvolutionMap.c,aten/src/THNN/generic/SpatialFullDilatedConvolution.c,aten/src/THNN/generic/SpatialMaxUnpooling.c,aten/src/THNN/generic/SpatialReflectionPadding.c,aten/src/THNN/generic/SpatialReplicationPadding.c,aten/src/THNN/generic/SpatialSubSampling.c,aten/src/THNN/generic/SpatialUpSamplingBilinear.c,aten/src/THNN/generic/SpatialUpSamplingNearest.c,aten/src/THNN/generic/Sqrt.c,aten/src/THNN/generic/Square.c,aten/src/THNN/generic/Tanh.c,aten/src/THNN/generic/TemporalConvolution.c,aten/src/THNN/generic/TemporalMaxPooling.c,aten/src/THNN/generic/TemporalReflectionPadding.c,aten/src/THNN/generic/TemporalReplicationPadding.c,aten/src/THNN/generic/TemporalRowConvolution.c,aten/src/THNN/generic/TemporalSubSampling.c,aten/src/THNN/generic/TemporalUpSamplingLinear.c,aten/src/THNN/generic/TemporalUpSamplingNearest.c,aten/src/THNN/generic/Threshold.c,aten/src/THNN/generic/VolumetricAdaptiveAveragePooling.c,aten/src/THNN/generic/VolumetricAdaptiveMaxPooling.c,aten/src/THNN/generic/VolumetricAveragePooling.c,aten/src/THNN/generic/VolumetricConvolution.c,aten/src/THNN/generic/VolumetricConvolutionMM.c,aten/src/THNN/generic/VolumetricDilatedConvolution.c,aten/src/THNN/generic/VolumetricDilatedMaxPooling.c,aten/src/THNN/generic/VolumetricFractionalMaxPooling.c,aten/src/THNN/generic/VolumetricFullDilatedConvolution.c,aten/src/THNN/generic/VolumetricMaxUnpooling.c,aten/src/THNN/generic/VolumetricReplicationPadding.c,aten/src/THNN/generic/VolumetricUpSamplingNearest.c,aten/src/THNN/generic/VolumetricUpSamplingTrilinear.c,aten/src/THNN/generic/unfold.c,tools/amd_build/pyHIPIFY/hipify-python.py,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/StorageSharing.cpp,torch/csrc/generic/serialization.cpp,torch/csrc/generic/utils.h",231.0,16,3,6.959639825,42.0,55495.0,40.0,4224321.982683983,3821.0,10672.33333,0.0,Corrective,1.0,1
pytorch,7c2103ad5ffdc1ef91231c966988f7f2a61b4166,0aa3c39e5f296dd0871d0f849e295d3b7644ff2e,Mike Ruberry,mruberry@devfair044.h1.fair,Mon Mar 21 03:24:16 2022 +0000,1647833056.0,"Extends OpInfo architecture with reference inputs, adds them for elementwise binary operators

This PR extends our OpInfo test architecture with ""reference inputs,"" an optional expansion of typical sample inputs that allows for more thorough testing. Currently only the elementwise binary operations implement an extended set of reference inputs. This PR also cleans up some smaller OpInfo-related issues, including several bugs, and it identified https://github.com/pytorch/pytorch/issues/74279.

A reference inputs function can be specified for an OpInfo by filling in its ""reference_inputs_func"" metadata. If this is done it's recommended that the reference inputs function first call the sample inputs function, then produce additional sample inputs. See `reference_inputs_elementwise_binary` for an example of this pattern.

In addition to implementing reference inputs for the elementwise binary operations, this PR improves their consistency and simplifies how their metadata is represented. The great majority now use a generic sample input function, and those that want extensions start by calling the generic sample input function and then adding additional samples. This removes many older sample input functions. The BinaryUfuncInfo subclass also now allows specifying scalar support more precisely, and reference inputs and error inputs are generated based on this metadata to ensure it's correct.

cc @kshitij12345 @pmeier @zou3519 @Chillee

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74280
Approved by: https://github.com/ngimel",796.0,1122.0,"test/test_binary_ufuncs.py,test/test_ops.py,test/test_testing.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_jit.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",7.0,4,2,0.925663926,4.0,27022.0,6.0,3148471.8571428573,1537.0,3721.0,0.0,Corrective,1.0,1
pytorch,bc7a41af7d541e64f8b8f7318a7a2248c0119632,0ac58d53b813ad129735af5435966d9a1fc14df6,Tongzhou Wang,SsnL@users.noreply.github.com,Wed Jan 10 22:36:26 2018 -0500,1515623786.0,"ATen conv param expansion; InstanceNorm use_running_stats fix (#4544)

* fix instancenorm and aten conv param expansion

* addressed colesbury 's comments

* improve conv input shape check",48.0,30.0,"aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/SpectralOps.cpp,test/test_torch.py,torch/nn/modules/instancenorm.py,torch/utils/data/dataloader.py",5.0,10,3,1.642299014,38.0,6784.0,5.0,869625.2,414.0,2235.0,0.0,Corrective,1.0,1
pytorch,fbe01d27e2f8d3e7209b710433968796f637248a,0ade178c6ed08b695aa557be01770ba704614d31,Richard Zou,zou3519@gmail.com,Mon Dec 13 01:05:22 2021 -0800,1639357522.0,"[functorch] Fix CI

Bunch of tests failing on CI. Also it seems like an OpInfo was removed
from PyTorch core that was blocking test_vmap from running",5.0,1.0,"functorch/test/test_eager_transforms.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,2,1,1.251629167,1.0,6544.0,3.0,0.3333333333333333,621.0,848.5,0.0,Corrective,1.0,1
pytorch,42328b70f71ed12b7bee20d45a44db99136d777d,0b000952c12b8e73bef6f668a6f10e1f19b222ba,Christian Sarofeen,csarofeen@nvidia.com,Wed Aug 02 23:55:05 2017 -0700,1501718105.0,Split batchnorm eval test into cpu and cuda functions. (#2273),28.0,25.0,test/test_nn.py,1.0,1,1,0,33.0,3927.0,1.0,602674.0,1291.0,17177.02883,0.0,,0.0,1
pytorch,54563e6288efda566be3653a547c8be2781cc547,0b22f5ae9f9830931453eb1b8c038e099de8daca,Edward Z. Yang,ezyang@fb.com,Thu Dec 15 08:37:25 2022 +0800,1671093445.0,"Deeply rework WeakIdKeyDictionary (#90825)

In the prior patch, I just YOLOed a mutable mapping implementation.
Many edge cases were not handled correctly.  In this PR, I just
copy paste the WeakKeyDictionary from CPython and the hacked it up
to use WeakIdRef instead of weakref.ref.  You can see each line
I changed with the comment CHANGED; there aren't many.

Being exactly API compatible with WeakKeyDictionary means I can also
rob all of the tests from CPython, which I also did for
test/test_weak.py

How to review?  You could either try taking the delta from CPython
(recommended), or review everything from scratch (not recommended).
Can post diff representing delta on request.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90825
Approved by: https://github.com/albanD",821.0,68.0,".lintrunner.toml,test/test_weak.py,torch/_subclasses/fake_tensor.py,torch/_subclasses/meta_utils.py,torch/utils/weak.py",5.0,4,2,1.097853399,3.0,2595.0,3.0,31836.75,10576.0,24160.5,0.0,Corrective,1.0,1
pytorch,96e3d1a76c5034679b68b26b45ef99edb9c67c09,0b2f68eadf5e9e99c0fdf8ff761ddcac126f224e,Horace He,horacehe2007@yahoo.com,Tue Nov 02 22:55:43 2021 -0700,1635893743.0,"Remove special FX OpInfo list (#67520)

Summary:
Most of the failing tests are since the test doesn't work with python functions (only builtins like `torch.add`).

I added a check for that and ported the remaining skips into the `skips` field.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67520

Reviewed By: ZolotukhinM

Differential Revision: D32046856

Pulled By: Chillee

fbshipit-source-id: 05fa3e3c40fa6cc4f776e0c24f667629b14afd25",38.0,90.0,"test/test_fx.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.679392935,2.0,16275.0,2.0,178263.0,16782.0,39361.5,0.0,Feature Addition,0.0,1
pytorch,ebc012ace69ada1508c66aa5b50a5cb8807346e3,0b44e1a74c7b66804ef313ff9d8bd5a13e827445,Jiong Gong,jiong.gong@intel.com,Thu Jul 18 08:06:59 2024 -0700,1721290019.0,"[inductor][cpp][gemm] optimize arbitrary N in packed gemm template (#130690)

Currently we require `n % register_block_n == 0` which typically bring good perf when `n` is a multiply of 8, 16, 32 etc. while will fall back to the reference micro gemm otherwise (where `register_block_n == 1`). This PR optimizes this by padding `n` to the multiple of `register_block_n` which is 8, 16, 32 etc. for packed weight. Therefore, the micro-gemm can work as is on the padded `n`. When the weight is padded, we will use the local accumulation buffer to get the result from micro-gemm and then unpadded (sliced) before storing back to the output buffer.

Performance numbers measured on ""Intel (R) Xeon (R) CPU Max 9480"", single core, bf16.

Before
AUTOTUNE linear_unary(512x768, 3073x768, 3073)
  _linear_pointwise 2.3563 ms 100.0%
  cpp_packed_gemm_0 710.5902 ms 0.3%

After
AUTOTUNE linear_unary(512x768, 3073x768, 3073)
  cpp_packed_gemm_0 1.8909 ms 100.0%
  _linear_pointwise 2.1016 ms 90.0%

Pull Request resolved: https://github.com/pytorch/pytorch/pull/130690
Approved by: https://github.com/leslie-fang-intel, https://github.com/jansel
ghstack dependencies: #130675",77.0,81.0,"test/inductor/test_cpu_select_algorithm.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_micro_gemm.py,torch/_inductor/codegen/cpp_template_kernel.py,torch/_inductor/utils.py",5.0,5,2,1.518878944,1.0,4150.0,4.0,191025.8,31445.0,79094.5,0.0,Feature Addition,0.0,1
pytorch,c6505cc3837eb903f98163e40fad638a1cfeb502,0b48d968952a6183ae122679d624940e5228567f,Patrick Kan,patrickkan@fb.com,Wed Sep 01 19:20:50 2021 -0700,1630524050.0,"[Bootcamp] Include both python unittest and parser parameters in --help and -h flag (#64297)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/45945

Creates a new thread to run -h or --help with unittest.main if the help flag is present, and keeps the add_help default for parameters.

Includes both python unittest and parser parameters in --help and -h flag and will remain up to date since both messages are displayed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64297

Test Plan:
Imported from GitHub

`python test/test_spectral_ops.py --help`

Output:
```
% python test/test_spectral_ops.py --help
usage: test_spectral_ops.py [-h] [-v] [-q] [--locals] [-f] [-c] [-b] [-k TESTNAMEPATTERNS] [tests [tests ...]]

positional arguments:
  tests                a list of any number of test modules, classes and test methods.

optional arguments:
  -h, --help           show this help message and exit
  -v, --verbose        Verbose output
  -q, --quiet          Quiet output
  --locals             Show local variables in tracebacks
  -f, --failfast       Stop on first fail or error
  -c, --catch          Catch Ctrl-C and display results so far
  -b, --buffer         Buffer stdout and stderr during tests
  -k TESTNAMEPATTERNS  Only run tests which match the given substring

Examples:
  test_spectral_ops.py                           - run default set of tests
  test_spectral_ops.py MyTestSuite               - run suite 'MyTestSuite'
  test_spectral_ops.py MyTestCase.testSomething  - run MyTestCase.testSomething
  test_spectral_ops.py MyTestCase                - run all 'test*' test methods
                                       in MyTestCase

usage: test_spectral_ops.py [-h] [--subprocess] [--seed SEED] [--accept] [--jit_executor JIT_EXECUTOR] [--repeat REPEAT]
                            [--test_bailouts] [--save-xml [SAVE_XML]] [--discover-tests] [--log-suffix LOG_SUFFIX]
                            [--run-parallel RUN_PARALLEL] [--import-slow-tests [IMPORT_SLOW_TESTS]]
                            [--import-disabled-tests [IMPORT_DISABLED_TESTS]]

optional arguments:
  -h, --help            show this help message and exit
  --subprocess          whether to run each test in a subprocess
  --seed SEED
  --accept
  --jit_executor JIT_EXECUTOR
  --repeat REPEAT
  --test_bailouts
  --save-xml [SAVE_XML]
  --discover-tests
  --log-suffix LOG_SUFFIX
  --run-parallel RUN_PARALLEL
  --import-slow-tests [IMPORT_SLOW_TESTS]
  --import-disabled-tests [IMPORT_DISABLED_TESTS]
  ```

Also ran some other tests to make sure tests still worked, and other tests with --help or -h flag

Reviewed By: seemethere

Differential Revision: D30677776

Pulled By: PatrickKan

fbshipit-source-id: eb3d6e3fa677137ec703ec3a23808efb99acc896",11.0,1.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2542.0,1.0,173107.0,15134.0,34671.5,0.0,Corrective,1.0,1
pytorch,cd68083cd92bceaf8360bc38d15a0345351adba2,0b5385c8cc2579247a23796864ba9270f64cdb1d,Samantha Andow,samdow@fb.com,Thu Mar 17 16:50:03 2022 -0700,1647535803.0,"[functorch] Refactor randomness tests (pytorch/functorch#593)

* refactor tests

* remove extra test, fix lint errors

* replace allclose calls

* fixes",240.0,121.0,functorch/test/test_vmap.py,1.0,2,1,0,1.0,3905.0,1.0,0.0,897.0,1249.0,0.0,Corrective,1.0,1
pytorch,be6ffac1b61853ed857147a8be1a87d731160386,0b606a4a7c3d8e4886fc5f63af120f4dc2e8fca8,Pritam Damania,pritam.damania@fb.com,Tue Jan 21 22:58:26 2020 -0800,1579647506.0,"Enhace DispatchStub to be thread safe from a TSAN point of view. (#32148)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32148

TSAN would complain about multiple threads reading and writing to the
`cpu_dispatch_ptr` without any sort of synchronization. Although, this is a
valid issue from a TSAN point of view, there wasn't a correctness issue since
both threads would compute the same value.

In order to fix this, I've used std::atomic for cpu_dispatch_ptr with relaxed
ordering guarantees.
ghstack-source-id: 96989435

Test Plan: Verify the TSAN tests pass.

Differential Revision: D19386082

fbshipit-source-id: 1ff0893e02529eddd06b2855d9565edf1bbf1196",8.0,4.0,aten/src/ATen/native/DispatchStub.h,1.0,4,1,0,1.0,201.0,1.0,8318798.0,14319.0,38911.83333,0.0,Corrective,1.0,1
pytorch,145560f49970616599498cfcfd08995446239ff2,0b693e9601973cda7261618151690e9a95f809dd,Xing Liu,xingl@fb.com,Tue May 05 16:53:32 2020 -0700,1588697612.0,"uninitialize output and bag_size in the fast path of EmbeddingBag to save overhead (#36681)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36681

Test Plan:
Imported from OSS

Unit tests:
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_failures_cpu
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_offsets_cpu
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_no_offsets_cpu
python test/test_nn.py TestNN.test_embeddingbag_from_pretrained
python test/test_nn.py TestNN.test_embeddingbag_from_pretrained_options

Finally run: python test/test_nn.py

Reviewed By: jspark1105

Differential Revision: D21058006

Pulled By: xing-liu

fbshipit-source-id: 65b36a788839e8b722db3e295e58215b5935d6e8",6.0,3.0,aten/src/ATen/native/EmbeddingBag.cpp,1.0,4,1,0,7.0,830.0,1.0,1289856.0,1687.0,4399.0,0.0,,0.0,1
pytorch,063d5b0d3f8e47b9930e1f387d0fbbf0be175e8d,0b9717b86a70b4a0a7a7b728d7f651ed2c149fde,Wojciech Baranowski,wbaranowski@protonmail.com,Mon Jul 06 14:54:18 2020 -0700,1594047258.0,"When linking libtorch_cpu.so, put AVX sources last in the input list (#40449)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/39600
Pull Request resolved: https://github.com/pytorch/pytorch/pull/40449

Reviewed By: VitalyFedyunin

Differential Revision: D22312501

Pulled By: colesbury

fbshipit-source-id: 4c09adb0173749046f20b84241d6c940b339ad77",41.0,1.0,"caffe2/CMakeLists.txt,cmake/Codegen.cmake",2.0,2,2,0.276195428,14.0,1661.0,2.0,2269851.5,3384.0,8051.0,0.0,Corrective,1.0,1
pytorch,2c0fe338da6a0e375d86575bb00dc58ac0c8f877,0bc9928f318a18b0ec44af7bbb84167b300f3a4a,Nikita Shulga,nshulga@fb.com,Fri Oct 22 20:36:41 2021 -0700,1634935001.0,"[ONNX] Symbolic: dynamic input for OneHot, bool for Einsum (#65940) (#66147)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66147

Symbolic: dynamic input for OneHot, bool for Einsum

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D31424094

fbshipit-source-id: 76bea22b29c93d1621c597fe7ab59deb3685087f

Co-authored-by: jiafatom <jiafa@microsoft.com>",29.0,9.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.49947168,3.0,13806.0,2.0,1543468.0,16506.0,38696.0,0.0,,0.0,1
pytorch,657b75d155aa76119049f72ea1ca9a3c6e228497,0c1420aa3c9726d817f48c351e0c3c9d7f1dbf4e,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Fri May 28 03:31:30 2021 -0700,1622172690.0,"OpInfo: `fmod` and `remainder` (#57941)

Summary:
See https://github.com/pytorch/pytorch/issues/54261

cc: mruberry Lezcano kshitij12345

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57941

Reviewed By: mrshenli

Differential Revision: D28744464

Pulled By: mruberry

fbshipit-source-id: 19847277d4f8d3a39a706c2b3c9eddf0dedcb20c",105.0,48.0,"aten/src/THC/generic/THCTensorMathPairwise.cu,test/test_jit_fuser_te.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",5.0,8,3,1.886495454,31.0,22197.0,4.0,5658519.4,12555.0,28489.5,0.0,,0.0,1
pytorch,e9bfe8ca92df0dfe1bddc4648c3296c51fe609bf,0c4b3f42713de5f818a759d3ddcbefa6213a1914,Neeraj Pradhan,prad.neeraj@gmail.com,Sat Dec 23 14:14:44 2017 -0800,1514038484.0,Adding Uniform distribution to PyTorch (#4328),116.0,1.0,"test/test_distributions.py,torch/distributions/distribution.py,torch/distributions/uniform.py",3.0,3,2,1.088260653,8.0,879.0,2.0,151871.5,869.0,6643.172317,0.0,Feature Addition,0.0,1
pytorch,8b2a9f81cc7cab9cb49cd2c96b9304a3f9313fca,0c6f409cdad073e8c64ef11b72c41d89ab7ee167,Jason Ansel,jansel@meta.com,Sat May 20 01:14:21 2023 +0000,1684545261.0,"[inductor] Refactor RNG operators (#100064)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100064
Approved by: https://github.com/ngimel",455.0,456.0,"test/inductor/test_cpp_wrapper.py,test/inductor/test_cpu_repro.py,test/inductor/test_cudagraph_trees.py,test/inductor/test_fused_attention.py,test/inductor/test_select_algorithm.py,test/inductor/test_torchinductor.py,test/inductor/test_torchinductor_codegen_dynamic_shapes.py,torch/_decomp/decompositions.py,torch/_dynamo/replay_record.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/cpp_prefix.h,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/compile_fx.py,torch/_inductor/config.py,torch/_inductor/decomposition.py,torch/_inductor/fx_passes/fuse_attention.py,torch/_inductor/fx_passes/joint_graph.py,torch/_inductor/fx_passes/pre_grad.py,torch/_inductor/fx_passes/replace_random.py,torch/_inductor/graph.py,torch/_inductor/inductor_prims.py,torch/_inductor/ir.py,torch/_inductor/lowering.py,torch/_inductor/overrides.py,torch/_inductor/pattern_matcher.py,torch/_inductor/triton_helpers.py,torch/_inductor/virtualized.py",29.0,8,2,3.663833094,5.0,35186.0,10.0,117555.44827586209,16056.0,36196.0,0.0,Perfective,0.0,1
pytorch,53c65ddc6a516d03b4848080d3250952d96c3226,0c9670ddf01e7b64f46100f1ac516c3221d96aa7,Adam Paszke,adam.paszke@gmail.com,Tue Oct 04 18:33:00 2016 -0700,1475605980.0,Allow remapping storages at load time and serialize data in little endian order,325.0,15.0,"test/test_cuda.py,torch/csrc/Storage.cpp,torch/csrc/THP.h,torch/csrc/byte_order.cpp,torch/csrc/byte_order.h,torch/csrc/generic/serialization.cpp,torch/serialization.py",7.0,4,2,2.09345628,9.0,806.0,1.0,143786.0,228.0,2263.14599,0.0,,0.0,1
pytorch,2279299c6c17b99a4d38b371e99f7f3a599463fc,0cf3c1ce66c31e735f0ddce58ad2e3ac3e77a6a9,Thomas Viehmann,tv.code@beamnet.de,Fri Oct 12 09:08:49 2018 -0700,1539335329.0,"Add copy= keyword to Tensor.to (#12571)

Summary:
Fixes: #12454
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12571

Differential Revision: D10356994

Pulled By: SsnL

fbshipit-source-id: d87416078a5a8e5ffa690cd73c09fa6b4e16aa25",62.0,52.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/TensorConversions.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/_tensor_docs.py,torch/csrc/autograd/utils/python_arg_parsing.h",9.0,13,4,2.981239911,41.0,17351.0,5.0,1486285.0,4577.0,13469.83333,0.0,Corrective,1.0,1
pytorch,281e34d1b71570e7e7ccd7fca54f277283d9cd5e,0d0f197682f6be142a8634450f6473cef9bdfb2c,Kai Arulkumaran,Kaixhin@users.noreply.github.com,Wed Dec 14 20:39:42 2016 +0000,1481747982.0,Add note on Huber loss (#310),3.0,1.0,"docs/nn_loss.md,torch/nn/modules/loss.py",2.0,4,2,0.811278124,14.0,678.0,1.0,69346.0,237.0,1852.406855,0.0,Feature Addition,0.0,1
pytorch,520f90f35964636be5da0fa7b56cb1d1f2aa9353,0d11dbf5119e1e4c69016ed1527bcf5013b208af,BowenBao,bowbao@microsoft.com,Fri May 14 16:49:58 2021 -0700,1621010998.0,"[ONNX] Support index_add_ function. (#56867) (#57830)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57830

This is PR is aiming to support tensor.index_add_() method in symbolic function. We leverage scatter_add() to implement this function while ONNX doesn't have a corresponding operator.

Notes:

      1.  4 tests have been added for some scenarios.
      2.  If there are duplicated value in 'index' parameter, the export will still execute successfully but the results are wrong. Add a warning for every call to this symbolic function. And if we detect that the rank of 'index' is greater than the size of the 'dim' dimension, will raise an exception to stop exporting an incorrect ONNX file.

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D28393518

Pulled By: SplitInfinity

fbshipit-source-id: f487ca2c63fec47c6ab74f1a7783dae7f3b8d1ef

Co-authored-by: Jay Zhang <jiz@microsoft.com>",168.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.924133542,3.0,11966.0,2.0,72541.5,12110.0,27456.0,0.0,Corrective,0.0,1
pytorch,1a9602d5db856f32d4e3201e4380ca58fc39b514,0d3cb91d8c8c1bb368128378e2300f96e9217344,Teng Li,tengli@fb.com,Fri Nov 30 02:46:22 2018 -0800,1543545982.0,"Make env init_method support both env and args for rank and size (#14494)

Summary:
Fixing: https://github.com/pytorch/pytorch/issues/14446

This was a supported behavior in old torch.distributed. We want to support it in the new release.

Test should cover all combination of scenario when we have either env or arg set up for rank or size or both
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14494

Differential Revision: D13253433

Pulled By: teng-li

fbshipit-source-id: c05974d84f1bdf969f74ec45763e11a841fe4848",77.0,15.0,"test/test_c10d.py,torch/distributed/distributed_c10d.py,torch/distributed/rendezvous.py",3.0,3,2,1.449441485,2.0,2924.0,3.0,467082.0,5716.0,17323.83333,0.0,Corrective,1.0,1
pytorch,9b173b87b2587ee085aaef65e1a0fdebd90d5229,0d4bbd1996ec27c1fb870f935f7e4346cc08495b,Andrew Gu,andgu@fb.com,Wed Jan 18 00:16:13 2023 +0000,1674000973.0,"[Lint] Add FSDP/composable API files to ufmt include (#90873)

This PR adds FSDP and composable API files to `.lintrunner.toml` so that (1) lintrunner enforces that those files are formatted and (2) `lintrunner f` formats those files for you.

There are two requirements here (see https://github.com/pytorch/pytorch/wiki/lintrunner for details):
1. Install lintrunner:
```
pip install lintrunner
lintrunner init
```
2. `lintrunner f` before you finalize your PR, which would now be enforced by CI after this PR.

The code changes in this PR outside of `.lintrunner.toml` are the result of `lintrunner f`.

---

I only plan to land this PR if all of the composable API developers agree that this is something that makes sense and is not too intrusive to the workflow.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90873
Approved by: https://github.com/yhcharles, https://github.com/mrshenli, https://github.com/rohan-varma",85.0,50.0,".lintrunner.toml,test/distributed/_composable/test_checkpoint.py,test/distributed/_composable/test_fully_shard.py,test/distributed/fsdp/test_fsdp_ignored_modules.py,test/distributed/fsdp/test_fsdp_mixed_precision.py,test/distributed/fsdp/test_fsdp_overlap.py,test/distributed/fsdp/test_fsdp_state_dict.py,test/distributed/fsdp/test_utils.py,torch/distributed/_composable/replicate.py,torch/distributed/fsdp/_init_utils.py,torch/distributed/fsdp/_runtime_utils.py,torch/distributed/fsdp/_state_dict_utils.py,torch/distributed/fsdp/flat_param.py,torch/distributed/fsdp/fully_sharded_data_parallel.py,torch/distributed/fsdp/sharded_grad_scaler.py,torch/testing/_internal/common_fsdp.py",16.0,10,2,3.334516645,3.0,13164.0,12.0,1138519.375,11373.0,26088.5,0.0,Feature Addition,0.0,1
pytorch,e67284d9ee1f9c8dbb14169c69c71d035014e38b,0d76299ff782224810b997d99f8af490cd96eede,Justin Chu,justinchuby@users.noreply.github.com,Fri May 20 01:56:24 2022 +0000,1653011784.0,"[ONNX] Clean up module imports (#77423)

Cleaning up onnx module imports to prepare for updating `__init__`.

- Simplify importing the `_C` and `_C._onnx` name spaces
- Remove alias of the symbolic_helper module in imports
- Remove any module level function imports. Import modules instead
    - Alias `symbilic_opsetx` as `opsetx`
- Fix some docstrings

Requires:
- https://github.com/pytorch/pytorch/pull/77448
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77423
Approved by: https://github.com/BowenBao",1981.0,1775.0,"torch/onnx/__init__.py,torch/onnx/_globals.py,torch/onnx/_patch_torch.py,torch/onnx/onnx_supported_ops.py,torch/onnx/operators.py,torch/onnx/symbolic_caffe2.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset14.py,torch/onnx/symbolic_opset15.py,torch/onnx/symbolic_opset16.py,torch/onnx/symbolic_opset7.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",19.0,2,1,2.850027845,14.0,12898.0,3.0,4374029.105263158,3467.0,8253.0,0.0,Corrective,1.0,1
pytorch,"ecc7579f446be2b012840c651b7a97f737c12d26,d112cbd7f675a8ffde3a8995ac37c69a4c84e5df",0d7d79ad759ac90c21cb3268d1bfe14e19ee68f5,Soumith Chintala,soumith@gmail.com,Fri Aug 25 11:39:02 2017 -0400,1503661142.0,Merge commit 'd112cbd7f675a8ffde3a8995ac37c69a4c84e5df',2057.0,883.0,"torch/lib/THCUNN/BatchNormalization.cu,torch/lib/THCUNN/CMakeLists.txt,torch/lib/THCUNN/FeatureLPPooling.cu,torch/lib/THCUNN/SpatialFullDilatedConvolution.cu,torch/lib/THCUNN/VolumetricFullConvolution.cu,torch/lib/THCUNN/VolumetricFullDilatedConvolution.cu,torch/lib/THCUNN/generic/FeatureLPPooling.cu,torch/lib/THCUNN/generic/SpatialConvolutionLocal.cu,torch/lib/THCUNN/generic/SpatialConvolutionMM.cu,torch/lib/THCUNN/generic/SpatialDepthWiseConvolution.cu,torch/lib/THCUNN/generic/SpatialDilatedConvolution.cu,torch/lib/THCUNN/generic/SpatialFullConvolution.cu,torch/lib/THCUNN/generic/SpatialFullDilatedConvolution.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THCUNN/generic/VolumetricDilatedConvolution.cu,torch/lib/THCUNN/generic/VolumetricFullConvolution.cu,torch/lib/THCUNN/generic/VolumetricFullDilatedConvolution.cu,torch/lib/THCUNN/im2col.h,torch/lib/THCUNN/vol2col.h",19.0,4,1,2.797755718,35.0,5574.0,1.0,3748076.0,1373.0,13787.85284,0.0,,0.0,1
pytorch,dcfb5620dfb7a01ea41c2d3e610d3eab3e43def2,0d8a3610c5206f3d4cbf5f07f7a8696ed4eb9a9a,Pieter Noordhuis,pietern@fb.com,Fri Apr 26 15:16:53 2019 -0700,1556291813.0,"Multiple module outputs and multiple calls to backward (#19799)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19799

A module that returns multiple outputs and where the called may end up
doing multiple calls to torch.autograd.backward did not work with
DistributedDataParallel. It expected the first call to
torch.autograd.backward to provide gradients for ALL parameters that
expect gradients and were used in computing the module output. If you
have outputs with disjoint autograd graphs it is fine to call
torch.autograd.backward on both and fill in the module's parameter
gradients in separate chunks.

With this change we delay queuing the finalizer callback until we have
marked all buckets as ready, instead of queueing it the first time we
receive an autograd hook. This returns the current implementation to
be functionally equivalent to the DistributedDataParallel
implementation before #18953 was merged.

Reviewed By: mrshenli

Differential Revision: D15097045

fbshipit-source-id: 2df023319713bc31e29a8b45108c78e6593fccd4",54.0,7.0,"test/test_c10d.py,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h",3.0,5,2,0.76004733,2.0,3060.0,1.0,572194.0,8347.0,24957.83333,0.0,,0.0,1
pytorch,3c0d7f08c65cd6b30bbcc08a7fa7b03233d7df56,0d9117d74caf59ee89d569c4fdcfa976909058c3,Richard Zou,zou3519@gmail.com,Fri Sep 24 15:20:33 2021 -0700,1632496833.0,[functorch] Fix CI,5.0,1.0,"functorch/test/common_utils.py,functorch/test/test_ops.py",2.0,2,1,0.650022422,1.0,732.0,1.0,0.0,374.0,551.0,0.0,Corrective,1.0,1
pytorch,918ede7a8590604491369ff441993c8b23e05f3b,0d94ae66a712a2bfa007a75ed43fd18d392d60af,Horace He,horacehe2007@yahoo.com,Thu Apr 29 01:42:16 2021 -0700,1619660536.0,[functorch] fixed dot batching rule and added vmap tests using opinfo,99.0,1.0,"functorch/functorch/__init__.py,functorch/functorch/_src/python_key.py,functorch/functorch/csrc/BatchRulesLinearAlgebra.cpp,functorch/test/test_vmap.py",4.0,5,1,1.21541732,1.0,2664.0,3.0,0.0,32.0,111.5,0.0,Corrective,1.0,1
pytorch,68c3b959de77978163262e1a3f1d40a5334a0f1c,0d95028bee5c05320c92979a302720a60a9c13ef,vishwakftw,cs15btech11043@iith.ac.in,Tue Feb 12 21:34:44 2019 -0800,1550007284.0,"Dispatch the correct legacy function for geqrf_out and ormqr_out (#16964)

Summary:
This fixes the segfault.

Changelog:
- Modify the function calls in LegacyDefinitions for `geqrf_out` and `ormqr_out`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16964

Differential Revision: D14025985

Pulled By: gchanan

fbshipit-source-id: aa50e2c1694cbf3642273ee14b09ba12625c7d33",34.0,8.0,"aten/src/ATen/native/LegacyDefinitions.cpp,test/test_cuda.py,test/test_torch.py",3.0,5,2,0.892942742,41.0,13778.0,3.0,342119.0,6981.0,21449.33333,0.0,Corrective,1.0,1
pytorch,8aa48315de868023a3c79d5e3aca26a300d6e989,0db704d2403f55fbbb3e85cb08b50a1c58184b94,Li-Huai (Allan) Lin,qqaatw@gmail.com,Fri May 26 01:38:59 2023 +0800,1685065139.0,"[OpInfo] Add multi_head_attention_forward (#100153)

<!--
copilot:summary
-->
### <samp>ð¤ Generated by Copilot at 8f8d620</samp>

This pull request improves the testing of the `nn.functional.multi_head_attention_forward` function by adding it to the `OpInfo` framework, adjusting the tolerance and skipping criteria for some test cases, and restricting the dtype for the `MetaProgrammingSystem` tests. These changes aim to address the randomness and numerical precision issues of the function.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100153
Approved by: https://github.com/drisspg",118.0,0.0,"test/functorch/test_ops.py,test/functorch/test_vmap.py,test/test_mps.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,2,0.637732215,7.0,38650.0,4.0,349970.75,16248.0,36729.0,0.0,Feature Addition,0.0,1
pytorch,c691fc6dc711814a06107d4a9b763f34bff5afca,0dbf871d9ec424f1a7897af77bf93219d3be23bf,Luca Antiga,luca.antiga@orobix.com,Wed Jun 07 00:21:20 2017 +0200,1496794880.0,Have median reduce over all dims and return just the value when dim is not provided,66.0,0.0,"generic/THCTensorMathReduce.cu,generic/THCTensorMathReduce.h",2.0,1,1,0.113274303,13.0,414.0,1.0,137156.0,1089.0,12742.4346,0.0,,0.0,1
pytorch,c8a50a26d2247d00d5fb79aa04bae5a1764def0b,0dcb8755c8ba23922a3940e0d835cbb3db5944ad,Richard Zou,zou3519@gmail.com,Fri Jul 26 15:36:52 2019 -0700,1564155412.0,"Implement tensor.set_names_, tensor.names setter

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23172

Test Plan:
- [namedtensor ci]

gh-metadata: pytorch pytorch 23172 gh/zou3519/74/head

Imported from OSS

Differential Revision: D16494364

Pulled By: zou3519

fbshipit-source-id: 8d0e26b33346d4eadba30b2e76610f6d7be7c373",81.0,6.0,"aten/src/ATen/NamedTensorUtils.cpp,aten/src/ATen/NamedTensorUtils.h,aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/native/NamedTensor.cpp,aten/src/ATen/native/native_functions.yaml,test/test_namedtensor.py,test/test_torch.py,tools/autograd/gen_python_functions.py,torch/csrc/autograd/python_variable.cpp,torch/csrc/utils/python_arg_parser.h",11.0,12,4,2.945095441,41.0,23514.0,5.0,135892.0,10216.0,29384.33333,0.0,,0.0,1
pytorch,0259d9c8d3e881ebd312a5227b2cd013c0f64d0a,0dd2521d4c8415a802aae3e3be746a39c8dbab72,bddppq,bai@in.tum.de,Fri May 04 20:21:32 2018 -0700,1525465292.0,Fix ONNX export for AveragePool with count_include_pad=True (#7279),25.0,22.0,torch/onnx/symbolic.py,1.0,2,1,0,9.0,1035.0,1.0,41153.0,1072.0,2786.805292,0.0,Corrective,1.0,1
pytorch,d899385a3d02977b61ee0779c4e1f5d5cc266d65,0de2ea305acdc43de5e7f278604d62a605e0ede2,Adam Paszke,adam.paszke@gmail.com,Sun Feb 12 18:32:06 2017 +0100,1486924326.0,Support retain_variables in cuDNN RNN,39.0,2.0,"test/test_nn.py,torch/autograd/function.py,torch/nn/_functions/rnn.py",3.0,5,2,1.384140653,22.0,2263.0,3.0,24571.33333333333,466.0,4383.616645,0.0,,0.0,1
pytorch,63472bcf290aa618cb9a7a12308c4b020e358297,0dff2b5e35daf71531a580da35541715d62b532b,Tongzhou Wang,SsnL@users.noreply.github.com,Wed Apr 11 02:09:36 2018 -0400,1523412576.0,"[fft] [3 of 3] Implements backward of fft ifft rfft irfft (#5537)

* change irfft signal_sizes arg to be the last

* add docs for fft, ifft, rfft, irfft; update doc for stft

* fix typo in window function docs

* improve gradcheck error message

* implement backward of fft, ifft, rfft, irfft

* add grad tests for fft, ifft, rfft, irfft

* fix nits and typos from #6118

* address comments",615.0,45.0,"aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/SpectralOpsUtils.h,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_tensor_docs.py,torch/_torch_docs.py,torch/autograd/gradcheck.py,torch/functional.py",14.0,14,5,2.266705258,40.0,22860.0,5.0,365123.6428571429,862.0,2028.305292,0.0,Corrective,1.0,1
pytorch,db8d01b248ba4c6bb306be056646d3abab6980ec,0e44db8b0d23431c6db4c79af8b7c66527e2cc73,Thomas Viehmann,tv.code@beamnet.de,Mon Oct 08 01:53:17 2018 -0700,1538963597.0,"Add check for backend of arguments to bmm cpu (#12434)

Summary:
Fixes: #12406
Thank you, jcjohnson, for reporting.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12434

Differential Revision: D10235799

Pulled By: soumith

fbshipit-source-id: 44ee35010bac3791901f604095f5b4bc66b0e7f8",5.0,0.0,"aten/src/ATen/native/LinearAlgebra.cpp,test/test_torch.py",2.0,5,2,0.721928095,40.0,9696.0,2.0,740244.0,4488.0,13210.33333,0.0,Corrective,1.0,1
pytorch,90faf43151352ea5d4f0fa6f37cd2017bcf473b2,0e4f9a78725975bef7040dd17bf6ab17c7370d75,Lillian Johnson,lillianjohnson@fb.com,Thu Dec 03 03:52:39 2020 -0800,1606967559.0,"Refactored OpInfo testing to support custom SampleInputs, added addmm to op_db to test (#48627)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48627

Several changes to the OpInfo testing suite:
- Changed test_ops.py to support sample.inputs that are longer than a single element
- Changed OpInfo class to use custom sample_input generator functions, changed UnaryUfuncInfo to use new format
- Added mvp addmm op to operator database to test out sample.inputs with a length greater than a single element

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D25234178

Pulled By: Lilyjjo

fbshipit-source-id: cca2c60af7e6deb849a1cc3770c04ed88865016c",69.0,29.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.88864667,2.0,2342.0,1.0,3.0,7155.0,16184.0,0.0,Feature Addition,0.0,1
pytorch,410191c4175eaae141306cdb3c3c1c1e8a495225,0ea2fa15a391f09b1a6407f7b9c48c7b7499da47,gchanan,gregchanan@gmail.com,Mon Jun 04 20:48:57 2018 -0400,1528145337.0,"Replace most remaining usages of TensorUtils<T>::DataType. (#8124)

As in https://github.com/pytorch/pytorch/pull/8056, this doesn't work with a single TensorImpl type.
This replaces the usages of with a templatized parameter and static_asserts that the new and old are equal.

After this we can get rid of the old template parameter, but I want to ensure they are equivalent across all builds first.",334.0,324.0,"aten/src/THC/THCApply.cuh,aten/src/THC/THCReduce.cuh,aten/src/THC/THCReduceAll.cuh,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorIndex.cu,aten/src/THC/THCTensorMathReduce.cu,aten/src/THC/THCTensorMathReduce.cuh,aten/src/THC/THCTensorSort.cu,aten/src/THC/THCTensorTypeUtils.cuh,aten/src/THC/generic/THCTensorCopy.cu,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorMode.cu,aten/src/THC/generic/THCTensorScatterGather.cu,aten/src/THC/generic/THCTensorSort.cu,aten/src/THC/generic/THCTensorTopK.cu,aten/src/THCS/generic/THCSTensor.cu,aten/src/THCS/generic/THCSTensorMath.cu,aten/src/THCUNN/generic/FusedRNNKernel.cu",19.0,8,1,3.286040394,6.0,7220.0,7.0,2131719.6842105263,1242.0,3448.805292,0.0,,0.0,1
pytorch,a2f9c7d4e38c2fc0c9aea8e0fd3a7cbf7dff2988,0ec717c83097c4600e883a860d673226a08fcda4,Qi Zhou,qiz@fb.com,Wed Nov 04 07:31:24 2020 -0800,1604475084.0,"Support int32 indices and offsets in nn.EmbeddingBag (#46758)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/46758

It's in general helpful to support int32 indices and offsets, especially when such tensors are large and need to be transferred to accelerator backends. Since it may not be very useful to support the combination of int32 indices and int64 offsets, here we enforce that these two must have the same type.

Test Plan: unit tests

Reviewed By: ngimel

Differential Revision: D24470808

fbshipit-source-id: 94b8a1d0b7fc9fe3d128247aa042c04d7c227f0b",1011.0,890.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/Indexing.cu,caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc,caffe2/perfkernels/embedding_lookup_idx.cc,caffe2/perfkernels/embedding_lookup_idx.h,caffe2/perfkernels/embedding_lookup_idx_avx2.cc,caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup_idx.cc,caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup_idx.h,caffe2/perfkernels/hp_emblookup_codegen.py,test/test_nn.py,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/nn/functional.py,torch/nn/modules/sparse.py",21.0,11,4,3.352139092,48.0,66227.0,16.0,8826176.333333334,6459.0,14739.0,0.0,,0.0,1
pytorch,d39e519ce20e0b9d8f862c67df9394a93356024b,0eec332e14ca4de59cef1801cc25116437b26714,SsnL,SsnL@users.noreply.github.com,Fri Oct 06 21:59:01 2017 -0400,1507327141.0,assert reflection padding in range (#3008),42.0,6.0,"torch/lib/THCUNN/generic/SpatialReflectionPadding.cu,torch/lib/THCUNN/generic/TemporalReflectionPadding.cu,torch/lib/THNN/generic/SpatialReflectionPadding.c,torch/lib/THNN/generic/TemporalReflectionPadding.c,torch/nn/functional.py",5.0,7,1,2.231679995,38.0,2040.0,1.0,8547.0,1935.0,23753.35823,0.0,Feature Addition,0.0,1
pytorch,50eb1a389f5029bb36f3e2785e9afcbfc9e78517,0f0271e255dd9b17b48c01f19b375c15520b43c3,Michael Carilli,mcarilli@nvidia.com,Tue Mar 24 15:58:40 2020 -0700,1585065520.0,"[RELAND2] Eager autocasting, out-of-place ops only (with MSVC 2017 fix) (#35102)

Summary:
This is the second reland attempt for https://github.com/pytorch/pytorch/pull/32140.

The first reland attempt https://github.com/pytorch/pytorch/pull/35011 failed due a [small incompatible change](https://github.com/pytorch/pytorch/pull/35011#issuecomment-601754216) in recent master (`skipIfRocm` was removed from `test_data_parallel.py`).

The present PR restores skipIfRocm.

Description from first reland attempt https://github.com/pytorch/pytorch/pull/35011:

> https://github.com/pytorch/pytorch/pull/32140 was approved and merged, but [reverted](https://github.com/pytorch/pytorch/commit/d0577e19f09a32a68f0f3faed635dec72971b019) because it broke builds with versions of Visual Studio older than 15.8 that were not represented in public CI.  The build failures were caused by a [known VS bug](https://developercommunity.visualstudio.com/content/problem/27729/allow-function-with-internal-linkage-as-template-n.html), fixed in versions 15.8 and newer.
>
> The present PR reverts the revert (restoring https://github.com/pytorch/pytorch/pull/32140 's diffs) and adds a workaround to enable compilation with VS < 15.8.  The workaround isn't pretty, but it's guarded by macros such that it's only used when compiling with VS < 15.8.  All other builds compile with the same code/control flow as was merged in https://github.com/pytorch/pytorch/pull/32140.
>
> Original description of https://github.com/pytorch/pytorch/pull/32140:
> > Initial integration of eager autocasting, supporting out-of-place ops only for easier review.
> Relevant issue/RFC: https://github.com/pytorch/pytorch/issues/25081
>
> > In-place ops and ops with user-supplied out=... can certainly be supported as well (my initial WIP https://github.com/pytorch/pytorch/issues/29552 handled many) but require substantially more complex special casing in the autocasting backend and tests. Support for these ops (much of which has already been written) will be broken into later PRs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35102

Differential Revision: D20596918

Pulled By: ezyang

fbshipit-source-id: 60caa279bb0ce4a9bb0b28c1d585d42cf1cc7e50",1856.0,72.0,"aten/src/ATen/autocast_VS2017_helper.h,aten/src/ATen/autocast_mode.cpp,aten/src/ATen/autocast_mode.h,c10/core/DispatchKey.cpp,c10/core/DispatchKey.h,c10/core/impl/LocalDispatchKeySet.cpp,docs/source/amp.rst,docs/source/notes/amp_examples.rst,docs/source/notes/extending.rst,test/distributed/test_data_parallel.py,test/test_cuda.py,torch/_overrides.py,torch/csrc/autograd/init.cpp,torch/cuda/amp/__init__.py,torch/cuda/amp/autocast_mode.py,torch/cuda/amp/grad_scaler.py,torch/testing/_internal/autocast_test_lists.py",17.0,18,5,2.997227236,41.0,5813.0,3.0,360324.2352941177,391.0,1117.5,0.0,Corrective,1.0,1
pytorch,42286432b29f9712f70f3059150be9bc9633e33c,0f1332b09eac6c1a8a9f518ce202dbec22f01041,Richard Zou,zou3519@users.noreply.github.com,Fri Apr 29 18:07:16 2022 -0400,1651255636.0,[functorch] Fix a bunch of linalg coverage issuez (pytorch/functorch#765),66.0,33.0,"functorch/codegen/gen_vmap_plumbing.py,functorch/functorch/csrc/BatchRulesLinearAlgebra.cpp,functorch/functorch/csrc/VmapGeneratedPlumbing.h,functorch/test/test_ops.py",4.0,5,1,1.670550024,1.0,7772.0,3.0,2.75,1028.0,1412.0,0.0,Corrective,1.0,1
pytorch,d860313903f21c26d10c2bdd3a9482466255b001,0f646b1d154fdd7a567b29598d79298bbca24841,Bin Bao,binbao@fb.com,Sat Sep 16 16:46:26 2023 +0000,1694882786.0,"[inductor] Add a C shim layer for libtorch (#109391)

Summary:
This PR adds a limited C shim layer for libtorch. The ultimate goal is to ban any direct reference to aten/c10 data structures or functions, to avoid ABI breakage by providing stable C interfaces.

To make the review and landing easier, we broke the changes into several steps. In this PR (a combination of https://github.com/pytorch/pytorch/pull/109022 and https://github.com/pytorch/pytorch/pull/109351), we add C interfaces for certain libtorch functions and modify the wrapper codegen to generate calls to those interfaces. There are a few other items to be addressed in future PRs:

* The AOTInductor runtime interface still takes lists of aten tensors as input and output
* The interaction with ProxyExecutor (general fallback support) needs to move away from aten tensor
* Remove all references to aten/c10 headers in the AOTInductor-generated code

Differential Revision: D49302669

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109391
Approved by: https://github.com/chenyang78",933.0,166.0,"build_variables.bzl,setup.py,test/inductor/test_aot_inductor.py,torch/_inductor/codecache.py,torch/_inductor/codegen/aot_runtime/interface.cpp,torch/_inductor/codegen/wrapper.py,torch/_inductor/config.py,torch/_inductor/ir.py,torch/csrc/inductor/aot_runtime/interface.h,torch/csrc/inductor/aot_runtime/model.h,torch/csrc/inductor/aot_runtime/model_container.h,torch/csrc/inductor/aoti_torch/c/shim.h,torch/csrc/inductor/aoti_torch/shim_common.cpp,torch/csrc/inductor/aoti_torch/shim_cuda.cpp,torch/csrc/inductor/aoti_torch/utils.h",15.0,11,2,2.531535577,45.0,14687.0,7.0,155819.0,19774.0,45111.5,0.0,Feature Addition,0.0,1
pytorch,459dadf04dad8fc9e23a33a4f97f790fc7395db9,0f86f64398b20bf6e9bdd5187737949fee044e45,gchanan,gregchanan@gmail.com,Wed Feb 28 19:41:57 2018 -0500,1519846917.0,"Add support for device python arguments with constructors. (#5384)

* Add support for device python arguments with constructors.

* Fix flake8.

* Simplify device handling.

* Dont use torch._C._VariableFunctions.

* Handle default values for functions that have tensor args (e.g. ones_like).",57.0,27.0,"test/test_autograd.py,test/test_torch.py,tools/autograd/gen_python_functions.py",3.0,3,2,1.172881034,39.0,9084.0,3.0,4279.333333333333,568.0,1729.905869,0.0,Corrective,1.0,1
pytorch,ce92c1cfe99bb03505dd5c3c27f7e7be1077cfc1,0fcdf936e703db544f5285dfa5fa2cb2e267f8a4,soulitzer,soulitzer@gmail.com,Mon Jul 25 15:47:44 2022 -0400,1658764064.0,"Skip tests that don't call gradcheck in slow gradcheck CI (#82117)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82117
Approved by: https://github.com/kit1980, https://github.com/albanD",11.0,4.0,"test/quantization/core/test_quantized_op.py,test/quantization/jit/test_quantize_jit.py,test/test_cpp_extensions_jit.py,test/test_fx.py",4.0,4,1,1.723231429,4.0,15035.0,4.0,3480828.5,5731.0,13352.5,0.0,,0.0,1
pytorch,04e896a4b4fe6ac240cbecf999d7c1419db751e3,0fecec14b8546e54f5b1330c87324158336c3a95,Soumith Chintala,soumith@fb.com,Sat Nov 26 06:45:51 2016 -0500,1480142751.0,fixing bug in indexing when given float indices,27.0,1.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.749595257,16.0,3365.0,2.0,201977.5,229.0,1735.748261,0.0,Corrective,1.0,1
pytorch,43613b4236d2692c31152b63de773053a75b699b,0ff0fea42bf9721b87e01fe15445dfd3ea5f2093,James Reed,jamesreed@fb.com,Tue Aug 11 20:28:39 2020 -0700,1597177719.0,"[FX] fix lint (#42866)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42866

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D23056813

Pulled By: jamesr66a

fbshipit-source-id: d30cdffe6f0465223354dec00f15658eb0b08363",7.0,1.0,"torch/fx/__init__.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/fx/node.py,torch/fx/symbolic_trace.py",5.0,2,1,2,1.0,566.0,1.0,14379.0,4230.0,9900.0,0.0,Corrective,1.0,1
pytorch,9b875e1256d679d96615bf4bfdde63d3d813bab4,0ff1696c753b4be1f16c287f390097ef44a2d679,Alban Desmaison,albandes@fb.com,Thu Nov 07 16:32:51 2019 -0800,1573144371.0,"add pybind version of HANDLE_TH_ERRORS

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/26614

Test Plan: Imported from OSS

Differential Revision: D18249634

Pulled By: albanD

fbshipit-source-id: 25503f368926e0f3633c5af0f222c9bb4729f342",216.0,23.0,"test/test_cpp_extensions.py,test/test_torch.py,torch/csrc/Exceptions.h,torch/csrc/README.md,torch/utils/cpp_extension.py,ubsan.supp",6.0,4,2,2.055768115,42.0,16708.0,5.0,8685009.5,12909.0,35701.83333,0.0,Feature Addition,0.0,1
pytorch,7a06dbb87e8d602d5a066f38aafb95c9ed360f6d,103e70ccc5e61581a34959f605e001d97691284a,Soumith Chintala,soumith@gmail.com,Wed Nov 02 14:25:58 2016 -0400,1478096758.0,adding cuda types for tensor methods (#194),71.0,70.0,"test/test_cuda.py,torch/csrc/generic/TensorMethods.cwrap",2.0,4,2,0.997059057,11.0,4396.0,1.0,35537.0,56.0,65.89007937,0.0,Feature Addition,0.0,1
pytorch,3397d41b8af2824495faaefc649c3c7e47f03bfb,1054ab213dc4bac34155ecebf1321f522e324f89,Rohan Varma,rvarm1@fb.com,Tue Oct 15 18:07:31 2019 -0700,1571162851.0,"improve error message for scatter in processGroupGloo (#27458)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27458

Same as the previous diff - improve error message by passing back the
size discrepancy.
ghstack-source-id: 91864213

Test Plan: `python test/test_c10d.py`

Differential Revision: D17785296

fbshipit-source-id: f939b8091aede768ea215f69df2c83e438c430cf",19.0,14.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.999337504,3.0,5491.0,2.0,299979.0,12290.0,34345.33333,0.0,Perfective,0.0,1
pytorch,964732fa8dc05794668cf3ae4f5ff069ee656abb,1065e7cd247e9e3a6b9133eae67daf2f76e54c18,Xiang Gao,qasdfgtyuiop@gmail.com,Tue Jan 15 16:24:27 2019 -0800,1547569467.0,"Add `itertools.{prod, combinations, combinations_with_replacement}` like op to pytorch (#9393)

Summary:
closes https://github.com/pytorch/pytorch/issues/7580
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9393

Differential Revision: D13659628

Pulled By: zou3519

fbshipit-source-id: 3a233befa785709395a793ba8833413be394a6fd",202.0,1.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/Itertools.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,torch/_torch_docs.py,torch/functional.py",6.0,7,3,2.15196576,42.0,21686.0,4.0,516037.0,6463.0,20004.83333,0.0,Feature Addition,0.0,1
pytorch,f60ae085e699e8535e9a519c8583ce890ce1a9d0,108936169c9f972ef944c7c6642854494d85f35d,Soumith Chintala,soumith@fb.com,Tue Jan 03 02:36:08 2017 -0500,1483410968.0,"implement more torch.* docs, remove zero, cauchy, log_normal from torch.* docs as they are not stateless",476.0,17.0,"docs/source/torch.rst,torch/docs.py",2.0,3,2,0.053542259,8.0,2020.0,2.0,0.0,296.0,6379.224559,0.0,Non Functional,0.0,1
pytorch,08041c526449319b6a7707a9545ab51e856cf705,10910758f462a7b673b60c4755c553ba2abb462d,Jason Ansel,jansel@fb.com,Tue Jan 31 05:03:07 2023 -0800,1675141387.0,"Make dynamo tests work under pytest (#93251)

This now runs without error:
```
pytest test/dynamo
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93251
Approved by: https://github.com/ezyang, https://github.com/voznesenskym, https://github.com/mlazos",25.0,15.0,"test/dynamo/test_dynamic_shapes.py,test/dynamo/test_optimizers.py,test/dynamo/test_repros.py,test/inductor/test_torchinductor.py,torch/_dynamo/testing.py",5.0,5,2,2.139448951,1.0,9609.0,5.0,833516.2,11885.0,27703.5,0.0,,0.0,1
pytorch,ed19580dc4adb4e4e44e706473c6bde2b8d689e6,10b1254edd9bcb4d3ab6a47f2bab31edef6485d0,Ojas Ahuja,ojas.ahuja2@gmail.com,Wed Aug 07 16:03:17 2019 -0700,1565193797.0,"fix crash on torch.Tensor.repeat() for 0 repeats (#23766)

Summary:
This PR fixes https://github.com/pytorch/pytorch/issues/23603
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23766

Differential Revision: D16644866

Pulled By: soumith

fbshipit-source-id: ee7d368afdfe874133d0bd90f4d03a191ee22b13",13.0,1.0,"aten/src/ATen/native/TensorShape.cpp,test/test_torch.py",2.0,5,2,0.863120569,40.0,13623.0,2.0,403271.0,10441.0,29772.83333,0.0,Corrective,1.0,1
pytorch,f86460a352375b7156e21ce9a5f656697abd0ab8,10b2a2450851885463eb8cf8e016ccd0397b44f4,Peter Bell,peterbell10@live.co.uk,Wed Jul 07 22:13:11 2021 -0700,1625695991.0,"Migrate log_sigmoid (forward and backward) to ATen (CUDA) (#60881)

Summary:
Fixes gh-24591, fixes gh-24590, closes gh-39642

Benchmarks were run with nvprof using contiguous inputs; they show improvement across the board.

#### Forward benchmarks

| Num Elements | Master (us) | This PR (us) |
|:------------:|:-----------:|:------------:|
|     10^4     |    2.5840   |    2.5230    |
|     10^5     |    4.6410   |    3.9280    |
|     10^6     |    33.772   |    23.025    |
|     10^7     |    299.67   |    206.35    |
|     10^8     |    3001.9   |    2052.8    |

#### Backward benchmarks

| Num Elements | Master (us) | This PR (us) |
|:------------:|:-----------:|:------------:|
|     10^4     |    2.7750   |    2.7080    |
|     10^5     |    5.2430   |    3.9010    |
|     10^6     |    46.198   |    32.878    |
|     10^7     |    447.18   |    296.18    |
|     10^8     |    4393.2   |    2938.0    |

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60881

Reviewed By: mruberry

Differential Revision: D29589455

Pulled By: ngimel

fbshipit-source-id: 70cd5db244bf6292e9ca367462640530a1d85f7d",129.0,258.0,"BUILD.bazel,aten/src/ATen/LegacyTHFunctionsCUDA.h,aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/Activation.h,aten/src/ATen/native/cpu/Activation.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/native_functions.yaml,aten/src/THCUNN/CMakeLists.txt,aten/src/THCUNN/LogSigmoid.cu,aten/src/THCUNN/generic/LogSigmoid.cu,aten/src/THCUNN/generic/THCUNN.h,torch/testing/_internal/common_methods_invocations.py",13.0,12,2,2.80332383,13.0,24778.0,6.0,9270487.461538462,13641.0,30864.5,0.0,Corrective,1.0,1
pytorch,6eb3969ac7f6ea1065b75ba15b0e7805573bf29c,10c60b601a63710b17e2d14e8384c7c9bb51f9ff,Iurii Zdebskyi,iuriiz@fb.com,Wed Jul 10 16:05:25 2019 -0700,1562774725.0,"Added Bfloat16 tensor for cpu with very limited support (#21860)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21860
ghimport-source-id: 5290755b63033cdfdeb911a4ecf4aa282b3db02d

Test Plan: Imported from OSS

Differential Revision: D15856091

Pulled By: izdeby

fbshipit-source-id: 54e7e17be1b5c5a2e80a41feaeaeba75dbb8108f",199.0,43.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/Dispatch.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Scalar.cpp,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native_parse.py,aten/src/ATen/preprocess_declarations.py,aten/src/TH/THTensor.h,aten/src/TH/THTensorEvenMoreMath.cpp,aten/src/TH/THTensorFill.cpp,aten/src/TH/THVector.cpp,aten/src/TH/THVector.h,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.h,c10/core/ScalarType.h,c10/util/BFloat16-inl.h,c10/util/BFloat16.h,test/common_utils.py,test/test_torch.py,torch/_tensor_str.py,torch/csrc/utils/python_scalars.h,torch/csrc/utils/tensor_types.cpp,torch/testing/__init__.py",25.0,15,4,3.699476779,41.0,21690.0,16.0,3068752.92,9851.0,28593.83333,0.0,Feature Addition,0.0,1
pytorch,ee08120b467f99cf42a41635a4cb04f731d6a2f0,10d24d8f841fba220dc0ab448a94dc1e4150a810,Sam Gross,colesbury@gmail.com,Mon Nov 20 18:58:12 2017 -0500,1511204292.0,"Add Tensor.slice() (#3750)

The slice function is very similar to narrow, except that it takes an
optional ""step"" argument. Unlike narrow, the arguments use the same
conventions as Python indexing: negative values wrap around and start
and stop are clamped to the size of the Tensor.",98.0,75.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/lib/THD/master_worker/worker/dispatch/Tensor.cpp",8.0,13,4,2.157583639,39.0,14643.0,5.0,321461.875,344.0,966.9058694,0.0,Feature Addition,0.0,1
pytorch,626e410e1dedcdb9d5a410a8827cc7a8a9fbcce1,10dd25dcd18457a53e69eb319f48749a49a48430,Iurii Zdebskyi,iuriiz@devfair004.maas,Mon Sep 07 17:28:26 2020 -0700,1599499706.0,"Add binary ops for _foreach APIs (#42536)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42536

[First PR: Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar)](https://github.com/pytorch/pytorch/pull/41554).

**Motivation**
[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)
Current PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.
As an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).
In order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and itâs FusedAdam optimizer.

**Current API restrictions**
- List can't be empty (will fixed in upcoming PRs).
- All tensors in the list must have the same dtype, device and size.

**Broadcasting**
At this point we don't support broadcasting.

**What is 'Fast' and 'Slow' route**
In particular cases, we cant process an op with a fast list CUDA kernel. Still, we can do with a regular for-loop where the op will be applied to each tensor individually through the dispatch mechanisms. There are a few checks that decide whether the op will be performed via a 'fast' or 'slow' path.
To go the fast route,
- All tensors must have strided layout
- All tensors must be dense and not have overlapping memory
- The resulting tensor type must be the same.

----------------
**In this PR**
Adding APIs:
```
torch._foreach_sub(TensorList tl1, TensorList tl2)
torch._foreach_sub_(TensorList self, TensorList tl2)
torch._foreach_mul(TensorList tl1, TensorList tl2)
torch._foreach_mul_(TensorList self, TensorList tl2)
torch._foreach_div(TensorList tl1, TensorList tl2)
torch._foreach_div_(TensorList self, TensorList tl2)

torch._foreach_sub(TensorList tl1, Scalar scalar)
torch._foreach_sub_(TensorList self, Scalar scalar)
torch._foreach_mul(TensorList tl1, Scalar scalar)
torch._foreach_mul_(TensorList self, Scalar scalar)
torch._foreach_div(TensorList tl1, Scalar scalar)
torch._foreach_div(TensorList self, Scalar scalar)
```

**Tests**
Tested via unit tests

**TODO**
1. Properly handle empty lists
2. Properly handle bool tensors

**Plan for the next PRs**
1. APIs
- Unary Ops for list
- Pointwise Ops

2. Complete tasks from TODO
3. Rewrite PyTorch optimizers to use for-each operators for performance gains.

Test Plan: Imported from OSS

Reviewed By: cpuhrsch

Differential Revision: D23331891

Pulled By: izdeby

fbshipit-source-id: 18c5937287e33e825b2e391e41864dd64e226f19",363.0,158.0,"aten/src/ATen/native/ForeachOpsKernels.cpp,aten/src/ATen/native/cuda/ForeachBinaryOpList.cu,aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu,aten/src/ATen/native/cuda/ForeachFunctors.cuh,aten/src/ATen/native/cuda/ForeachTensorAddList.cu,aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu,aten/src/ATen/native/native_functions.yaml,test/test_foreach.py,tools/codegen/model.py",9.0,8,3,2.992884542,12.0,8971.0,4.0,348480.28571428574,4908.0,11353.5,0.0,Corrective,1.0,1
pytorch,dfd5d8d0fe8540361b9d4e9414920046fe43f4ae,10e23943b348e0f3b8f7709beadf3bc1541c763c,Sam Gross,colesbury@gmail.com,Tue Jul 11 22:23:35 2017 -0400,1499811815.0,Fix missing _forward_pre_hooks in serialized modules (#2057),40.0,43.0,".gitignore,test/common.py,test/test_nn.py,test/test_torch.py,test/test_utils.py,torch/nn/_functions/thnn/upsampling.py,torch/nn/modules/module.py,torch/nn/modules/rnn.py,torch/nn/parallel/distributed.py",9.0,7,2,2.62786476,35.0,10200.0,1.0,28211.0,252.0,1102.875885,0.0,Corrective,1.0,1
pytorch,0476a2346b9f364e98971002c96269f64568a785,11056528d19da8088d01ec1147edc94b19ad83db,Sam Gross,colesbury@gmail.com,Wed Mar 14 02:22:20 2018 -0400,1520994140.0,"Fixes Variable::data() on UndefinedTensor (#5756)

The save_mean and save_std are undefined if training is false.
Previously, we unpacked them even though we did not use them in the
computation.

We also don't need to re-pack the mean/variance variables.",6.0,21.0,tools/autograd/templates/Functions.cpp,1.0,3,1,0,10.0,1156.0,1.0,383367.0,487.0,2377.0,0.0,Corrective,1.0,1
pytorch,dc17fb68e4482b1afdf68a4ecf8a6968159d7ad9,112728cbe93c943de0b8338aafac21f5650315bf,Alykhan Tejani,alykhan.tejani@gmail.com,Mon Jul 24 22:16:27 2017 -1000,1500934587.0,"reformulate bce_with_logits to not use abs (#2195)

* reformulate bce_with_logits to not use abs

* flake8 fixes",10.0,2.0,"test/test_nn.py,torch/nn/functional.py",2.0,3,2,0.979868757,32.0,4983.0,1.0,267260.0,1237.0,14735.52883,0.0,Corrective,1.0,1
pytorch,edce6b138d92117088fcffe065039df7cac1bddb,11598da229a58ea9e36a6fcde2cf6ab6c73c14f4,James Reed,jamesreed@fb.com,Tue Dec 22 05:39:40 2020 -0800,1608615580.0,"[FX] Fix python code having spurious newlines from placeholders (#49720)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49720

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D25675825

Pulled By: jamesr66a

fbshipit-source-id: a9028acad9c8feb877fff5cd09aedabed52a3f4b",3.0,1.0,torch/fx/graph.py,1.0,2,1,0,1.0,850.0,1.0,300005.0,7707.0,17364.5,0.0,Corrective,1.0,1
pytorch,799ebce3aa445601fd8fbe5b1895b8d13dc6a110,1181628d853844c8c1e2d4d2d3251470e2e2596c,Nikita Shulga,nshulga@fb.com,Fri Nov 12 06:00:13 2021 -0800,1636696813.0,"BE: Use TORCH_CHECK instead of explicit c10::Error (#68187)

Summary:
`if (cond) { raise c10::error("""", msg)}` is identical to `TORCH_CHECK(!cond, msg);`, but with better attribution

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68187

Reviewed By: xuzhao9

Differential Revision: D32360956

Pulled By: malfet

fbshipit-source-id: e554b99926d7ad0c79a1cd54d35f47339fa2429d",1.0,3.0,aten/src/ATen/core/ivalue.h,1.0,4,1,0,2.0,1252.0,1.0,823516.0,17048.0,40085.5,0.0,,0.0,1
pytorch,4a1e099974f510a10d29b8e19b91edbe7a47f065,11852e5c2211a42adcc59e44fa5d74bc19bfac82,Adam Paszke,adam.paszke@gmail.com,Tue Sep 06 17:35:34 2016 -0700,1473183334.0,"Add new flags for THMapAllocator

* TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
  mapping (and steals it)
* TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
  until the mapping is freed
* TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
  mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek,
which alters the fd state).",108.0,35.0,"THAllocator.c,THAllocator.h",2.0,0,0,0.251198817,8.0,463.0,1.0,503932.0,147.0,1766.679257,0.0,Feature Addition,0.0,1
pytorch,1a299d8f1b040339860fb83e959e969af3fd9dc6,1188d89a1dd2c52a1d515f76accad472ced2f3bc,Peter Bell,peterbell10@live.co.uk,Tue Dec 14 19:08:40 2021 -0800,1639508920.0,"TestMathBits: Call functions with original sample input values (#68947)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68947

`_test_math_view` currently calls the operator with different values
than those specified in the `SampleInput`. This is undesirable as it
could break mathematical properties required by the operator. Instead,
this calls `math_op_view(math_op_physical(sample.input))` to get a
view that represents the same value as the original input.

`test_neg_view` already did this by returning `torch._neg_view(-x)`
from `math_op_view` but this moves the handling into `_test_math_view`
to make it apply to all view op tests.

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D33064327

Pulled By: anjali411

fbshipit-source-id: 4d87e0c04fc39b95f8dc30dcabda0d554d16a1d8",7.0,24.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.458685816,2.0,16634.0,2.0,109271.0,17723.0,41870.5,0.0,,0.0,1
pytorch,f9b3dcba0d0ceefe08d3562f5ddee8b68e82aa81,119b3eccdaa4fcc32d9b553776e7d5043f7e4cc2,anjali411,chourdiaanjali123@gmail.com,Fri Apr 16 16:59:54 2021 -0700,1618592394.0,"Revert ""Revert D27598681: Add OpInfo tests for torch.addbmm"" (#55908)

Summary:
This reverts commit fd450ff1b93e4c498e7326cb35c7c26760c5ddbf.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55908

Reviewed By: agolynski

Differential Revision: D27800571

Pulled By: anjali411

fbshipit-source-id: f04144afe7768872acb3fc2f5f242bb0093abc5e",46.0,10.0,"test/test_autograd.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.222284831,42.0,13890.0,2.0,71558.0,10910.0,24070.5,0.0,Feature Addition,0.0,1
pytorch,9a858aba5f6b1f5188bf986f7cc0a9187aa490d1,11b30653236c1f401ee9cf0966482c06249da081,Edward Yang,ezyang@fb.com,Fri Dec 06 15:21:19 2019 -0800,1575645679.0,"Run method_tests on CUDA. (#30821)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30821

While investigating while our tests didn't catch #30704 I noticed that none
of our tests in method_tests() were being run on CUDA.  This diff moves
those tests into the new device-generic test framework so that we also get
CUDA coverage.  For expediency, I blacklisted all tests which didn't work
on CUDA (rather than fix them); that's something we can leave for future PRs.
This is done by way of a new expectedFailure gadget.

Note that all occurences of skipIfNoLapack needed to be replaced with
skipCPUIfNoLapack.

I punted for test_jit; it's possible those tests should also run CUDA but a JIT
expert should take a look here.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Differential Revision: D18840089

Pulled By: ezyang

fbshipit-source-id: 66b613b5024c91d3e391c456bb642be7e73d4785",144.0,90.0,"test/common_device_type.py,test/common_methods_invocations.py,test/test_autograd.py,test/test_jit.py",4.0,1,1,1.312512719,43.0,23155.0,4.0,1892740.25,13685.0,37381.33333,0.0,Corrective,1.0,1
pytorch,8d676a6e8e551d5ecd09d8ffc84987ef6bb1593f,12116aee6852df2b040255b8fcc7deb52b897792,Xinya Zhang,Xinya.Zhang@amd.com,Thu Mar 28 00:27:38 2024 +0000,1711585658.0,"Add Flash Attention support on ROCM (#121561)

This patch addresses the major limitations in our previous [PR #115981](https://github.com/pytorch/pytorch/pull/115981) through the new dedicated repository [AOTriton](https://github.com/ROCm/aotriton)

- [x] Only supports MI200 series GPU (i.e., `gcnArchName == gfx90a:sramecc+:xnack-`).
    * MI300X is supported. More architectures will be added once Triton support them.
- [x] Only supports power of two sequence lengths.
    * Now it support arbitrary sequence length
- [ ] No support for varlen APIs.
    * varlen API will be supported in future release of AOTriton
- [x] Only support head dimension 16,32,64,128.
    * Now it support arbitrary head dimension <= 256
- [x] Performance is still being optimized.
    * Kernel is selected according to autotune information from Triton.

Other improvements from AOTriton include
* Allow more flexible Tensor storage layout
* More flexible API

This is a more extensive fix to #112997

Pull Request resolved: https://github.com/pytorch/pytorch/pull/121561
Approved by: https://github.com/huydhn",264.0,326.0,"CMakeLists.txt,aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip,caffe2/CMakeLists.txt,cmake/Dependencies.cmake,cmake/External/aotriton.cmake,cmake/External/oort.cmake,test/test_transformers.py,torch/testing/_internal/common_cuda.py",9.0,15,5,1.431353923,73.0,10619.0,4.0,569055.6666666666,26741.0,63561.0,0.0,Corrective,1.0,1
pytorch,5a1a9dc354e4098a96cb03adce3b3460809ca23a,1250032c2eb0cb55bcd376b499982bcc9ab231f8,Oguz Ulgen,oulgen@meta.com,Sun Oct 29 06:19:05 2023 -0700,1698560345.0,"[Inductor] Add triton.autotune support for user defined triton kernels with complex grids (#112290)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/112290
Approved by: https://github.com/jansel",177.0,37.0,"test/dynamo/test_functions.py,torch/_dynamo/variables/functions.py,torch/_higher_order_ops/triton_kernel_wrap.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py,torch/_inductor/triton_heuristics.py",6.0,8,2,1.842984421,2.0,13837.0,3.0,239759.16666666663,21296.0,48581.5,0.0,Feature Addition,0.0,1
pytorch,b0055f62297d3a4da7d6fc3d97e35500e0167522,1259a0648bfc8fbf210551016fc610e91ac0bd3f,Adam Paszke,adam.paszke@gmail.com,Sun Jan 15 16:59:02 2017 +0100,1484499542.0,Make nn containers copyable,24.0,0.0,"test/test_nn.py,torch/nn/backends/thnn.py",2.0,4,2,0.870864469,21.0,1557.0,2.0,118826.5,332.0,4815.476424,0.0,,0.0,1
pytorch,b07358b329974dc528b3acc1e3d7af2ab22c1ef8,126a1cc398de64af4139f0322ff17b3b8c9d2411,Sam Gross,sgross@fb.com,Fri Dec 23 21:28:04 2016 -0800,1482528484.0,Add Sphinx docs,7451.0,6248.0,"docs-old/docutils/doc2md.py,docs-old/docutils/gendocs.sh,docs-old/docutils/torch-cwrap-gen.py,docs-old/nn.md,docs-old/nn_activation.md,docs-old/nn_container.md,docs-old/nn_convolution.md,docs-old/nn_core.md,docs-old/nn_dropout.md,docs-old/nn_linear.md,docs-old/nn_loss.md,docs-old/nn_normalization.md,docs-old/nn_pooling.md,docs-old/nn_recurrent.md,docs-old/nn_sparse.md,docs-old/optim.md,docs-old/tensor.md,docs-old/tensor_ref.md,docs-old/torch.md,docs/Makefile,docs/docutils/doc2md.py,docs/docutils/gendocs.sh,docs/docutils/torch-cwrap-gen.py,docs/make.bat,docs/nn.md,docs/nn_activation.md,docs/nn_container.md,docs/nn_convolution.md,docs/nn_core.md,docs/nn_dropout.md,docs/nn_linear.md,docs/nn_loss.md,docs/nn_normalization.md,docs/nn_pooling.md,docs/nn_recurrent.md,docs/nn_sparse.md,docs/optim.md,docs/requirements.txt,docs/source/autograd.rst,docs/source/conf.py,docs/source/cuda.rst,docs/source/data.rst,docs/source/index.rst,docs/source/legacy.rst,docs/source/multiprocessing.rst,docs/source/nn.rst,docs/source/optim.rst,docs/source/tensors.rst,docs/source/torch.rst,docs/tensor.md,docs/tensor_ref.md,docs/torch.md,torch/__init__.py,torch/csrc/Module.cpp,torch/csrc/utils.h,torch/docs.py,torch/optim/__init__.py,torch/optim/sgd.py",104.0,8,3,4.537039114,18.0,7572.0,1.0,6673.0,338.0,2469.012243,0.0,Feature Addition,0.0,1
pytorch,d9b25b1a2a03a15a1afd376db654453c53fb6a42,126fd93c21417b2e62bb3e4bcc14e7e8522c1019,samdow,samdow@fb.com,Mon May 09 23:06:41 2022 -0400,1652137601.0,[functorch] windows being dumb,1.0,1.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1289.0,1.0,0.0,1056.0,1446.0,0.0,,0.0,1
pytorch,820143f4af0128dad56d6a610354e8a79510d877,1290e586fbc3d6266423f3417723d6620267054b,Sam Gross,colesbury@gmail.com,Tue Sep 12 15:36:01 2017 -0400,1505230561.0,"Use at::Tensor based autograd Variable (#2676)

Variable is now a subclass of at::Tensor backed by a VariableImpl* pImpl. The implementation of the ATen functions is defined in the auto-generated VariableType.h/cpp file.

Currently, only functions which fall through to the base type, such as sizes() and isCuda() are implemented. Differentiable ops like add() and mul() will be added in a subsequent PR.",1221.0,555.0,".gitignore,setup.py,test/test_autograd.py,tools/autograd/__init__.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/Functions.h,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/function_hook.h,torch/csrc/autograd/functions/accumulate_grad.cpp,torch/csrc/autograd/functions/accumulate_grad.h,torch/csrc/autograd/functions/basic_ops.cpp,torch/csrc/autograd/functions/basic_ops.h,torch/csrc/autograd/functions/batch_normalization.cpp,torch/csrc/autograd/functions/batch_normalization.h,torch/csrc/autograd/functions/convolution.cpp,torch/csrc/autograd/functions/convolution.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/jit_closure.cpp,torch/csrc/autograd/functions/special.cpp,torch/csrc/autograd/functions/special.h,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/functions/utils.cpp,torch/csrc/autograd/input_buffer.cpp,torch/csrc/autograd/input_buffer.h,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_hook.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/autograd/saved_variable.cpp,torch/csrc/autograd/saved_variable.h,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/autograd/variable_version.h,torch/csrc/jit/python_tracer.cpp,torch/csrc/jit/tracer.cpp,torch/csrc/jit/tracer.h,torch/csrc/jit/tracer_state.h",48.0,9,3,4.441070075,41.0,9914.0,3.0,358161.6666666667,281.0,906.3663601,0.0,Feature Addition,0.0,1
pytorch,fa6e5c5bffde9149bdc5ff66cc6477e4cc972206,12bed8dc0d8e08ff4c68cce77504474c8c78e5f0,Adam Paszke,adam.paszke@gmail.com,Fri Aug 12 14:46:46 2016 -0700,1471013206.0,Add CUDA device selection,214.0,18.0,"setup.py,test/test_cuda.py,tools/cwrap/plugins/AutoGPU.py,torch/Tensor.py,torch/csrc/Module.cpp,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Module.h,torch/csrc/cuda/Tensor.cpp,torch/csrc/generic/TensorMethods.cwrap,torch/cuda/__init__.py",10.0,9,3,2.976687617,5.0,4973.0,4.0,371351.4444444445,89.0,1156.433333,0.0,Feature Addition,0.0,1
pytorch,f595467e5c6569d4a457033d7ee46c7b9b1d28b1,12cb26509a30077f4bd11972dea9baa03c8d943f,Huy Do,huydhn@gmail.com,Fri Jul 22 02:19:50 2022 +0000,1658456390.0,"Apply ufmt to torch internal (#81643)

This is a big bang PR, merge conflicts are probably expected and will be addressed at merge.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81643
Approved by: https://github.com/ezyang",8343.0,4469.0,".lintrunner.toml,torch/_VF.py,torch/__config__.py,torch/__future__.py,torch/_appdirs.py,torch/_classes.py,torch/_decomp/__init__.py,torch/_decomp/decompositions.py,torch/_deploy.py,torch/_jit_internal.py,torch/_lazy/__init__.py,torch/_lazy/computation.py,torch/_lazy/config.py,torch/_lazy/debug.py,torch/_lazy/extract_compiled_graph.py,torch/_lazy/ir_cache.py,torch/_lazy/metrics.py,torch/_lazy/ts_backend.py,torch/_linalg_utils.py,torch/_lobpcg.py,torch/_lowrank.py,torch/_masked/__init__.py,torch/_meta_registrations.py,torch/_namedtensor_internals.py,torch/_ops.py,torch/_prims/__init__.py,torch/_prims/context.py,torch/_prims/executor.py,torch/_prims/nvfuser_executor.py,torch/_python_dispatcher.py,torch/_refs/__init__.py,torch/_refs/fft.py,torch/_refs/linalg/__init__.py,torch/_refs/nn/functional/__init__.py,torch/_refs/special/__init__.py,torch/_six.py,torch/_sources.py,torch/_storage_docs.py,torch/_subclasses/__init__.py,torch/_subclasses/fake_tensor.py,torch/_subclasses/meta_utils.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/_utils.py,torch/_utils_internal.py,torch/_vmap_internals.py",48.0,11,1,2.830723225,42.0,37990.0,32.0,7713952.791666667,5613.0,13139.0,0.0,Feature Addition,0.0,1
pytorch,08017f45988ff05e72a51624ab6c3efb203ca869,12cbd6975a0ac861f210592bb653b97dfe7009b2,Negin Raoof,neginmr@utexas.edu,Fri Feb 19 18:52:54 2021 -0800,1613760774.0,"[ONNX] Fix for sequence of mutations in blocks (#51577) (#52347)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52347

Fixes consecutive mutations in a tensor inside blocks.
Also, support append and pop in blocks.

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D26490328

Pulled By: SplitInfinity

fbshipit-source-id: f0cdc706d2793e1f4eb0503d3e0f63f4127ea47a",282.0,116.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp",3.0,7,2,1.158902451,3.0,8692.0,3.0,860309.0,9050.0,20275.0,0.0,Corrective,1.0,1
pytorch,37e05485d9fc30ba7dd3cd4d2759ce1f27011cb2,12efd53dbaffddd265f315a9d33249a27f2a6edc,Sergey Zagoruyko,zagoruyko2@gmail.com,Wed Mar 01 18:39:44 2017 +0100,1488393584.0,ConstantPad2d and F.pad (#856),118.0,1.0,"test/test_nn.py,torch/nn/_functions/padding.py,torch/nn/functional.py",3.0,4,2,1.303943368,23.0,2818.0,2.0,2262.0,501.0,4646.478466,0.0,,0.0,1
pytorch,edc5ef1afb585005408ef8c64ea4fa571ae4c3a0,12f5a328639104353a84ef623423d3b8abd76c87,Sebastian Messmer,messmer@fb.com,Wed Apr 29 03:09:09 2020 -0700,1588129749.0,"Don't use NonVariableTypeMode in custom ops (#37355)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37355

Potentially fixes https://github.com/pytorch/pytorch/issues/37306
ghstack-source-id: 103073537

Test Plan: waitforsandcastle

Differential Revision: D21261946

fbshipit-source-id: 454652b528dcf942bec5438f89201822de40bbf0",27.0,39.0,"aten/src/ATen/core/VariableFallbackKernel.cpp,test/test_cpp_extensions_jit.py",2.0,5,2,0.976020648,2.0,901.0,2.0,445988.5,1488.0,3944.0,0.0,Corrective,1.0,1
pytorch,12efef166aff92742a0816c6f62dca225a5a4c2e,130d55a5f4470690885668bbe1df10a2183638ac,Peter Goldsborough,psag@fb.com,Wed Sep 12 23:40:30 2018 -0700,1536795630.0,"Allow building the C++ API without cereal (#11498)

Summary:
I am working on unifying the C++ extensions and C++ API, and one constraint for this is that we will want to be able to build the C++ API without cereal, since we won't want to ship it with the Python `torch` package.

For this I introduce a `TORCH_WITH_CEREAL` option to CMake. If on, the C++ API will be built with cereal and thus serialization support. If off, serialization functions will throw exceptions, but the library will otherwise still compile the same. __This option is on by default, so for regular C++ API users nothing will change__. However, from C++ extensions, we'll be able to turn it off. This effectively means we won't be searching for any cereal headers from C++ API headers, which wouldn't be installed in the Python package.

ebetica ezyang soumith
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11498

Differential Revision: D9784803

Pulled By: goldsborough

fbshipit-source-id: 5d0a1f2501993012d28cf3d730f45932b483abc4",116.0,60.0,"CMakeLists.txt,cmake/Summary.cmake,tools/build_libtorch.py,tools/build_pytorch_libs.sh,torch/CMakeLists.txt,torch/csrc/api/include/torch/optim/adagrad.h,torch/csrc/api/include/torch/optim/adam.h,torch/csrc/api/include/torch/optim/lbfgs.h,torch/csrc/api/include/torch/optim/rmsprop.h,torch/csrc/api/include/torch/optim/sgd.h,torch/csrc/api/include/torch/serialization.h",11.0,8,3,2.884059589,69.0,2012.0,7.0,1240872.6363636365,4065.0,11323.33333,0.0,Feature Addition,0.0,1
pytorch,9b95f757afd34857ffd83d55060328f5f35a7daf,13120bf67742e01d5e047b04f3b1e29e0a313b80,Mike Ruberry,mruberry@devfair044.maas,Wed May 27 13:28:05 2020 -0700,1590586085.0,"Updates assertEqual to require atol and rtol, removes positional atol (#38872)

Summary:
This updates assertEqual and assertEqual-like functions to either require both or neither of atol and rtol be specified. This should improve clarity around handling precision in the test suite, and it allows us to remove the legacy positional atol argument from assertEqual. In addition, the ""message"" kwarg is replace with a kwarg-only ""msg"" argument whose name is consistent with unittest's assertEqual argument.

In the future we could make ""msg"" an optional third positional argument to be more consistent with unittest's assertEqual, but requiring it be specified should be clear, and we can easily update the signature to make ""msg"" an optional positional argument in the future, too.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38872

Differential Revision: D21740237

Pulled By: mruberry

fbshipit-source-id: acbc027aa1d7877a49664d94db9a5fff91a07042",847.0,835.0,"test/cpp_api_parity/functional_impl_check.py,test/cpp_api_parity/module_impl_check.py,test/distributed/test_c10d.py,test/distributed/test_c10d_spawn.py,test/distributed/test_data_parallel.py,test/quantization/test_qat_module.py,test/quantization/test_quantize.py,test/quantization/test_quantized_module.py,test/quantization/test_quantized_op.py,test/quantization/test_workflow_module.py,test/test_autograd.py,test/test_cpp_extensions_aot.py,test/test_cpp_extensions_jit.py,test/test_cuda.py,test/test_distributions.py,test/test_jit.py,test/test_multiprocessing.py,test/test_namedtensor.py,test/test_nn.py,test/test_optim.py,test/test_serialization.py,test/test_sparse.py,test/test_tensorboard.py,test/test_torch.py,test/test_type_promotion.py,test/test_utils.py,torch/testing/_internal/common_distributed.py,torch/testing/_internal/common_nn.py,torch/testing/_internal/common_utils.py",29.0,7,2,2.947922634,48.0,92881.0,3.0,41594.31034482759,2346.0,5947.5,0.0,Feature Addition,0.0,1
pytorch,7557407653ec6754dc6188714670a1a94b34ba19,1324410f2ec3a1ed0bfc3f0b689c861167170497,David Berard,dberard@fb.com,Thu Apr 21 18:50:03 2022 +0000,1650567003.0,"[JIT] Reuse traced fn for jit opinfos

Previously, jit opinfos would only run the traced function once. This is a problem for NNC and NVFuser, where the fused implementation only runs on the second invocation.

This caches the traced function and calls the cached implementation, so that subsequent calls actually perform fusion and use the fused implementation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76000

Approved by: https://github.com/eellison",81.0,13.0,"test/test_jit_cuda_fuser.py,test/test_jit_fuser_te.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_metaprogramming_utils.py",4.0,4,2,1.164626868,5.0,24851.0,4.0,651276.0,2544.0,6002.0,0.0,,0.0,1
pytorch,6d14ef8083d1b46e33dda2c8329d61460df6ca21,1335b7c1da6c1f05f0a581096ca0579622ad438e,ngimel,ngimelshein@nvidia.com,Thu Jan 19 16:08:43 2017 -0800,1484842123.0,Fix unpooling docs (#492),8.0,9.0,torch/nn/modules/pooling.py,1.0,3,1,0,20.0,640.0,1.0,490189.0,365.0,4883.976424,0.0,Corrective,1.0,1
pytorch,7eba9849c15bf471c68bd1830e163066694a5d16,1350f76b628bd4630439223adbe21ee993873f44,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Sun Sep 02 06:01:34 2018 -0700,1535868094.0,"Fix max and min with inf on CUDA (#11091)

Summary:
Fixes #10237 #11084

cc vishwakftw
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11091

Differential Revision: D9582859

Pulled By: SsnL

fbshipit-source-id: 3991c0a2af65ba82fa815b82f9e6b2107912fd10",85.0,14.0,"aten/src/ATen/cuda/NumericLimits.cuh,aten/src/THC/THCNumerics.cuh,aten/src/THC/generic/THCTensorMathReduce.cu,test/test_cuda.py,test/test_torch.py",5.0,7,2,2.082600811,41.0,11834.0,4.0,827405.2,3813.0,10529.33333,0.0,Corrective,1.0,1
pytorch,24ce3a7c3453734bc9870661b1e44d778c2a9384,13538c88b38b8fe038e344ab0a588bb2462193c9,Charlie Yan,yanhao.charles@gmail.com,Fri Mar 17 03:12:23 2023 +0000,1679022743.0,"[1/n] Consolidate `replicate` and `DDP`: setup ufmt for `distributed.py` (#96597)

As we already enabled ufmt for composable APIs in https://github.com/pytorch/pytorch/pull/90873, it seems a good idea to enable ufmt for other distributed APIs as well. This change setup ufmt for DDP.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96597
Approved by: https://github.com/rohan-varma",80.0,111.0,".lintrunner.toml,torch/nn/parallel/distributed.py",2.0,3,1,0.047205972,20.0,3202.0,2.0,140153.0,13456.0,31198.0,0.0,,0.0,1
pytorch,53a163a015b67299e2b519661ea4ca58edcd4fc4,136abf5aff986473ccd8e2a72eba7e47dbb0f69b,Nikita Shulga,nshulga@fb.com,Fri Oct 22 20:36:41 2021 -0700,1634935001.0,"[ONNX] Update sum symbolic to handle dtypes (#64289) (#66141)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66141

* Update aten::sum symbolic for dtype

* Remove nesting and modify opeartor tests

* Fix expect files

[ONNX] Fix expect files added in #64289 (#65356)

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D31424091

fbshipit-source-id: d4af21e9f0d7e1c68bf6ef2f3e385db84b4c53f3",455.0,16.0,"test/onnx/expect/TestOperators.test_mean_dtype.expect,test/onnx/expect/TestOperators.test_prod_dtype.expect,test/onnx/expect/TestOperators.test_reduced_mean_dtype.expect,test/onnx/expect/TestOperators.test_reduced_prod_dtype.expect,test/onnx/expect/TestOperators.test_reduced_sum_dtype.expect,test/onnx/expect/TestOperators.test_sum_dtype.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset9.py",10.0,5,2,3.004983409,5.0,14809.0,4.0,2442257.25,16500.0,38686.0,0.0,Corrective,1.0,1
pytorch,c082e2184de79d8025c52a9a01fe8e1f2735ae0e,137f2a385af9a32a71296b8b6e00735c6b1017d7,neginraoof,neginmr@utexas.edu,Thu Jan 21 23:29:19 2021 -0800,1611271759.0,"[ONNX] Handle sequence output for models (#50599)

Summary:
Duplicate of https://github.com/pytorch/pytorch/issues/46542

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50599

Reviewed By: SplitInfinity

Differential Revision: D25928897

Pulled By: bzinodev

fbshipit-source-id: a898cef7b2d15a287aedd9798ce1423cebf378d4",243.0,72.0,"scripts/onnx/test.sh,test/onnx/expect/TestOperators.test_arange_dynamic.expect,test/onnx/expect/TestOperators.test_empty_like.expect,test/onnx/expect/TestOperators.test_full.expect,test/onnx/expect/TestOperators.test_full_like.expect,test/onnx/expect/TestOperators.test_ones_like.expect,test/onnx/expect/TestOperators.test_topk.expect,test/onnx/expect/TestOperators.test_topk_smallest_unsorted.expect,test/onnx/expect/TestOperators.test_unique.expect,test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect,test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect,test/onnx/expect/TestOperators.test_zeros_like.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/_C/__init__.pyi.in,torch/csrc/jit/passes/onnx/list_model_parameters.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.h,torch/csrc/jit/python/init.cpp,torch/csrc/jit/serialization/export.cpp,torch/onnx/utils.py",20.0,14,3,2.894914377,7.0,12611.0,2.0,530913.25,8257.0,18628.0,0.0,,0.0,1
pytorch,bdb11e716a3affc6c7aa50f0a27776b721da2396,13814d6744e29c2cf74517eab5b7f7fcfcea5028,Peter Goldsborough,psag@fb.com,Tue Aug 14 19:58:06 2018 -0700,1534276686.0,"Remove use of data() in optimizers (#10490)

Summary:
After talking to users of the C++ API we found that having the tensor type be `autograd::Variable` causes more complications than having it be `at::Tensor`. It used to be a problem because `at::Tensor` didn't have the ""autograd API"" of variable (e.g. `detach()` or `grad()` methods), but those methods are now on `at::Tensor`. As such, we want to make a last big breaking change to have the tensor type be `at::Tensor`, while factory methods like `torch::ones` will return `Variable`s disguised as `at::Tensor`. This will make many things easier, like calling functions in ATen that take vectors of tensors.

This PR makes a small step in this direction by updating the optimizer classes to not use `.data()` on `Variable` to access the underlying `at::Tensor`. Using `.data()` is effectively a hack to work around our modification rules for tensors that require grad. The proper way of doing things is to use `with torch.no_grad` or equivalently `NoGradGuard` in C++ to guard in-place operations.

The next step can then simply redefine `torch::Tensor` to be `at::Tensor`. This transition should be smooth, since all methods available on `Variable` are at this point available on `at::Tensor`.

For this PR I:

1. Modified the implementations of optimizers to not use `.data()`. This means the implementations are now different from PyTorch, which still uses the legacy method of using `.data`.
2. To properly verify (1), I added more fine-grained test cases to our optimizer tests, e.g. `SGD` with and without `weight_decay`, then with `nesterov` etc. Generally more tests = more happy!
3. Minor cleanup of the optimizer codebase

ebetica apaszke
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10490

Differential Revision: D9318229

Pulled By: goldsborough

fbshipit-source-id: fb386700f37840542bc5d323f308ea88fe5ea5c5",1094.0,289.0,"test/cpp/api/optim.cpp,test/cpp/api/optim_baseline.h,test/cpp/api/optim_baseline.py,torch/csrc/api/include/torch/optim/adagrad.h,torch/csrc/api/include/torch/optim/adam.h,torch/csrc/api/include/torch/optim/optimizer.h,torch/csrc/api/include/torch/optim/rmsprop.h,torch/csrc/api/include/torch/optim/sgd.h,torch/csrc/api/include/torch/serialization.h,torch/csrc/api/src/nn/modules/embedding.cpp,torch/csrc/api/src/optim/adagrad.cpp,torch/csrc/api/src/optim/adam.cpp,torch/csrc/api/src/optim/optimizer.cpp,torch/csrc/api/src/optim/rmsprop.cpp,torch/csrc/api/src/optim/sgd.cpp",15.0,13,2,1.895007379,6.0,1570.0,4.0,910201.2666666668,3453.0,9309.833333,0.0,Feature Addition,0.0,1
pytorch,469d6d45febf09571b84bcf63c0b1a7f8b446373,1392843e7bec4f412935c013c1733e1f213fb6b8,Yang Chen,yangche@fb.com,Tue Dec 12 18:22:10 2023 -0800,1702405330.0,"[inductor] make sure bitcast input and target type have the same bitwidth (#115619)

This PR fixed #104791

bitcast requires the source and target have the bitwidth.
Because the input tensor's dtype could be promoted, e.g. from float16 to
float, we have to cast the tensor to its original source dtype before
invoking bitcast in such cases. After that, we also need to convert
the bit-casted tensor back to float to make sure we keep using higher
precision values for the rest of the computation.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/115619
Approved by: https://github.com/jansel, https://github.com/eellison",71.0,8.0,"test/inductor/test_torchinductor.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/triton.py,torch/_inductor/lowering.py",4.0,5,2,1.887577401,3.0,20752.0,4.0,225482.5,23018.0,52223.0,0.0,Corrective,1.0,1
pytorch,8e32e4c04c83f907994c1c1cdf9ec235e57394f9,13a5090695fd5ed89c4005ff03d7359d5739e7f1,Zhou Chang,achang.zhou@gmail.com,Sat Feb 25 13:53:30 2017 +0800,1488030810.0,"Added a size change in MaxPool1d module and improved tests (#771) (#832)

Backend is SpatialDilatedMaxPooling, so change 3D input (N*C*L)
to 4D size (N*C*1*L). Then output indices will range from 0 to L.
This range will not cause UnMaxPool1D error.

Signed-off-by: Zhou Chang <achang.zhou@gmail.com>",32.0,21.0,"test/test_nn.py,torch/nn/_functions/thnn/pooling.py",2.0,5,2,0.999743186,23.0,2260.0,2.0,396395.0,435.0,3905.575745,0.0,Feature Addition,0.0,1
pytorch,57373c7c29e014ccc62443507b3a214979a62ee6,13e34b467990d7192fcadd37bc1ebd6ad7bc8bf4,Adam Paszke,adam.paszke@gmail.com,Sat Jan 28 00:18:42 2017 +0100,1485562722.0,Fix multiprocessing tests,9.0,8.0,test/test_multiprocessing.py,1.0,1,1,0,20.0,413.0,1.0,171.0,386.0,3831.696975,0.0,Corrective,1.0,1
pytorch,b31cf0ebd4ffc0e25801b4e40762266ad54721d6,141f8921ac33270811bc0eb652c1175908045448,Varun Agrawal,varagrawal@gmail.com,Sun Sep 10 17:48:33 2017 -0400,1505065713.0,MultiLabelMarginLoss doc fix (#2683),1.0,1.0,torch/nn/modules/loss.py,1.0,3,1,0,33.0,679.0,1.0,178178.0,1696.0,19105.75618,0.0,Corrective,1.0,1
pytorch,2b2d56d8460d335daf5aa79774442a111d424f90,147612e64a8ec42b9e6043371b12be9bf5242aeb,li-roy,8813817+li-roy@users.noreply.github.com,Tue Feb 13 15:51:57 2018 -0800,1518537117.0,"add reduce=True arg to SoftMarginLoss (#5071)

* add reduce=True arg to SoftMarginLoss

* add reference function for SoftMarginLoss

* Rebase onto master

* Address comments

* Fix flake8

* Fix rebase error",175.0,39.0,"aten/src/ATen/nn.yaml,aten/src/THCUNN/SoftMarginCriterion.cu,aten/src/THCUNN/generic/SoftMarginCriterion.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/SoftMarginCriterion.c,aten/src/THNN/generic/THNN.h,test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/legacy/nn/SoftMarginCriterion.py,torch/nn/functional.py,torch/nn/modules/loss.py",13.0,16,4,3.286636565,39.0,16228.0,7.0,3000334.153846154,2368.0,24613.35823,0.0,Corrective,1.0,1
pytorch,b738b0960691284cf5ccb68ff76f26b5bac24244,1486d880b0be575bc1ebdf23204b1553b2fb1b2b,Sam Gross,sgross@fb.com,Wed Sep 07 15:13:29 2016 -0700,1473261209.0,"Add Storage.from_buffer

The from_buffer is similar to numpy's frombuffer. It decodes a Python
buffer object into a Storage object. For byte and char storages, it
simply copies the bytes.",197.0,2.0,"setup.py,test/test_torch.py,torch/csrc/Storage.cpp,torch/csrc/byte_order.cpp,torch/csrc/byte_order.h,torch/csrc/generic/StorageMethods.cpp",6.0,4,2,1.748750725,7.0,2691.0,3.0,620414.5,22.0,331.75,0.0,Feature Addition,0.0,1
pytorch,139afd0ea723e13e03abd71f14a05e66a91fec6a,1487137c5bf7aa69e559cee88a461b960c102278,cshesse,48501609+cshesse@users.noreply.github.com,Wed Feb 12 04:27:22 2020 -0800,1581481642.0,"add missing default value for LRScheduler.step() (#32411)

Summary:
see also other type errors in https://github.com/pytorch/pytorch/pull/30576 and https://github.com/pytorch/pytorch/pull/30441
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32411

Differential Revision: D19697245

Pulled By: ezyang

fbshipit-source-id: d0295d747541adec5d6fad646f4cf4bb2f04abf5",1.0,1.0,torch/optim/lr_scheduler.pyi,1.0,2,1,0,1.0,40.0,1.0,11865297.0,14685.0,39520.83333,0.0,Feature Addition,0.0,1
pytorch,977630bc15a7ba5cd41103e5a55502bee8765e2f,1487278fdf449670ab192ddf09177fc0498491ec,Adam Paszke,adam.paszke@gmail.com,Wed Mar 01 15:49:40 2017 -0800,1488383380.0,"Allow backprop through cuDNN RNN in eval mode

Handling of dropout descriptors has been improved too.",78.0,29.0,"test/test_nn.py,torch/backends/cudnn/__init__.py,torch/backends/cudnn/rnn.py",3.0,4,2,1.58255529,23.0,3278.0,2.0,2912.0,503.0,4649.978466,0.0,Perfective,0.0,1
pytorch,275a89a7ee45772b5d353ec488ec2cd0c78a69c3,149190c01453b33f1150556b9c5d2df3aad287d8,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu Nov 12 20:12:24 2020 -0800,1605211944.0,"Added CUDA support for complex input for torch.solve (#47045)

Summary:
`torch.solve` now works for complex inputs on GPU.
I moved the existing tests to `test_linalg.py` and modified them to test complex and float32 dtypes.
Differentiation also works correctly with complex inputs.

Fixes https://github.com/pytorch/pytorch/issues/41084
Ref. https://github.com/pytorch/pytorch/issues/33152

anjali411 I hope you don't mind that I took over https://github.com/pytorch/pytorch/pull/42737

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47045

Reviewed By: nikithamalgifb

Differential Revision: D24921503

Pulled By: anjali411

fbshipit-source-id: 4c3fc4f193a84b6e28c43c08672d480715000923",157.0,113.0,"aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,test/test_autograd.py,test/test_linalg.py,test/test_torch.py,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/linalg/__init__.py,torch/testing/_internal/common_methods_invocations.py",9.0,14,4,2.165461555,45.0,48018.0,5.0,202514.3333333333,6719.0,15277.0,0.0,Corrective,1.0,1
pytorch,16fcb07846f9e056e9d0b23fe25550fd7f7b1e90,1491bae277668fac459937c874a49c3bb8adedcb,Adnan Akhundov,aakhundov@meta.com,Wed Aug 23 19:36:50 2023 -0700,1692819410.0,"[reland][inductor] Adjust dynamic SMEM limit when above default in AOT (#107814)

Summary:

This relands #107601, which was reverted due to the new test failing in the internal CI. Here we skip the new test (as well as the existing tests in `test_aot_inductor.py`, as those are also failing in the internal CI).

Test Plan:

```
$ python test/inductor/test_aot_inductor.py
...
----------------------------------------------------------------------
Ran 5 tests in 87.309s

OK
```

Reviewers:

Subscribers:

Tasks:

Tags:

Differential Revision: [D48623171](https://our.internmc.facebook.com/intern/diff/D48623171)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/107814
Approved by: https://github.com/eellison",51.0,7.0,"test/inductor/test_aot_inductor.py,torch/_inductor/codegen/wrapper.py",2.0,5,2,0.872696504,1.0,1682.0,1.0,117181.0,18968.0,43059.0,0.0,,0.0,1
pytorch,10376522243cea049a3ad1c3e6992dc6690af4f7,149c646b7450296fc740ad476c438abde228bbe8,Pritam Damania,pritam.damania@fb.com,Thu Aug 22 23:10:29 2019 -0700,1566515429.0,"Detect and handle NCCL errors appropriately in ProcessGroupNCCL. (#25012)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25012

Resubmitting https://github.com/pytorch/pytorch/pull/22907 with build fix.

This change adds the following functionality:
1) WorkNCCL isCompleted, isSuccess methods check for NCCL errors and set the
appropriate exception.
2) Added a watchdog thread to ProcessGroupNCCL which checks for errors in the
cached communicators and removes them from the cache.
3) Use ncclCommAbort in NCCLComm destructor since ncclCommDestroy can block
forever waiting for work.
4) Added a simulate_nccl_errors.py script to simulate NCCL errors.

https://github.com/pytorch/pytorch/issues/17882
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22907

Test Plan: 1) Run the simulate_nccl_errors.py to verify NCCL errors are caught.

Differential Revision: D16958078

fbshipit-source-id: 662b0b8b8ee250e2b6d15bdfc9306d71c4f66219",627.0,39.0,"test/common_distributed.py,test/simulate_nccl_errors.py,test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/NCCLUtils.hpp,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/lib/c10d/test/CMakeLists.txt,torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp",9.0,8,2,2.519291686,5.0,4719.0,1.0,109923.0,10819.0,30731.33333,0.0,Corrective,1.0,1
pytorch,0fb1356a98b532318cfe39293daff1687c627fb9,14f06759030eeb9c0eb240abf835413ca517a9d5,Bowen Bao,bowbao@microsoft.com,Tue Nov 10 22:32:55 2020 -0800,1605047575.0,"[ONNX] Fix dtype for log_softmax export (#46627)

Summary:
Previously dtype was not converted from constant node to python number properly.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46627

Reviewed By: houseroad

Differential Revision: D24657535

Pulled By: bzinodev

fbshipit-source-id: 33b0b9087d969f2cb0a2fa608fcf6e10956c06bf",10.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.845350937,3.0,7886.0,1.0,613.0,6625.0,15056.0,0.0,Corrective,1.0,1
pytorch,fe9aac72d1dfecf875dd42ac3e656bc87029d78c,1501f81a8e18b0f0fad2a5937eed913994be6ab7,Richard Zou,zou3519@users.noreply.github.com,Tue Nov 30 01:24:22 2021 -0500,1638235462.0,"[functorch] functorch jvp quick fixes (pytorch/functorch#298)

Quick fixes for jvp support.",66.0,30.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/functorch/csrc/BatchRulesDynamic.cpp,functorch/functorch/csrc/BatchRulesFactory.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",5.0,4,1,1.529042151,1.0,4722.0,5.0,0.0,563.0,782.0,0.0,Corrective,1.0,1
pytorch,fb45ec5ac3ad97c271d3402182fa43bc1b9f062a,151e7de893c00e11fb1de6b02b386087c1c21721,Thomas Viehmann,tv.code@beamnet.de,Wed Aug 15 22:10:16 2018 -0700,1534371016.0,"varargs for einsum (#10067)

Summary:
Implemented via a wrapper, thank you Richard for the suggestion!

Fixes: #9929
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10067

Differential Revision: D9083388

Pulled By: soumith

fbshipit-source-id: 9ab21cd35278b01962e11d3e70781829bf4a36da",79.0,70.0,"test/test_torch.py,torch/_torch_docs.py,torch/functional.py",3.0,2,2,1.15063689,41.0,15034.0,3.0,556583.0,3491.0,9469.333333,0.0,Corrective,1.0,1
pytorch,de4659659bf1564ab7f775e6ee63c3e5b5ac4f4c,1527b37c26f3eff1e64b0badd2b528f60e342437,JoÃ£o Felipe Santos,joao.eel@gmail.com,Tue Feb 07 17:59:27 2017 -0500,1486490367.0,"Fixed typo and rendering of some equations (#693)

* Fixed typo and rendering of some equations

* Few more fixes to MSELoss docs

* Cleaning up whitespace to make pep8 happy",10.0,8.0,torch/nn/modules/loss.py,1.0,3,1,0,20.0,431.0,1.0,475125.0,452.0,4356.616645,0.0,Corrective,1.0,1
pytorch,d4dbdee5288aadaa3598a00687f8d18d4f5295e3,152d65ae1dd3a18d78183d694cbe7e44e4b993b6,Bin Bao,binbao@fb.com,Thu Apr 06 21:00:39 2023 +0000,1680814839.0,"[reland][inductor] Enable CudaWrapperCodeGen for non-AOT mode (#98534)

Summary: This is a reland of #98264.

When _inductor.config.cpp_wrapper is specified, we run a
two-pass wrapper codegen to generate wrapper code in cpp which calls
cuLaunchKernel to launch pre-compiled cuda kernels, and then call
load_inline to load that generated wrapper back into the python world.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98534
Approved by: https://github.com/huydhn",242.0,160.0,"test/inductor/test_cpp_wrapper.py,torch/_inductor/codecache.py,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/compile_fx.py,torch/_inductor/graph.py,torch/_inductor/triton_heuristics.py",7.0,5,2,2.426943451,2.0,6247.0,2.0,22297.14285714286,14272.0,32643.5,0.0,,0.0,1
pytorch,ad5fdef6acb11862b74bb711734d00f46035896f,15377ac39143c0e387aeca84cf2d49bc92cabd3e,Sam Gross,colesbury@gmail.com,Mon Oct 31 16:12:29 2016 -0400,1477930349.0,Copy Module._buffers in nn.parallel.replicate (#180),31.0,14.0,"test/common_nn.py,test/test_nn.py,torch/nn/parallel/replicate.py",3.0,4,2,1.009946386,10.0,1707.0,2.0,304030.6666666667,55.0,64.89007937,0.0,,0.0,1
pytorch,e845158b1abecb5178b65a3a4d67112079833336,154eca03096a9bb36c1d72d2895b440b16a2487a,kshitij12345,kshitijkalambarkar@gmail.com,Mon May 03 05:09:28 2021 -0700,1620018568.0,"OpInfo: ravel, view, view_as (#56910)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56910

Reviewed By: ngimel

Differential Revision: D28141867

Pulled By: mruberry

fbshipit-source-id: bff49d40d7e3bb36bc83d1405bd77f5529eeffe9",107.0,16.0,"test/test_fx.py,test/test_fx_experimental.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,1.317254001,43.0,18546.0,3.0,21747.75,11581.0,26213.0,0.0,,0.0,1
pytorch,af9fd35d82b09de13bf0e91647d3409e396e141c,157f949cefb9b15d01160d2becb16c6572023536,gchanan,gregchanan@gmail.com,Tue Nov 28 17:56:51 2017 -0500,1511891811.0,"Implement python scalar conversions via ATen; allow localScalar if numel == 1 (#3908)

* Have localScalar work with all 1 element tensors, not just scalars.

Also have toCFloat, etc. call localScalar so 1 element tensors work as well.

* Implement python number conversions.

* Implement __bool__, __nonzero__ as ATen functions.

* Remove merge artifacts.

* Simplify by dispatching to toCDouble.",133.0,22.0,"aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/templates/TensorDense.cpp,aten/src/ATen/templates/TensorDerived.cpp,aten/src/ATen/templates/TensorMethods.h,test/test_autograd.py,tools/autograd/templates/python_variable_methods.cpp,torch/autograd/variable.py,torch/csrc/autograd/utils/wrap_outputs.h,torch/csrc/utils/python_numbers.h",10.0,15,4,2.544445968,38.0,3999.0,6.0,573037.1,351.0,1008.405869,0.0,,1.0,1
pytorch,4102fbdf08430dcfadc2faeb588c34b88c449e3a,15864d170384f584e9c8a06118781e9817ef8cc5,Sinan Nasir,sinannasir@fb.com,Sat Jun 27 02:18:22 2020 -0700,1593224302.0,"Skip allreducing `local_used_maps_dev_` when `find_unused_param=False`

Summary:
1. In reducer.cpp, we have a new boolean `find_unused_param_` and its value is set in `Reducer::prepare_for_backward`.
If `!find_unused_param_`, then it avoids `allreduce(local_used_maps_dev_)`.
2. Solves issue [38942](https://github.com/pytorch/pytorch/issues/38942).
3. Fixes incorrect `find_unused_parameters_` passing like checking `outputs.empty()` or `unused_parameters_.empty()`.

ghstack-source-id: 106693089

Test Plan:
1. Run `test/distributed/test_c10d.py` and make sure all tests pass.
2. A new test case `test_find_unused_parameters_when_unused_parameters_empty` is included. Old `reducer.cpp` was failing in that unit test because it was checking `find_unused_parameters_` by `unused_parameters_.empty()`. Current `reducer.cpp` passes this unit test.
3. Two test cases were failing `test_forward_backward_unused_parameters` and `test_forward_backward_optimizer` , because `find_unused_parameter_` of their `reducer` object was not set properly. I fixed that as well.

Imported from OSS

**Output of version 14:**
```
................s.....s...............................................test/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  tensor = torch.full([100, 100], self.rank)
test/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  tensor = torch.full([100, 100], self.rank)
test/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  tensor = torch.full([100, 100], self.rank)
test/distributed/test_c10d.py:1531: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  tensor = torch.full([100, 100], self.rank)
.test/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  self.assertEqual(torch.full([10, 10], self.world_size), tensor)
test/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  self.assertEqual(torch.full([10, 10], self.world_size), tensor)
test/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  self.assertEqual(torch.full([10, 10], self.world_size), tensor)
test/distributed/test_c10d.py:1554: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning. (Triggered internally at  ../aten/src/ATen/native/TensorFactories.cpp:364.)
  self.assertEqual(torch.full([10, 10], self.world_size), tensor)
.....s...............................
----------------------------------------------------------------------
Ran 108 tests in 214.210s

OK (skipped=3)
```

Differential Revision: D22176231

fbshipit-source-id: b5d15f034e13a0915a474737779cc5aa8e068836",194.0,106.0,"test/distributed/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h,torch/nn/parallel/distributed.py",5.0,8,2,1.130562532,19.0,6173.0,4.0,266703.6,3247.0,7815.5,0.0,Corrective,1.0,1
pytorch,815bfad28c8dc7fd3bd89c5e5a821679f73702fc,158cdece65a9599656f788f926e7b835845e9f77,lezcano,lezcano-93@hotmail.com,Tue Apr 06 15:39:49 2021 -0700,1617723589.0,"Correct many OpInfos ""test_out"" skips. (#55141)

Summary:
Partially solves https://github.com/pytorch/pytorch/issues/54061

This PR solves many of the ""easy to solve"" problems with `out=` not notifying when it resizes a tensor. It also reports the cause of some fails of the `out=` operation in the tests. Hopefully this way we will be able to catch some errors that do not come simply from not using `resize_output`.
cc mruberry anjali411

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55141

Reviewed By: anjali411

Differential Revision: D27568755

Pulled By: mruberry

fbshipit-source-id: a32546555fef8d241de2ef635a99e5615461ed09",59.0,76.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOpsUtils.h,aten/src/ATen/native/Resize.h,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cuda/Indexing.cu,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/cuda/ScanKernels.cu,aten/src/ATen/native/cuda/TriangularOps.cu,test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",14.0,10,3,3.013308231,10.0,17517.0,12.0,618771.2857142857,10462.0,23219.0,0.0,Corrective,0.0,1
pytorch,ee79413b6a3f895227ff6b5b6e562f2b0bcc130c,159a2404bdc7fc54918c78d4bd290e5fa830dca7,Peter Bell,peterbell10@live.co.uk,Fri May 07 08:29:32 2021 -0700,1620376172.0,"fft: Increase tolerance for nd-fft tests (#57576)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/56820

The test only fails for inverse n-dim functions with `norm=""forward""`. The relative error for isn't actually any bigger than other norm modes though. It's just that the magnitude of the result is bigger, so the absolute tolerance is less relative each element. So, I just increase the relative tolerance  to compensate.

This `precisionOverride` is already applied to `fftn` and `rfftn` for exactly the same reason.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57576

Reviewed By: albanD

Differential Revision: D28249222

Pulled By: mruberry

fbshipit-source-id: 734c7c1ae8236b253d6e3cd2218c05d21901c567",12.0,2.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,6436.0,1.0,583.0,11764.0,26544.0,0.0,Corrective,1.0,1
pytorch,838fdd6f99cbbe403c63289ae5620b2312abbbb2,15a75208ee965e07b03bafba69aa4891bfbd8093,Will Feng,willfeng@fb.com,Thu Jun 28 22:02:55 2018 -0700,1530223375.0,"Use std::random_device for generating storage handle (#8971)

Summary:
Currently the `test_RNG_after_pickle` in the PR would fail because pickling a tensor changes the RNG state. This PR aims to fix it.
Closes https://github.com/pytorch/pytorch/pull/8971

Reviewed By: ezyang

Differential Revision: D8677474

Pulled By: yf225

fbshipit-source-id: 1713d9611699ad288b66d92dbb29ce9feb34b8cf",16.0,1.0,"test/test_torch.py,torch/csrc/generic/StorageSharing.cpp",2.0,4,2,0.787126586,40.0,8258.0,1.0,173679.0,2756.0,6259.333333,0.0,Corrective,1.0,1
pytorch,5bcacb21d512c359a2a4e7f19d020b68b3103284,15b657af843d0dd95fa43d44348161d3785ed0e9,gchanan,gregchanan@gmail.com,Wed Dec 27 21:36:50 2017 -0500,1514410610.0,"Support ATen GPU pointwise apply and torch.where. (#4304)

* Support ATen GPU pointwise apply and torch.where.

Like the CPU version, this implements an apply template that is almost identical to the
apply template already in THC, but using the ATen API.  Much of this involves stripping out
the TensorUtils code (which is basically templated ATen-style), although a couple of functions
remain that are apply specific (and thus don't seem worth porting to ATen), namely
overlappingIndices, canUse32BitIndexMath, and getTensorInfo.  We can make those generally
available if there's a need.

* Use int64_t instead of ptrdiff_t.

* Use snake case for _copyIgnoringOverlaps_.",1458.0,50.0,"aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/Declarations.cwrap,aten/src/ATen/Dispatch.h,aten/src/ATen/cuda/CUDAApplyUtils.cuh,aten/src/ATen/cuda/detail/IndexUtils.cu,aten/src/ATen/cuda/detail/IndexUtils.cuh,aten/src/ATen/cuda/detail/TensorInfo.cuh,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/cuda/NativeFunctionsCuda.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/scalar_test.cpp,test/test_autograd.py,tools/autograd/templates/Functions.cpp",19.0,12,3,1.813803426,38.0,9652.0,11.0,673017.6,405.0,1274.405869,0.0,,0.0,1
pytorch,18a76f54a61c9ec67e31460098163ae70dc3ec77,15eae9543e8d916656f3394df65d46d3e23f992b,cjsg,johann2706@hotmail.com,Sat Mar 03 10:49:01 2018 +0100,1520074141.0,Fixed dimensions in docs of conv and conv_transpose (#5543),5.0,5.0,torch/nn/functional.py,1.0,2,1,0,28.0,2119.0,1.0,62957.0,2418.0,24711.35823,0.0,Corrective,1.0,1
pytorch,2c44a9f9cdad95db9cfd3e03e1a6a1cb45d1cfab,16055663888669b5dc46ffd40cc1e5d38a870e81,Jerry Zhang,jerryzh@fb.com,Fri Oct 06 20:20:09 2017 -0700,1507321209.0,"Add map input for predictor

Summary: Added TensorMap input for run function in predictor.cc

Reviewed By: bwasti

Differential Revision: D5847103

fbshipit-source-id: cd9755a0491b50adc35177164ffe7a50e73ff80f",33.0,0.0,"caffe2/core/predictor.cc,caffe2/core/predictor.h,caffe2/core/predictor_test.cc",3.0,2,1,1.404826958,3.0,277.0,1.0,681059.0,2073.0,5179.833333,0.0,Feature Addition,0.0,1
pytorch,c68d0a7042e850cebc4cbe7f717fc11aedf6b9d7,161ea463e690dcb91a30faacbf7d100b98524b6b,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Fri Aug 25 19:34:55 2023 +0000,1692992095.0,"Revert ""Remove remaining global `set_default_dtype` calls from tests (#107246)""

This reverts commit aa8ea1d787a9d21b064b664c5344376265feea6c.

Reverted https://github.com/pytorch/pytorch/pull/107246 on behalf of https://github.com/facebook-github-bot due to Diff reverted internally ([comment](https://github.com/pytorch/pytorch/pull/107246#issuecomment-1693838522))",874.0,900.0,"test/distributed/test_data_parallel.py,test/distributions/test_distributions.py,test/jit/test_freezing.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_complex.py,test/test_cpp_api_parity.py,test/test_jit_cuda_fuser.py,test/test_linalg.py,test/test_matmul_cuda.py,test/test_ops_fwd_gradients.py,test/test_ops_gradients.py,test/test_ops_jit.py,test/test_tensor_creation_ops.py,test/test_torch.py,test/test_type_info.py,torch/testing/_internal/common_nn.py,torch/testing/_internal/common_utils.py",17.0,8,2,1.037968426,49.0,54476.0,2.0,96969.64705882352,19045.0,43239.5,0.0,Non Functional,0.0,1
pytorch,932e48402977bbf4a8f8e0435307f658c6c6ac5f,165d0897e4b54a83b026bf5109cbf688497a8199,Fritz Obermeyer,fritz.obermeyer@gmail.com,Sat Dec 02 00:10:08 2017 -0800,1512173408.0,Implement distributions.Gamma (#3841),202.0,11.0,"aten/src/TH/THRandom.c,aten/src/TH/THRandom.h,aten/src/TH/generic/THTensorRandom.c,aten/src/TH/generic/THTensorRandom.h,test/test_distributions.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorRandom.cwrap,torch/distributions.py",9.0,10,3,2.21379034,39.0,2814.0,8.0,1456676.4444444445,829.0,6561.172317,0.0,,0.0,1
pytorch,3c709f5b263e2c4c21f86bbc2eece9d11e0b637a,1661370ac5f88ef11fedbeac8d0398e8369fc1f3,SsnL,SsnL@users.noreply.github.com,Wed Nov 29 22:52:14 2017 -0500,1511995934.0,Signal handling in DataLoader workers; Timeout option (#3474),336.0,18.0,"test/test_dataloader.py,torch/csrc/DataLoader.cpp,torch/csrc/Module.cpp,torch/utils/data/dataloader.py",4.0,5,2,1.52889937,38.0,1631.0,3.0,1465279.3333333333,827.0,6556.172317,0.0,,0.0,1
pytorch,69265ea5bcf99469c24fd9ef7d55921bf106666b,1681d07199376256a3611149e7fe9ed582e41508,Will Feng,yf225@cornell.edu,Wed Dec 20 10:30:21 2017 +0800,1513765821.0,Disable tests and fix issues with Windows CUDA build (#4251),36.0,15.0,"aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/templates/Type.h,test/common.py,test/test_dataloader.py,test/test_jit.py,test/test_multiprocessing.py,test/test_nccl.py,test/test_nn.py,test/test_torch.py",9.0,6,2,2.873556236,38.0,13066.0,8.0,463683.1111111111,855.0,6615.172317,0.0,Corrective,1.0,1
pytorch,41c8fee3e7166e6aa8f6d454de07411c80bf9b17,169ca67a4e721471d7a61c090dd6d6a36543901a,Soumith Chintala,soumith@gmail.com,Fri Jul 07 15:37:53 2017 -0400,1499441873.0,Adding Spatial Transformers w/CuDNN support,583.0,0.0,"setup.py,test/test_nn.py,torch/csrc/cudnn/AffineGridGenerator.cpp,torch/csrc/cudnn/AffineGridGenerator.h,torch/csrc/cudnn/Descriptors.h,torch/csrc/cudnn/GridSampler.cpp,torch/csrc/cudnn/GridSampler.h,torch/csrc/cudnn/cuDNN.cwrap,torch/nn/_functions/vision.py,torch/nn/functional.py",10.0,6,2,2.934517534,34.0,5361.0,1.0,5654.0,1129.0,15452.61187,0.0,Feature Addition,0.0,1
pytorch,58a88d1ac053d009111d09eb32f88ea66d7285df,16a09304b47353bbe2b014a24d9e574399c8a379,Jean Senellart,jsenellart@systran.fr,Fri Jan 20 17:01:50 2017 +0100,1484931710.0,fix documentation of LSTM cell (#525),2.0,2.0,torch/nn/modules/rnn.py,1.0,3,1,0,18.0,495.0,1.0,28146.0,368.0,4887.976424,0.0,Corrective,1.0,1
pytorch,dc41b23e41bb4c73a03cd0c44cfa67bb414eb8b2,16c253e62e070c2cd175d54fcce955317dffdc65,Yangqing Jia,jiayq84@gmail.com,Mon Jul 06 05:24:13 2015 -0700,1436160253.0,"Some non-trivial refactoring:

(1) added blob serialization.
(2) registry can now use key types other than string.
(3) changed load_save_op so they interface with a db.
(4) change sgd iter op: it does increments so we can resume an iter.
(5) mnist linear classifier tests snapshot functionality.
(6) added protodb which is a small wrapper over TensorProtos.",506.0,94.0,"caffe2/core/BREW,caffe2/core/blob.h,caffe2/core/blob_serialization.cc,caffe2/core/blob_serialization.h,caffe2/core/blob_serialization_gpu.cc,caffe2/core/registry.h,caffe2/db/BREW,caffe2/db/protodb.cc,caffe2/operators/load_save_op.cc,caffe2/operators/load_save_op.cu,caffe2/operators/load_save_op.h,caffe2/operators/utility_ops.h,caffe2/sgd/iter_op.cc,pycaffe2/core_gradients.py",14.0,6,2,2.801281101,1.0,1018.0,5.0,670041.1,24.0,1069.0,0.0,Feature Addition,0.0,1
pytorch,7920a970c6b0e4a9ac59a50f5a012ea8d520693d,16d9bcd725e9dc9b54ce9beecd3fde80b4fd9dc8,Di Wu,allwu@fb.com,Tue Apr 07 15:23:42 2020 -0700,1586273022.0,"Fix test_avg_pool3d issue in pytorch_paralleltbb_linux_xenial_py3_6_gcc5_4_test (#36103)

Summary:
Fix parallel execution issue introduced by https://github.com/pytorch/pytorch/issues/35740
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36103

Test Plan: test_quantized.py

Differential Revision: D20879323

Pulled By: allwu

fbshipit-source-id: a2deaaf5c933cbef3096a399c19c44d28935bd69",2.0,3.0,"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,test/quantization/test_quantized.py",2.0,9,2,0.721928095,2.0,4468.0,2.0,226492.0,841.0,2359.5,0.0,Corrective,1.0,1
pytorch,3580c937165dd7efc943b364b4e2942b6b16d3bf,171476e870b51a392a726d63c273592635c394fb,Jianyu Huang,jianyuhuang@fb.com,Wed Apr 22 18:27:14 2020 -0700,1587580034.0,"CUDA implementation of Sparse Adagrad Fusion for GPUs (#35762)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35762

We implement the following operators for Regular and RowWise SparseAdagrad Fusion with SLS and SLWS gradient
- SparseAdagradFusedWithSparseLengthsSumGradient
- RowWiseSparseAdagradFusedWithSparseLengthsSumGradient
- SparseAdagradFusedWithSparseLengthsWeightedSumGradient
- RowWiseSparseAdagradFusedWithSparseLengthsWeightedSumGradient

Test Plan:
- SparseAdagradFusedWithSparseLengthsSumGradient
- RowWiseSparseAdagradFusedWithSparseLengthsSumGradient

```
buck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \(caffe2\.caffe2\.fb\.net_transforms\.tests\.fuse_sparse_ops_test\.TestFuseSparseOps\)' --print-passing-details
```

- SparseAdagradFusedWithSparseLengthsWeightedSumGradient
- RowWiseSparseAdagradFusedWithSparseLengthsWeightedSumGradient

```
buck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_weighted_sum_gradient \(caffe2\.caffe2\.fb\.net_transforms\.tests\.fuse_sparse_ops_test\.TestFuseSparseOps\)' --print-passing-details
```

Benchmark code:
```
buck run mode/dev-nosan //caffe2/caffe2/fb/optimizers:adagrad_fused_bench_gpu
```

Reviewed By: jspark1105

Differential Revision: D20453096

fbshipit-source-id: bc209348232e3454af0d1d909bbd8ab7f07f69fd",1110.0,7.0,"caffe2/sgd/adagrad_fused.cc,caffe2/sgd/adagrad_fused_op_gpu.cu,caffe2/sgd/rowwise_adagrad_fused.cc",3.0,2,1,0.061047594,1.0,332.0,1.0,2311268.0,1300.0,3466.0,0.0,Perfective,0.0,1
pytorch,7e4c95695539c0fe0675ab0c91de761247143d8a,1723ab53c493cf0fb1a3510c7a19fb81f20bc972,BowenBao,bowbao@microsoft.com,Thu Jan 28 01:41:50 2021 -0800,1611798110.0,"[ONNX] Update Reducesum operator for opset 13 (#50532) (#50907)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50907

* udpate symbolic for squeeze/unsqueeze

* update c++ unsqueeze/squeeze creation

* clang format

* enable tests

* clang format

* remove prints

* remove magic number

* add helper function

* fix build issue

* update opset9 symbolic with helper function

* fix utility test

* fix prim_fallthrough opset skip

* enable reducesum opset 13

* enable embedding_bag which contain reducesum op

* add ReduceSum helper

* remove block_listed_operators

* remove local test code

* remove embedding_bag() in opset13 file

* remove unuse import

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D26050888

Pulled By: SplitInfinity

fbshipit-source-id: 88307af6a7880abf94eac126ec1638e962de8c1f

Co-authored-by: BowenBao <bowbao@microsoft.com>
Co-authored-by: hwangdeyu <deyhuang@qq.com>",57.0,16.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset9.py",6.0,4,2,1.694788973,3.0,11137.0,2.0,203978.16666666663,8411.0,18990.0,0.0,Corrective,1.0,1
pytorch,403905a37b29e3b4a22b390e5f09537a3985941e,1726c6f7a7723aa8222539db4e7747284c4b14de,kshitij12345,kshitijkalambarkar@gmail.com,Tue Mar 28 03:35:41 2023 +0000,1679974541.0,"[fix] vmap: fix segfault on data access (#97237)

Fixes #97161

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97237
Approved by: https://github.com/zou3519",45.0,0.0,"aten/src/ATen/functorch/BatchRulesDecompositions.cpp,aten/src/ATen/functorch/BatchedTensorImpl.cpp,aten/src/ATen/functorch/BatchedTensorImpl.h,test/functorch/test_vmap.py",4.0,6,2,1.826894565,1.0,5761.0,4.0,5623438.75,13805.0,31818.5,0.0,Corrective,1.0,1
pytorch,96456bfa4cf9394c9c926b143cf724a09901908d,173f224570017b4b1a3a1a13d0bff280a54d9cd9,Edward Yang,ezyang@fb.com,Sat Mar 30 15:58:10 2019 -0700,1553961490.0,"Turn on F401: Unused import warning. (#18598)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18598
ghimport-source-id: c74597e5e7437e94a43c163cee0639b20d0d0c6a

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18598 Turn on F401: Unused import warning.**

This was requested by someone at Facebook; this lint is turned
on for Facebook by default.  ""Sure, why not.""

I had to noqa a number of imports in __init__.  Hypothetically
we're supposed to use __all__ in this case, but I was too lazy
to fix it.  Left for future work.

Be careful!  flake8-2 and flake8-3 behave differently with
respect to import resolution for # type: comments.  flake8-3 will
report an import unused; flake8-2 will not.  For now, I just
noqa'd all these sites.

All the changes were done by hand.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Differential Revision: D14687478

fbshipit-source-id: 30d532381e914091aadfa0d2a5a89404819663e3",175.0,333.0,".circleci/cimodel/lib/visualization.py,.flake8,.jenkins/pytorch/perf_test/compare_with_baseline.py,benchmarks/fastrnns/__init__.py,benchmarks/fastrnns/profile.py,benchmarks/fastrnns/test.py,docs/cpp/source/conf.py,docs/source/conf.py,setup.py,test/common_methods_invocations.py,test/common_nn.py,test/custom_operator/test_custom_ops.py,test/onnx/debug_embed_params.py,test/onnx/export_onnx_tests_filter.py,test/onnx/export_onnx_tests_generator.py,test/onnx/model_defs/__init__.py,test/onnx/model_defs/squeezenet.py,test/onnx/model_defs/super_resolution.py,test/onnx/model_defs/word_language_model.py,test/onnx/test_models.py,test/onnx/test_operators.py,test/onnx/test_pytorch_common.py,test/onnx/test_pytorch_helper.py,test/onnx/test_pytorch_onnx_caffe2.py,test/optim/test.py,test/test_autograd.py,test/test_cuda.py,test/test_dataloader.py,test/test_distributed.py,test/test_docs_coverage.py,test/test_indexing.py,test/test_jit.py,test/test_jit_fuser.py,test/test_module/future_div.py,test/test_module/no_future_div.py,test/test_multiprocessing.py,test/test_nn.py,test/test_quantized.py,test/test_sparse.py,test/test_thd_distributed.py,test/test_torch.py,test/test_type_hints.py,test/test_utils.py,tools/amd_build/build_amd.py,tools/autograd/gen_variable_type.py,tools/build_libtorch.py,tools/build_pytorch_libs.py,tools/clang_format.py,tools/cwrap/__init__.py,tools/cwrap/plugins/ArgumentReferences.py,tools/cwrap/plugins/CuDNNPlugin.py,tools/cwrap/plugins/GILRelease.py,tools/cwrap/plugins/OptionalArguments.py,tools/cwrap/plugins/__init__.py,tools/download_mnist.py,tools/jit/gen_jit_dispatch.py,tools/nnwrap/__init__.py,tools/nnwrap/generate_wrappers.py,tools/pyi/gen_pyi.py,tools/setup_helpers/dist_check.py,tools/setup_helpers/miopen.py,tools/setup_helpers/nccl.py,tools/setup_helpers/nvtoolext.py,tools/shared/__init__.py,torch/__init__.py,torch/_six.py,torch/_tensor_str.py,torch/_thnn/utils.py,torch/autograd/__init__.py,torch/autograd/_functions/__init__.py,torch/autograd/_functions/utils.py,torch/autograd/gradcheck.py,torch/autograd/profiler.py,torch/cuda/__init__.py,torch/cuda/comm.py,torch/distributed/__init__.py,torch/distributed/distributed_c10d.py,torch/distributed/launch.py,torch/distributions/__init__.py,torch/distributions/categorical.py,torch/distributions/chi2.py,torch/distributions/fishersnedecor.py,torch/distributions/gamma.py,torch/distributions/half_cauchy.py,torch/distributions/half_normal.py,torch/distributions/kl.py,torch/distributions/log_normal.py,torch/distributions/logistic_normal.py,torch/distributions/pareto.py,torch/distributions/studentT.py,torch/distributions/utils.py,torch/for_onnx/__init__.py,torch/hub.py,torch/jit/__init__.py,torch/jit/_pickle.py,torch/jit/annotations.py,torch/jit/frontend.py,torch/jit/quantized.py,torch/multiprocessing/__init__.py,torch/multiprocessing/reductions.py,torch/nn/__init__.py,torch/nn/_functions/thnn/__init__.py,torch/nn/_functions/thnn/auto.py,torch/nn/_reduction.py,torch/nn/functional.py,torch/nn/init.py,torch/nn/modules/distance.py,torch/nn/modules/loss.py,torch/nn/modules/normalization.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/modules/upsampling.py,torch/nn/parallel/__init__.py,torch/nn/parallel/deprecated/distributed.py,torch/nn/parallel/distributed.py,torch/nn/utils/__init__.py,torch/nn/utils/spectral_norm.py,torch/onnx/__init__.py,torch/onnx/symbolic.py,torch/onnx/utils.py,torch/optim/__init__.py,torch/optim/lr_scheduler.py,torch/optim/rprop.py,torch/random.py,torch/utils/bottleneck/__main__.py,torch/utils/collect_env.py,torch/utils/data/__init__.py,torch/utils/data/_utils/__init__.py,torch/utils/data/_utils/pin_memory.py,torch/utils/data/_utils/signal_handling.py",131.0,51,7,6.365692146,49.0,83291.0,93.0,8645674.633587787,7784.0,23630.33333,0.0,Corrective,1.0,1
pytorch,8cd45b4c4633eb19f80eafca2c118711de26542d,17831648dd0395283b28f5ec29c0ce1b085e7df6,James Reed,jamesreed@fb.com,Fri Aug 30 04:18:52 2019 -0700,1567138732.0,"Quantized vec256 + vectorized quantized::add

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/25202

Test Plan: Imported from OSS

Differential Revision: D17061047

Pulled By: jamesr66a

fbshipit-source-id: b08a61a9b4a258a4c1b6a97a6da1db05c3a6b0f7",650.0,86.0,"aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/qadd.cpp,aten/src/ATen/quantized/Quantizer.cpp,aten/src/ATen/quantized/Quantizer.h,test/test_quantized.py",6.0,10,2,1.299300547,4.0,2515.0,4.0,354420.4,11046.0,31183.33333,0.0,Feature Addition,0.0,1
pytorch,2e9eb5afa21a005a9be7b534597daadebf807859,17e5ba44f1f67eb734fdf007c52c4374abbd71ab,kshitij12345,kshitijkalambarkar@gmail.com,Wed Apr 07 15:18:56 2021 -0700,1617808736.0,"[testing] Support input samples where `self` is broadcasted. (#53014)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/50747

Reference https://github.com/pytorch/pytorch/issues/50006

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53014

Reviewed By: SplitInfinity, ngimel

Differential Revision: D27615320

Pulled By: mruberry

fbshipit-source-id: a48bccf06aef1ee8f66a89e6b2bbe79736700b2b",193.0,152.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.909353644,2.0,5591.0,2.0,49361.0,10514.0,23298.5,0.0,Corrective,1.0,1
pytorch,a06b95b2badc287355d75e43b944b6ebcea4a0ae,17f8c329dfb5c00df1f1c6de4c2d9257529abb6f,Nick Gibson,nickg@fb.com,Tue Oct 20 02:35:01 2020 -0700,1603161301.0,"[NNC] IRSimplifier rules for Compare and Mod (#46412)

Summary:
Adds new rules to the NNC IRSimplifier to take care of the following cases:

* Comparisons which are symbolic but have a constant difference. E.g. this is most useful in cases like `if (x > x + 4) ...` which we can now eliminate.

* Simplification of `Mod` nodes, including simple rules such as `0 % x` and `x % 1`, but also factorization of both sides to find common symbolic multiples. E.g. `(x * y) % x` can be cancelled out to `0`.

See tests for many more examples!

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46412

Reviewed By: navahgar

Differential Revision: D24396151

Pulled By: nickgg

fbshipit-source-id: abb954dc930867d62010dcbcd8a4701430733715",442.0,18.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",4.0,7,2,1.083965668,2.0,7246.0,3.0,1117839.5,6086.0,14040.0,0.0,Feature Addition,0.0,1
pytorch,9a0d1c5446c02f84b94a312193f7b26704965a47,17fbb617ad7b575df8d88ca48824b4decbc27a23,Mike Ruberry,mruberry@devfair044.h1.fair,Sat Apr 16 01:26:01 2022 +0000,1650072361.0,"Adds soft_margin_loss opinfo

per title
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75888
Approved by: https://github.com/ngimel",13.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,17148.0,1.0,473.0,2349.0,5498.5,0.0,Feature Addition,0.0,1
pytorch,101950ce926d36131fa342a5b786f922a10d9316,183b3aacd2db811e8a6e7509f8f306ab5f454272,Adam Lerer,alerer@fb.com,Mon Dec 19 00:39:09 2016 -0800,1482107949.0,Hold CuDNN PRNG state between RNN iterations,84.0,18.0,"test/test_nn.py,torch/backends/cudnn/rnn.py,torch/nn/functions/rnn.py,torch/nn/modules/rnn.py",4.0,7,2,1.828782999,18.0,2354.0,3.0,58915.75,351.0,3022.196975,0.0,,0.0,1
pytorch,23633bdb5cdd29a77583b8c2ecd17e6598825091,1842364b30bb37a4199e97356ad7df0dbdfa3cee,Pearu Peterson,pearu.peterson@gmail.com,Wed Dec 01 18:43:55 2021 -0800,1638384235.0,"Strided masked normalize. (#68694)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68694

Test Plan: Imported from OSS

Reviewed By: samdow

Differential Revision: D32724552

Pulled By: cpuhrsch

fbshipit-source-id: 82f579a86b0b265e0b9b3715a8a327b775dd55e1",87.0,19.0,"test/test_masked.py,torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.517716749,2.0,15055.0,2.0,110961.33333333331,17420.0,40911.0,0.0,,0.0,1
pytorch,9388d3529326f063946772718a495877d681342a,1848cad10802db9fa0aa066d9de195958120d863,Tongzhou Wang,SsnL@users.noreply.github.com,Thu Feb 22 16:56:41 2018 -0500,1519318601.0,"[ready] Layer Normalization (#4922)

* at::maybe_data_ptr and Check.h => TensorUtils.h

* THNN support for optional BN running_*

* ATen support for optional BN running_*

* Python nn.* support for optional BN running_*; Improve IN and BN doc

* Add tests for IN and BN new option

* Layer Norm

* Fix LRN doc

* functional interface for LN and IN

* Layer norm tests

* fix BN double backward returning undefined tensors

* fix jit test using wrong dim inputs for BN

* add/improve BN, IN and LN GPU tests with half type

* Udpate docs to be consistent with Conv notation
Fix onnx
Clarified onnx symbokic wrapper

* fix typo

* Address comments",1192.0,531.0,"aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Check.cpp,aten/src/ATen/Check.h,aten/src/ATen/TensorUtils.cpp,aten/src/ATen/TensorUtils.h,aten/src/ATen/cuda/CUDAApplyUtils.cuh,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/native/BatchNorm.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/Pooling.cpp,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,aten/src/ATen/native/cudnn/BatchNorm.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/cudnn/GridSampler.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/THCUNN/BatchNormalization.cu,aten/src/THCUNN/generic/BatchNormalization.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/BatchNormalization.c,aten/src/THNN/generic/THNN.h,docs/source/nn.rst,test/common.py,test/expect/TestJit.test_batchnorm.expect,test/test_jit.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/csrc/utils/tensor_apply.cpp,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/batchnorm.py,torch/nn/modules/instancenorm.py,torch/nn/modules/normalization.py,torch/onnx/__init__.py",37.0,25,5,3.687240416,38.0,23825.0,23.0,2631954.1714285715,2394.0,24678.35823,0.0,Corrective,1.0,1
pytorch,7c825bad10fa81e330f8ecd4e8cac5aa17143b21,1875c2e4bda4addf290cad73ca54003ff9ed7090,Hameer Abbasi,einstein.edison@gmail.com,Fri Apr 10 16:07:15 2020 -0700,1586534835.0,"Add torch.Tensor.as_subclass method. (#34369)

Summary:
This is according to pytorch/rfcs#3.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34369

Differential Revision: D20963929

Pulled By: ezyang

fbshipit-source-id: e618af6fd36e1dfaeda617162314ad5840f55358",83.0,2.0,"docs/source/tensors.rst,test/test_torch.py,torch/_tensor_docs.py,torch/csrc/autograd/python_variable.cpp",4.0,6,3,1.315947746,42.0,21674.0,3.0,652480.0,959.0,2573.5,0.0,Feature Addition,0.0,1
pytorch,147c88ef2d123dd8e3f6274e2234fb32614942d4,18876b572275fce59c284d41a9b486ed1cd02bc8,anjali411,chourdiaanjali123@gmail.com,Tue Sep 29 19:50:05 2020 -0700,1601409005.0,"Update backward formula for torch.dot and add backward definition for torch.vdot (#45074)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/45074

TODO: Add R -> C tests in https://github.com/pytorch/pytorch/pull/44744 (blocked on some JIT changes)

Test Plan: Imported from OSS

Reviewed By: gchanan

Differential Revision: D23975361

Pulled By: anjali411

fbshipit-source-id: 3512bd2962b588a198bc317673bd18cc96ac823f",15.0,5.0,"test/test_autograd.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",5.0,8,3,1.821928095,43.0,13589.0,3.0,141089.8,5580.0,13093.5,0.0,Feature Addition,0.0,1
pytorch,ea0ee77c61beacfe67c0209de4e142e22ce136f1,18e5fd36c2d8b55520aee0998258a61dc696602a,Myle Ott,myleott@fb.com,Mon Sep 10 20:42:51 2018 -0700,1536612171.0,"Normalize gradients before reduction in DistributedDataParallelC10d (#11109)

Summary:
Normalizing by the world size before the reduction is less likely to cause overflow in FP16 training.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11109

Differential Revision: D9594708

Pulled By: myleott

fbshipit-source-id: 93ab53cb782ee1cbe1264e529b333490a0940338",34.0,1.0,"test/test_c10d.py,torch/nn/parallel/distributed_c10d.py",2.0,4,2,0.512709142,2.0,1094.0,2.0,249250.0,3976.0,11101.33333,0.0,,0.0,1
pytorch,902579602bc8ec8e884fe6501205008d3da629a4,18ed2160b070acde35736e850281399281e6cf13,gchanan,gregchanan@gmail.com,Thu Apr 26 23:13:23 2018 -0400,1524784403.0,"Use Index rather than Long for IntList parsing (#6674)

* Use Index rather than Long for IntList, so floating-point types convertible to ints fail the parsing.

Basically, our unpackLong code works with floating-point types that are convertible to ints, but this isn't often what you want (because of truncation).
What you actually want is to convert to an index, which will usually find such issues.

I made this the minimal change I could because:
1) I didn't want to change unpackLong because the existing code call checkLong before unpackLong, so this should be a non-issue most of the time.  And fixing this properly requires calling checkLong again, which will slow everything down.
2) An exception above is with IntList, which only checks that 1) it is a tuple or 2) it is a varargs tuple (i.e. torch.ones(1, 2, 3)).

* Fix bug.

* Don't conflict tensor and IntList bindings.

* Change function to be consistent between python 2 and 3.

* Check Index.

* Move IntList overloads in legacy new functions to below Tensor overloads.",91.0,40.0,"test/test_torch.py,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/python_numbers.h,torch/csrc/utils/tensor_new.cpp",5.0,4,2,1.630313181,39.0,8451.0,3.0,259938.6,985.0,2392.305292,0.0,Corrective,1.0,1
pytorch,7ffa8649534deed77f73eac1dc60e4eada7772ca,1906305c075e8043014e1ec4c6b9a3fafd99eca5,Pieter Noordhuis,pietern@fb.com,Tue Nov 06 19:50:08 2018 -0800,1541533808.0,"Consolidate argument checkers (#13623)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13623

Moves the bulk of shared argument checkers in the gloo backend to Utils.hpp.

Reviewed By: teng-li

Differential Revision: D12934598

fbshipit-source-id: 7b80e67ccc3425f21498c30fbe7837af314f96f2",145.0,133.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/Utils.hpp",3.0,4,2,1.321949562,3.0,3266.0,3.0,64755.333333333336,5187.0,15505.33333,0.0,,0.0,1
pytorch,b600aed237fb984ff03abbfdfa9d4f488fbd7c32,192477b5ba991b5e160d7264230945b49695c0e0,Oleg Bulatov,oleg@bulatov.me,Tue Oct 24 22:43:43 2023 +0000,1698187423.0,"Enable flake8-bugbear B020 lint (#110823)

Fixes part of https://github.com/pytorch/pytorch/issues/106571

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110823
Approved by: https://github.com/Skylion007",70.0,67.0,".flake8,caffe2/python/net_builder_test.py,caffe2/python/schema.py,pyproject.toml,scripts/release_notes/test_release_notes.py,test/distributed/elastic/agent/server/test/local_elastic_agent_test.py,test/distributed/fsdp/test_fsdp_dtensor_state_dict.py,test/distributed/fsdp/test_hsdp_dtensor_state_dict.py,test/functorch/test_vmap.py,test/jit/test_freezing.py,test/nn/test_embedding.py,test/quantization/core/test_workflow_ops.py,test/test_linalg.py,test/test_nn.py,test/torch_np/numpy_tests/lib/test_function_base.py,torch/_inductor/lowering.py,torch/_inductor/triton_heuristics.py,torch/autograd/functional.py,torch/fx/experimental/migrate_gradual_types/constraint_generator.py,torch/nn/parallel/distributed.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py,torch/utils/data/datapipes/_hook_iterator.py",23.0,32,4,4.189626972,51.0,73617.0,22.0,4679058.956521739,21091.0,48124.5,0.0,Corrective,1.0,1
pytorch,7e13138eb6f6fcc6705ee66ca8cddc1432e1fb3e,19367537084a0e9352646947c0f4974b89e2aae5,Brooks,tbrx@users.noreply.github.com,Mon Mar 19 22:22:46 2018 +0000,1521498166.0,Added an implementation of a multivariate normal distribution (#4950),316.0,5.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/constraints.py,torch/distributions/multivariate_normal.py,torch/distributions/utils.py",6.0,5,3,1.730548791,8.0,3933.0,4.0,1355251.0,1000.0,6884.172317,0.0,Feature Addition,0.0,1
pytorch,68b5d9437136fdb8ce87c7b27aca3b8bf1b3cc53,19515520bbb33c0c1fb86dc5ec883c5e7badcaaf,Edward Z. Yang,ezyang@fb.com,Wed Nov 08 05:53:39 2017 +0800,1510120419.0,"Make prelu an ATen op.

This operator is a warmup I was doing before tackling convolution, as it
has many properties that make it a ""first"" for implementing things.  In
particular, it is the first operator whose backwards have multiple
returns; this means its double backwards is the first backwards for a
function with multiple differentiable outputs.  This exercises new code
for output_mask and set_flags.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",128.0,133.0,"tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/VariableType.cpp,torch/nn/_functions/thnn/activation.py,torch/nn/functional.py",6.0,7,2,2.179490655,28.0,4481.0,5.0,216020.16666666663,360.0,2176.0,0.0,,0.0,1
pytorch,a91535930fc39f40cb564f6c03215729af5f98ea,195362d74cce3c11c3f3ada71e4edcdace399c57,Nick Gibson,nickg@fb.com,Wed Apr 08 18:53:07 2020 -0700,1586371987.0,"[TensorExpr] scalar factorization of Div (#36154)

Summary:
Add support for the TensorExpr IR Simplifier to factorize common terms on either side of a Div node. e.g. `(8 * x) / (4 * y) => (2 * x) / y`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36154

Differential Revision: D20910580

Pulled By: nickgg

fbshipit-source-id: ee071d93bc4711b1e710be312de599d18ab506f3",173.0,13.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",4.0,7,2,1.188779942,2.0,3311.0,4.0,413688.5,875.0,2413.0,0.0,Feature Addition,0.0,1
pytorch,8d4c8df33a58dc5f905dfdcee1cd124a79aaf2d8,19701267f36fc966074267af96835a2ee4a05b53,Kulin Seth,kulinseth@gmail.com,Fri May 20 17:43:14 2022 +0000,1653068594.0,"MPS: Add back the memory leak fixes. (#77964)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77964
Approved by: https://github.com/albanD",13.0,13.0,"aten/src/ATen/mps/MPSDevice.mm,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Convolution.mm",3.0,7,1,1.238901257,1.0,1005.0,1.0,11554.0,3493.0,8299.5,0.0,Corrective,1.0,1
pytorch,520982d1df0cf62b464fa4bf5a3ae58327d96d4d,19c675178fb629832b0df5703863b30b1ed30dbb,Iurii Zdebskyi,iuriiz@fb.com,Mon Aug 05 14:37:51 2019 -0700,1565015871.0,"Updated docs and added deprecation warnings to acknowledge a bool tensor (#22261)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22261
ghimport-source-id: 1611d62d056a04c0ad15ef662e594a3d206a78e2

Test Plan: Imported from OSS

Differential Revision: D16005990

Pulled By: izdeby

fbshipit-source-id: 2413824aa75a0755719e4df11acd21e6607e5a85",200.0,168.0,"aten/src/ATen/native/IndexingUtils.h,aten/src/ATen/native/LegacyDefinitions.cpp,aten/src/ATen/native/cuda/LegacyDefinitions.cpp,docs/source/tensors.rst,test/test_indexing.py,test/test_torch.py,tools/pyi/gen_pyi.py,torch/__init__.py,torch/__init__.pyi.in,torch/_tensor_docs.py,torch/_torch_docs.py",11.0,11,5,2.787797998,42.0,25544.0,9.0,1314590.9090909092,10392.0,29693.83333,0.0,Feature Addition,0.0,1
pytorch,a36f11a3a58e9fe026297fe2f0822f8fa706c037,19da1d22fe337e19a98d4cbc711b888e1c90b0ee,Nick Gibson,nickg@fb.com,Thu Oct 08 01:11:16 2020 -0700,1602119476.0,"[NNC] Registerizer V2, supporting partial and conditional replacement (#45574)

Summary:
This is a rewrite of the Registerizer, supporting scalar replacement in *vastly* more situations. As a refresher, the registerizer does this:

Before:
``` A[0] = 0;
for (int x = 0; x < 10; x++) {
  A[0] = (A[0]) + x;
}
```
After:
```
int A_ = 0;
for (int x = 0; x < 10; x++) {
  A_ = x + A_;
}
A[0] = A_;
```

Which can greatly reduce the number of accesses to main memory in a kernel. There are cases where doing this gets complicated, and the existing implementation bails out whenever encountering multiple partial overlaps of the same buffer, or conditional accesses under any circumstances. This makes it much less useful in the presence of complex (ie. real world not example) kernels. This new version should work optimally in almost all cases (I have a few minor follow ups).

I tested this version extensively, and found quite a few bugs in the original implementation I'd prefer not to back port fixes for - so I'm in favor of landing this even if we don't immediately see a perf win. I believe the killer app for this kind of optimization is fused reductions and we haven't enabled many examples of that yet.

It is safe to move two accesses of the same Tensor element to a local scalar Var if between all usages of the element there are no other Loads or Stores that may refer to it. In the comments I refer to this as overlapping the access, or ""cutting"" the existing AccessInfo. In the case where a candidate for registerization is cut, it may be possible to finalize the access early by writing it back to the Tensor and then create a new scalar variable after the overlapping access is complete. We will attempt to do this when it saves memory accesses.

There are a few cases that make this more challenging:

 - For: Loops change the number of real usages of a buffer by the loop extent, but only if we can pull the definition and finalization of the scalar variable out of the loop block. For loops often create accesses which are conditional on a loop var and will overlap large ranges of elements.

E.g. Before:
```
A[0] = 2;
for (int x1 = 0; x1 < 10; x1++) {
  A[0] = (A[0]) + x1;
}
for (int x2 = 1; x2 < 10; x2++) {
  A[x2] = A[x2 - 1];
}
for (int x3 = 0; x3 < 10; x3++) {
  A[0] = (A[0]) + x3;
}
```
After:
```
int A_1 = 2;
for (int x1 = 0; x1 < 10; x1++) {
  A_1 = A_1 + x1;
}
A[0] = A_1;
for (int x2 = 1; x2 < 10; x2++) {
  A[x2] = A[x2 - 1];
}
int A_2 = A[0];
for (int x3 = 0; x3 < 10; x3++) {
  A_2 = A_2 + x3;
}
A[0] = A_2;
```
- Cond: Conditions complicate lifting scalars out of internal scopes. Generally we cannot lift an access outside of a conditional scope unless there is already a reference to that same access at the higher scope, since we don't know if the condition was guarding an array access not safe at the higher scope. In the comments I refer to this as the condition ""hiding"" the access, and the outer access ""unhiding"" it.

E.g. this example:
```
if (x<5 ? 1 : 0) {
  A[x] = (A[x]) + 1;
}
A[x] = (A[x]) + 1;
if (x>5 ? 1 : 0) {
  A[x] = (A[x]) + 1;
}
```
The A[x] access can be registerized due to the unconditional access between the two conditions:
```
int A_1 = A[x];
if (x<5 ? 1 : 0) {
  A_1 = A_1 + 1;
}
A_1 = A_1 + 1;
if (x>5 ? 1 : 0) {
  A_1 = A_1 + 1;
}
A[x] = A_1;
```
But this example has no accesses that can be registerized:
```
if (x<5 ? 1 : 0) {
  A[x] = (A[x]) + 1;
}
if (x>5 ? 1 : 0) {
  A[x] = (A[x]) + 1;
}
```

- IfThenElse: Same situation as Cond, except since IfThenElse is an Expr rather than a Stmt we cannot insert the scalar definition or finalizer within the conditional scope. Accesses inside an IfThenElse can be safely combined with external accesses but cannot exist completely within.

E.g in this example the `B[x]` cannot be registerized as there is no safe place to define it.
```
A[x] = IfThenElse(x<3 ? 1 : 0, (B[x]) + (B[x]), B[x]);
```

But the equivalent kernel using Cond can be registerized:
```
if (x<3 ? 1 : 0) {
  float B_1 = B[x];
  A[x] = B_1 + B_1;
} else {
  A[x] = B[x];
}
```
- Let: Accesses dependent on local variables via Let Stmts, or loop vars, cannot be raised outside of the scope of the dependent var.

E.g. no accesses in this example can be registerized:
```
for (int x = 0; x < 10; x++) {
  int y = 30;
  A[y] = x + (A[y]);
}
```

But they can in this example:
```
int y = 30;
for (int x = 0; x < 10; x++) {
  A[y] = x + (A[y]);
}
```

**Testing**

The majority of this PR is tests, over 3k lines of them, because there are many different rules to consider and they can interact together more or less arbitrarily. I'd greatly appreciate any ideas for situations we could encounter that are not covered by the tests.

**Performance**

Still working on it, will update. In many FastRRNS sub kernels this diff reduces the number of total calls to Store or Load by 4x, but since those kernels use Concat very heavily (meaning a lot of branches) the actual number encountered by any particular thread on GPU is reduced only slightly. Overall perf improved by a very small amount.

Reductions is where this optimization should really shine, and in particular the more complex the kernel gets (with extra fusions, etc) the better this version of the registerizer should do compared the existing version.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45574

Reviewed By: albanD

Differential Revision: D24151517

Pulled By: nickgg

fbshipit-source-id: 9f0b2d98cc213eeea3fda16fee3d144d49fd79ae",4506.0,747.0,"test/cpp/tensorexpr/test_cuda.cpp,test/cpp/tensorexpr/test_registerizer.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/registerizer.cpp,torch/csrc/jit/tensorexpr/registerizer.h,torch/csrc/jit/tensorexpr/stmt.h",7.0,7,2,1.522694412,2.0,6853.0,4.0,1134840.4285714286,5806.0,13520.5,0.0,Corrective,1.0,1
pytorch,4dc13ecdd8fd61f6c053ac3c6236c3346a51aaf2,19f2f1a9d3f6a5fd51704a2e3a042ac800f71ef1,Adam Paszke,adam.paszke@gmail.com,Mon Oct 24 08:34:34 2016 +0200,1477298074.0,Buffer values when constructing a CUDA tensor from a sequence,43.0,7.0,"test/test_cuda.py,torch/csrc/cuda/override_macros.h,torch/csrc/cuda/undef_macros.h,torch/csrc/generic/Tensor.cpp",4.0,5,2,1.340328389,11.0,1316.0,3.0,441228.5,261.0,2326.14599,0.0,,0.0,1
pytorch,73f11a0b23d1deec39b5bcfdcc731722ff78d3cc,1a0b95e7e46a056546a49f7f8b40aaf6ffa11dde,xiaobingsuper,xiaobing.zhang@intel.com,Wed Apr 15 00:15:45 2020 -0700,1586909745.0,"bfloat16: enable basic math function (#35172)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/35172

Test Plan: Imported from OSS

Differential Revision: D20721146

Pulled By: ngimel

fbshipit-source-id: 25b2176d0a431706c51a7086e0642aff814d7148",196.0,38.0,"aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,c10/util/BFloat16-inl.h,c10/util/BFloat16-math.h,test/test_torch.py",8.0,10,3,2.120225149,41.0,19673.0,6.0,5417689.571428572,1069.0,2827.5,0.0,,0.0,1
pytorch,0a1aaff0dea5953928cefc506b4f4d39e0cb8a4d,1a1fb31cfa3135e56c533da037b5d8dc6981b7fa,Anirudh Dagar,anirudhdagar6@gmail.com,Tue Sep 07 06:55:53 2021 -0700,1630997753.0,"Support `torch.concat` alias, add `cat` OpInfo & remove OpInfo test_out skips {cat, stack, hstack, vtack, dstack} (#62560)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/61767

## Changes

- [x] Add `torch.concat` alias to `torch.cat`
- [x] Add OpInfo for `cat`/`concat`
- [x] Fix `test_out` skips (Use `at::native::resize_output` or `at::native::resize_output_check`)
  - [x] `cat`/`concat`
  - [x] `stack`
  - [x] `hstack`
  - [x] `dstack`
  - [x] `vstack`/`row_stack`
- [x] Remove redundant tests for `cat`/`stack`

~I've not added `cat`/`concat` to OpInfo `op_db` yet, since cat is a little more tricky than other OpInfos (should have a lot of tests) and currently there are no OpInfos for that. I can try to add that in a subsequent PR or maybe here itself, whatever is suggested.~
**Edit**: cat/concat OpInfo has been added.

**Note**: I've added the named tensor support for `concat` alias as well, maybe that's out of spec in `array-api` but it is still useful for consistency in PyTorch.

Thanks to krshrimali for guidance on my first PR :))

cc mruberry rgommers pmeier asmeurer leofang AnirudhDagar asi1024 emcastillo kmaehashi heitorschueroff krshrimali

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62560

Reviewed By: saketh-are

Differential Revision: D30762069

Pulled By: mruberry

fbshipit-source-id: 6985159d1d9756238890488a0ab3ae7699d94337",137.0,90.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/Resize.cpp,aten/src/ATen/native/Resize.h,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/Shape.cu,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_autograd.py,test/test_fx_experimental.py,test/test_tensor_creation_ops.py,torch/_torch_docs.py,torch/csrc/jit/passes/normalize_ops.cpp,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",15.0,15,4,2.821466319,44.0,53774.0,15.0,2056239.4,15224.0,34879.5,0.0,Corrective,1.0,1
pytorch,87d7c362b1617d5a7ab72c3c27b4bb11e3371145,1a21c92364ea0527d7e8d3b0c6728e8e768e727e,Spandan Tiwari,sptiwari@microsoft.com,Thu Aug 27 23:43:24 2020 -0700,1598571804.0,"[ONNX] Update in scatter ONNX export when scalar src has different type (#43440)

Summary:
`torch.scatter` allows `src` to be of different type when `src` is a scalar. This requires a an explicit cast op to be inserted in the ONNX graph because ONNX `ScatterElements` does not allow different types. This PR updates the export of `torch.scatter` with this logic.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43440

Reviewed By: hl475

Differential Revision: D23352317

Pulled By: houseroad

fbshipit-source-id: c9eeddeebb67fc3c40ad01def134799ef2b4dea6",24.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.396535423,3.0,7401.0,3.0,376314.3333333333,4597.0,10645.5,0.0,,0.0,1
pytorch,71225ecc8ce802d44929087ee659b97469af55d5,1a2574734222193c344dffb7904def10db18b874,vishwakftw,vishwaks@cs.cmu.edu,Thu Feb 20 20:12:11 2020 -0800,1582229531.0,"Check for consistent devices in at::where (#33432)

Summary:
Changelog:
- Add a check to ensure that all inputs to `where` lie on the same device
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33432

Test Plan:
- Added test_where_invalid_device

Fixes https://github.com/pytorch/pytorch/issues/33422

Differential Revision: D19981115

Pulled By: VitalyFedyunin

fbshipit-source-id: 745896927edb53f61f3dd48ba9e1e6cd10d35434",16.0,1.0,"aten/src/ATen/native/TensorCompare.cpp,test/test_torch.py",2.0,5,2,0.936667382,40.0,15585.0,2.0,405700.5,14866.0,39872.83333,0.0,Corrective,1.0,1
pytorch,3fdf567752b484c5542efea02afc64e4f875eeca,1a2ec10bd4a73f4ebeefeb7a7cc781a80b14794c,Zachary DeVito,zdevito@fb.com,Thu Dec 20 22:26:06 2018 -0800,1545344766.0,"Support enough of closures to write autograd functions (#15411)

Summary:
This PR adds enough of the infra for supporting closures (inner script functions) in order to allow us to expression symbolic gradients using them. We do not actually ever run graphs that contain these closures. The symbolic_script infrastructure just extracts them out of the original forward graph and turns them into discrete forward/backward pairs. This cuts down on the type annotations necessary to write forward/backward pairs and aligns closely with the ""differentiator"" function approach to expression reverse-mode AD.

Example:

This code:
```
import torch

r = torch.jit.CompilationUnit(
'''
def mul_forward(self, other):
    def backward(grad_output):
        grad_self = (grad_output * other).sum_to_size(self.size())
        grad_other = (grad_output * self).sum_to_size(other.size())
        return grad_self, grad_other
    return self * other, backward
''')

print(r.module.code)
```

Will produce this graph (pretty printed for clarity):

```
def mul_forward(self,
    self: Tensor,
    other: Tensor) -> Tuple[Tensor, Tuple[None, Tuple[Tensor, Tensor]]]:
  backward = (self.__lambda, (other, self))
  return (torch.mul(self, other), backward)

def __lambda(self,
    context: Tuple[Tensor, Tensor],
    grad_output: Tensor) -> Tuple[Tensor, Tensor]:
  other, self, = context
  grad_self = torch.sum_to_size(torch.mul(grad_output, other), torch.size(self))
  grad_other = torch.sum_to_size(torch.mul(grad_output, self), torch.size(other))
  return (grad_self, grad_other)
```

symbolic_script will then do some modifications to remove the unsuppored prim::Function node, yielding:

```
def mul_forward(self,
    self: Tensor,
    other: Tensor) -> Tuple[Tensor, Tuple[None, Tuple[Tensor, Tensor]]]:
  return (torch.mul(self, other), (other, self))

def backward(self,
    context: Tuple[Tensor, Tensor],
    grad_output: Tensor) -> Tuple[Tensor, Tensor]:
  other, self, = context
  grad_self = torch.sum_to_size(torch.mul(grad_output, other), torch.size(self))
  grad_other = torch.sum_to_size(torch.mul(grad_output, self), torch.size(other))
  return (grad_self, grad_other)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15411

Differential Revision: D13523340

Pulled By: zdevito

fbshipit-source-id: 4d4a269460e595b16802c00ec55ae00e3e682d49",248.0,112.0,"aten/src/ATen/core/interned_strings.h,test/expect/TestFuser.test_lstm_cuda-backward.expect,test/expect/TestFuser.test_milstm_cuda-backward.expect,test/expect/TestJit.test_cpp_cuda.expect,test/test_jit.py,torch/csrc/jit/passes/alias_analysis.cpp,torch/csrc/jit/passes/python_print.cpp,torch/csrc/jit/script/compiler.cpp,torch/csrc/jit/script/parser.cpp,torch/csrc/jit/script/tree_views.h,torch/csrc/jit/symbolic_script.cpp",11.0,11,3,2.338330727,12.0,17378.0,7.0,231739.54545454544,6171.0,19191.33333,0.0,Feature Addition,0.0,1
pytorch,e9f9fd3727bca299111c6b7b23a5fdbc61832484,1a57979f41e9773ff61aa52553a73d7cac8e6a2c,Adam Paszke,adam.paszke@gmail.com,Thu Aug 11 13:43:41 2016 -0700,1470923021.0,Add cutorch tests,3945.0,3708.0,"test/common.py,test/legacy/nn.py,test/smoke.py,test/test.py,test/test_cuda.py,test/test_legacy_nn.py,test/test_torch.py",7.0,2,1,2.192542094,5.0,3711.0,2.0,245445.0,85.0,1135.933333,0.0,Feature Addition,0.0,1
pytorch,cb4867a71a5944baaf6655bd765652cf37864443,1a87c25fe19f117fd413ec469d7c21ac6ff44a62,Nikita Shulga,nshulga@meta.com,Thu Oct 13 04:25:41 2022 +0000,1665635141.0,"Add functorch shard to sm86-periodic workflow (#86820)

After https://github.com/pytorch/pytorch/pull/86799 was landed there shouldn't be a need to increase tolerances

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86820
Approved by: https://github.com/zou3519",6.0,17.0,".github/workflows/periodic.yml,test/functorch/test_ops.py,test/functorch/test_vmap.py",3.0,4,2,0.912316303,1.0,6564.0,2.0,53481.66666666666,8304.0,19692.0,0.0,Feature Addition,0.0,1
pytorch,295ed7e2647b6c314dd3e604fed9abb9370a3926,1aaa24d99bbb3d8476f7c992aa557ce4f9327d5b,Soumith Chintala,soumith@gmail.com,Tue Jul 04 20:52:36 2017 -0400,1499201556.0,add medianall prototype to docs,20.0,4.0,"torch/_torch_docs.py,torch/csrc/generic/methods/TensorCompare.cwrap",2.0,4,1,0.543564443,28.0,5515.0,1.0,288.0,1096.0,16352.9346,0.0,Feature Addition,0.0,1
pytorch,3a1cb5be106b4ca35d8811f62a223555031c5ad9,1abeb6a54106b2b98c0f7c2dafdb556f0ae5db78,Samantha Andow,samdow@fb.com,Wed Apr 06 18:22:05 2022 -0400,1649269325.0,[functorch] fix multinomial (pytorch/functorch#664),100.0,8.0,"functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/test/test_vmap.py",2.0,4,1,0.908756962,1.0,4526.0,2.0,1.0,941.0,1300.0,0.0,Corrective,1.0,1
pytorch,d04574b1fc73929faa1d6743e0486cda6586fd95,1ae10a48319d8ae5e1e027d7504a7ede1d4c3471,albanD,alban@robots.ox.ac.uk,Mon Oct 30 17:26:06 2017 +0000,1509384366.0,add test to check zero_strided tensors in blas level 2 and 3 functions,98.0,0.0,test/test_torch.py,1.0,1,1,0,38.0,4584.0,1.0,340139.0,2023.0,23888.35823,0.0,Feature Addition,0.0,1
pytorch,bc66d982482e6d6c586d12de5176d9d4bf38eec5,1af1b0c2a5d1dc708f5ff1d2acf69ff186a67a1a,Gregory Chanan,gchanan@fb.com,Fri Jul 27 15:41:27 2018 -0700,1532706087.0,"Remove THTensor::_dim, temporarily remove THTensor_nDimension. (#9895)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9895

The primary goal here was to remove THTensor::_dim, which isn't part of the API moving forward.
Instead, we provide 3 options for getting the dimensionality (this is temporary although non-trivial to remove!):
```
nDimension                 corresponds to the ""true"" ATen dimension. TODO: implement.
nDimensionLegacyNoScalars  correpsonds to the ATen dimension, except scalars are viewed as 1-dimensional tensors.
nDimensionLegacyAll        corresponds to the ATen dimension, except scalars are viewed as 1-dimensional tensors
                           and tensors with a dimension of size zero are collapsed to 0-dimensional tensors.
```
So in this patch, nDimension -> nDimensionLegacyNoScalars and _dim/_nDimension goes to nDimensionLegacyAll.
These are just codemods.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9835

Reviewed By: ezyang

Differential Revision: D8999338

Pulled By: gchanan

fbshipit-source-id: a4d676ac728f6f36ca09604a41e888d545ae9311",468.0,450.0,"aten/src/TH/THTensor.hpp,aten/src/TH/THTensorApply.h,aten/src/TH/THTensorDimApply.h,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensor.h,aten/src/TH/generic/THTensorApply.hpp,aten/src/TH/generic/THTensorConv.cpp,aten/src/TH/generic/THTensorCopy.cpp,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/TH/generic/THTensorRandom.cpp,aten/src/THC/THCApply.cuh,aten/src/THC/THCDeviceTensorUtils-inl.cuh,aten/src/THC/THCDeviceTensorUtils.cuh,aten/src/THC/THCReduce.cuh,aten/src/THC/THCReduceAll.cuh,aten/src/THC/THCReduceApplyUtils.cu,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensor.hpp,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorMathReduce.cuh,aten/src/THC/THCTensorSort.cu,aten/src/THC/THCTensorTypeUtils.cuh,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensor.h,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMathBlas.cu,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorMathScan.cu,aten/src/THC/generic/THCTensorMode.cu,aten/src/THC/generic/THCTensorRandom.cu,aten/src/THC/generic/THCTensorScatterGather.cu,aten/src/THC/generic/THCTensorSort.cu,aten/src/THC/generic/THCTensorTopK.cu,aten/src/THCUNN/common.h,aten/src/THCUNN/generic/BatchNormalization.cu,aten/src/THCUNN/generic/ClassNLLCriterion.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/FeatureLPPooling.cu,aten/src/THCUNN/generic/FusedRNNKernel.cu,aten/src/THCUNN/generic/Im2Col.cu,aten/src/THCUNN/generic/IndexLinear.cu,aten/src/THCUNN/generic/LookupTable.cu,aten/src/THCUNN/generic/LookupTableBag.cu,aten/src/THCUNN/generic/PReLU.cu,aten/src/THCUNN/generic/SparseLinear.cu,aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu,aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu,aten/src/THCUNN/generic/SpatialFractionalMaxPooling.cu,aten/src/THCUNN/generic/SpatialGridSamplerBilinear.cu,aten/src/THCUNN/generic/SpatialReflectionPadding.cu,aten/src/THCUNN/generic/SpatialReplicationPadding.cu,aten/src/THCUNN/generic/SpatialUpSamplingNearest.cu,aten/src/THCUNN/generic/TemporalReflectionPadding.cu,aten/src/THCUNN/generic/TemporalReplicationPadding.cu,aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu,aten/src/THCUNN/generic/VolumetricGridSamplerBilinear.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricReplicationPadding.cu,aten/src/THCUNN/generic/VolumetricUpSamplingNearest.cu,aten/src/THNN/generic/ClassNLLCriterion.c,aten/src/THNN/generic/Col2Im.c,aten/src/THNN/generic/FeatureLPPooling.c,aten/src/THNN/generic/HardTanh.c,aten/src/THNN/generic/Im2Col.c,aten/src/THNN/generic/IndexLinear.c,aten/src/THNN/generic/Linear.c,aten/src/THNN/generic/LookupTable.c,aten/src/THNN/generic/PReLU.c,aten/src/THNN/generic/SpatialClassNLLCriterion.c,aten/src/THNN/generic/SpatialFractionalMaxPooling.c,aten/src/THNN/generic/SpatialUpSamplingNearest.c,aten/src/THNN/generic/Sqrt.c,aten/src/THNN/generic/Square.c,aten/src/THNN/generic/Tanh.c,aten/src/THNN/generic/TemporalUpSamplingNearest.c,aten/src/THNN/generic/VolumetricFractionalMaxPooling.c,aten/src/THNN/generic/VolumetricUpSamplingNearest.c,aten/src/THNN/init.cpp,tools/cwrap/plugins/AssertNDim.py,torch/lib/THD/master_worker/master/generic/THDTensor.cpp,torch/lib/THD/master_worker/master/generic/THDTensor.h,torch/lib/THD/master_worker/master/generic/THDTensorMath.cpp,torch/lib/THD/master_worker/master/generic/THDTensorRandom.cpp",92.0,19,3,5.598373001,27.0,33196.0,26.0,1808538.7717391304,3148.0,8087.333333,0.0,,1.0,1
pytorch,04e55d69f94f0e74bfb2bb0e64e13dbfbf5761d5,1b18adb7e8c8a90fecf12ef81c91bba0e4ca8f97,BowenBao,bowbao@microsoft.com,Tue Aug 04 01:46:58 2020 -0700,1596505618.0,"[ONNX] Export static as_strided (#41569)

Summary:
`as_strided` creates a view of an existing tensor with specified `sizes`, `strides`, and `storage_offsets`. This PR supports the export of `as_strided` with static argument `strides`. The following scenarios will not be supported:
* Calling on tensor of dynamic shape, i.e. the tensor shape differs between model runs and different model inputs.
* In-place operations, i.e. updates to the original tensor that are expected to reflect in the `as_strided` output, and vice versa.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41569

Reviewed By: VitalyFedyunin

Differential Revision: D22845295

Pulled By: bzinodev

fbshipit-source-id: 7d1aa88a810e6728688491478dbf029f17ae7201",43.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.884115122,3.0,6524.0,2.0,322.0,4016.0,9405.0,0.0,,0.0,1
pytorch,ca39c5b04e30a67512589cafbd9d063cc17168a5,1b2ee4d0e11ec818028724b009488c3bbd20541c,soulitzer,soulitzer@gmail.com,Wed Dec 28 00:24:10 2022 -0500,1672187050.0,"Update functorch supported autograd.Function to allow mark_dirty (#91222)

Fixes https://github.com/pytorch/pytorch/issues/90225
Uses what was originally in https://github.com/pytorch/pytorch/pull/89860/commit/32a57bcdb6328c6f46a4908d96d652ab539fdf83

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91222
Approved by: https://github.com/zou3519",102.0,80.0,"test/functorch/test_eager_transforms.py,test/functorch/test_ops.py,test/test_autograd.py,torch/_C/_functorch.pyi,torch/_functorch/autograd_function.py,torch/csrc/autograd/python_function.cpp,torch/testing/_internal/autograd_function_db.py",7.0,9,2,2.185487046,44.0,19284.0,5.0,290326.14285714284,10890.0,24791.0,0.0,Corrective,1.0,1
pytorch,7c2f53b363fde8279454f9b6fe79d84a3cf831a6,1b40daac743d2d732440b71cb4b7c2e6cae08b5d,Nikita Vedeneev,nik@quansight.com,Mon Oct 11 15:28:39 2021 -0700,1633966119.0,"pinv: forward/backward AD which is Frechet-defined in a rank-preserving neighborhood. (#66092)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/65911. Also enables complex support/tests for `linalg_pinv` in OpInfo.

cc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233

Pull Request resolved: https://github.com/pytorch/pytorch/pull/66092

Reviewed By: ejguan

Differential Revision: D31503072

Pulled By: albanD

fbshipit-source-id: 52018e826826ae62beaad76becb5edf880be253f",158.0,1.0,"aten/src/ATen/native/native_functions.yaml,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",6.0,11,3,1.641641047,16.0,29270.0,4.0,525616.8333333334,16136.0,37327.5,0.0,Corrective,1.0,1
pytorch,48d1ad1adad8bc6286993c03a943b46c3120ae11,1b6d18aa7c15428bb282183eb8d9f9889e738198,Igor Gitman,iggitma@microsoft.com,Wed Dec 16 19:19:30 2020 -0800,1608146370.0,"Adding support for CuDNN-based LSTM with projections (#47725)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/46213

I didn't yet update the documentation, will add those change soon. A few other things that I didn't do, but want to clarify if I maybe should.

1. I didn't expose projections in c++ API: torch/csrc/api/src/nn/modules/rnn.cpp. Let me know if this is desirable and I will add those changes.
2. I didn't expose projections in ""lstm_cell"" function and ""_thnn_differentiable_lstm_cell_backward"" functions from aten/src/ATen/native/RNN.cpp. As far as I understand, they are not needed for nn.LSTM CPU execution. For lstm_cell, projections don't bring any real benefit, since if cell is used separately, it can be easily added in Python. For ""_thnn_differentiable_lstm_cell_backward"", I'm actually not sure where exactly that function is used, so I also disabled projections there for now. Please let me know if I should change that.
3. I added check that projections are not supported for quantized LSTMs to quantized_lstm_<data/input> functions. But I didn't add any checks to LSTMCell code. It seems that since I disabled projections in ""lstm_cell"" function, they should also not be available for quantized models through any other API than quantized_lstm_<data/input>. Please let me know if I'm not correct and I will add checks to other places.
4. Projections are not supported for CuDNN versions < 7.1.2. Should I add the check for CuDNN version and disable projections in that case? If so, what will be the best way to do that?
5. Currently I added projection weight as the last weight, so the layout is ""w_ih, w_hh, b_ih, b_hh, w_hr"". This breaks the assumption that biases come after weights and thus I had to add additional if-s in various places. Alternative way would be to have ""w_ih, w_hh, w_hr, b_ih, b_hh"" layout, in which case the assumption will be true. But in that case I will need to split the loop in get_parameters function from aten/src/ATen/native/cudnn/RNN.cpp. And in some cases, I will still need to add an ""undefined"" tensor in the 3rd position, because we get all 5 weights from CuDNN most of the time. So I'm not sure which way is better. Let me know if you think I should change to the weights-then-biases layout.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47725

Reviewed By: zou3519

Differential Revision: D25449794

Pulled By: ngimel

fbshipit-source-id: fe6ce59e481d1f5fd861a8ff7fa13d1affcedb0c",984.0,234.0,"aten/src/ATen/cudnn/AutocastRNN.cpp,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/cudnn/RNNUtils.h,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py,test/cpp/api/rnn.cpp,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/nn/modules/rnn.h,torch/csrc/api/include/torch/nn/options/rnn.h,torch/csrc/api/src/nn/modules/rnn.cpp,torch/nn/modules/rnn.py,torch/onnx/symbolic_opset9.py",17.0,27,4,2.771519168,47.0,43900.0,14.0,8069033.05882353,7547.0,16870.5,0.0,Corrective,1.0,1
pytorch,ddeeb561c388b502f09ba8399d489a7fd17a4c4b,1bb5209f7e6173582f260bcf0dd1720e84e3646a,Edward Yang,ezyang@fb.com,Tue Nov 12 01:31:34 2019 -0800,1573522294.0,"Back out ""Revert D18299298: [pytorch][PR] Migrate conv3d from TH to ATen (CPU)"" (#29286)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29286

Original commit changeset: 33057d5a91d1
ghstack-source-id: 93638554

Test Plan: sandcastle and ossci

Differential Revision: D18349945

fbshipit-source-id: 9d9ddb0c185248a2073ade1063bb69ffbfa48b46",1076.0,818.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/ConvolutionMM3d.cpp,aten/src/ATen/native/LegacyNNDefinitions.cpp,aten/src/ATen/native/Unfold3d.cpp,aten/src/ATen/native/Unfold3d.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/nn.yaml,aten/src/THNN/generic/THNN.h,aten/src/THNN/generic/VolumetricConvolutionMM.c,aten/src/THNN/init.cpp,test/backward_compatibility/check_backward_compatibility.py,tools/autograd/derivatives.yaml",13.0,11,3,2.024916415,14.0,11426.0,4.0,351804.0,13044.0,35959.83333,0.0,,0.0,1
pytorch,e027740e7745bb0843d31337be3a17b805f4f712,1bb609ad47902353018948f4cd04a0aee9542e43,Sean Ross-Ross,srossross@gmail.com,Wed Oct 12 19:22:47 2022 -0500,1665602567.0,"Added new test test_compare_cpu that checks if cpu and gpu results are consistent (#85011)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85011
Approved by: https://github.com/lezcano, https://github.com/mruberry",104.0,13.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/linalg.py",3.0,6,2,1.317229696,7.0,22268.0,3.0,46225.66666666666,8378.0,19999.0,0.0,Feature Addition,0.0,1
pytorch,06ae9b79ed9dacb241817800f29bff906aa5aea9,1be6a070bca41ce19e4adaf28e976351e3127f37,Guilherme Leobas,guilhermeleobas@gmail.com,Wed Dec 20 02:44:49 2023 -0300,1703040289.0,"Add support for torch.cond in vmap (#114523)

Fixes: https://github.com/pytorch/pytorch/issues/114136

Patch enables conversion of a BatchedTensor into FakeTensor and write
torch.cond vmap support using torch.where

Pull Request resolved: https://github.com/pytorch/pytorch/pull/114523
Approved by: https://github.com/zou3519",331.0,96.0,"test/dynamo/test_higher_order_ops.py,test/functorch/test_control_flow.py,test/functorch/test_eager_transforms.py,test/functorch/test_vmap.py,test/test_fake_tensor.py,torch/_dynamo/variables/higher_order_ops.py,torch/_dynamo/variables/tensor.py,torch/_functorch/vmap.py,torch/_higher_order_ops/cond.py,torch/_subclasses/fake_tensor.py,torch/_subclasses/meta_utils.py,torch/fx/passes/shape_prop.py",12.0,11,2,2.8933337,2.0,23259.0,12.0,2364639.25,23289.0,52888.5,0.0,Corrective,1.0,1
pytorch,c43c911662600e3d0a780ce3d23bd9d853ded590,1c01eabd3cc8402e6de815245774278af3ad0c7f,Tongzhou Wang,SsnL@users.noreply.github.com,Wed Apr 18 02:06:54 2018 -0400,1524017214.0,"Codemod to update our codebase to 0.4 standard (#6641)

* Codemod to update our codebase to 0.4 standard

* Update some of the test scri[ts

* remove Variable in test_clip_grad_value

* fix _symbolic_override_wrapper_maker",1312.0,1496.0,"docs/source/nn.rst,test/common.py,test/common_nn.py,test/test_autograd.py,test/test_cuda.py,test/test_distributions.py,test/test_indexing.py,test/test_nn.py,test/test_utils.py,torch/autograd/__init__.py,torch/autograd/_functions/utils.py,torch/autograd/function.py,torch/autograd/gradcheck.py,torch/backends/cudnn/rnn.py,torch/csrc/autograd/python_engine.cpp,torch/distributions/bernoulli.py,torch/distributions/binomial.py,torch/distributions/categorical.py,torch/distributions/constraint_registry.py,torch/distributions/dirichlet.py,torch/distributions/distribution.py,torch/distributions/exp_family.py,torch/distributions/gamma.py,torch/distributions/kl.py,torch/distributions/multinomial.py,torch/distributions/one_hot_categorical.py,torch/distributions/poisson.py,torch/distributions/relaxed_bernoulli.py,torch/distributions/relaxed_categorical.py,torch/distributions/transforms.py,torch/distributions/uniform.py,torch/distributions/utils.py,torch/functional.py,torch/jit/__init__.py,torch/legacy/nn/Bilinear.py,torch/legacy/nn/Index.py,torch/legacy/nn/utils.py,torch/nn/_functions/dropout.py,torch/nn/_functions/rnn.py,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/auto_double_backwards.py,torch/nn/functional.py,torch/nn/grad.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/parallel/data_parallel.py,torch/nn/parallel/parallel_apply.py,torch/nn/parallel/scatter_gather.py,torch/nn/parameter.py,torch/nn/utils/convert_parameters.py,torch/nn/utils/rnn.py,torch/nn/utils/weight_norm.py,torch/onnx/__init__.py,torch/onnx/utils.py,torch/optim/lbfgs.py,torch/optim/optimizer.py,torch/testing/__init__.py,torch/utils/data/dataloader.py,torch/utils/data/sampler.py,torch/utils/ffi/__init__.py,torch/utils/serialization/read_lua_file.py,torch/utils/trainer/trainer.py",65.0,28,3,3.453792881,42.0,36346.0,29.0,1014860.5076923076,903.0,2189.305292,0.0,Corrective,1.0,1
pytorch,d89c719160716e57a834f4f0a5ee65d4e98c69d9,1c2dfdf30cd612bae5857a43ba9d90182c83bf7f,Li-Huai (Allan) Lin,qqaatw@gmail.com,Mon Jun 05 16:29:48 2023 +0000,1685982588.0,"Add renorm forward-ad (#100798)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100798
Approved by: https://github.com/soulitzer",58.0,2.0,"test/functorch/test_ops.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,1.067842887,18.0,33921.0,5.0,1029685.2,16533.0,37346.5,0.0,Feature Addition,0.0,1
pytorch,5c695e3a60e570d67d3eaa0cbab92af4c768fdd6,1c3580b6fef05a11666e98c51c97b83429a9223a,vishwakftw,cs15btech11043@iith.ac.in,Wed Jul 18 00:09:14 2018 -0700,1531872554.0,"Added hash for device (#9246)

Summary:
If this is good, I could write some tests to ensure collision doesn't occur within a given range.

Closes #7228
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9246

Differential Revision: D8872608

Pulled By: ezyang

fbshipit-source-id: 0ed29a73188f4167b42756f59a5c9a3d5cb37326",30.0,1.0,"aten/src/ATen/Device.h,test/test_torch.py,torch/csrc/Device.cpp",3.0,6,3,1.506890126,40.0,8529.0,2.0,1829972.3333333333,2977.0,6998.833333,0.0,Feature Addition,0.0,1
pytorch,e03e4f3a2ddb280462cceeb6a56347dbfe6e6f77,1c42b9466b9ff35acc2cd70538f4db621ac1384d,BowenBao,bowbao@microsoft.com,Mon Feb 03 20:56:29 2020 -0800,1580763389.0,"[ONNX] Update support of exporting bool type index mask (#32445)

Summary:
e.g. `tensor[torch.tensor([0, 1, 0], dtype=torch.bool)]`
Previously the mask is of type uint8. Both uint8 and bool should be supported for export.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32445

Reviewed By: hl475

Differential Revision: D19610713

Pulled By: houseroad

fbshipit-source-id: 8df636e0c3cb0b82919a689242a962c79220209c",8.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.99107606,2.0,5001.0,2.0,206156.0,14558.0,39350.33333,0.0,,0.0,1
pytorch,61813cfd977b366bfa964d987d5066a3906879b8,1c51c185a1152ae7efe7d8d490102a9cdb7782e7,peterjc123,peter_jiachen@163.com,Sun Sep 17 03:50:15 2017 +0800,1505620215.0,Improve Windows Compatibility(for lib/THC) (#2440),843.0,831.0,"torch/lib/THC/CMakeLists.txt,torch/lib/THC/THCAllocator.c,torch/lib/THC/THCAllocator.h,torch/lib/THC/THCApply.cuh,torch/lib/THC/THCAsmUtils.cuh,torch/lib/THC/THCAtomics.cuh,torch/lib/THC/THCBlas.cu,torch/lib/THC/THCBlas.h,torch/lib/THC/THCCachingAllocator.h,torch/lib/THC/THCDeviceTensor-inl.cuh,torch/lib/THC/THCDeviceTensor.cuh,torch/lib/THC/THCGenerateByteType.h,torch/lib/THC/THCGenerateCharType.h,torch/lib/THC/THCGenerateIntType.h,torch/lib/THC/THCGenerateLongType.h,torch/lib/THC/THCGenerateShortType.h,torch/lib/THC/THCNumerics.cuh,torch/lib/THC/THCReduce.cuh,torch/lib/THC/THCReduceAll.cuh,torch/lib/THC/THCReduceApplyUtils.cu,torch/lib/THC/THCSleep.cu,torch/lib/THC/THCSleep.h,torch/lib/THC/THCSortUtils.cu,torch/lib/THC/THCSortUtils.cuh,torch/lib/THC/THCTensorConv.cu,torch/lib/THC/THCTensorConv.h,torch/lib/THC/THCTensorIndex.cu,torch/lib/THC/THCTensorInfo.cuh,torch/lib/THC/THCTensorMath.cu,torch/lib/THC/THCTensorMathPairwise.cu,torch/lib/THC/THCTensorMathPointwise.cuh,torch/lib/THC/THCTensorMathReduce.cuh,torch/lib/THC/THCTensorMode.cuh,torch/lib/THC/THCTensorRandom.cpp,torch/lib/THC/THCTensorRandom.cu,torch/lib/THC/THCTensorRandom.cuh,torch/lib/THC/THCTensorRandom.h,torch/lib/THC/THCTensorScatterGather.cu,torch/lib/THC/THCTensorSort.cu,torch/lib/THC/THCTensorSort.cuh,torch/lib/THC/THCTensorTopK.cuh,torch/lib/THC/THCTensorTypeUtils.cu,torch/lib/THC/THCTensorTypeUtils.cuh,torch/lib/THC/generic/THCDeviceTensorUtils.cu,torch/lib/THC/generic/THCTensor.c,torch/lib/THC/generic/THCTensor.h,torch/lib/THC/generic/THCTensorIndex.cu,torch/lib/THC/generic/THCTensorMasked.cu,torch/lib/THC/generic/THCTensorMath.cu,torch/lib/THC/generic/THCTensorMath.h,torch/lib/THC/generic/THCTensorMathBlas.cu,torch/lib/THC/generic/THCTensorMathMagma.cu,torch/lib/THC/generic/THCTensorMathPairwise.cu,torch/lib/THC/generic/THCTensorMathPointwise.cu,torch/lib/THC/generic/THCTensorMathReduce.cu,torch/lib/THC/generic/THCTensorMathReduce.h,torch/lib/THC/generic/THCTensorMathScan.cu,torch/lib/THC/generic/THCTensorMathScan.h,torch/lib/THC/generic/THCTensorMode.cu,torch/lib/THC/generic/THCTensorRandom.cu,torch/lib/THC/generic/THCTensorScatterGather.cu,torch/lib/THC/generic/THCTensorSort.cu,torch/lib/THC/generic/THCTensorTopK.cu,torch/lib/THC/generic/THCTensorTopK.h",64.0,4,1,5.132002608,37.0,18127.0,1.0,281524.0,1735.0,22399.05562,0.0,Perfective,0.0,1
pytorch,6b3f335641e1874c5b0ae0aa719687ed5f42ab45,1c5c289b6218eb1026dcb5fd9738231401cfccea,Jianyu Huang,jianyuhuang@fb.com,Wed Jul 29 08:14:44 2020 -0700,1596010484.0,"[pt] Add incude_last_offset option to EmbeddingBag mean and max (#42215)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42215

Specifically on https://github.com/pytorch/pytorch/pull/27477#discussion_r371402079

We would like to supported with include_last=True overall for other reduction types like mean and max. It now causes further code fragmentation in DPER (https://www.internalfb.com/intern/diff/D22794469/).

More details: https://www.internalfb.com/intern/diff/D22794469/?dest_fbid=309597093427021&transaction_id=631457624153457

ghstack-source-id: 108733009

Test Plan:
```
buck test mode/dev-nosan //caffe2/test:nn -- ""test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu""
```

```
(base) [jianyuhuang@devbig281.ftw3.facebook.com: ~/fbsource/fbcode/caffe2/test] $ TORCH_SHOW_CPP_STACKTRACES=1 buck test mode/dev-nosan //caffe2/test:
nn -- ""test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu"" --print-passing-details
Parsing buck files: finished in 1.2 sec
Building: finished in 5.5 sec (100%) 10130/10130 jobs, 2 updated
  Total time: 6.7 sec
More details at https://www.internalfb.com/intern/buck/build/dbdc2063-69d8-45cb-9146-308a9e8505ef
First unknown argument: --print-passing-details.
Falling back to TestPilot classic.
Trace available for this run at /tmp/testpilot.20200728-195414.1422748.log
TestPilot test runner for Facebook. See https://fburl.com/testpilot for details.
Testpilot build revision cd2638f1f47250eac058b8c36561760027d16add fbpkg f88726c8ebde4ba288e1172a348c7f46 at Mon Jul 27 18:11:43 2020 by twsvcscm from /usr/local/fbprojects/packages/testinfra.testpilot/887/t.par
Discovering tests
Running 1 test
Started new test run: https://our.intern.facebook.com/intern/testinfra/testrun/844425097242375
      â caffe2/test:nn - test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu (test_nn.TestNNDeviceTypeCPU) 0.162 1/1 (passed)
Test output:
> /data/users/jianyuhuang/fbsource/fbcode/buck-out/dev/gen/caffe2/test/nn#binary,link-tree/torch/_utils_internal.py:103: DeprecationWarning: This is a NOOP in python >= 3.7, its just too dangerous with how we write code at facebook. Instead we patch os.fork and multiprocessing which can raise exceptions if a deadlock would happen.
>   threadSafeForkRegisterAtFork()
> /usr/local/fbcode/platform007/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__
and __path__
>   return f(*args, **kwds)
> test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu (test_nn.TestNNDeviceTypeCPU) ... Couldn't download test skip set, leaving all tests enabled...
> ok
>
> ----------------------------------------------------------------------
> Ran 1 test in 0.162s
>
> OK
Finished test run: https://our.intern.facebook.com/intern/testinfra/testrun/844425097242375
Summary (total time 5.54s):
  PASS: 1
  FAIL: 0
  SKIP: 0
  FATAL: 0
  TIMEOUT: 0
  OMIT: 0
Did _not_ run with tpx. See https://fburl.com/tpx for details.
```

Reviewed By: dzhulgakov

Differential Revision: D22801881

fbshipit-source-id: 80a624465727081bb9bf55c28419695a3d79c6e5",102.0,48.0,"aten/src/ATen/native/EmbeddingBag.cpp,test/test_nn.py,torch/csrc/api/include/torch/nn/options/embedding.h,torch/nn/modules/sparse.py",4.0,14,3,1.235736277,43.0,13374.0,4.0,4043927.75,3913.0,9215.5,0.0,Feature Addition,1.0,1
pytorch,1dbf44c00d4f77e72fcdf48ecf7628c3bcac4f96,1c6ff53b60610b03a515918819fd3f1f3f5c7142,Adam Paszke,adam.paszke@gmail.com,Sun Jan 15 19:37:54 2017 +0100,1484509074.0,Make storages unresizable once exported to numpy,17.0,0.0,"test/test_torch.py,torch/csrc/generic/methods/TensorSerialization.cwrap",2.0,5,2,0.672294817,21.0,2877.0,1.0,237653.0,334.0,4816.976424,0.0,,0.0,1
pytorch,122798916f6e533fd9dcb4a52c49c67b313943e7,1c776d209c0dfc43b04065d6ae1bfd275d8e6364,Natalia Gimelshein,ngimel@fb.com,Thu May 05 15:53:07 2022 +0000,1651765987.0,"Adds amax and amin references

Also extends reference testing to error inputs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76855
Approved by: https://github.com/mruberry",91.0,29.0,"test/test_ops.py,torch/_prims/utils.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",4.0,6,2,1.219047544,5.0,21234.0,4.0,72612.0,2897.0,6960.5,0.0,Feature Addition,0.0,1
pytorch,9ea576d06839b7c888865921c2bd66db1fbc3648,1c96809cf8b6ae04399858e9814fe614cca166db,gchanan,gregchanan@gmail.com,Thu Dec 07 22:34:51 2017 -0800,1512686091.0,"Bind cauchy_, exponential_, normal_, uniform_ functions to THPVariable. (#3945)

* Bind cauchy_, exponential_, normal_, uniform_ functions to THPVariable.

Also changes the error messages around Generator parser; previously, you'd get an error
like: torch._C.Generator is not a torch.Generator; now the check is proper but returns
that only None is supported.

* Support passing Generators to ATen Variable-bound methods.

This involves changing THPGenerator to have an at::Generator rather than a THGenerator.
TH getRNGState, setRNGState are still called directly because they are not bound from ATen yet;
they should probably be on the Generators and return (opaque) GenerateState objects.

* Fix default values.

* Properly use THRandom_initialSeed.

* update standard gamma to use new default generator.",94.0,68.0,"aten/src/ATen/CPUGenerator.cpp,aten/src/ATen/CUDAGenerator.cpp,aten/src/ATen/Declarations.cwrap,aten/src/ATen/Generator.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/templates/GeneratorDerived.h,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/cwrap/plugins/StandaloneExtension.py,tools/cwrap/plugins/THPPlugin.py,torch/csrc/Generator.cpp,torch/csrc/Generator.h,torch/csrc/Module.cpp,torch/csrc/generic/StorageSharing.cpp,torch/csrc/generic/methods/TensorRandom.cwrap,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h",17.0,13,3,3.119438688,39.0,10495.0,11.0,1487304.0588235294,367.0,1107.405869,0.0,Corrective,1.0,1
pytorch,34d7bf6865bed674888cbecaaf232b5b23e88d8f,1cc3add0e7bc6221aecc3fc2022676481d203d03,Richard Zou,zou3519@users.noreply.github.com,Fri Apr 01 02:22:52 2022 -0400,1648779772.0,[functorch] Update functorch lagging op db (pytorch/functorch#652),123.0,11.0,"functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py,functorch/test/xfail_suggester.py",5.0,2,1,1.816624367,1.0,6792.0,3.0,0.0,924.0,1286.5,0.0,,0.0,1
pytorch,f8f30a5e27e92e55b2e3ef72e4544ab1ad1bc652,1ccaec02384db7d1916740fef4775381563de62a,Mikhail Zolotukhin,mvz@fb.com,Fri Apr 02 02:44:04 2021 -0700,1617331444.0,"[TensorExpr] Cleanup IRNodeType enum. (#55001)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55001

The enum is only used for precedence computation thus we only need to
enum node-types for which we know the precedence priority.

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D27446410

Pulled By: ZolotukhinM

fbshipit-source-id: 217dd63c4fd086155030ebf0c3e1772605109f7b",7.0,17.0,"torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/ir.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",4.0,4,1,1.680155967,2.0,4173.0,4.0,1867007.75,10349.0,22868.0,0.0,,0.0,1
pytorch,d1f48f05cef9e2b3b01c64a21a6e2abc3ddab323,1cd6ebe0958ab8eff2b7ba715d9544f067dfe59e,Kazuaki Ishizaki,ishizaki@jp.ibm.com,Thu Nov 17 04:18:10 2022 +0000,1668658690.0,"Fix typos in messages under torch (#89049)

This PR fixes typos of messages in `.py` files under torch directory.
Only in `torch/onnx/symbolic_opset16.py`, fix a typo in comment to make the operator name correct.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89049
Approved by: https://github.com/lezcano",28.0,28.0,"torch/_refs/nn/functional/__init__.py,torch/ao/nn/intrinsic/qat/modules/linear_fused.py,torch/ao/quantization/fx/_model_report/model_report.py,torch/ao/quantization/observer.py,torch/backends/xeon/run_cpu.py,torch/cuda/memory.py,torch/distributed/benchmarks/benchmark_ddp_rpc.py,torch/distributed/elastic/multiprocessing/api.py,torch/distributed/elastic/rendezvous/etcd_rendezvous.py,torch/distributions/mixture_same_family.py,torch/fx/experimental/accelerator_partitioner.py,torch/fx/experimental/graph_gradual_typechecker.py,torch/fx/passes/split_module.py,torch/jit/frontend.py,torch/nn/utils/parametrize.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset16.py,torch/profiler/_pattern_matcher.py,torch/serialization.py,torch/testing/_internal/common_distributed.py,torch/testing/_internal/composite_compliance.py,torch/utils/benchmark/examples/fuzzer.py,torch/utils/benchmark/examples/sparse/fuzzer.py,torch/utils/data/datapipes/dataframe/dataframes.py,torch/utils/data/datapipes/iter/callable.py",25.0,39,1,4.593069208,44.0,17435.0,23.0,14304992.44,9612.0,22371.0,0.0,Corrective,1.0,1
pytorch,4047c972669238ac62073a5afe47be14e1cb48be,1ce188c510a160cac37fb08eb45c8ebcd157075a,Brennan Vincent,btv@fb.com,Tue Feb 05 16:27:04 2019 -0800,1549384024.0,"logsumexp for multiple dimensions (#16475)

Summary:
Move `logsumexp` and `max_values` to `TensorIterator` and use it to make `logsumexp` work for multiple dimensions.

Timings on a tensor of shape `(10,1000000,10)`, for each combination of (cpu, single-threaded cpu, gpu) and dimension:

**before**
208 ms Â± 2.72 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
279 ms Â± 5.07 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
199 ms Â± 2.64 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.11 s Â± 33.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.25 s Â± 25.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.11 s Â± 6.83 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
15.4 ms Â± 1.02 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
132 ms Â± 30.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
39.6 ms Â± 19.1 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

**after**
199 ms Â± 8.23 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
307 ms Â± 8.73 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
207 ms Â± 7.62 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
1.16 s Â± 8.92 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.26 s Â± 47.6 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.13 s Â± 13.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
15.4 ms Â± 868 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
132 ms Â± 27.6 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
39.6 ms Â± 21.8 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16475

Differential Revision: D13855746

Pulled By: umanwizard

fbshipit-source-id: aaacc0b967c3f89073487e1952ae6f76b7bd7ad3",225.0,90.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOps.h,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cuda/ReduceOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_torch_docs.py,torch/csrc/jit/passes/shape_analysis.cpp",15.0,17,4,3.049181625,42.0,30845.0,9.0,812537.8666666667,6826.0,21100.33333,0.0,,0.0,1
pytorch,8f64f918f7b48547f24e650e4c6f2d189ebb7ad4,1ce5431aaf5318e3f707961337e17924515d02e3,Thomas Viehmann,tv.github@beamnet.de,Sun May 13 19:44:24 2018 +0200,1526240664.0,"Documentation improvements (#7537)

- improve scatter documentation (fixes #7518)
- refine KLDivLoss documentation (fixes #7464)
- fix some sphinxbuild warnings

Thank you, Hugh Perkins for reporting!",18.0,16.0,"docs/source/nn.rst,torch/_tensor_docs.py,torch/nn/modules/loss.py",3.0,5,2,1.260771794,37.0,4723.0,3.0,802467.6666666666,645.0,3611.0,0.0,Corrective,1.0,1
pytorch,d8e5f2aa6d00f4e734e180a179d5096ab0890df7,1cebfef8a4ab939c32491f950df3f25ccd753109,Fuzzkatt,zonghan2000@gmail.com,Mon Jul 31 17:59:40 2023 +0000,1690826380.0,"sm90 efficient attention test fixes (#105978)

Fixes the following two test cases involving efficient attention on sm90:

Explanations:

functorch/test_ops.py: test_vjp_nn_functional_scaled_dot_product_attention_cuda_float32
* originally the test had xfail for all sm
* in https://github.com/pytorch/pytorch/issues/102029, we found that it was unexpectedly passing on sm90
* I made https://github.com/pytorch/pytorch/pull/102131 to update the test to let it pass
* @drisspg seems to have made changes to the behavior such that the original xfail was getting triggered (https://github.com/pytorch/pytorch/issues/102029#issuecomment-1560071148)
* the CI began complaining about the failure again: https://github.com/pytorch/pytorch/issues/102663
* I'm now reverting https://github.com/pytorch/pytorch/pull/102131 to bring back the original xfail now that the behavior has been fixed by @drisspg to trigger the xfail in sm90 similar to all other sm

test_transformers.py: test_mem_efficient_fail_sm90_cuda
* the test as it's currently written seems to expect the sdp dispatcher to fail for mem efficient attention on sm90; however, testing this on H100, it actually succeeds, so I'm disabling the test for now as the current expected result may be outdated

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105978
Approved by: https://github.com/eqy, https://github.com/kshitij12345, https://github.com/zou3519",2.0,16.0,"test/functorch/test_ops.py,test/test_transformers.py,torch/testing/_internal/common_cuda.py",3.0,5,2,1.134969975,5.0,5100.0,3.0,1056531.3333333333,18190.0,41349.0,0.0,Corrective,1.0,1
pytorch,d2336edcfb13c458549b104e8a3c2e827efde76e,1d3f3a1a0cc0f66b51d8e8f4006fb200e18e2818,Pieter Noordhuis,pietern@fb.com,Wed Nov 27 12:44:37 2019 -0800,1574858677.0,"Add pybind11 trampoline class for c10d.Store (#30415)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30415

This enables subclassing of c10d.Store and implementing its interface in Python.
ghstack-source-id: 94586627

Test Plan: New tests passes.

Reviewed By: vladbelous

Differential Revision: D18693018

fbshipit-source-id: fa1eba4bd11cc09a3d6bf3f35369c885033c63c0",152.0,1.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp",2.0,5,2,0.828908453,3.0,3879.0,2.0,1599921.5,13521.0,36970.83333,0.0,Feature Addition,0.0,1
pytorch,6b085d5cadffb10591c450623f93a21dd3dd786d,1d6a188d08829b1aee28eb1e6255d5bf43a77f16,lezcano,lezcano-93@hotmail.com,Sat Nov 19 01:00:03 2022 +0000,1668819603.0,"Reland Dispatch torch.norm to linalg.vector_norm and linalg.matrix_norm (#81761) (#84624)

Reland https://github.com/pytorch/pytorch/pull/81761

Differential Revision: [D39332292](https://our.internmc.facebook.com/intern/diff/D39332292)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84624
Approved by: https://github.com/kit1980",100.0,62.0,"aten/src/ATen/autocast_mode.cpp,aten/src/ATen/functorch/BatchRulesDecompositions.cpp,aten/src/ATen/functorch/BatchRulesReduceOps.cpp,aten/src/ATen/native/LinearAlgebra.cpp,test/functorch/test_vmap.py,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_utility_funs.py,test/test_decomp.py,test/test_linalg.py,test/test_reductions.py,torch/functional.py,torch/testing/_internal/common_methods_invocations.py",13.0,11,3,2.788882937,35.0,57137.0,12.0,1044535.6153846154,9763.0,22612.0,0.0,,1.0,1
pytorch,66b8cb95e942eafcc40a774c72f8f77679ca0f96,1dbbef6b4823b3eee441102fb250849e8efaec9d,Ilia Cherniavskii,iliacher@fb.com,Wed Oct 11 01:53:13 2017 -0700,1507686793.0,"Fix crash in blob deallocation

Summary: We have to use copy constructor in Concat when copying non-primitive types

Reviewed By: Yangqing

Differential Revision: D6002883

fbshipit-source-id: 0aebc955079975bb6423291589ed09ce0660acf3",77.0,19.0,"caffe2/operators/channel_shuffle_op.h,caffe2/operators/concat_split_op.h,caffe2/operators/prepend_dim_op.h,caffe2/operators/reshape_op.h,caffe2/python/test/blob_deallocation_test.py,caffe2/utils/math.h,caffe2/utils/math_cpu.cc,caffe2/utils/math_gpu.cu",8.0,5,1,2.651937876,5.0,4728.0,1.0,1046461.0,2097.0,5212.833333,0.0,Corrective,1.0,1
pytorch,a2db5d34a53e5c61042f1d152a975d1e2efb3a0b,1dc2b52764a81288cb0e31bb7a72fe8c4cc826a1,Mikhail Zolotukhin,mvz@fb.com,Tue Aug 17 20:39:36 2021 -0700,1629232776.0,"[TensorExpr] Add a wrapper for all expr and stmt pointers. (#63195)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63195

This helps us to later switch from using KernelArena with raw pointers
to shared pointers without having to change all our source files at
once.

The changes are mechanical and should not affect any functionality.

With this PR, we're changing the following:
 * `Add*` --> `AddPtr`
 * `new Add(...)` --> `alloc<Add>(...)`
 * `dynamic_cast<Add*>` --> `to<Add>`
 * `static_cast<Add*>` --> `static_to<Add>`

Due to some complications with args forwarding, some places became more
verbose, e.g.:
 * `new Block({})` --> `new Block(std::vector<ExprPtr>())`

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D30292779

Pulled By: ZolotukhinM

fbshipit-source-id: 150301c7d2df56b608b035827b6a9a87f5e2d9e9",4972.0,4859.0,"test/cpp/tensorexpr/test_approx.cpp,test/cpp/tensorexpr/test_aten.cpp,test/cpp/tensorexpr/test_boundsinference.cpp,test/cpp/tensorexpr/test_conv.cpp,test/cpp/tensorexpr/test_cpp_codegen.cpp,test/cpp/tensorexpr/test_cuda.cpp,test/cpp/tensorexpr/test_expr.cpp,test/cpp/tensorexpr/test_external_calls.cpp,test/cpp/tensorexpr/test_ir_printer.cpp,test/cpp/tensorexpr/test_ir_verifier.cpp,test/cpp/tensorexpr/test_kernel.cpp,test/cpp/tensorexpr/test_llvm.cpp,test/cpp/tensorexpr/test_loopnest.cpp,test/cpp/tensorexpr/test_memdependency.cpp,test/cpp/tensorexpr/test_reductions.cpp,test/cpp/tensorexpr/test_registerizer.cpp,test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/test_utils.h,test/cpp/tensorexpr/tutorial.cpp,torch/csrc/jit/runtime/static/ops.cpp,torch/csrc/jit/tensorexpr/analysis.h,torch/csrc/jit/tensorexpr/block_codegen.cpp,torch/csrc/jit/tensorexpr/block_codegen.h,torch/csrc/jit/tensorexpr/bounds_inference.cpp,torch/csrc/jit/tensorexpr/bounds_inference.h,torch/csrc/jit/tensorexpr/bounds_overlap.cpp,torch/csrc/jit/tensorexpr/bounds_overlap.h,torch/csrc/jit/tensorexpr/codegen.cpp,torch/csrc/jit/tensorexpr/codegen.h,torch/csrc/jit/tensorexpr/cpp_codegen.cpp,torch/csrc/jit/tensorexpr/cpp_codegen.h,torch/csrc/jit/tensorexpr/cuda_codegen.cpp,torch/csrc/jit/tensorexpr/cuda_codegen.h,torch/csrc/jit/tensorexpr/eval.cpp,torch/csrc/jit/tensorexpr/eval.h,torch/csrc/jit/tensorexpr/exceptions.h,torch/csrc/jit/tensorexpr/expr.cpp,torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/fwd_decls.h,torch/csrc/jit/tensorexpr/half_support.h,torch/csrc/jit/tensorexpr/hash_provider.cpp,torch/csrc/jit/tensorexpr/hash_provider.h,torch/csrc/jit/tensorexpr/ir.cpp,torch/csrc/jit/tensorexpr/ir.h,torch/csrc/jit/tensorexpr/ir_cloner.cpp,torch/csrc/jit/tensorexpr/ir_cloner.h,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_mutator.h,torch/csrc/jit/tensorexpr/ir_printer.cpp,torch/csrc/jit/tensorexpr/ir_printer.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/ir_verifier.cpp,torch/csrc/jit/tensorexpr/ir_verifier.h,torch/csrc/jit/tensorexpr/ir_visitor.cpp,torch/csrc/jit/tensorexpr/ir_visitor.h,torch/csrc/jit/tensorexpr/kernel.cpp,torch/csrc/jit/tensorexpr/kernel.h,torch/csrc/jit/tensorexpr/llvm_codegen.cpp,torch/csrc/jit/tensorexpr/llvm_codegen.h,torch/csrc/jit/tensorexpr/loopnest.cpp,torch/csrc/jit/tensorexpr/loopnest.h,torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp,torch/csrc/jit/tensorexpr/mem_dependency_checker.h,torch/csrc/jit/tensorexpr/operators/conv2d.cpp,torch/csrc/jit/tensorexpr/operators/matmul.cpp,torch/csrc/jit/tensorexpr/operators/softmax.cpp,torch/csrc/jit/tensorexpr/reduction.cpp,torch/csrc/jit/tensorexpr/reduction.h,torch/csrc/jit/tensorexpr/registerizer.cpp,torch/csrc/jit/tensorexpr/registerizer.h,torch/csrc/jit/tensorexpr/stmt.h,torch/csrc/jit/tensorexpr/tensor.cpp,torch/csrc/jit/tensorexpr/tensor.h,torch/csrc/jit/tensorexpr/tensorexpr_init.cpp,torch/csrc/jit/tensorexpr/unique_name_manager.cpp,torch/csrc/jit/tensorexpr/unique_name_manager.h,torch/csrc/jit/tensorexpr/var_substitutor.h",78.0,10,2,5.225278197,2.0,61904.0,14.0,1451606.4285714286,14701.0,33707.5,0.0,Feature Addition,0.0,1
pytorch,7d4e9bdba144e162882fb854324430c4b92fb267,1e03a2505f9cee92587bf45fbbbbfedede5cb9ec,mingfeima,mingfei.ma@intel.com,Tue Apr 20 22:00:55 2021 -0700,1618956055.0,"add channels last for MaxPool2d (#56361)

Summary:
add channels last support for MaxPool2d.
this one is a replacement of https://github.com/pytorch/pytorch/pull/48917

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56361

Reviewed By: heitorschueroff

Differential Revision: D27874142

Pulled By: VitalyFedyunin

fbshipit-source-id: bc9604def9c974d7b59621fc709a39948088b992",465.0,291.0,"aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/native/DilatedMaxPool2d.cpp,aten/src/ATen/native/Pool.h,aten/src/ATen/native/cpu/MaxPoolKernel.cpp,aten/src/ATen/test/vec256_test_all_types.cpp,test/test_nn.py,tools/build_variables.bzl",10.0,10,3,1.594291592,43.0,21865.0,5.0,746155.7,11044.0,24366.5,0.0,Feature Addition,0.0,1
pytorch,3710edc86b6a68a5b77c977f33c8584325accf1f,1e25a8499362c51b803af52c68cac5cc5acd836b,Natalia Gimelshein,ngimel@fb.com,Mon Sep 13 23:31:07 2021 -0700,1631575867.0,"kill SkipInfo (#64878)

Summary:
Per offline discussion, replaces SkipInfo with DecorateInfo. SkipInfo class itself is not removed yet to give functorch time to replace its SkipInfos.
cc zou3519

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64878

Reviewed By: mruberry

Differential Revision: D30908052

Pulled By: ngimel

fbshipit-source-id: 5124180b25c6e32517722883b9f3a2b488e3fe20",528.0,522.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,9889.0,1.0,30739.0,15404.0,35221.5,0.0,,0.0,1
pytorch,c6271c63f21cd886ded73787e53844588cff5e74,1e76649d30cb8600fefadb6ddec2801d3111edc5,lixinyu,lixinyu@devgpu175.prn2.facebook.com,Sat Feb 15 04:32:49 2020 -0800,1581741169.0,"fast setup for output tensor in tensor iterator (#33165)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33165

Test Plan: Imported from OSS

Differential Revision: D19825853

Pulled By: glaringlee

fbshipit-source-id: 8f908f2e93a4e377306a77e8a771208603b20e72",168.0,26.0,"aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,test/test_torch.py",3.0,5,2,1.119461997,40.0,16572.0,2.0,908928.3333333334,14778.0,39699.33333,0.0,,0.0,1
pytorch,73bdb661feb195a8b98366db5750b998c025f709,1e76ade9dc144d797a0daf2fa9190822a14eb390,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Thu Jan 04 21:57:47 2018 +0500,1515103067.0,Implementation of Pareto Distribution (#4459),150.0,44.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/cauchy.py,torch/distributions/pareto.py",5.0,5,3,1.124778958,8.0,1208.0,3.0,309261.0,894.0,6687.172317,0.0,,0.0,1
pytorch,0282c5ae6996584b908946cb694b02443dfd3c9a,1e8ed021c6bbb74be3af23278c39ecdf68e57541,Bowen Bao,semisqg@gmail.com,Wed Nov 27 16:30:24 2019 -0800,1574872224.0,"Support logsoftmax with dim != -1 (#30433)

Summary:
PyTorch dim and ONNX axis have different meanings.
ONNX only supports log_softmax with dim = -1. Transpose must be added before and after log_softmax to support other cases.
This requires input rank to be known at export time.
Fixes https://github.com/pytorch/pytorch/issues/17918
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30433

Reviewed By: hl475

Differential Revision: D18723520

Pulled By: houseroad

fbshipit-source-id: d0ed3b3f051d08d46495a7abfa854edd120dca3a",32.0,3.0,"test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.483126122,2.0,6819.0,2.0,30822.666666666668,13523.0,36974.83333,0.0,Corrective,1.0,1
pytorch,12bed8dc0d8e08ff4c68cce77504474c8c78e5f0,1e905eb4d527154d7e416139d72c09421c8ed0f4,Adam Paszke,adam.paszke@gmail.com,Fri Aug 12 16:26:33 2016 -0700,1471019193.0,copy -> copy_,218.0,218.0,"test/common.py,test/test_cuda.py,test/test_legacy_nn.py,test/test_torch.py,torch/Storage.py,torch/Tensor.py,torch/TensorPrinting.py,torch/cuda/__init__.py,torch/legacy/nn/Add.py,torch/legacy/nn/AddConstant.py,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/CAddTable.py,torch/legacy/nn/CDivTable.py,torch/legacy/nn/CMul.py,torch/legacy/nn/CMulTable.py,torch/legacy/nn/CSubTable.py,torch/legacy/nn/ClassSimplexCriterion.py,torch/legacy/nn/Concat.py,torch/legacy/nn/ConcatTable.py,torch/legacy/nn/Contiguous.py,torch/legacy/nn/Copy.py,torch/legacy/nn/Cosine.py,torch/legacy/nn/CosineDistance.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/DepthConcat.py,torch/legacy/nn/DotProduct.py,torch/legacy/nn/Dropout.py,torch/legacy/nn/Euclidean.py,torch/legacy/nn/GradientReversal.py,torch/legacy/nn/HingeEmbeddingCriterion.py,torch/legacy/nn/JoinTable.py,torch/legacy/nn/L1HingeEmbeddingCriterion.py,torch/legacy/nn/L1Penalty.py,torch/legacy/nn/Log.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/MarginRankingCriterion.py,torch/legacy/nn/MaskedSelect.py,torch/legacy/nn/MixtureTable.py,torch/legacy/nn/Module.py,torch/legacy/nn/Mul.py,torch/legacy/nn/MulConstant.py,torch/legacy/nn/Narrow.py,torch/legacy/nn/Normalize.py,torch/legacy/nn/Padding.py,torch/legacy/nn/PairwiseDistance.py,torch/legacy/nn/Parallel.py,torch/legacy/nn/Power.py,torch/legacy/nn/Reshape.py,torch/legacy/nn/Select.py,torch/legacy/nn/SoftMin.py,torch/legacy/nn/SoftSign.py,torch/legacy/nn/SpatialConvolution.py,torch/legacy/nn/SpatialConvolutionLocal.py,torch/legacy/nn/SpatialConvolutionMap.py,torch/legacy/nn/SpatialCrossMapLRN.py,torch/legacy/nn/SpatialDivisiveNormalization.py,torch/legacy/nn/SpatialDropout.py,torch/legacy/nn/SpatialFullConvolution.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/legacy/nn/SpatialZeroPadding.py,torch/legacy/nn/SplitTable.py,torch/legacy/nn/Sum.py,torch/legacy/nn/TanhShrink.py,torch/legacy/nn/Transpose.py,torch/legacy/nn/VolumetricConvolution.py,torch/legacy/nn/VolumetricDropout.py,torch/legacy/nn/VolumetricFullConvolution.py,torch/legacy/nn/WeightedEuclidean.py,torch/legacy/nn/WeightedMSECriterion.py,torch/legacy/nn/utils.py,torch/legacy/optim/adadelta.py,torch/legacy/optim/adagrad.py,torch/legacy/optim/adam.py,torch/legacy/optim/adamax.py,torch/legacy/optim/asgd.py,torch/legacy/optim/cg.py,torch/legacy/optim/nag.py,torch/legacy/optim/rprop.py,torch/legacy/optim/sgd.py",79.0,6,2,5.981080286,5.0,10861.0,7.0,209004.06329113923,90.0,1195.933333,0.0,,0.0,1
pytorch,a809297d63f6b5df7766a8c949a6e9a283818ee4,1eb454f59781347c9b9135c5ed5c5052d7afa177,Richard Zou,zou3519@users.noreply.github.com,Mon Dec 20 03:06:42 2021 -0500,1639969602.0,"[functorch] Fix vmapvjp tests (pytorch/functorch#354)

Previously they were not vmapping over non-differentiable arguments.
That case is important to test.

Test Plan:
- run tests",78.0,9.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1109.0,1.0,0.0,671.0,915.5,0.0,Corrective,1.0,1
pytorch,ddf1598ef8e1a0fe2248e5b3e416a03dcfc4da27,1ed488da4f88ec7b85ba5f6a4113908dda3681e3,Adam Paszke,adam.paszke@gmail.com,Sun Sep 25 18:56:54 2016 -0700,1474829814.0,Make custom precision of CUDA tests work in inplace mode as well,7.0,7.0,test/test_cuda.py,1.0,1,1,0,6.0,345.0,1.0,141332.0,194.0,3860.032937,0.0,,0.0,1
pytorch,98206c326ecbd9194ff44b052661830e0608d7ab,1ef1dd9cadb00ac91f3b922978fb3e0d2ddf3a09,yunjey,yunjey47@naver.com,Tue Jul 11 06:02:56 2017 +0900,1499752976.0,Add comments for readability (#2005),14.0,2.0,"torch/nn/functional.py,torch/nn/modules/loss.py",2.0,3,1,0.69621226,30.0,1604.0,1.0,55952.0,669.0,5393.172317,0.0,Feature Addition,0.0,1
pytorch,deb4100928f306c2f92a9e006393d9a2c79389c7,1f09f7ea448f404fff135b5e4c1c3d63d157de79,anjali411,chourdiaanjali123@gmail.com,Fri May 01 18:44:10 2020 -0700,1588358650.0,"Python API for Complex Storage and storage copy logic (#35771)

Summary:
Following up on this: https://github.com/pytorch/pytorch/pull/35851 cross dtype storage copy is not being used internally, so I have not included cross dtype copy for complex.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35771

Differential Revision: D21319650

Pulled By: anjali411

fbshipit-source-id: 07c72996ee598eba0cf401ad61534494d6f5b5b3",723.0,241.0,"aten/src/ATen/DLConvertor.cpp,aten/src/ATen/function_wrapper.py,aten/src/TH/CMakeLists.txt,aten/src/TH/THGenerateComplexDoubleType.h,aten/src/TH/THGenerateComplexFloatType.h,aten/src/TH/THGenerateComplexTypes.h,aten/src/TH/THStorageFunctions.cpp,aten/src/TH/THStorageFunctions.h,aten/src/TH/THTensor.cpp,aten/src/TH/THTensor.h,aten/src/TH/THTensor.hpp,aten/src/TH/generic/THStorage.h,aten/src/TH/generic/THStorageCopy.cpp,aten/src/TH/generic/THStorageCopy.h,aten/src/TH/generic/THTensor.h,aten/src/THC/CMakeLists.txt,aten/src/THC/THCGeneral.h.in,aten/src/THC/THCGenerateComplexDoubleType.h,aten/src/THC/THCGenerateComplexFloatType.h,aten/src/THC/THCGenerateComplexTypes.h,aten/src/THC/THCStorage.cpp,aten/src/THC/THCStorage.cu,aten/src/THC/THCStorage.h,aten/src/THC/THCStorageCopy.cpp,aten/src/THC/THCStorageCopy.cu,aten/src/THC/THCStorageCopy.h,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensor.cu,aten/src/THC/THCTensor.h,aten/src/THC/THCTensor.hpp,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorCopy.h,aten/src/THC/THCTensorIndex.cu,aten/src/THC/THCTensorRandom.cu,aten/src/THC/generic/THCStorage.h,aten/src/THC/generic/THCStorageCopy.cpp,aten/src/THC/generic/THCStorageCopy.cu,aten/src/THC/generic/THCStorageCopy.h,aten/src/THC/generic/THCTensor.h,c10/util/TypeCast.h,test/test_torch.py,torch/__init__.py,torch/_storage_docs.py,torch/csrc/Module.cpp,torch/csrc/Storage.cpp,torch/csrc/Storage.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Storage.cpp,torch/csrc/cuda/Storage.h,torch/csrc/cuda/serialization.cpp,torch/csrc/cuda/serialization.h,torch/csrc/cuda/utils.cpp,torch/csrc/cuda/utils.h,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/utils.h,torch/csrc/serialization.cpp,torch/csrc/serialization.h,torch/csrc/utils.cpp,torch/csrc/utils.h,torch/csrc/utils/byte_order.cpp,torch/csrc/utils/byte_order.h,torch/cuda/__init__.py,torch/storage.py",64.0,16,4,4.945658318,45.0,27108.0,27.0,13545083.965517242,1596.0,4195.5,0.0,,0.0,1
pytorch,b349f58c21d93de755561a3be605fc3050556788,1f0cfbaaad09921f588adf549751041b8cb2e283,Zachary DeVito,zdevito@fb.com,Sun Aug 23 22:35:08 2020 -0700,1598222108.0,"[fx] add type annotations (#43083)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/43083

This adds type annotations to all classes, arguments, and returns
for fx. This should make it easier to understand the code, and
encourage users of the library to also write typed code.

Test Plan: Imported from OSS

Reviewed By: ezyang

Differential Revision: D23145853

Pulled By: zdevito

fbshipit-source-id: 648d91df3f9620578c1c51408003cd5152e34514",117.0,64.0,"torch/fx/graph.py,torch/fx/graph_module.py,torch/fx/node.py,torch/fx/proxy.py,torch/fx/symbolic_trace.py",5.0,2,1,2.062510103,1.0,550.0,1.0,64.0,4499.0,10484.5,0.0,Feature Addition,0.0,1
pytorch,22ba8726da84ac246c945c095953c505a6091fab,1f36caceb2822c3f4969c1cdcad77a8640e8a63a,Peter Goldsborough,peter@goldsborough.me,Tue Jun 26 17:13:14 2018 -0700,1530033194.0,"[C++ API]  Rework optimization package (#8815)

* Rework optim folder

* Removed TORCH_OPTIMIZER_CLASS macro

* Got rid of CRTP/Impl

* Removed TORCH_AUTOGRAD_KWARG

* Differentiate between Optimizer and LossClosureOptimizer

* Make Optimizers parameters based instead of model based

* Allow construction of optimizer from arbitrary vector

* Added test for zero grad

* Added test for external parameter vectors

* Now comparing against baseline values

* Documentation

* Post rebase fixes

* Different strategy for creating and accessing buffers in optimizers

* Fix member ordering",1545.0,718.0,"test/cpp/api/cursor.cpp,test/cpp/api/integration.cpp,test/cpp/api/optim.cpp,test/cpp/api/optim_baseline.h,test/cpp/api/optim_baseline.py,test/cpp/api/rnn.cpp,test/cpp/api/serialization.cpp,torch/CMakeLists.txt,torch/csrc/api/include/torch/nn/cursor.h,torch/csrc/api/include/torch/nn/modules.h,torch/csrc/api/include/torch/nn/modules/modules.h,torch/csrc/api/include/torch/optim.h,torch/csrc/api/include/torch/optim/adagrad.h,torch/csrc/api/include/torch/optim/adam.h,torch/csrc/api/include/torch/optim/lbfgs.h,torch/csrc/api/include/torch/optim/optimizer.h,torch/csrc/api/include/torch/optim/rmsprop.h,torch/csrc/api/include/torch/optim/sgd.h,torch/csrc/api/include/torch/optimizers.h,torch/csrc/api/include/torch/serialization.h,torch/csrc/api/include/torch/torch.h,torch/csrc/api/src/nn/cursor.cpp,torch/csrc/api/src/optim/adagrad.cpp,torch/csrc/api/src/optim/adam.cpp,torch/csrc/api/src/optim/lbfgs.cpp,torch/csrc/api/src/optim/optimizer.cpp,torch/csrc/api/src/optim/rmsprop.cpp,torch/csrc/api/src/optim/sgd.cpp,torch/csrc/api/src/optimizers.cpp,torch/csrc/api/src/utils.cpp",30.0,14,2,4.114099424,5.0,3182.0,5.0,450421.9285714286,1420.0,4293.805292,0.0,Corrective,1.0,1
pytorch,307421930a0e1ac610c42c07470813ea14f53964,1f38225b56b873c944196241ea448445a61798fd,Nikita Karetnikov,nkaretnikov@quansight.com,Fri Aug 19 18:51:57 2022 +0000,1660935117.0,"[primTorch] Add ref for `new_empty_strided` (#82466)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82466
Approved by: https://github.com/ezyang, https://github.com/ngimel",122.0,12.0,"functorch/test/test_vmap.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,6,2,0.800404734,7.0,27040.0,3.0,34216.333333333336,6619.0,15282.5,0.0,Feature Addition,0.0,1
pytorch,87748ffd4cc360a3a4f481a2208eb2aa60730ccd,1f5951693a36c8acf55f2f0162754806d94ee2e3,Adam Paszke,adam.paszke@gmail.com,Thu Dec 01 16:51:07 2016 +0100,1480611067.0,Change torch.randperm to return Long tensors,19.0,8.0,"test/test_nn.py,test/test_torch.py,torch/csrc/Module.cpp",3.0,3,2,1.305803563,18.0,4711.0,2.0,1094.0,315.0,2403.512243,0.0,,0.0,1
pytorch,40179cd61c80e2d402dc087e3e3723e05c4763e8,1f64c2ef91a6251e548222488ee93daecae5e914,Fritz Obermeyer,fritz.obermeyer@gmail.com,Sat Nov 18 21:10:07 2017 -0800,1511039407.0,"Rename pyro.distributions.Multinomial -> .Categorical (#3766)

* Rename distributions.Multinomial -> distributions.Categorical

* Rename Multinomial -> Categorical

* Update docs

* Update variable.py

* Update distributions.py

* Update variable.py",24.0,19.0,"docs/source/distributions.rst,test/test_distributions.py,torch/autograd/_functions/stochastic.py,torch/autograd/variable.py,torch/distributions.py",5.0,6,3,1.94697079,37.0,810.0,4.0,1320456.4,2137.0,24109.85823,0.0,Non Functional,0.0,1
pytorch,1f8939937a8ac651be808ffa8449a6522aa94534,1f6f82dbcf324c1acd359f46fd12f1d90454d65e,Adam Paszke,adam.paszke@gmail.com,Sat Feb 25 21:57:17 2017 -0800,1488059837.0,Fall back to indexing compatible with numpy,87.0,77.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.377646321,24.0,3824.0,2.0,36856.5,494.0,4635.478466,0.0,,0.0,1
pytorch,b5f2574f36dec86a71ff39d0e07e16d39178abde,1f74e082e252a03aa54ecc111112aedb41d80f53,Philip Meier,github.pmeier@posteo.de,Thu Feb 17 02:25:35 2022 -0800,1645064735.0,"only compare attributes for meta tensors (#72508)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72508

Todo:

- [x] document this behavior
- [x] add tests

Test Plan: Imported from OSS

Reviewed By: zou3519

Differential Revision: D34262452

Pulled By: ezyang

fbshipit-source-id: bc5c9653d5c3ad5c6efccc9c8e0efc0d28e15104
(cherry picked from commit 233142c88e4cff02825c7e233aba9411a6df3e9f)",254.0,213.0,"test/test_binary_ufuncs.py,test/test_modules.py,test/test_tensor_creation_ops.py,test/test_testing.py,test/test_torch.py,test/test_type_promotion.py,test/test_view_ops.py,torch/testing/_comparison.py,torch/testing/_internal/common_methods_invocations.py",9.0,4,2,1.930766323,45.0,38274.0,8.0,1718961.111111111,805.0,1977.5,0.0,Feature Addition,0.0,1
pytorch,d7fb500252390ac8a461240ce3978e2359c77d7e,1f7822f65097d09df7fd0e7fafaf7941e08b63df,Richard Zou,zou3519@users.noreply.github.com,Mon Dec 06 22:08:57 2021 -0500,1638828537.0,[functorch] Update lagging op db (pytorch/functorch#318),221.0,28.0,"functorch/test/functorch_additional_op_db.py,functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py,functorch/test/xfail_suggester.py",6.0,2,1,2.176114738,1.0,5491.0,5.0,1.4,600.0,823.5,0.0,,0.0,1
pytorch,b3d41a5f967abe8b1e5591c323e34b620e17cf44,1f8939937a8ac651be808ffa8449a6522aa94534,Adam Paszke,adam.paszke@gmail.com,Sat Feb 25 19:28:40 2017 -0800,1488050920.0,Allow using expand to broadcast tensors,52.0,20.0,"test/test_autograd.py,test/test_torch.py,torch/autograd/_functions/tensor.py,torch/tensor.py",4.0,4,2,1.758576517,24.0,5142.0,4.0,102703.75,493.0,4634.478466,0.0,,0.0,1
pytorch,8d99d6127ef49db8286f6fb5dc7ae7e634c92a22,1fae890a07f180c64825faa214916dc0cbd6cb58,samdow,samdow@fb.com,Fri Sep 30 15:26:27 2022 -0400,1664551587.0,"fix grad silent correctness issue from view fn followed by an inplace fn (#85374)

From https://github.com/pytorch/functorch/issues/1007, which was an issue where we would wrap aliases of unwrapped tensors and miss the inplace error message where we should have gotten it. Instead of keeping aliases unwrapped like I had originally wanted, this simplifies it slightly such that:
(1) All tensors that were previously wrapped are still wrapped. This is occasionally important because of the 1-1 relationship between a tensor and autograd meta. By keeping the same number of wrapper tensors before, we'll never have autograd try to write multiple autograd metas to the same tensor when it wouldn't before
(2) The tensors that either were unwrapped tensors or aliases of unwrapped tensors now get a flag on them (now called `alias_of_unwrapped`). This way, they are still wrapper tensors (and don't have to potentially break autograd) but we can identify that they should be treated like an unwrapped tensor
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85374
Approved by: https://github.com/zou3519",227.0,36.0,"aten/src/ATen/core/function_schema.h,aten/src/ATen/functorch/ADInterpreters.cpp,aten/src/ATen/functorch/ADInterpreters.h,aten/src/ATen/functorch/DynamicLayer.cpp,aten/src/ATen/functorch/DynamicLayer.h,aten/src/ATen/functorch/FunctionalizeInterpreter.cpp,aten/src/ATen/functorch/FunctionalizeInterpreter.h,aten/src/ATen/functorch/Interpreter.cpp,aten/src/ATen/functorch/Interpreter.h,aten/src/ATen/functorch/TensorWrapper.cpp,aten/src/ATen/functorch/TensorWrapper.h,aten/src/ATen/functorch/VmapInterpreter.cpp,aten/src/ATen/functorch/VmapInterpreter.h,functorch/_src/eager_transforms.py,functorch/test/test_ops.py",15.0,8,2,2.576925412,4.0,5318.0,4.0,725330.8,7889.0,18723.5,0.0,Corrective,1.0,1
pytorch,33d1ec396bad4ff8d53b154e64166b56c9054a75,1fb6b431a3f0938bf133e1319cacb33bc2492558,Spandan Tiwari,sptiwari@microsoft.com,Sun Jan 20 03:50:20 2019 -0800,1547956220.0,"Replace use of ConstantLike with with ConstantOfShape (#16095)

Summary:
Submitting this PR as an update to existing PR (https://github.com/pytorch/pytorch/pull/15938) on houseroad 's request.

This PR replaces the use of ONNX op `ConstantLike` with `ConstantOfShape` in the ONNX exporter. In addition to removing the call sites in `symbolic.py`, it also replace the call site in `peephole.cpp`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16095

Differential Revision: D13745723

Pulled By: houseroad

fbshipit-source-id: e2a5f534f01adf199df9e27544f7afcfa540e1f0",201.0,9.0,"aten/src/ATen/core/interned_strings.h,caffe2/onnx/backend.cc,test/onnx/expect/TestOperators.test_full_like.expect,test/onnx/expect/TestOperators.test_ones_like.expect,test/onnx/expect/TestOperators.test_zeros_like.expect,torch/csrc/jit/passes/onnx/peephole.cpp,torch/onnx/symbolic.py",7.0,15,4,2.234100553,15.0,4232.0,5.0,772722.1428571428,6563.0,20355.83333,0.0,Feature Addition,0.0,1
pytorch,b3d8fae042176f72c1f3bc919e1cb57d64c35e30,1fd119948ebde3b6e3864fa5ead03a4fffde8348,Xuehai Pan,XuehaiPan@pku.edu.cn,Wed Mar 01 23:50:52 2023 +0000,1677714652.0,"[3/3] Update `.pyi` Python stub files and enable `'UFMT'` linter (#95268)

Changes:

- #95200

1. Recognize `.py.in` and `.pyi.in` files as Python in VS Code for a better development experience.
2. Fix deep setting merge in `tools/vscode_settings.py`.

- #95267

3. Use `Namedtuple` rather than `namedtuple + __annotations__` for `torch.nn.utils.rnn.PackedSequence_`:

    `namedtuple + __annotations__`:

    ```python
    PackedSequence_ = namedtuple('PackedSequence_',
                                 ['data', 'batch_sizes', 'sorted_indices', 'unsorted_indices'])

    # type annotation for PackedSequence_ to make it compatible with TorchScript
    PackedSequence_.__annotations__ = {'data': torch.Tensor, 'batch_sizes': torch.Tensor,
                                       'sorted_indices': Optional[torch.Tensor],
                                       'unsorted_indices': Optional[torch.Tensor]}
    ```

    `Namedtuple`: Python 3.6+

    ```python
    class PackedSequence_(NamedTuple):
        data: torch.Tensor
        batch_sizes: torch.Tensor
        sorted_indices: Optional[torch.Tensor]
        unsorted_indices: Optional[torch.Tensor]
    ```

- => this PR: #95268

4. Sort import statements and remove unnecessary imports in `.pyi`, `.pyi.in` files.
5. Format `.pyi`, `.pyi.in` files and remove unnecessary ellipsis `...` in type stubs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95268
Approved by: https://github.com/huydhn",1817.0,944.0,".lintrunner.toml,.vscode/extensions.json,.vscode/settings_recommended.json,torch/_C/_VariableFunctions.pyi.in,torch/_C/__init__.pyi.in,torch/_C/_autograd.pyi,torch/_C/_cudnn.pyi,torch/_C/_distributed_autograd.pyi,torch/_C/_distributed_c10d.pyi,torch/_C/_distributed_rpc.pyi,torch/_C/_distributed_rpc_testing.pyi,torch/_C/_dynamo/eval_frame.pyi,torch/_C/_functions.pyi,torch/_C/_functorch.pyi,torch/_C/_lazy.pyi,torch/_C/_lazy_ts_backend.pyi,torch/_C/_monitor.pyi,torch/_C/_nn.pyi.in,torch/_C/_profiler.pyi,torch/_C/return_types.pyi.in,torch/fx/__init__.pyi,torch/nn/functional.pyi.in,torch/nn/parallel/__init__.pyi,torch/nn/parallel/common_types.pyi,torch/nn/parallel/data_parallel.pyi,torch/nn/parallel/parallel_apply.pyi,torch/nn/parallel/replicate.pyi,torch/nn/parallel/scatter_gather.pyi,torch/nn/parameter.pyi,torch/nn/utils/rnn.pyi,torch/optim/__init__.pyi,torch/optim/_multi_tensor/__init__.pyi,torch/optim/adadelta.pyi,torch/optim/adagrad.pyi,torch/optim/adam.pyi,torch/optim/adamax.pyi,torch/optim/adamw.pyi,torch/optim/asgd.pyi,torch/optim/lbfgs.pyi,torch/optim/lr_scheduler.pyi,torch/optim/nadam.pyi,torch/optim/optimizer.pyi,torch/optim/radam.pyi,torch/optim/rmsprop.pyi,torch/optim/rprop.pyi,torch/optim/sgd.pyi,torch/optim/sparse_adam.pyi,torch/optim/swa_utils.pyi,torch/utils/data/datapipes/datapipe.pyi.in",49.0,13,2,3.144781528,4.0,4749.0,35.0,46847368.18367347,12947.0,30204.0,0.0,Corrective,1.0,1
pytorch,c897651392798663c7b7261150e42d1197ae2f94,1fd2cd26a04cb691d7704233482c5a111d475870,Jiong Gong,jiong.gong@intel.com,Wed Jun 12 22:00:55 2024 -0700,1718229655.0,"[inductor][cpp] support bf16/fp16 gemm template epilogue fusion (#126545)

As part of #125683, this PR adds epilogue fusion support for bf16/fp16 gemms. The key changes are as follows:
1. bf16 linear w/ epilogue fusion of some ops was originally supported via ATen oneDNN linear pointwise ops. In order to match the ATen op semantics, in-template epilogue support is added to the cpp gemm template so that we would have: ""gemm + in-template epilogues -> template buffer"". If the template is chosen for codegen, the in-template epilogues will be concatenated with the out-of-template epilogues that are appended during the scheduling.
2. Support bf16/fp16 legalization for `codegen_loop_bodies` which is used to generate the epilogue loops.
3. We used to leverage the in-place buffer mechanism to handle the in-place buffers in the epilogue codegen, in particular, for the reuses for output buffers of GEMM, template and epilogues. This is not correct since the output buffer is an ""output"" not an ""in-place"" buffer of the template kernel itself. Now, we use a dedicated ""aliases"" dict to manage such buffer reuses and the intermediate aliasing buffers are removed after codegen.
4. Add `localize_buffer` method to `LocalBufferScope` to allow the replacement of a global buffer with a local one in the given inductor IR nodes. This helps the fused loops to work on smaller-sized local buffers for better data locality.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/126545
Approved by: https://github.com/jansel",583.0,129.0,"test/inductor/test_cpu_select_algorithm.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_template.py,torch/_inductor/codegen/cpp_template_kernel.py,torch/_inductor/codegen/cpp_utils.py,torch/_inductor/kernel/mm.py,torch/_inductor/mkldnn_lowerings.py",8.0,6,2,2.260669826,3.0,6819.0,3.0,157848.625,29960.0,74275.0,0.0,Corrective,0.0,1
pytorch,ffce2492afa88328b7a19ea2a02a97fe3f419a90,1fdc88f87746f85713ef681d53faada1b22c0000,chunyuan,chunyuan.wu@intel.com,Fri Jul 14 14:46:44 2023 +0000,1689346004.0,"Inductor cpp wrapper: fix codegen of FallbackKernel with kwargs (#104575)

Fix cpp wrapper failure on TorchBench model `hf_Reformer` with `randn`:
```
random_rotations = torch.randn(rotations_shape, device=vectors.device, dtype=vectors.dtype)
```

For cpp wrapper, when `kwargs` is not empty, for `OpOverloadPacket` kernel, we need to know the exact overload schema to handle the `kwargs` properly when calling the cpp kernel: including finding the correct order of the kwargs and getting the default value for optional args without provided value when calling the function (`layout` in the above case).

The current support in this PR is conservative and we'll extend the functionality in subsequent PRs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/104575
Approved by: https://github.com/jgong5, https://github.com/desertfire",188.0,10.0,"test/inductor/test_cpp_wrapper.py,test/inductor/test_torchinductor.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py,torch/_inductor/utils.py",5.0,5,2,1.641893797,2.0,15143.0,4.0,59204.0,17699.0,39783.5,0.0,Corrective,1.0,1
pytorch,899bddeeb6054e89c30cda704f9fe816b5e18f9c,2019f6cd51c94a85bc38140031528bf5552a06b0,Chenyang Yu,chenyangyu@instagram.com,Thu May 09 17:07:12 2019 -0700,1557421632.0,"Add unit test to ensure no gradients sync when calling ddp.module(input) (#20282)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20282

Add unit test to ensure no gradients sync when calling ddp.module(input), e.g not invoking prepare_for_backward

PyText is depending on DDP for data parallel distributed training. To support accumulate gradients locally before gradients sync, we are calling orig_model.forward instead of ddp_model.forward. Add a unit test to avoid changes break the assumption.

Reviewed By: pietern, mrshenli

Differential Revision: D15263155

fbshipit-source-id: 7734e174f507690fb23ea6c52dffff4a93f9b151",51.0,0.0,test/test_c10d.py,1.0,1,1,0,2.0,2602.0,1.0,903780.0,8576.0,25389.33333,0.0,Feature Addition,0.0,1
pytorch,7fe6e8e5a20bf1d279a296184b1ee6c28d86763d,201ad938b26133075000d6aa3108c91764464d80,Jeffrey Wan,jw3468@fb.com,Tue Apr 27 14:51:54 2021 -0700,1619535114.0,"Enable fixed fast_mode for complex (#55699)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55699

Todo:
- error message should be updated to say whether the failure is for fn's real or imaginary component

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D28007887

Pulled By: soulitzer

fbshipit-source-id: 1819201f59c8586a1d9631db05983969438bde66",83.0,24.0,torch/autograd/gradcheck.py,1.0,2,1,0,28.0,1142.0,1.0,66.0,11324.0,24967.0,0.0,Corrective,1.0,1
pytorch,2540f866ff1eff10dbed7ca47ea9c432e8583da2,201ddafc22e22c387b4cd654f397e05354d73d09,francescocastelli,francesco8.castelli@mail.polimi.it,Sat Apr 30 04:06:14 2022 +0000,1651291574.0,"Make Opinfo a dataclass to get a more useful repr

Fixes #74320. If we make OpInfo a dataclass we should change the method `default_test_dtypes` name because there is also an argument with the same name (or change the argument name). Right now I've changed the method name because it seems to be used only twice.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75663
Approved by: https://github.com/mruberry",288.0,207.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,17280.0,1.0,93829.0,2728.0,6533.5,0.0,Corrective,1.0,1
pytorch,898d062bfd6a0e7bbf5aa1d71f31c06a13b52592,20397285c6353abda0bc3722831ac97237b0e612,Ailing,ailzhang@users.noreply.github.com,Wed May 27 22:26:50 2020 -0700,1590618410.0,"Replace use of np.allclose in tests. (#34287)

Summary:
fixes https://github.com/pytorch/pytorch/issues/34096
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34287

Differential Revision: D21735525

Pulled By: ailzhang

fbshipit-source-id: 611da17cfc5a3fee77d482abccf8f9854f504263",20.0,23.0,"test/test_nn.py,test/test_torch.py",2.0,1,1,0.518569732,45.0,30283.0,2.0,17638.0,2371.0,5978.5,0.0,Corrective,1.0,1
pytorch,8ec6bc72921b3451a82480eda0f08942ae54dadc,204f985fc30837deb6d1c242bc7d5faa4cdd7d0a,Nick Gibson,nickg@fb.com,Thu Sep 17 01:40:18 2020 -0700,1600306818.0,"[NNC] Add simplification of Loop + Condition patterns. (#44764)

Summary:
Adds a new optimization to the IRSimplifier which changes this pattern:
```
for ...
  if ...
   do thing;
```
into:
```
if ...
  for ...
    do thing;
```

Which should be almost strictly better.

There are many cases where this isn't safe to do, hence tests. Most  obviously when the condition depends on something modified within the loop.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44764

Reviewed By: mruberry

Differential Revision: D23734463

Pulled By: nickgg

fbshipit-source-id: 51617e837de96b354fb702d0090ac65ddc523d36",294.0,2.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/analysis.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_visitor.cpp,torch/csrc/jit/tensorexpr/stmt.h",6.0,7,2,1.447372062,2.0,6653.0,4.0,162600.33333333334,5193.0,11919.0,0.0,Feature Addition,0.0,1
pytorch,d01302431c41691ee29ccf2b710cc5738128993e,207883600554ed9f0940c37c7bbf2141ebdf2c8b,Jeffrey Wan,jw3468@fb.com,Fri Apr 23 02:42:20 2021 -0700,1619145740.0,"Clean up raise exception logic (#55656)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55656

### For release notes
What:
 - All errors that are silenced by ""raise_exception=False"" are now GradcheckError (which inherits from RuntimeError).

Why:
 - Due to a refactor of gradcheck

Workaround:
 - If you catch for 'RuntimeError' with `except RuntimeError`, since GradcheckError inherits from RuntimeError, no changes are necessary. However if you explicitly check for the errors type via `type(error)`, you'll need to update your code to check for `GradcheckError` instead.

Factors out all the logic handling involving `fail_test`, `raise_exception` into 1) a wrapper around gradcheck that uses try/except 2) gradcheck_helper that always raises exception.
This allows us to avoid having to write the `if not x: return False` logic that is scattered throughout gradcheck currently.

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D27920809

Pulled By: soulitzer

fbshipit-source-id: 253aef6d9a3b147ee37a6e37a4ce06437981929a",104.0,96.0,"test/test_autograd.py,torch/autograd/gradcheck.py",2.0,3,2,0.529360865,42.0,9753.0,2.0,10980.5,11182.0,24720.0,0.0,Perfective,0.0,1
pytorch,43ab91118226b330be6d2274a154b98da233d879,20b5e82155ec052b2c168e3fb1aff0fd2dd3c8a4,Sam Gross,colesbury@gmail.com,Tue Jan 02 20:44:46 2018 -0500,1514925886.0,"Implement embedding in ATen (#4322)

Implements nn.Embedding (lookup table) in ATen.

Breaking change: new optional argument padding_idx in F.embedding to
match nn.Embedding.

Note that there are a few bugs in Embedding that are inherited from the
previous code:

 - CUDA renorm has race conditions if index contains duplicate entries
 - sparse gradient doesn't work with scale_grad_by_freq",626.0,137.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/cuda/AccumulateType.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/native_functions.yaml,aten/src/THC/THCNumerics.cuh,tools/autograd/derivatives.yaml,torch/nn/_functions/thnn/sparse.py,torch/nn/functional.py,torch/nn/modules/sparse.py,torch/onnx/symbolic.py",12.0,15,3,2.310781276,36.0,6072.0,8.0,1353815.0,410.0,1292.905869,0.0,Corrective,1.0,1
pytorch,a3da3653ebb80d274bcec29c81c0538b410e527a,2113ea6fbf20cb820ea6a504b1dfe30297ce4d7c,Gregory Chanan,gchanan@fb.com,Wed Apr 03 14:52:54 2019 -0700,1554303174.0,"Add device and dtype to storage. (#18749)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18749
ghimport-source-id: 9026a037f5e11cdb9ccd386f4b6b5768b9c3259b

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18751 Disallow changing the device of a tensor via set_.
* #18750 Use non-legacy constructors for tensor deserialization.
* **#18749 Add device and dtype to storage.**

The goal here is to fix our serialization, which currently depends on the legacy constructors.  Having dtype and device on Storage allows us to use the non-legacy constructors.

This fits somewhat along our goal of removing Storage, my having Storage act like a Tensor.

Differential Revision: D14729516

fbshipit-source-id: bf4a3e8669ad4859931f4a3fa56df605cbc08dcb",49.0,0.0,"test/test_torch.py,torch/csrc/Storage.cpp,torch/csrc/cuda/Storage.cpp,torch/csrc/generic/Storage.cpp",4.0,5,2,1.407578811,41.0,11458.0,3.0,1312443.75,7854.0,23767.33333,0.0,Corrective,1.0,1
pytorch,e05ca753bf5fa6fef67a9450841bc6cf7e3a0249,211d31afc931d0f2230e0ef94cb11610ddf18e74,lezcano,lezcano-93@hotmail.com,Mon Apr 12 16:43:29 2021 -0700,1618245809.0,"symeig supports complex backward (#55085)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/53651
I did not put much effort in improving the docs, as I will go over all these docs in future PRs
cc anjali411

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55085

Reviewed By: nikithamalgifb

Differential Revision: D27493604

Pulled By: anjali411

fbshipit-source-id: 413363013e188bc869c404b2d54ce1f87eef4425",26.0,25.0,"test/test_autograd.py,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",4.0,6,3,1.596818744,42.0,24689.0,4.0,92878.25,10655.0,23557.0,0.0,Corrective,1.0,1
pytorch,1c9073b43a7ceffeea489aad5c07f7d138af22c2,213540cd8557d5bb3624cbdf53b0aceeedf436c9,"Gao, Xiang",qasdfgtyuiop@gmail.com,Thu Jul 05 18:21:54 2018 -0700,1530814914.0,"Add meshgrid to PyTorch (#8581)

Summary:
Part of this issue https://github.com/pytorch/pytorch/issues/7580
Closes https://github.com/pytorch/pytorch/pull/8581

Differential Revision: D8661660

Pulled By: soumith

fbshipit-source-id: 4a72fb5152ed6eb4d57f14de691bf09a2a2e5b0c",81.0,0.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,torch/_torch_docs.py",4.0,6,3,1.720813597,42.0,16120.0,2.0,202032.75,2818.0,6406.833333,0.0,Feature Addition,0.0,1
pytorch,7fba30c2be4c1373c1e4424111e5ec2b878a85da,215679573ebff5a03238a7f9aa801a6c00826f19,Alex Suhan,asuhan@fb.com,Wed Sep 23 06:46:32 2020 -0700,1600843592.0,"[TensorExpr] Fix operator order in combineMultilane (#45157)

Summary:
combineMultilane used the wrong order when ramp was on the left hand side,
which matters for subtract.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45157

Test Plan: test_tensorexpr --gtest_filter=TensorExprTest.SimplifyRampSubBroadcast

Reviewed By: ailzhang

Differential Revision: D23851751

Pulled By: asuhan

fbshipit-source-id: 864d1611e88769fb43327ef226bb3310017bf858",16.0,1.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",3.0,7,2,0.834347023,2.0,6446.0,2.0,98796.66666666669,5353.0,12217.5,0.0,Corrective,1.0,1
pytorch,2a0e98a3344d0806f3fb81ac77d5ac2b20bdf7a9,21609e0fd061fbf1a2161d818ee3d51928598e66,Chintak Sheth,chintaksheth@gmail.com,Thu Jun 14 15:38:04 2018 -0700,1528990684.0,"``bincount`` feature implementation (#6688)

* Implement CPU bincount feature support

* Incorporate feedback on renaming to SummaryOps file and other nits

* bincount gpu implementation

* refactor cuda code and incorporate nits

* doc fix

* cuda bincount - cast weights to double if integral type

* fix: signed unsigned comparison error

* fix: ssize_t error

* refactor

* make template typenames readable and other nist

* make compatible with v0.5

* incorporate comments

* update test cases to ensure CUDA code coverage",494.0,3.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/cuda/CUDAApplyUtils.cuh,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/native_functions.yaml,test/test_cuda.py,test/test_torch.py,torch/_torch_docs.py",8.0,8,3,1.963551288,40.0,17192.0,6.0,867332.1666666666,2729.0,25210.85823,0.0,Corrective,1.0,1
pytorch,8d674c0d510dd6e2dfcc0b10f31da2efa86ef3b4,2184e3f9338be4ca5fd964bebb256f93e90aff7f,cpuhrsch,cpuhrsch@googlemail.com,Thu Jun 14 14:40:21 2018 -0400,1528987221.0,Use MKL VML if available (#8458),174.0,75.0,"aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h",7.0,7,1,1.983114823,6.0,797.0,4.0,1527007.1666666667,1328.0,3771.305292,0.0,,0.0,1
pytorch,f7bcba33a6888383832575626b26ca6006d2d2ae,21ba9b3c6d650d7124118cf16b8206e2d2212d7f,James Reed,jamesreed@fb.com,Fri Sep 06 04:41:25 2019 -0700,1567744885.0,"Copy quantize routine to vec256 (#25685)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25685

This saves a bunch of dynamic linking/function call overhead

Benchmark script

```
import torch
import time

x = torch.rand(1, 256, 56, 56)
y = torch.rand(1, 256, 56, 56)

print('dtype', 'ms/iter (float)', 'ms/iter (quant)', 'quant / float', sep='\t')

for dtype in [torch.quint8, torch.qint8, torch.qint32]:
    qX = torch.quantize_linear(x, 0.1, 5, dtype).permute([0, 3, 1, 2])
    qY = torch.quantize_linear(y, 0.1, 5, dtype).permute([0, 3, 1, 2])

    _x = x.permute([0, 3, 1, 2])
    _y = y.permute([0, 3, 1, 2])

    NITER = 1000

    # Test float
    s = time.time()
    for i in range(NITER):
        _x + _y
    elapsed_float = time.time() - s
    ms_per_iter_float = elapsed_float / NITER * 1000

    # Test quantized
    s = time.time()
    for i in range(NITER):
        torch.ops.quantized.add(qX, qY, 0.1, 5)
    elapsed = time.time() - s
    ms_per_iter = elapsed / NITER * 1000

    print(str(dtype), ms_per_iter_float, ms_per_iter, ms_per_iter / ms_per_iter_float, sep='\t')
```

Before this change (DynDisp to AVX2)
```
dtype   ms/iter (float) ms/iter (quant) quant / float
torch.quint8    0.47539472579956055     0.5174136161804199      1.0883873717996941
torch.qint8     0.46573758125305176     0.5322310924530029      1.1427703365080666
torch.qint32    0.47144651412963867     4.043398380279541       8.576579228174513
```

After this change (DynDisp to AVX2)

```
dtype   ms/iter (float) ms/iter (quant) quant / float
torch.quint8    0.48140883445739746     0.3396260738372803      0.705483675263412
torch.qint8     0.4651052951812744      0.3467671871185303      0.7455670591395397
torch.qint32    0.4986207485198975      4.015796899795532       8.053810259031533
```

Test Plan: Imported from OSS

Differential Revision: D17199438

Pulled By: jamesr66a

fbshipit-source-id: d518500c2b5f4e3a202d9ebc2a5862b4062ef118",89.0,6.0,aten/src/ATen/cpu/vec256/vec256_qint.h,1.0,5,1,0,1.0,746.0,1.0,4.0,11193.0,31553.33333,0.0,Feature Addition,0.0,1
pytorch,3a943e9f824f196b90f72fdfdf62aeb64d8a1930,220b91660f5b1a098b7238d8f6f5ae95048685cd,Joel Schlosser,jbschlosser@fb.com,Mon Dec 14 22:51:11 2020 -0800,1607986271.0,"[pytorch] Expand PixelShuffle to support any number of batch dims (#49187)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/49187

Expands the implementation of PixelShuffle to support any number of batch dimensions

Test Plan: `buck test caffe2/test:nn -- test_pixel_shuffle`

Reviewed By: mruberry

Differential Revision: D25399058

fbshipit-source-id: ab0a7f593b276cafc9ebb46a177e2c1dce56d0de",107.0,38.0,"aten/src/ATen/native/PixelShuffle.cpp,test/test_nn.py,torch/nn/modules/pixelshuffle.py",3.0,8,3,1.166140379,43.0,13865.0,3.0,26582652.666666668,7487.0,16766.5,0.0,,0.0,1
pytorch,1cafb1027f223f2174f842945dd337cfa0fc120e,2255911f8ab281c42a5b7e33d4c3205fa3da65a8,albanD,desmaison.alban@gmail.com,Fri Aug 05 16:12:06 2022 +0000,1659715926.0,"Make M1 tests green (#82213)

This is skipping all the failing tests and add a new master job to test on M1

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82213
Approved by: https://github.com/seemethere, https://github.com/soulitzer, https://github.com/malfet",244.0,143.0,".github/workflows/_mac-build.yml,.github/workflows/_mac-test-arm64.yml,.github/workflows/_mac-test-mps.yml,.github/workflows/_mac-test.yml,.github/workflows/trunk.yml,.jenkins/pytorch/macos-common.sh,.jenkins/pytorch/macos-test.sh,test/quantization/core/test_docs.py,test/quantization/eager/test_numeric_suite_eager.py,test/quantization/fx/test_quantize_fx.py,test/test_ao_sparsity.py,test/test_cpp_api_parity.py,test/test_cpp_extensions_jit.py,test/test_cpp_extensions_open_device_registration.py,test/test_dataloader.py,test/test_dispatch.py,test/test_linalg.py,test/test_ops.py,test/test_profiler_tree.py,test/test_utils.py,torch/testing/_internal/common_utils.py",21.0,12,4,3.462040713,42.0,28772.0,19.0,4210229.9,6148.0,14318.5,0.0,Feature Addition,0.0,1
pytorch,96007ec6c00b5248d47eae8ce781481e1e32ab70,227ef1fb6085b966a4035117732daa911612ed25,Sam Gross,colesbury@gmail.com,Tue Dec 19 20:45:33 2017 -0500,1513716333.0,"Move adaptive avg pooling 2d/3d to ATen (#4254)

Move adaptive avg pooling 2d/3d to ATen

Also use ATen for softshrink",71.0,507.0,"aten/src/ATen/nn.yaml,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/nn/_functions/thnn/activation.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py",6.0,10,3,1.469013678,28.0,4521.0,5.0,318154.3333333333,378.0,1177.905869,0.0,,0.0,1
pytorch,a32e98b700abb620ba773ec668112ea080bdd788,22ec5f37cad13c97b57cedf1386e9ed82f300f5d,gchanan,gregchanan@gmail.com,Tue Aug 22 07:57:45 2017 -0400,1503388665.0,Support double backwards with parallel nn autograd functions. (#2508),73.0,44.0,"test/test_autograd.py,test/test_nn.py,torch/nn/parallel/_functions.py,torch/nn/parallel/replicate.py,torch/nn/parallel/scatter_gather.py",5.0,4,2,1.170900364,35.0,6510.0,1.0,468263.0,1349.0,15525.56175,0.0,,0.0,1
pytorch,6285348f06f59b7ed31bd57254dc1b9ccfb4e0b3,22f36353dc6ac81e317c5ad2eb9c8fbc7e84bb35,Michael Suo,suo@fb.com,Thu Sep 30 23:04:52 2021 -0700,1633043092.0,"Revert D31137652: [pytorch][PR] Skip failing tests when LAPACK and MAGMA are not available

Test Plan: revert-hammer

Differential Revision:
D31137652 (https://github.com/pytorch/pytorch/commit/dd354117ef06f32e5e0fb6a6725078815d67cfce)

Original commit changeset: c969f75d7cf1

fbshipit-source-id: bc4cde4eeb5d38ac940ebb471abbd8b9009b3aee",40.0,49.0,"aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.323562577,2.0,21602.0,2.0,44838.333333333336,15850.0,36630.5,0.0,,0.0,1
pytorch,200c34318454d47da316f7b0d14248efe1a0e387,23174ca71b580b0a7f53e92fcf11bf3db1666938,Xiang Gao,qasdfgtyuiop@gmail.com,Thu Jul 16 03:58:31 2020 -0700,1594871911.0,"[reland] Enable TF32 support for cuBLAS (#41498)

Summary:
fix rocm

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41498

Reviewed By: mruberry

Differential Revision: D22560572

Pulled By: ngimel

fbshipit-source-id: 5ee79e96cb29e70d9180830d058efb53d1c6c041",248.0,25.0,"aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cuda/CublasHandlePool.cpp,aten/src/ATen/native/cuda/MiscUtils.h,aten/src/THC/THCBlas.cu,docs/source/notes/cuda.rst,test/test_cuda.py,test/test_torch.py,torch/backends/cuda/__init__.py,torch/csrc/Module.cpp,torch/testing/_internal/common_cuda.py",12.0,17,4,3.008171762,45.0,25606.0,3.0,41265.16666666666,3631.0,8602.0,0.0,Corrective,1.0,1
pytorch,f96f3c312d54eaa4db5294b889bd8f86d8c896cf,232530cc28bce864e04ab7af2a43873c37226a3a,gchanan,gregchanan@gmail.com,Tue Feb 13 21:44:21 2018 -0500,1518558261.0,Move scalar tests from common_nn to legacy_nn. (#5223),172.0,170.0,"test/common_nn.py,test/test_nn.py",2.0,1,1,0.999901322,37.0,7121.0,1.0,21144.0,177.0,12870.22461,0.0,,0.0,1
pytorch,eb8547e93910bb53865860a5cfdee427e9d217ed,2354ff5fabbbfee331240a8fabfab0f86ca44d6c,Kshiteej K,kshitijkalambarkar@gmail.com,Fri Jan 06 15:00:36 2023 +0000,1673017236.0,"[functorch] test: try using reference_inputs in vmap tests (#91355)

Ref https://github.com/pytorch/functorch/issues/1090

Timings:

`test_vmap_exhaustive`

After PR
```
== 1168 passed, 55 skipped, 2353 deselected, 153 xfailed in 195.07s (0:03:15) ==
```

Before PR
```
== 1134 passed, 55 skipped, 2316 deselected, 150 xfailed in 77.18s (0:01:17) ==
```

`test_op_has_batch_rule`

After PR
```
== 988 passed, 57 skipped, 2353 deselected, 331 xfailed in 144.70s (0:02:24) ==
```

Before PR
```
== 969 passed, 57 skipped, 2316 deselected, 313 xfailed in 65.86s (0:01:05) ==
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91355
Approved by: https://github.com/zou3519",101.0,24.0,"test/functorch/common_utils.py,test/functorch/test_vmap.py",2.0,2,1,0.118350011,1.0,5314.0,1.0,18195.0,11063.0,25241.0,0.0,,0.0,1
pytorch,a1386bd95084d84b7af92e379112562a987a5f70,23695ab23fd7c3866953a095009494d17dfb5ce5,Alexander Golynski,agolynski@fb.com,Mon Nov 04 16:33:02 2019 -0800,1572885182.0,"Moving python allgather_coalesced impl from Py to C. (#29059)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29059
This is a resubmit of reverted diff D18209289 ( PR #28857 ).

Test Plan:
buck test caffe2/test:c10d
buck test caffe2/test:distributed_gloo

Reviewed By: pietern

Differential Revision: D18277097

fbshipit-source-id: aecfd7206d70829f0cac66182bf02fccee410fed",222.0,72.0,"test/test_c10d.py,test/test_distributed.py,torch/csrc/distributed/c10d/init.cpp,torch/distributed/distributed_c10d.py,torch/lib/c10d/ProcessGroup.hpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/ProcessGroupGloo.hpp,torch/lib/c10d/ProcessGroupMPI.cpp,torch/lib/c10d/ProcessGroupMPI.hpp,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/lib/c10d/Utils.hpp",12.0,8,2,2.646608437,29.0,12838.0,2.0,1121692.0,12772.0,35348.83333,0.0,,0.0,1
pytorch,25ecc4889de895fa2041b556e1ea4dc057c33712,2386cd2945498ce7261b761a8d9bd5b59d06c5a1,Khushi Agrawal,khushiagrawal411@gmail.com,Thu Sep 15 19:34:44 2022 +0000,1663270484.0,"[reland] [numpy] add torch.concatenate, alias of torch.cat (#85073)

Previous PR: #82946

Fixes #81161

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85073
Approved by: https://github.com/mruberry",117.0,82.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,functorch/test/test_vmap.py,test/test_tensor_creation_ops.py,torch/_torch_docs.py,torch/csrc/jit/passes/normalize_ops.cpp,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",9.0,15,5,2.014755789,40.0,60203.0,3.0,72666.66666666667,7374.0,17331.5,0.0,Corrective,1.0,1
pytorch,126d00c8ddcca996a5045f6a6c59cf86c120936c,23e5f6a7be756ea16a187a58bed2085152e1ef45,Vasiliy Kuznetsov,vasiliy@fb.com,Thu Apr 09 15:58:24 2020 -0700,1586447904.0,"Add avx2 integer horizontal sum and sum of squares to vec256 qint types (#35693)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35693

Adds utility functions to quantized int types of vec256 to calculate
horizontal sums and sums of squares using avx2 intrinsics.

This is useful for quantized implementations of various normalization
layers (LayerNorm, GroupNorm, InstanceNorm), where we need to calculate
the mean and variance of a layer of quantized ints.

Test Plan:
Adhoc c++ tester for the correctness of the avx2 functions:
https://gist.github.com/vkuzo/0380f450793cd5c05abbeacb6d3883ae
Run with:
```
-lstdc++ -mavx2 -lm -ldl -o main main.cpp && ./main
```

The integration bits and performance will be tested in the next PR in the stack
where we will hook quantized Layernorm to use this.

Imported from OSS

Differential Revision: D20768804

fbshipit-source-id: 4720dd358dde0dabbab8e1a33a67be55925d98f9",220.0,0.0,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,1.0,7,1,0,2.0,1710.0,1.0,67313.0,912.0,2465.0,0.0,Corrective,0.0,1
pytorch,f6a4c80a5acf7454e90814449c4d6f4b73f7fad9,240e8d5cc5a6913375cd8507cf7d22727334b953,Samantha Andow,samdow@fb.com,Fri Nov 05 19:11:59 2021 -0700,1636139519.0,"Updated searchsorted functionality (#66818)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/60492

Updates searchsorted API to be more consistent with numpy and adds an OpInfo for searchsorted

Pull Request resolved: https://github.com/pytorch/pytorch/pull/66818

Reviewed By: mruberry

Differential Revision: D31745142

Pulled By: samdow

fbshipit-source-id: 0b9600afc3cb0720afb5811212404ee96d2a7d93",385.0,111.0,"aten/src/ATen/autocast_mode.cpp,aten/src/ATen/native/Bucketization.cpp,aten/src/ATen/native/BucketizationUtils.h,aten/src/ATen/native/cuda/Bucketization.cu,aten/src/ATen/native/native_functions.yaml,test/test_reductions.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",8.0,9,3,2.496575758,33.0,39682.0,6.0,3110129.25,16900.0,39706.0,0.0,Corrective,1.0,1
pytorch,4bd8ae13c68644b977e10ab87ac57e7332fb7994,24242e86fab134d4993b19807a7cf3e3585c4117,Pritam Damania,pritam.damania@fb.com,Tue Oct 08 00:37:58 2019 -0700,1570495078.0,"Ensure NCCL error handling code is disabled for NCCL versions < 2.4 (#27124)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27124

ncclCommAbort() and ncclGetAsyncError() were two APIs added in NCCL
2.4 to detect errors in NCCL communicators. These were used as part of
ProcesGroupNCCL and we also enforced that only NCCL versions 2.4+ were
supported. Although, there is still legitimate use for older NCCL versions and
hence we should still support those.

For that purpose, in this change I've ensured we disable NCCL error checking
for versions < 2.4.
ghstack-source-id: 91452959

Test Plan:
1) Test with 2.4.8
2) Test with 2.2.13
3) unit tests.

Differential Revision: D17178988

fbshipit-source-id: 5dc44b5f7b4b00466c67fd452315f1d4f5c47698",97.0,40.0,"caffe2/CMakeLists.txt,test/common_distributed.py,test/test_c10d.py,torch/CMakeLists.txt,torch/csrc/cuda/nccl.cpp,torch/csrc/cuda/nccl.h,torch/csrc/cuda/python_nccl.cpp,torch/lib/c10d/NCCLUtils.hpp,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/test/CMakeLists.txt,torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp",11.0,8,3,3.112305007,20.0,7081.0,7.0,2704481.0,12080.0,33872.83333,0.0,Feature Addition,0.0,1
pytorch,8cedd1ae2eb7f62f5a91af302d09dcc3108a0201,244197a2a6c3177a6afbba66315c5799cb9b0e54,Richard Zou,zou3519@users.noreply.github.com,Fri Apr 22 20:59:09 2022 -0400,1650661149.0,[functorch] Update lagging op db (pytorch/functorch#737),107.0,59.0,"functorch/test/functorch_additional_op_db.py,functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py,functorch/test/xfail_suggester.py",6.0,2,1,2.187245392,1.0,7510.0,4.0,1.0,988.0,1371.0,0.0,,0.0,1
pytorch,403a53382754bcb17e6b818c4eb4272f4a263355,2443fcac0bb78aed78a459c4787159e29f81da16,Priya Goyal,prigoyal@devgpu205.prn2.facebook.com,Thu Sep 28 21:33:30 2017 -0700,1506634410.0,Deterministic cudnn algorithms,363.0,147.0,"test/test_nn.py,torch/backends/cudnn/__init__.py,torch/csrc/autograd/functions/convolution.cpp,torch/csrc/autograd/functions/convolution.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/cudnn/Conv.cpp,torch/csrc/cudnn/Conv.h,torch/csrc/cudnn/cuDNN.cwrap,torch/nn/functional.py",9.0,9,2,1.345785816,36.0,8367.0,3.0,324041.7777777777,1958.0,23781.85823,0.0,,0.0,1
pytorch,41765d4681bcea8d480b35811b0d5cb06a3f8c1d,2444b4d12245dbfde986e4bb9bc3538806a9d267,Howard Huang,howardhuang@fb.com,Tue Mar 02 22:29:08 2021 -0800,1614724148.0,"Add wait_for_worker param to TCPStore and fix port in use flaky test failures (#52888)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52888

Test Plan: Imported from OSS

Reviewed By: glaringlee

Differential Revision: D26678707

Pulled By: H-Huang

fbshipit-source-id: 5662e60c4d06d88d2e57834f496b52fb7600de29",39.0,73.0,"test/distributed/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/testing/_internal/common_utils.py",3.0,8,2,0.900482176,4.0,8629.0,3.0,209694.3333333333,9360.0,20721.0,0.0,Corrective,1.0,1
pytorch,2aee2d312fcdd56dcc30e956cb5ed4e492e503d3,24476090df4dbfe898f5040d2cd549e9dc382010,Adam Paszke,adam.paszke@gmail.com,Wed Aug 24 15:38:44 2016 -0700,1472053124.0,Add volatile variables,79.0,14.0,"test/test_autograd.py,test/test_nn.py,torch/autograd/function.py,torch/autograd/variable.py,torch/nn/modules/criterion.py",5.0,5,2,2.203842917,1.0,503.0,2.0,60126.2,132.0,2244.528571,0.0,Feature Addition,0.0,1
pytorch,b32dd4a876914cae2bb7910e13c133a48cbb006f,24a2f2e3a047963fd446c5995d6834f90e364bb0,Sam Gross,colesbury@gmail.com,Fri Jan 13 19:36:25 2017 -0500,1484336185.0,Add MaxUnpool1d module (#447),224.0,141.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py",6.0,8,3,1.771825924,21.0,3185.0,3.0,65649.0,125.0,825.6565415,0.0,Feature Addition,0.0,1
pytorch,8a2933883706f5175ef6b491f8fb08996f6f99f6,24af02154c1a599e2d785ed667c8c2968e230e87,Sam Gross,colesbury@gmail.com,Thu Dec 29 01:34:23 2016 -0500,1482975263.0,"Use ForkingPickler for sharing tensor/storages across processes (#344)

This hooks into the (internal) ForkingPickler class in multiprocessing
to reduce tensors, storages, and CUDA events instead of our queue from
joblib. This makes it easier to use the standard multiprocessing classes
in later versions of Python.

This also exposes:

 - Tensor/Storage.share_memory_()
 - Module.share_memory()

These methods move the CPU tensors and storages to shared memory. If
you're using the ""fork"" method of multiprocessing, these objects can be
directly inherited instead of serialized through a queue.",538.0,782.0,"test/test_multiprocessing.py,torch/__init__.py,torch/csrc/Module.cpp,torch/csrc/generic/StorageSharing.cpp,torch/cuda/__init__.py,torch/lib/libshm/core.cpp,torch/multiprocessing/__init__.py,torch/multiprocessing/_storage.py,torch/multiprocessing/_tensor.py,torch/multiprocessing/common.py,torch/multiprocessing/pool.py,torch/multiprocessing/queue.py,torch/multiprocessing/reductions.py,torch/nn/modules/module.py,torch/storage.py,torch/tensor.py,torch/utils/data/dataloader.py",17.0,12,2,3.323026338,20.0,3683.0,2.0,35664.75,103.0,70.15764778,0.0,,0.0,1
pytorch,4d0097fab83c9550e109a6a41c8ace735e8d23be,24b49314625e13f98cd761dec939db1d825420d9,Simeon Monov,sdmonov@us.ibm.com,Mon Apr 16 18:33:50 2018 -0700,1523903630.0,"Improve run_test.py to support running individual test classes and methods (#6344)

* Improve run_test.py to support running individual test classes and methods

Added support in run_test.py for running individual test classes and methods.
The -i/--include option can specify a list of test modules, classes or methods
like this:

python run_test.py -i autograd torch.TestTorch.test_abs \
  torch.TestTorch.test_add utils.TestBottleneck

-f, -l and -x behaviour stays the same as before

* Fixed some code formatting

* Multiple fixes according to the reviews in #6344",78.0,17.0,test/run_test.py,1.0,1,1,0,6.0,286.0,1.0,375671.0,586.0,3474.5,0.0,Corrective,1.0,1
pytorch,cf5a21e4a1cd923c82f0089025536368db169481,24e958a0a779e3dcd7bda19ef3fa702ee654e9dc,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Wed Sep 19 23:17:18 2018 -0700,1537399038.0,"Move bernoulli into ATen (#10273)

Summary:
+ https://github.com/pytorch/pytorch/issues/10236 : torch.bernoulli's out kwarg is broken
  fixed in moving `bernoulli_out` to ATen
+ https://github.com/pytorch/pytorch/issues/9917 : BUG torch.bernoulli(p.expand(shape)) is broken
  fixed in moving all `bernoulli` ops in ATen to use the modern apply utils methods
+ https://github.com/pytorch/pytorch/issues/10357 : torch.bernoulli inconsistent gpu/cpu results
  fixed by adding CUDA asserts

In order to use `curand_uniform4`, I made some changes to `CUDAApplyUtils.cuh`. Specifically, I introduced an optional template parameter `int step` to the `CUDA_tensor_applyN` methods, representing that we want to process `step` values at each time for each of the `N` tensors.

The calling convention for `step = 1` (default) isn't changed. But if `step > 1`, the given lambda `op` must take in `int n` as its first argument, representing the number of valid values, because there may not be full `step` values at the boundary. E.g., here is what the `bernoulli(self, p_tensor)` call look like:
```cpp

  // The template argument `4` below indicates that we want to operate on four
  // element at each time. See NOTE [ CUDA_tensor_applyN helpers ] for details.
  at::cuda::CUDA_tensor_apply2<scalar_t, prob_t, 4>(
      ret, p,
      [seeds] __device__(
          int n, scalar_t& v1, scalar_t& v2, scalar_t& v3, scalar_t& v4,
          const prob_t& p1, const prob_t& p2, const prob_t& p3, const prob_t& p4) {
        curandStatePhilox4_32_10_t state;
        curand_init(
            seeds.first,
            blockIdx.x * blockDim.x + threadIdx.x,
            seeds.second,
            &state);
        float4 rand = curand_uniform4(&state);
        switch (n) {
          case 4: {
            assert(0 <= p4 && p4 <= 1);
            v4 = static_cast<scalar_t>(rand.w <= p4);
          }
          case 3: {
            assert(0 <= p3 && p3 <= 1);
            v3 = static_cast<scalar_t>(rand.z <= p3);
          }
          case 2: {
            assert(0 <= p2 && p2 <= 1);
            v2 = static_cast<scalar_t>(rand.y <= p2);
          }
          case 1: {
            assert(0 <= p1 && p1 <= 1);
            v1 = static_cast<scalar_t>(rand.x <= p1);
          }
        }
      }
    );
```

Benchmarking on `torch.rand(200, 300, 400)` 20 times, each time with 20 loops:

post patch
```
â  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py
torch.bernoulli(x)
6.841588497161865 +- 0.05413117632269859
torch.bernoulli(xc)
0.05963418632745743 +- 0.0008014909108169377
x.bernoulli_()
0.4024486541748047 +- 0.0021550932433456182
xc.bernoulli_()
0.02167394384741783 +- 2.3818030967959203e-05

```

pre-patch
```
â  ~ numactl --cpunodebind 1 --membind 1 -- taskset -c 12,13,14,15,16,17,18,19,20,21,22,23 env CUDA_LAUNCH_BLOCKING=1 python bern.py
torch.bernoulli(x)
12.394511222839355 +- 0.0966421514749527
torch.bernoulli(xc)
0.08970972150564194 +- 0.0038722590543329716
x.bernoulli_()
1.654480218887329 +- 0.02364428900182247
xc.bernoulli_()
0.058352887630462646 +- 0.003094920190051198

```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10273

Differential Revision: D9831294

Pulled By: SsnL

fbshipit-source-id: 65e0655a36b90d5278b675d35cb5327751604088",1098.0,623.0,"aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Declarations.cwrap,aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cuda/CUDAApplyUtils.cuh,aten/src/ATen/gen.py,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h,aten/src/ATen/native/cuda/Distributions.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/THRandom.cpp,aten/src/TH/THRandom.h,aten/src/TH/generic/THTensorRandom.cpp,aten/src/TH/generic/THTensorRandom.h,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.cpp,aten/src/TH/generic/THVectorDispatch.cpp,aten/src/TH/vector/AVX.cpp,aten/src/TH/vector/AVX.h,aten/src/TH/vector/SSE.cpp,aten/src/THC/THCTensorRandom.cu,aten/src/THC/generic/THCTensorRandom.cu,aten/src/THC/generic/THCTensorRandom.h,test/test_cuda.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/jit/ir.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/nn/init.py",37.0,23,4,3.206301173,42.0,39230.0,27.0,2327679.783783784,4222.0,11844.83333,0.0,Corrective,1.0,1
pytorch,35a8242226445c6a704a4b891d92b8df35e98128,2504af5ec98f3a46d816c25b3a51577f3e4fadf2,eqy,eddiey@nvidia.com,Wed Jun 28 16:13:29 2023 +0000,1687968809.0,"[cuDNN][cuDNN V8 API] Improve safety of `ParamsHash` keys (#104122)

In anticipation of adding some enhancements to the cuDNN benchmark cache (e.g., LRU eviction for memory savings), this PR adds some safety improvements to the handling of cache keys.

Currently, cache keys are dangerous to use, as e.g., a single inadvertent pass-by-value will potentially instantiate a copy with uninitialized padding bytes that will unwittingly hash differently and compare as unequal. This behavior is the result of `ParamsHash` using raw-bytes for hashing and comparison. I've been bitten by this in the past and would like to hopefully eliminate this class of errors.

Additionally, I'm not sure the standard guarantees that default copy/move constructors copy structs byte-for-byte, and this could be problematic when using maps as insertion could call these default constructors in order to instantiate a `std::pair`. Someone knowledgeable in C++ can correct me on this, but it seems that we are potentially relying on the good graces of common compiler implementations rather than the actual standard here.

This PR adds a variant of `ParamsHash` that expects a wrapped POD that has custom byte-for-byte constructors. It modifies the cuDNN V8 API benchmark cache to use this variant, and replaces the `setCacheKey` style code with constructor usage. If this approach looks good to folks I will also port other `ParamsHash` usage (e.g., in cuDNN v7 and other backends) and we can remove `ParamsHash`.

CC @malfet
@ngimel (who originally wanted constructors for keys, but I didn't have this solution in mind at the time)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/104122
Approved by: https://github.com/zasdfgbnm, https://github.com/colesbury",106.0,54.0,"aten/src/ATen/native/cudnn/Conv_v8.cpp,aten/src/ATen/native/utils/ParamsHash.h",2.0,6,1,0.922406262,3.0,777.0,2.0,543051.5,17232.0,38810.0,0.0,Corrective,0.0,1
pytorch,c2a75926cac36b770c9afcca894d07feb307592e,25110d61fb868dcae5f1aee6edae38c8bea74523,Brennan Vincent,btv@fb.com,Sat Dec 08 04:13:31 2018 -0800,1544242411.0,"Implement `std` for multiple dimensions on CPU devices. (#14535)

Summary:
Tested on a tensor with 1 billion elements and 3 dimensions on a powerful, highly
multi-core Linux machine.

parallelized: All operations (e.g., `t.std(1)`) that could be done in the old code are now several times faster. All
new operations (e.g., `t.std((0,2))` are significantly faster than the NumPy equivalents.
`t.std((0, 1, 2))`, a new operation, is logically equivalent to the
old `t.std()`, but faster.

serial: The above comment about old operationos now being faster still
holds, but `t.std((t1, ..., tn))` is now a few
times slower than `t.std()`. If this turns out to be important, we can
special-case that to use the old algorithm.

The approach is to create a new method, `TensorIterator::foreach_reduced_elt`,
valid for `TensorIterator`s that represent a dimension reduction. This
method calls a supplied function for each element in the output,
supplying it with the input elements that correspond to that output.

Given that primitive, we can implement reductions like the following pseudocode:

If there is more than one output element:
```
PARALLEL FOR EACH element IN output:
    accumulator = identity
    SERIAL FOR EACH data_point IN element.corresponding_input:
        accumulator.update(data_point)
    element = accumulator.to_output()
```

If there is only one output element, we still want to parallelize, so we
do so along the *input* instead:

```
accumulators[n_threads]
PARALLEL FOR EACH input_chunk IN input.chunks():
    accumulators[thread_num()] = identity
    SERIAL FOR EACH data_point IN input_chunk:
        accumulators[thread_num()].update_with_data(data_point)
accumulator = identity
SERIAL FOR EACH acc in accumulators:
    accumulator.update_with_other_accumulator(acc)
output_element = accumulator.to_output()
```

Note that accumulators and data points do not have to be the same type
in general, since it might be necessary to track arbitrary amounts of
data at intermediate stages.

For example, for `std`, we use a parallel version of Welford's
algorithm, which requies us to track the mean, second moment, and number
of elements, so the accumulator type for `std` contains three pieces of
data.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14535

Differential Revision: D13283887

Pulled By: umanwizard

fbshipit-source-id: 8586b7bf00bf9f663c55d6f8323301e257f5ec3f",312.0,81.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOps.h,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/TensorIteratorReduce.cpp,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/csrc/jit/passes/shape_analysis.cpp",15.0,14,4,3.001898964,41.0,22925.0,9.0,1056231.3333333333,5928.0,17914.83333,0.0,Non Functional,0.0,1
pytorch,092b2f7fee0f35fcbebde38f58276d0e51e2ddf3,257b61495eb8c1dd5afd5cb490803c0109f0fe90,Lu Fang,lufang@fb.com,Fri Sep 27 00:11:17 2019 -0700,1569543077.0,"Revert D17610292: [pytorch][PR] Choose num_threads in parallel_for based on GRAIN_SIZE

Test Plan: revert-hammer

Differential Revision:
D17610292

Original commit changeset: 60b9fe4b0eec

fbshipit-source-id: cfa0be39eef5bf306ef128c134f86a135bb3d5c9",3.0,9.0,aten/src/ATen/ParallelOpenMP.h,1.0,3,1,0,1.0,100.0,1.0,11075.0,11815.0,33109.83333,0.0,,0.0,1
pytorch,a850d8a526d0d964159493dbd17f94c181af7339,25abdcb3d1a86beb76c857ce8ccad78c12927967,Nick Gibson,nickg@fb.com,Wed Apr 22 00:56:04 2020 -0700,1587516964.0,"[TensorExpr] add Block flattening to IR Simplifier (#37013)

Summary:
Some IR optimizations were leaving superfluous Blocks in the IR, this PR adds simplification and merging of enclosing Block statements to the IR Simplifier, e.g.

```
Block {
   Stmt 1
   Block {
       Stmt 2
   }
   Block {}
}
```

becomes

```
Block {
   Stmt 1
   Stmt 2
}
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37013

Differential Revision: D21166208

Pulled By: nickgg

fbshipit-source-id: 6dcdf863980d94731a8ddf184882c4a5b7259381",128.0,26.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",4.0,7,2,0.726874323,2.0,4127.0,2.0,135932.75,1278.0,3354.5,0.0,Feature Addition,0.0,1
pytorch,e33df2b88a31c08567c81f49c45f1eb530cd7ef4,25b166ed1f58fe1ba255cf218f8b44bf7cc9bb5b,Soumith Chintala,soumith@gmail.com,Mon Nov 13 04:26:42 2017 -0500,1510547202.0,add depthwise convolution terminology as a note,30.0,0.0,torch/nn/modules/conv.py,1.0,3,1,0,36.0,675.0,1.0,314053.0,2116.0,24069.85823,0.0,Feature Addition,0.0,1
pytorch,50c153926e1f25d1ab4fc49f83ae3b77e0803c3d,25bcfbf59eeceb476825973afce205228a6de705,Andreas KÃ¶pf,andreas.koepf@xamla.com,Wed Dec 16 00:20:35 2015 +0100,1450225235.0,Add THCUNN/ffi conversion of Abs and AbsCriterion,47.0,61.0,"Abs.cu,AbsCriterion.cu,CMakeLists.txt,THCUNN.h",4.0,0,0,1.896154149,29.0,160.0,2.0,-6303487.666666667,47.0,323.9047619,0.0,Feature Addition,0.0,1
pytorch,425f173f9d8cb41988ff03fff316ff8424bcd521,2626cd3ba4452234ac82501d7959a4c5ca6bf063,Elias Ellison,eellison@devfair044.h1.fair,Wed Sep 15 20:43:12 2021 -0700,1631738592.0,"Add Maxpool to shape analysis / Opinfo (#63530)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63530

how to review: pretty much just check that the inputs generated are a good representation of the op semantics, that should be sufficient for correctness, and then you can also double check the op size semantics by going to https://codebrowser.bddppq.com/pytorch/pytorch/ typing in native::{op_name} and looking at the op implementation as a bonus if you want

Test Plan: Imported from OSS

Reviewed By: driazati

Differential Revision: D30738147

Pulled By: eellison

fbshipit-source-id: cf52339e572ee04e0d6167fd95d8a82d58ea7706",112.0,2.0,"torch/csrc/jit/runtime/symbolic_shape_registry.cpp,torch/testing/_internal/common_methods_invocations.py",2.0,6,1,0.934849024,2.0,10582.0,2.0,339028.0,15471.0,35373.5,0.0,Corrective,0.0,1
pytorch,2f598b53ddfbd2dbbaddb76a0f30018a713f3c7a,2639c4e6b37c530285772ea8592ee24e530f9aaf,Rong Rong (AI Infra),rongr@fb.com,Mon Apr 26 17:37:22 2021 -0700,1619458642.0,"fix bug in rocm device type (#56646)

Summary:
related to https://github.com/pytorch/pytorch/issues/56156.

https://github.com/pytorch/pytorch/issues/55808 effectively turned dtypeIfROCM off but let some legacy issues unfixed. Given the fact that we still need to deal with discrepancy between the 2 platform. This PR turns dtypeIfROCM default pointing to dtypeIfCUDA and only override when user specifies.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56646

Reviewed By: mruberry

Differential Revision: D27968959

Pulled By: walterddr

fbshipit-source-id: 6a11987b8ddf4417577b3d0d5054eaab169de42c",4.0,4.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,6052.0,1.0,2883.0,11273.0,24895.5,0.0,Corrective,1.0,1
pytorch,ba952d8308cd8d64430c3e36dbf7a0633695ed6a,2679aa47897232827771ad7bb18e14bb4be3cae8,Nikita Shulga,nshulga@fb.com,Wed May 25 14:59:57 2022 +0000,1653490797.0,"[MPS] Lazy initialize allocators (#78227)

Do not construct MPS allocators at load time, but rather create them
lazily when needed

This significantly reduces `libtorch.dylib` load time and prevents weird
flicker, when during import torch when Intel MacBook runs switches from
integrated to discrete graphics

Before the change `python3 -c ""import timeit;import importlib;print(timeit.timeit(lambda: importlib.import_module('torch'), number=1))""` takes about 1 sec, after the change it drops down to .6 sec

Minor changes:
 - Deleted unused `__block id<MTLBuffer> buf = nil;` from
   HeapAllocatorImpl
 - Add braces for single line if statements

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78227
Approved by: https://github.com/kulinseth, https://github.com/albanD",39.0,17.0,"aten/src/ATen/mps/MPSAllocator.mm,aten/src/ATen/mps/MPSDevice.mm",2.0,4,1,0.301378644,1.0,413.0,2.0,630356.5,3632.0,8619.0,0.0,Corrective,1.0,1
pytorch,8bd80a6b7488ad07af6cc9b2ab1046ab36a14c0f,267e1ec112b34bdb925c78fd399ef049f66777d6,Thomas Viehmann,tv@beamnet.de,Wed Sep 05 17:17:08 2018 -0700,1536167828.0,"Accept more numpy scalars as doubles (#9659)

Summary:
Allows mulitplication of e.g. numpy.float32 with tensors.

This came up with #9468

If you want this and after the other patch is done, I'll add tests (but that would be conflicting, so I prefer to wait).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9659

Differential Revision: D8948078

Pulled By: weiyangfb

fbshipit-source-id: c7dcc57b63e2f100df837f70e1299395692f1a1b",31.0,13.0,"test/test_torch.py,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_numbers.h,torch/csrc/utils/tensor_numpy.cpp,torch/csrc/utils/tensor_numpy.h",5.0,4,2,1.931086228,40.0,9749.0,5.0,3194556.6,3859.0,10763.83333,0.0,Feature Addition,1.0,1
pytorch,57e37080cd4b6dc03661d53b008f6a73d6cc38c8,268cc117a899ca2d42d63697107b47cf645e620d,anjali411,chourdiaanjali123@gmail.com,Tue Apr 27 14:36:38 2021 -0700,1619534198.0,"Add OpInfos for torch.{complex, view_as_real, view_as_complex} (#56524)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56524

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D27909165

Pulled By: anjali411

fbshipit-source-id: 38592cdb357386549c8309792ef7c3218665d286",40.0,56.0,"test/test_autograd.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.673654715,42.0,14667.0,2.0,15024.5,11322.0,24965.5,0.0,Feature Addition,0.0,1
pytorch,5177f8de2b9a53b4dec05d4ec00b00d1fe8f76ec,26a9012f842c48dcd2bc9a50aee7a9a84d87122b,Zachary DeVito,zdevito@fb.com,Mon Oct 05 22:15:43 2020 -0700,1601936143.0,"[fx] import used modules for code gen (#45471)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/45471

Intead of assuming that 'torch' is the only module used by generated code,
use the qualified names of builtin functions to generate import statements
for all builtins. This allows user-captured functions to also get code generated correctly.

Test Plan: Imported from OSS

Reviewed By: jamesr66a

Differential Revision: D23978696

Pulled By: zdevito

fbshipit-source-id: ecbff150e3de38532531cdadbfe4965468f29a38",26.0,7.0,"test/test_fx.py,torch/fx/graph.py,torch/fx/graph_module.py",3.0,3,2,1.233515936,1.0,1444.0,2.0,185386.3333333333,5730.0,13398.5,0.0,Corrective,0.0,1
pytorch,12b4e8996f4f2d1c3604b42ad6d1c8a7a9fa781d,26beda8ed5db28e7c559f975d43a306bcaecb466,Rong Rong (AI Infra),rongr@fb.com,Wed Jun 09 15:15:44 2021 -0700,1623251744.0,"[BE] unsupported backward failing on single sample (#59455)

Summary:
Echo on https://github.com/pytorch/pytorch/pull/58260#discussion_r637467625

similar to `test_unsupported_dtype` which only check exception raised on the first sample. we should do similar things for unsupported_backward as well. The goal for both test is to remind developer to
1. add a new dtype to the support list if they are fulling runnable without failure (over all samples)
2. replace the skip mechanism which will indefinitely ignore tests without warning

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59455

Test Plan: CI.

Reviewed By: mruberry

Differential Revision: D28927169

Pulled By: walterddr

fbshipit-source-id: 2993649fc17a925fa331e27c8ccdd9b24dd22c20",129.0,47.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.604123458,2.0,8215.0,2.0,62519.0,12862.0,29178.5,0.0,Feature Addition,0.0,1
pytorch,b82453cbd46485fa34282914ec22b6204492dc6d,26cdec6ce47cc108c7c94d591e58a0f044e1a942,Akifumi Imanishi,imanishi@preferred.jp,Thu Jun 24 06:52:00 2021 -0700,1624517520.0,"Support `torch.bitwise_{left/right}_shift` and `__rlshift__`, `__rrshift__` (#59544)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/58121

This PR implements `torch.bitwise_left_shift` and `torch.bitwise_right_shift` and `torch.Tensor.{__rlshift__/__rrshift__}`for compatibility with Python array API standard.
(cc: mruberry, rgommers, emcastillo, kmaehashi)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59544

Reviewed By: ngimel

Differential Revision: D29348869

Pulled By: mruberry

fbshipit-source-id: 329aee296cf890735e8a9f858bccfe87c03d06ca",305.0,9.0,"aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_binary_ufuncs.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",13.0,12,4,2.961016576,36.0,42994.0,11.0,2024297.307692308,13313.0,30096.5,0.0,Corrective,1.0,1
pytorch,071e68d99d67374533491f1dbaa364e97284de27,26d626a47c9c676af456ce6b6025ce5575ad4b2f,Soumith Chintala,soumith@fb.com,Mon Sep 19 15:45:15 2016 -0400,1474299915.0,"adding docs for loss functions, container, module and fix typos",798.0,12.0,"docs/nn.md,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/loss.py,torch/nn/modules/module.py",5.0,4,2,1.769264129,14.0,2708.0,3.0,131845.2,220.0,1716.748261,0.0,Corrective,1.0,1
pytorch,7ad39add016026a307358018343ffba8663d3421,26ead7753f8ce32c36cabbb8287c801d06bddb37,Horace He,horacehe2007@yahoo.com,Wed Oct 27 18:53:20 2021 -0700,1635360800.0,[functorch] Update op db and fix failing tests,133.0,15.0,"functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",4.0,2,1,1.643194145,1.0,4661.0,4.0,0.75,481.0,675.0,0.0,Corrective,1.0,1
pytorch,1ae884ff86122f00aedf9f7c5e6a9e41e79f4d93,27265503ad1b4c63e2caceb6b991a7abe998d7bd,Tongzhou Wang,SsnL@users.noreply.github.com,Thu Mar 01 23:11:39 2018 -0500,1519945899.0,"nn.* doc update after Variable/Tensor merge (#5459)

The nn.* counterpart of #5443 . Mostly removed Variable wrapper. Also added doc for nn.RReLU.

Notice that torch.randn(*, requires_grad=True) isn't documented until #5462 is done.",337.0,351.0,"torch/nn/functional.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/distance.py,torch/nn/modules/dropout.py,torch/nn/modules/fold.py,torch/nn/modules/instancenorm.py,torch/nn/modules/linear.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/modules/normalization.py,torch/nn/modules/padding.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/modules/upsampling.py",18.0,3,1,3.277327867,37.0,9171.0,16.0,3990952.444444445,180.0,12880.72461,0.0,Feature Addition,0.0,1
pytorch,35f7925aad7ae3e22126810fa48ff9cffb49c197,27455e9c788a1e95411cd7e0fcbabf6f866ded42,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Wed Jul 18 17:39:39 2018 -0700,1531935579.0,"Use _six for inf and nan (#9500)

Summary:
Things like `float('inf')` are actually quite expensive.
```py
In [1]: import math

In [2]: %timeit -n 200 math.inf
49.3 ns Â± 1.42 ns per loop (mean Â± std. dev. of 7 runs, 200 loops each)

In [3]: %timeit -n 200 float('inf')
194 ns Â± 39.1 ns per loop (mean Â± std. dev. of 7 runs, 200 loops each)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9500

Reviewed By: soumith

Differential Revision: D8876229

Pulled By: SsnL

fbshipit-source-id: 78602b76bb53d5588910b58270930c0bd413d2d7",154.0,129.0,"test/common.py,test/test_autograd.py,test/test_cuda.py,test/test_distributions.py,test/test_nn.py,test/test_optim.py,test/test_torch.py,torch/_six.py,torch/_tensor_str.py,torch/distributions/categorical.py,torch/distributions/cauchy.py,torch/distributions/fishersnedecor.py,torch/distributions/half_cauchy.py,torch/distributions/half_normal.py,torch/distributions/kl.py,torch/distributions/multinomial.py,torch/distributions/studentT.py,torch/functional.py,torch/legacy/nn/Normalize.py,torch/legacy/optim/cg.py,torch/nn/utils/clip_grad.py,torch/optim/lr_scheduler.py",22.0,9,2,3.480646562,45.0,29718.0,16.0,2583577.727272727,2994.0,7039.833333,0.0,,0.0,1
pytorch,e274c2e4fd5f5039a4837ac9668cd564e2e50463,2763b5080362489e40a0653de2ebb9b4df18c248,Fuzzkatt,zonghan2000@gmail.com,Thu May 25 00:30:57 2023 +0000,1684974657.0,"update thresholds for various ops in functorch/test_ops.py (#102016)

update thresholds for following ops:

linalg.multi_dot, pca_lowrank
matrix_exp
matmul, __rmatmul__
Pull Request resolved: https://github.com/pytorch/pytorch/pull/102016
Approved by: https://github.com/ngimel, https://github.com/zou3519",16.0,5.0,test/functorch/test_ops.py,1.0,2,1,0,1.0,2230.0,1.0,1628522.0,16200.0,36664.5,0.0,,0.0,1
pytorch,4bf076e964a6304fdcfbfc3fc0475da65de00fc9,278958686b6879b9310bf530705f297dadf35e02,Fabio Rocha,frocha@quansight.com,Fri Jul 08 09:00:10 2022 +0000,1657270810.0,"Added OpInfo for nn.functional.dropout3d (#81045)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81045
Approved by: https://github.com/Lezcano, https://github.com/mruberry",21.0,9.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,6.0,21429.0,1.0,67274.0,5142.0,12142.5,0.0,Feature Addition,0.0,1
pytorch,008a62b18ab28368684ce79a8a1a0a0efed40ae4,27bd3df71bfced095682784560fb50916675e44a,Adam Fisch,adamfisch15@gmail.com,Wed Aug 23 11:12:21 2017 -0400,1503486741.0,"Patching EmeddingBag to accept 2D input (#2429)

* Patching EmeddingBag to accept 2D input

* fix for CUDA inputs

* fix lint",24.0,12.0,"test/test_nn.py,torch/nn/modules/sparse.py",2.0,4,2,0.581321499,34.0,4254.0,2.0,319178.5,1351.0,15527.06175,0.0,Corrective,1.0,1
pytorch,"da17414b3f94485680558b1c498a641e5fe2591d,6ee77b4edd1552d3a9a2e5389ffc351e513a8089",27c4c6e0af63ff3339425b509a4eab5c23174e79,Sam Gross,sgross@fb.com,Sat Jan 28 01:16:41 2017 -0800,1485566201.0,Merge commit '6ee77b4edd1552d3a9a2e5389ffc351e513a8089',585.0,2.0,"torch/lib/THCUNN/Abs.cu,torch/lib/THCUNN/CMakeLists.txt,torch/lib/THCUNN/ELU.cu,torch/lib/THCUNN/HardTanh.cu,torch/lib/THCUNN/LeakyReLU.cu,torch/lib/THCUNN/LogSigmoid.cu,torch/lib/THCUNN/PReLU.cu,torch/lib/THCUNN/RReLU.cu,torch/lib/THCUNN/Sigmoid.cu,torch/lib/THCUNN/SoftPlus.cu,torch/lib/THCUNN/SoftShrink.cu,torch/lib/THCUNN/SpatialClassNLLCriterion.cu,torch/lib/THCUNN/SpatialReflectionPadding.cu,torch/lib/THCUNN/SpatialReplicationPadding.cu,torch/lib/THCUNN/Sqrt.cu,torch/lib/THCUNN/Square.cu,torch/lib/THCUNN/THCUNN.h,torch/lib/THCUNN/Tanh.cu,torch/lib/THCUNN/TemporalRowConvolution.cu,torch/lib/THCUNN/Threshold.cu,torch/lib/THCUNN/VolumetricReplicationPadding.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THCUNN/generic/TemporalRowConvolution.cu,torch/lib/THCUNN/row2col.h",24.0,4,1,1.479427394,22.0,2328.0,1.0,671257.0,94.0,4985.651435,0.0,,0.0,1
pytorch,770089c2b8ef8693151a2b5911f1138abbe78ecb,27d1daab45dc19953345b3c44185952116c249ad,Diego Estrada,haidar.lara@gmail.com,Mon Jun 03 17:55:51 2019 -0700,1559584551.0,"Export ONNX Dropout for opset 10 (#20710)

Summary:
Remove Dropout from the opset 10 blacklist.
ONNX Dropout was modified in opset 10, but only the output ""mask"" was modified, which is not exported in pytorch opset 9. So we can still fallback on the opset 9 op.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20710

Differential Revision: D15571248

Pulled By: houseroad

fbshipit-source-id: 15267eb63308a29a435261034b2f07324db1dea6",41.0,5.0,"test/onnx/test_onnx_opset.py,test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.444747003,1.0,3654.0,3.0,1222908.25,9099.0,26672.83333,0.0,,0.0,1
pytorch,bdb05c2243f4798c671520a234c8a6fabf691910,27d7182d6c7e223e04166f33d5ec46ef8b510944,Siddharth Mittal,smittal6@gmail.com,Sun Jan 14 17:06:47 2018 +0500,1515949607.0,"replace full stop by comma

From (batch. hidden_size) to (batch, hidden_size)",1.0,1.0,torch/nn/modules/rnn.py,1.0,3,1,0,33.0,761.0,1.0,1111824.0,916.0,6719.172317,0.0,,0.0,1
pytorch,95233870f284565df602471bf09b8aad565540b8,27f7d1c2865355c694fb964609df45974748615b,Kurt Mohler,kmohler@quansight.com,Thu Dec 10 07:26:06 2020 -0800,1607585166.0,"Port `eig` CPU from TH to ATen (#43215)

Summary:
Also consolidates shared logic between `eig` CPU and CUDA implementations

Fixes https://github.com/pytorch/pytorch/issues/24693

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43215

Reviewed By: VitalyFedyunin, zhangguanheng66

Differential Revision: D23862622

Pulled By: ngimel

fbshipit-source-id: ca1002428850520cd74cd5b7ed8cb4d12dbd9c52",169.0,208.0,"aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/LegacyTHFunctionsCPU.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THLapack.cpp,aten/src/TH/generic/THLapack.h,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorLapack.h,aten/src/THC/generic/THCTensorMathMagma.h,test/test_linalg.py,test/test_torch.py",14.0,10,2,3.002429447,44.0,27924.0,9.0,11340809.0,7350.0,16544.0,0.0,Corrective,1.0,1
pytorch,6581063583434a71442ddbe0901f1fc36aa6e83c,282dfe8ba4d514b0b1ad41edcfcb253cac9582f6,Bin Bao,binbao@fb.com,Fri Dec 09 01:53:12 2022 +0000,1670550792.0,"[inductor][Reland] Use decomposition for _to_copy (#90494)

Summary: also contains a fix for https://github.com/pytorch/pytorch/issues/89633

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90494
Approved by: https://github.com/ngimel",7.0,32.0,"test/inductor/test_torchinductor_opinfo.py,torch/_decomp/decompositions.py,torch/_inductor/decomposition.py,torch/_inductor/lowering.py",4.0,5,2,0.998486165,3.0,7424.0,1.0,80564.0,10370.0,23723.5,0.0,Corrective,1.0,1
pytorch,9257de7efa190134f5c9600b24ca604b4b37207c,28549b618a0ea1d95492aa242874cb5bbd9781aa,Douglas Lehr,Doug.Lehr@amd.com,Fri Feb 11 22:05:22 2022 -0800,1644617122.0,"[ROCm] Enable skipped ROCm unit tests (#67706)

Summary:
A number of ROCm tests were skipped via the skipCUDAIfRocm flag.
A majority of the testcases are now supported on the ROCm platform. This fix enabled all of the test_ops tests for ROCm and enables most Operators in  common_methods_invocations.py minus the SpectralFuncInfo class which still has some fft issues.

Partially Fixes https://github.com/pytorch/pytorch/issues/51303

cc jeffdaily sunway513 jithunnair-amd ROCmSupport KyleCZH amathews-amd

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67706

Reviewed By: seemethere, janeyx99

Differential Revision: D34153457

Pulled By: malfet

fbshipit-source-id: 95f4420f306ca7580cd438d3b5cc0b24efbfae99
(cherry picked from commit 0d178fffd3762579fdcc1129ea921090f2694c8a)",97.0,74.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.190564877,4.0,17068.0,2.0,184412.0,668.0,1568.5,0.0,Corrective,1.0,1
pytorch,b69b885e8215d6c7125d9ccef5020c672178a99e,285a9e2452f38b8e3fcd2b5c31ae7189a552e7a2,gchanan,gregchanan@gmail.com,Thu Mar 01 19:06:55 2018 -0500,1519931215.0,"Add dtype to torch.Tensor constructors and accept them in set_default_tensor_type (#5444)

* Add dtype to torch.Tensor, torch.FloatTensor, etc.

* Support passing dtypes to set_default_tensor_type.

* Check dtype exception.

* Correctly handle new type initialization order.

* Move handling of torch.Storage alias to C++.

* Delete function that erroneously reappeared.",147.0,36.0,"test/test_torch.py,torch/__init__.py,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Module.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/tensor_dtypes.cpp",7.0,5,2,1.889305486,39.0,6821.0,5.0,179741.42857142858,575.0,1755.905869,0.0,Corrective,0.0,1
pytorch,0ab68b8db49e6c74fe5b1d199ecb14281c254359,28890b20461453d21564142295e727dec58044c1,Richard Zou,zou3519@users.noreply.github.com,Wed Dec 13 17:48:00 2017 -0500,1513187280.0,"Add rnn args check (#3925)

* Add rnn args check

* Check both hidden sizes for LSTM

* RNN args check test",98.0,10.0,"test/test_nn.py,torch/backends/cudnn/rnn.py,torch/nn/modules/rnn.py",3.0,6,2,1.298287301,37.0,6164.0,2.0,1509416.6666666667,2181.0,24185.85823,0.0,Feature Addition,0.0,1
pytorch,30943d16107f5f42b54ef7494aea2e09e681421f,28a36546680ddb2d629566d4962907c4a8a7df1b,Edward Z. Yang,ezyang@fb.com,Mon Apr 18 19:20:19 2022 -0400,1650309619.0,"Make PYTORCH_TEST_WITH_SLOW_GRADCHECK consistent with other test envvars

Signed-off-by: Edward Z. Yang <ezyangfb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75987

Approved by: https://github.com/soulitzer",3.0,3.0,".jenkins/pytorch/test.sh,torch/testing/_internal/common_utils.py",2.0,5,2,0.918295834,7.0,3835.0,2.0,220027.0,2396.0,5600.0,0.0,,0.0,1
pytorch,bff07eb7ad2d1a31da29b0fa97d07f80494801ee,28a908d7ba29b280a6c137c515184c99cd5499a5,samdow,samdow@fb.com,Thu May 12 15:34:22 2022 -0400,1652369662.0,[functorch] typo,1.0,1.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1290.0,1.0,1.0,1066.0,1456.0,0.0,,0.0,1
pytorch,9afdf017dc99a9d948b0daa754f77159ba6dcc4a,28bfdbb0663c3fd7884ef09cfc061886fa0dad1b,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Tue Sep 21 18:34:08 2021 -0700,1632249248.0,"OpInfo for `nn.functional.batch_norm` (#63218)

Summary:
Addresses https://github.com/facebookresearch/functorch/issues/78 and https://github.com/pytorch/pytorch/issues/54261.

* There exists `torch.batch_norm` but it takes an extra arg: `cudnn_enabled` (not there in functional variant). This is passed from the functional variant to `torch.batch_norm` here: https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L2282. `test_variant_consistency_jit` fails with an error: (when passed an alias)
    ```python
    File ""/home/krshrimali/Documents/Projects/Quansight/pytorch/test/test_ops.py"", line 457, in _test_consistency_helper
    variant_forward = variant(cloned,
    TypeError: batch_norm() missing 1 required positional arguments: ""cudnn_enabled""
    ```
    * I'm not sure of a solution to this, as AFIK - there is no way to pass a lambda wrapper for an alias. Hence, I've skipped adding this as an alias there.
    * On second thought, is this even an alias?

cc: mruberry zou3519 kshitij12345

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63218

Reviewed By: bdhirsh

Differential Revision: D31019785

Pulled By: zou3519

fbshipit-source-id: 2a834d05835da975289efc544a7ad7e98c99438f",102.0,3.0,"torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",2.0,3,1,0.512709142,2.0,11376.0,2.0,291353.0,15623.0,35932.0,0.0,Feature Addition,0.0,1
pytorch,1461859fdebe1346accf50751854b5448b832be8,28c24ec3e85a54056980cf26293ad7ab667193d9,kshitij12345,kshitijkalambarkar@gmail.com,Tue May 04 19:21:46 2021 -0700,1620156106.0,"[numpy] polygamma: int -> float promotion (#57462)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57462

Reviewed By: mrshenli

Differential Revision: D28187104

Pulled By: ezyang

fbshipit-source-id: 4072589ad1cb9766e7721d006d43701820922d56",44.0,36.0,"aten/src/ATen/native/Math.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/UnaryGammaKernels.cu,aten/src/ATen/native/native_functions.yaml,torch/testing/_internal/common_methods_invocations.py",5.0,8,2,1.732746269,13.0,18272.0,4.0,488500.6,11645.0,26330.5,0.0,,0.0,1
pytorch,ba61eee074385767151c2a0575317d1b16661b47,28f056fed2b82701b155a1e783e57cb2fbd5517d,li-roy,royli2@illinois.edu,Mon Feb 05 17:28:51 2018 -0800,1517851731.0,"add reduce=True argument to MultiLabelMarginLoss (#4924)

* add reduce=True argument to MultiLabelMarginLoss

* Fix lint

* Addressed comments

* Remove unneeded syncthreads calls",277.0,40.0,"aten/src/ATen/nn.yaml,aten/src/THCUNN/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/MultiLabelMarginCriterion.c,aten/src/THNN/generic/THNN.h,test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/legacy/nn/MultiLabelMarginCriterion.py,torch/nn/functional.py,torch/nn/modules/loss.py",12.0,15,4,2.856390487,39.0,14622.0,8.0,3076545.083333333,2333.0,24501.35823,0.0,Corrective,1.0,1
pytorch,0a30d64c83297d83fd7248c3173d5d0d1be88443,28fc59d13d57bd12b8eca25f54b38665f704aff8,Akshit Khurana,axit@fb.com,Wed Apr 28 21:52:37 2021 -0700,1619646757.0,"Add xnnpack hardswish op (#56714)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56714

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55800

For mobile use xnnpack implementation of hardswish

Test Plan: buck test //xplat/caffe2:pt_xnnpack_test

Reviewed By: kimishpatel

Differential Revision: D27712306

fbshipit-source-id: c7f0b70482aeef2aaa1966e2c669f79ecd29caa7",155.0,0.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/xnnpack/Activation.cpp,aten/src/ATen/native/xnnpack/Engine.h,aten/src/ATen/test/xnnpack_test.cpp,tools/build_variables.bzl",5.0,7,2,1.481769021,7.0,1981.0,3.0,1448510.0,11408.0,25786.0,0.0,Feature Addition,0.0,1
pytorch,2626cd3ba4452234ac82501d7959a4c5ca6bf063,29514bfcdbe460b15900b762576ed9bb1eea45d5,Elias Ellison,eellison@devfair044.h1.fair,Wed Sep 15 20:43:12 2021 -0700,1631738592.0,"Max Pool with indices (#64121)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64121

Add support for aten operators which return multiple outputs

Test Plan: Imported from OSS

Reviewed By: driazati

Differential Revision: D30738142

Pulled By: eellison

fbshipit-source-id: 0d7e51187bd5e3e9b43f0fdb5178366a97aec943",103.0,39.0,"test/test_ops.py,torch/csrc/jit/passes/symbolic_shape_analysis.cpp,torch/csrc/jit/runtime/symbolic_shape_registry.cpp,torch/testing/_internal/common_jit.py,torch/testing/_internal/common_methods_invocations.py",5.0,8,2,1.975447131,2.0,12586.0,4.0,354065.4,15472.0,35376.0,0.0,Feature Addition,0.0,1
pytorch,710a83d09fee3a57fa1211d69d31d1e44c1e8637,29753339b78e3a64060277e203346ef064d31011,Rong Rong (AI Infra),rongr@fb.com,Mon May 10 17:38:03 2021 -0700,1620668283.0,"Do not download slow test when on sandcastle (#57953)

Summary:
Downloading slow_test list on SC causes timeout, this is even a bigger issue since `common_utils.py` is reused in many internal projects/modules.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57953

Test Plan: CI

Reviewed By: janeyx99

Differential Revision: D28325527

fbshipit-source-id: ae47c9e43ad6f416008005bb26ceb2f3d6966f2e",5.0,2.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2278.0,1.0,1106213.0,11843.0,26968.0,0.0,,0.0,1
pytorch,f6f9d2222813c3fd47b433cb6af46fd61b3182ad,297c938729a0adbd98a235a5970c972f6148adfe,Iurii Zdebskyi,iuriiz@devfair004.maas,Wed Sep 02 19:15:40 2020 -0700,1599074140.0,"Add _foreach_add(TensorList tl1, TensorList tl2) and _foreach_add_(TensorList tl1, TensorList tl2) APIs (#42533)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42533

[First PR: Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar)](https://github.com/pytorch/pytorch/pull/41554).

**Motivation**
[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)
Current PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.
As an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).
In order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and itâs FusedAdam optimizer.

**Current API restrictions**
- List can't be empty (will fixed in upcoming PRs).
- All tensors in the list must have the same dtype, device and size.

**Broadcasting**
At this point we don't support broadcasting.

**What is 'Fast' and 'Slow' route**
In particular cases, we cant process an op with a fast list CUDA kernel. Still, we can do with a regular for-loop where the op will be applied to each tensor individually through the dispatch mechanisms. There are a few checks that decide whether the op will be performed via a 'fast' or 'slow' path.
To go the fast route,
- All tensors must have strided layout
- All tensors must be dense and not have overlapping memory
- The resulting tensor type must be the same.

----------------
**In this PR**
- Adding a `_foreach_add(TensorList tl1, TensorList tl2)` API
- Adding a `_foreach_add_(TensorList tl1, TensorList tl2)` API

**Tests**
Tested via unit tests

**TODO**
1. Properly handle empty lists

**Plan for the next PRs**
1. APIs
- Binary Ops for list with Scalar
- Binary Ops for list with list
- Unary Ops for list
- Pointwise Ops

2. Complete tasks from TODO
3. Rewrite PyTorch optimizers to use for-each operators for performance gains.

Test Plan: Imported from OSS

Reviewed By: zou3519

Differential Revision: D23331894

Pulled By: izdeby

fbshipit-source-id: 876dd1bc82750f609b9e3ba23c8cad94d8d6041c",364.0,15.0,"aten/src/ATen/native/ForeachOpsKernels.cpp,aten/src/ATen/native/ForeachUtils.h,aten/src/ATen/native/cuda/ForeachFunctors.cuh,aten/src/ATen/native/cuda/ForeachTensorAddList.cu,aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu,aten/src/ATen/native/native_functions.yaml,test/test_foreach.py,tools/codegen/model.py",8.0,8,3,2.434138064,12.0,8580.0,3.0,330810.5714285714,4784.0,11124.0,0.0,Corrective,1.0,1
pytorch,267b554b6fd7d8dfdfb9885c663fbe48fb26b2f8,298db672201f4a48da719e982a2702f5a48b27f1,kshitij12345,kshitijkalambarkar@gmail.com,Mon Apr 26 04:24:39 2021 -0700,1619411079.0,"[OpInfo] Add Function Variant and Opinfo for permute (#56125)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56125

Reviewed By: ezyang

Differential Revision: D27960312

Pulled By: mruberry

fbshipit-source-id: b9dd89f7e69d7dff29f3b53828656c13df898fa5",43.0,16.0,"aten/src/ATen/native/native_functions.yaml,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",6.0,8,3,1.879683646,45.0,39802.0,4.0,279899.8333333333,11252.0,24859.5,0.0,Feature Addition,0.0,1
pytorch,f1b94461aabec129164685f230d914e789ac42ba,298f01d9a2d872051016a43e8a3c96bdad791896,Bin Bao,binbao@meta.com,Thu Oct 05 15:04:37 2023 -0700,1696518277.0,"[aotinductor] Avoid generating redundant kernel loading code (#110510)

Summary: 1) Stop forcing triton.unique_kernel_names to True for AOTInductor, because the unique kernel name can be read from metadata; 2) Only generate load_kernel once for each kernel since we don't have control flow in our generated code.  This solves https://github.com/pytorch/pytorch/issues/105553.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110510
Approved by: https://github.com/chenyang78, https://github.com/jansel",51.0,24.0,"test/inductor/test_aot_inductor.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/compile_fx.py,torch/_inductor/triton_heuristics.py",4.0,5,2,1.34647025,1.0,5494.0,3.0,41421.75,20427.0,46722.5,0.0,Preventative,0.0,1
pytorch,dec0b6b792cc2cdff55a0f667634649c511fcf6f,2991bfdbe01bd961b4d5b12e2b5fa4a617379e91,Pieter Noordhuis,pietern@fb.com,Thu Sep 26 15:08:04 2019 -0700,1569510484.0,"Add bitwise distributed reduction ops (#26824)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26824

These ops are named after the bitwise reduction ops in MPI.

This is based on the work done by knottb in #22449.

Closes #22449.

Test Plan: Imported from OSS

Differential Revision: D17600210

Pulled By: pietern

fbshipit-source-id: 44c7041ce01bc5de170a4591c5a696e4f24431ef",137.0,5.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/Types.hpp",4.0,7,2,1.350605138,4.0,5948.0,4.0,1188452.25,11781.0,33048.33333,0.0,Feature Addition,0.0,1
pytorch,cfddfce0d378e335d2243563b2d84f923cbc790c,29b2131c626ee5fcd7d6e5200c05bda75121afa9,Oguz Ulgen,oulgen@meta.com,Wed Feb 21 08:28:25 2024 -0800,1708504105.0,"[Inductor] Fix bug around out of order constexprs in inductor (#120287)

Inductor signature/config generation code assumes that all constexprs come as last arguments of the function. This is not always true for user defined kernels.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/120287
Approved by: https://github.com/jansel",78.0,30.0,"test/dynamo/test_triton_kernels.py,torch/_inductor/codegen/triton_utils.py,torch/_inductor/codegen/wrapper.py",3.0,5,2,1.475133334,1.0,3048.0,3.0,46679.333333333336,25385.0,57261.5,0.0,Corrective,1.0,1
pytorch,373a20ad4a64240168446d6b0375de60ecf36698,29c4290a8d21b6e8a20b2e7aa7119de40efe7e29,Richard Barnes,rbarnes@fb.com,Thu Feb 25 02:42:05 2021 -0800,1614220925.0,"Use c10::irange for great good (#52153)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52153

Test Plan: Sandcastle

Reviewed By: ngimel

Differential Revision: D26407087

fbshipit-source-id: ea8ce1c17299cb9d89621e4a39f31edc2faa9fd6",109.0,94.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/Linear.cpp,aten/src/ATen/test/cuda_stream_test.cpp,c10/cuda/CUDAStream.cpp,c10/util/irange.h,test/cpp/jit/test_fuser.cpp,torch/csrc/jit/codegen/cuda/ir_nodes.cpp,torch/csrc/jit/codegen/cuda/lower_validation.cpp,torch/csrc/jit/codegen/cuda/mutator.cpp,torch/csrc/jit/codegen/cuda/tensor_view.cpp",10.0,16,4,2.833889584,8.0,6080.0,8.0,9215362.6,9172.0,20468.0,0.0,,0.0,1
pytorch,31ff0ecd2b400b4863741bcbc41748f2ad01745c,29ea08616bd10f50b31afcff077f3e8feb3d9792,Edward Yang,ezyang@fb.com,Tue Apr 09 18:09:31 2019 -0700,1554833371.0,"Add torch.__config__.show(), reporting detailed version of all libraries. (#18579)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18579
ghimport-source-id: 65124c95e49423de4ad1008c65e75057fea09b94

Differential Revision: D14778507

Pulled By: ezyang

fbshipit-source-id: 1e4bb79f4800a116ce8fb7af2fefbd34da8d102c",259.0,0.0,".jenkins/pytorch/test.sh,aten/src/ATen/ATen.h,aten/src/ATen/Version.cpp,aten/src/ATen/Version.h,aten/src/ATen/cuda/CUDAConfig.h.in,aten/src/ATen/cuda/detail/CUDAHooks.cpp,aten/src/ATen/cuda/detail/CUDAHooks.h,aten/src/ATen/detail/CUDAHooksInterface.h,docs/source/__config__.rst,docs/source/index.rst,test/test_docs_coverage.py,test/test_torch.py,torch/__config__.py,torch/__init__.py,torch/csrc/Module.cpp",15.0,13,5,2.295426441,41.0,12949.0,10.0,5435873.181818182,7999.0,24143.33333,0.0,Feature Addition,0.0,1
pytorch,a7db3a75913db93213ff8e97fbbd422a6da90873,2a104f7383845fef26bd95e19680913d67453443,Ilia Cherniavskii,iliacher@fb.com,Wed May 08 17:25:06 2019 -0700,1557336306.0,"Port ATen/native to ATen/Parallel (#20043)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20043
ghimport-source-id: 8003ef8ca335d4c4717a886a3a75fd78ec53ade5

Differential Revision: D15248505

Pulled By: ilia-cher

fbshipit-source-id: 7be500ed8bfb23cc36f1dd7108e344319e3e5332",1428.0,1436.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/AdaptiveAveragePooling.cpp,aten/src/ATen/native/AdaptiveMaxPooling2d.cpp,aten/src/ATen/native/AdaptiveMaxPooling3d.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/NNPACK.cpp,aten/src/ATen/native/ReflectionPad.cpp,aten/src/ATen/native/ReplicationPadding.cpp,aten/src/ATen/native/TensorTransformations.cpp,aten/src/ATen/native/WeightNorm.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp",20.0,7,1,3.586698883,7.0,10158.0,11.0,3916300.1,8541.0,25329.83333,0.0,,0.0,1
pytorch,c561ef540692a92ba866b8add8c3ed401a1315e9,2a2007e5aca575eb495e42dbf4404451a351eeca,Richard Zou,zou3519@gmail.com,Wed Apr 10 01:08:59 2019 -0700,1554858539.0,"EmbeddingBag CPU forward with per_sample_weights. (#18735)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18735
ghimport-source-id: d81bef54dafd7167d2451250d7be478d3c013920

Reviewed By: cpuhrsch

Differential Revision: D14851415

Pulled By: zou3519

fbshipit-source-id: cea6039e760ad571b90f0a536e420498f34be325",300.0,59.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,tools/autograd/derivatives.yaml,torch/nn/functional.py,torch/nn/modules/sparse.py,torch/onnx/symbolic.py",8.0,12,4,2.005687409,44.0,20998.0,7.0,568065.625,8010.0,24162.33333,0.0,,0.0,1
pytorch,55d45458bdb0b404bcaba540978123cbfccfbefa,2a24a2418a54234f0a4466f20653c9ebf16ddec0,Jane Xu,janeyx@fb.com,Fri Apr 09 16:23:31 2021 -0700,1617985411.0,"common_utils.py use new file names for disabled/slow tests (#55620)

Summary:
Following these changes in renaming the files:
https://github.com/pytorch/pytorch/pull/55618
https://github.com/pytorch/test-infra/pull/3

We should update the use sites in common_utils.py

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55620

Reviewed By: samestep

Differential Revision: D27651884

Pulled By: janeyx99

fbshipit-source-id: 298a981e55e0b7c95202294d9bc4b3fcce359590",2.0,2.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2150.0,1.0,120738.0,10601.0,23465.0,0.0,,0.0,1
pytorch,e182401062323ab613ca9f1e3786272e5ffc6eb4,2aa19f33c6272cb016c629792ddc70cf9636fc9a,BowenBao,bowbao@microsoft.com,Wed Aug 18 20:25:19 2021 -0700,1629318319.0,"[ONNX] Fix for batchnorm training op mode (#52758) (#62760)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62760

* Rebase

# Conflicts:
#	torch/csrc/jit/passes/onnx/eval_peephole.cpp

# Conflicts:
#	test/onnx/test_utility_funs.py
#	torch/onnx/symbolic_opset9.py

* Update symbolic_opset12.py

* Update test.sh
# Conflicts:
#	.jenkins/caffe2/test.sh

* Merge

* Fix utility tests

# Conflicts:
#	test/onnx/test_pytorch_onnx_onnxruntime.py
#	test/onnx/test_utility_funs.py

* Fix for comment

* Enable BN tests

* Fix for test

* Update test_pytorch_onnx_onnxruntime.py

* Update test_pytorch_onnx_onnxruntime.py

* Update test_utility_funs.py

* Update test_pytorch_onnx_onnxruntime.py

Test Plan: Imported from OSS

Reviewed By: SplitInfinity

Differential Revision: D30349060

Pulled By: msaroufim

fbshipit-source-id: 93312c17607974731c17099ae181acb6e4c1c409",135.0,111.0,".jenkins/caffe2/test.sh,test/onnx/expect/TestOperators.test_dropout_default.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_utility_funs.py,torch/csrc/jit/passes/onnx/eval_peephole.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset14.py,torch/onnx/symbolic_opset9.py",10.0,11,3,2.100136703,8.0,16374.0,4.0,4399067.3,14745.0,33767.5,0.0,Corrective,1.0,1
pytorch,9d80969fa47a4f921f43adc60762cbec895bdaf8,2abcafcfd8beb4f6a22e08532d58f9f09c490f0f,Pearu Peterson,pearu.peterson@gmail.com,Thu Mar 16 16:45:55 2023 +0200,1678985155.0,"Add masked_grad kw argument to to_dense (#96095)

As in the title.

The `masked_grad` kw argument is required for `to_dense` backward to distinguish the expected semantics of sparse tensors. `masked_grad=True` means that the `to_dense` backward will apply a mask to the returned gradient where the mask is defined by the input indices. The default semantics implies `masked_grad==True` for BC but see the [comment](https://github.com/pytorch/pytorch/pull/96095/files#diff-d4df180433a09071e891d552426911c227b30ae9b8a8e56da31046e7ecb1afbeR501-R513) in `to_dense_backward`.

As a consequence, existing code that is run through autograd engine must replace `.to_dense()` calls with `.to_dense(masked_grad=False)`. For example,
```python
torch.autograd.gradcheck(lambda x: torch.sum(x, [0]).to_dense())
torch.autograd.gradcheck(lambda x: torch.sparse.sum(x, [0]).to_dense())
```
(recall, gradcheck has `masked=False` as default) must be updated to
```python
torch.autograd.gradcheck(lambda x: torch.sum(x, [0]).to_dense(masked_grad=False))
torch.autograd.gradcheck(lambda x: torch.sparse.sum(x, [0]).to_dense(masked_grad=True), masked=True)
```

Fixes https://github.com/pytorch/pytorch/issues/95550

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96095
Approved by: https://github.com/cpuhrsch",79.0,65.0,"aten/src/ATen/native/TensorConversions.cpp,aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseTensor.cpp,test/test_sparse.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/autograd/gradcheck.py,torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp,torch/overrides.py",10.0,14,4,2.494623748,35.0,37118.0,9.0,2387216.0,13441.0,31171.0,0.0,Corrective,1.0,1
pytorch,695990e7c314d7e819b6c1309870d4dac6a2e77e,2ade2226424ed8744f2f36aac5a96bd2d708f736,Richard Zou,zou3519@users.noreply.github.com,Tue Dec 14 22:49:12 2021 -0500,1639522152.0,"[functorch] jvp transform fully composes with vmap (pytorch/functorch#340)

Test Plan:
- new tests",122.0,4.0,"functorch/functorch/csrc/BatchRulesFactory.cpp,functorch/test/test_ops.py,functorch/test/xfail_suggester.py",3.0,4,1,0.782978148,1.0,1199.0,3.0,0.6666666666666666,652.0,871.0,0.0,,0.0,1
pytorch,41f9c142975b1ad04bb4bfe69bc29272f17f806c,2aea8077f9a7e04cf3ef145eed864bb74784cccb,soumith,soumith@fb.com,Tue Dec 27 21:17:04 2016 -0800,1482873424.0,renaming test to avoid dot in test name,1.0,1.0,test/test_cuda.py,1.0,1,1,0,17.0,661.0,1.0,278.0,195.0,8684.289675,0.0,Preventative,0.0,1
pytorch,"2e44511b13e1619b00987e78105a8095ab9f95c1,42e835ebb81a3ecf8f76e15bb1866c1427f61d74",2af3098e5af822689850de061be123b0239e6093,Sam Gross,sgross@fb.com,Thu Oct 27 20:49:23 2016 -0700,1477601363.0,Merge commit '42e835ebb81a3ecf8f76e15bb1866c1427f61d74',6.0,1.0,"torch/lib/THCUNN/BatchNormalization.cu,torch/lib/THCUNN/CMakeLists.txt",2.0,3,1,0.863120569,9.0,398.0,1.0,878582.0,62.0,3967.263889,0.0,,0.0,1
pytorch,ccf4d69b756919fe831f0c9bcb5beb9aa7832c03,2af64ba3ede8906c4b48e1d0988665520ad03c9d,Peter Bell,peterbell10@live.co.uk,Thu Mar 05 23:54:00 2020 -0800,1583452440.0,"Allow output to zero-strided tensors if the size is <= 1 along that dim (#34100)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/33812
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34100

Differential Revision: D20267778

Pulled By: ngimel

fbshipit-source-id: 1b84c4f6e6bf5d29c3698daa3cb71554b25c1eee",15.0,3.0,"aten/src/ATen/MemoryOverlap.cpp,test/test_torch.py",2.0,4,2,0.99107606,40.0,15627.0,2.0,2602400.0,15243.0,40889.83333,0.0,Corrective,1.0,1
pytorch,71b1120ba8577324ee7c18b8351facb7462b7ed1,2b2a7dc2ad24b3692425f4faf29a1e5f7eb6c604,Pan He,mybestsonny@gmail.com,Thu Jan 11 17:29:43 2018 -0500,1515691783.0,small fix on MaxPool2d __repr__ (#4591),1.0,1.0,torch/nn/modules/pooling.py,1.0,3,1,0,36.0,1056.0,1.0,1635741.0,2271.0,24362.85823,0.0,Corrective,1.0,1
pytorch,3a0f37735c6935a46c3401195527e7197af8c905,2b3d955ffdcc79f8c30d9d457216c618775bbc1c,Nikita Karetnikov,nikita@karetnikov.org,Fri Jun 09 19:27:12 2023 +0200,1686338832.0,"[pt2] add meta and `SymInt` support for `linalg_matrix_exp` (#102945)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102945
Approved by: https://github.com/lezcano",16.0,13.0,"test/functorch/test_aotdispatch.py,test/test_meta.py,test/test_proxy_tensor.py,torch/_meta_registrations.py,torch/csrc/autograd/FunctionsManual.cpp",5.0,5,2,1.733727239,3.0,16958.0,5.0,135865.4,16690.0,37610.0,0.0,Feature Addition,0.0,1
pytorch,bbdadab306f52dc520cb9afc199186645b0f788e,2b54cec7e8c0f9787e9abfa5878480a44d1a64b1,Jeffrey Wan,jw3468@fb.com,Sat May 01 00:36:00 2021 -0700,1619829360.0,"Clean up naming and comments (#56964)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56964

This PR does many things but does not update any logic:
 - Prefixes all function names that are not `gradcheck`, `gradgradcheck`, `get_numerical_jacobian`, and `get_analytical_jacobian` with underscore to indicate that they aren't part of the public API (https://github.com/pytorch/pytorch/issues/55714).
 - Improve naming to avoid referencing Jacobian rows or Jacobian cols when we really mean vjp and jvp as suggested by zou3519
 - Try to reduce comment line length so they are more consistent and easier to read
 - Other misc improvements to documentaiton

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D28096571

Pulled By: soulitzer

fbshipit-source-id: d372b5f8ee080669e525a987402ded72810baa0c",250.0,242.0,"torch/autograd/gradcheck.py,torch/testing/_internal/common_nn.py",2.0,4,1,0.143262593,28.0,6659.0,2.0,334929.0,11534.0,26036.0,0.0,Corrective,1.0,1
pytorch,b09d66e60dd2a55877e1c1fc6fc34cbbc756f952,2b5a38b1a80f40d5fef9cccfa393b8d178deb906,Richard Zou,zou3519@users.noreply.github.com,Wed Nov 15 20:37:34 2017 -0500,1510778254.0,"Add missing trtrs, orgqr, ormqr docs (#3720)

* trtrs docs

* orgqr and ormqr docs",94.0,11.0,torch/_torch_docs.py,1.0,1,1,0,26.0,4956.0,1.0,545105.0,2127.0,24093.85823,0.0,Feature Addition,0.0,1
pytorch,945d333ae485673d7a603ca71822c9a39ca4775a,2b7236a0e1c0d2339165103b2cd42e25debee99d,Zachary DeVito,zdevito@gmail.com,Sun Oct 16 05:17:20 2022 +0000,1665897440.0,"[torchdynamo] Use ProcessPoolExecutor for triton compiles (#87032)

This patch significantly improves the parallel compilation performance for cThis patch significantly improves the parallel compilation performance for compiling triton kernels
by using ProcessPoolExecutor to create persistent pool of compilation
workers.

Previously os.fork overhead and GIL contention limited the achieved
parallelism. This patch replaces
the worker threads with a pool of processes to do the raw compilation,
and does serial work on the main thread
for everything else. This other work couldn't be parallelized anyway
since it is mostly in python.

In cold start situations, the time to get the worker threads started can
be significant portion of the time.
This patch starts the workers earlier so they are ready to perform
compilation (see code comments) when dynamo
gets to that point.

Just tested this on one example benchmark (tf_efficientnet_b0), but the
results are significant, almost eliminating the difference between a
warm and cold compilation.

```
39.613s - warm
41.290s - cold, this patch

2m53.197s - cold, single threaded:
1m7.092s - cold, old setup n = 8 (its best config)
```
 (cold compilation is done after running `rm -rf
/tmp/torchinductor_$USER`).ompiling triton kernels
by using ProcessPoolExecutor to create persistent pool of compilation workers.

Previously os.fork overhead and GIL contention limited the achieved parallelism. This patch replaces
the worker threads with a pool of processes to do the raw compilation, and does serial work on the main thread
for everything else. This other work couldn't be parallelized anyway since it is mostly in python.

In cold start situations, the time to get the worker threads started can be significant portion of the time.
This patch starts the workers earlier so they are ready to perform compilation (see code comments) when dynamo
gets to that point.

Just tested this on one example benchmark (tf_efficientnet_b0), but the results are significant, almost eliminating the difference between a warm and cold compilation.

```
39.613s - warm
41.290s - cold, this patch

2m53.197s - cold, single threaded:
1m7.092s - cold, old setup n = 8 (its best config)
```
 (cold compilation is done after running `rm -rf /tmp/torchinductor_$USER`).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87032
Approved by: https://github.com/soumith, https://github.com/jansel",114.0,29.0,"torch/_inductor/codecache.py,torch/_inductor/config.py,torch/_inductor/triton_ops/autotune.py",3.0,3,1,0.914325327,1.0,1105.0,2.0,225802.66666666663,8411.0,20104.5,0.0,Perfective,1.0,1
pytorch,1a83c372ecb0c81bdd5110c516cc6e9f9554a469,2b9765ad02b666d623566935b385fc3d058ad33d,IraKorshunova,i.hmarynka@gmail.com,Thu Sep 21 01:23:45 2017 +0100,1505957025.0,Erf and erfinv (#2799),300.0,10.0,"docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_cuda.py,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/autograd/_functions/pointwise.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorMath.cwrap,torch/lib/TH/THMath.h,torch/lib/TH/generic/THTensorMath.c,torch/lib/TH/generic/THTensorMath.h,torch/lib/TH/generic/THVector.h,torch/lib/TH/generic/THVectorDefault.c,torch/lib/THC/THCNumerics.cuh,torch/lib/THC/generic/THCTensorMathPointwise.cu,torch/lib/THC/generic/THCTensorMathPointwise.h",19.0,14,3,3.194473979,39.0,25129.0,7.0,221732.47368421053,1778.0,24827.05562,0.0,,0.0,1
pytorch,deefafb01dfe82fd08182729be1d288c2259e0c4,2baff9476e1c228767ff4ed753abba1b095e6236,Alexander Fix,alexander.fix@fb.com,Fri Apr 24 20:09:54 2020 -0700,1587758994.0,"Test test_is_nonzero make expected exception inline (#37128)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37128

In certain build modes (in fbcode, building a .par) the mechanism to get test output ""expect"" files doesn't work.
All other tests in test_torch.py already had assertExpectedInline instead of assertExpected, with the expected result inline in the file.
There was no equivalent for assertExpectedRaises, so I added one, and changed the tests for test_is_nonzero (the only test using this)

Test Plan: CI, specifically the test test_is_nonzero should pass

Reviewed By: malfet

Differential Revision: D21197651

fbshipit-source-id: 2a07079efdcf1f0b0abe60e92cadcf55d81d4b13",19.0,2.0,"test/test_torch.py,torch/testing/_internal/expecttest.py",2.0,4,2,0.985228136,41.0,17974.0,2.0,4002994.0,1373.0,3689.5,0.0,Feature Addition,0.0,1
pytorch,41c08fe4a184668beefd221e73e4bdf9f2f22996,2bb7e480c1b10c3f745b86f1e98c11ac9d3060c1,Sam Gross,colesbury@gmail.com,Thu Jun 21 21:16:32 2018 -0400,1529615792.0,"Define conversions and operations on at::Half (#8660)

The goal is to be able to use at::Half throughout ATen, including in
CUDA kernels and have it operate like built-in types. This avoids the
need for cuda::from_type and cuda::to_type before every
AT_DISPATCH_ALL_TYPES_AND_HALF call.",500.0,253.0,"aten/src/ATen/AccumulateType.h,aten/src/ATen/Half-inl.h,aten/src/ATen/Half.cpp,aten/src/ATen/Half.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/cuda/CUDAHalf.cu,aten/src/ATen/cuda/CUDATypeConversion.cuh,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/cuda/Distributions.cu,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/cuda/TensorCompare.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cuda/TensorTransformations.cu,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/half_test.cpp,aten/src/THC/THCAtomics.cuh,aten/src/THC/THCStorage.hpp,aten/src/THC/THCTensorCopy.cu,aten/src/THC/generic/THCStorage.cpp",23.0,10,1,3.49058448,6.0,3432.0,13.0,1945906.380952381,1395.0,4141.805292,0.0,Preventative,1.0,1
pytorch,e9a40de1af2ff6374f2fcbc19f58064784785f86,2bbcc80de3e2c69c332b696749352fc8997f3040,Jane Xu,janeyx@fb.com,Fri Jul 09 15:58:20 2021 -0700,1625846300.0,"Enable disabling test cases on specific platforms (#61427)

Summary:
This adds functionality to our common_utils.py to allow disabling test cases for platforms Mac, Windows, and Linux.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61427

Test Plan:
CI should not change as no issues currently have the line ""Platforms:...""

I tested locally by making sure `test_async_script` is skipped while running `python test/test_jit.py -k TestAsync.test_async_script` with a cached modified `.pytorch-disabled-tests.json`:
```
{
  ""total_count"": 32,
  ""incomplete_results"": false,
  ""items"": [
    {
      ""url"": ""https://api.github.com/repos/pytorch/pytorch/issues/60652"",
      ""repository_url"": ""https://api.github.com/repos/pytorch/pytorch"",
      ""labels_url"": ""https://api.github.com/repos/pytorch/pytorch/issues/60652/labels{/name}"",
      ""comments_url"": ""https://api.github.com/repos/pytorch/pytorch/issues/60652/comments"",
      ""events_url"": ""https://api.github.com/repos/pytorch/pytorch/issues/60652/events"",
      ""html_url"": ""https://github.com/pytorch/pytorch/issues/60652"",
      ""id"": 929288995,
      ""node_id"": ""MDU6SXNzdWU5MjkyODg5OTU="",
      ""number"": 60652,
      ""title"": ""DISABLED test_async_script (jit.test_async.TestAsync)"",
      ""user"": {
        ""login"": ""ezyang"",
        ""id"": 13564,
        ""node_id"": ""MDQ6VXNlcjEzNTY0"",
        ""avatar_url"": ""https://avatars.githubusercontent.com/u/13564?v=4"",
        ""gravatar_id"": """",
        ""url"": ""https://api.github.com/users/ezyang"",
        ""html_url"": ""https://github.com/ezyang"",
        ""followers_url"": ""https://api.github.com/users/ezyang/followers"",
        ""following_url"": ""https://api.github.com/users/ezyang/following{/other_user}"",
        ""gists_url"": ""https://api.github.com/users/ezyang/gists{/gist_id}"",
        ""starred_url"": ""https://api.github.com/users/ezyang/starred{/owner}{/repo}"",
        ""subscriptions_url"": ""https://api.github.com/users/ezyang/subscriptions"",
        ""organizations_url"": ""https://api.github.com/users/ezyang/orgs"",
        ""repos_url"": ""https://api.github.com/users/ezyang/repos"",
        ""events_url"": ""https://api.github.com/users/ezyang/events{/privacy}"",
        ""received_events_url"": ""https://api.github.com/users/ezyang/received_events"",
        ""type"": ""User"",
        ""site_admin"": false
      },
      ""labels"": [
        {
          ""id"": 1301397902,
          ""node_id"": ""MDU6TGFiZWwxMzAxMzk3OTAy"",
          ""url"": ""https://api.github.com/repos/pytorch/pytorch/labels/module:%20flaky-tests"",
          ""name"": ""module: flaky-tests"",
          ""color"": ""f7e101"",
          ""default"": false,
          ""description"": ""Problem is a flaky test in CI""
        },
        {
          ""id"": 679953883,
          ""node_id"": ""MDU6TGFiZWw2Nzk5NTM4ODM="",
          ""url"": ""https://api.github.com/repos/pytorch/pytorch/labels/oncall:%20distributed"",
          ""name"": ""oncall: distributed"",
          ""color"": ""f7e101"",
          ""default"": false,
          ""description"": ""Add this issue/PR to distributed oncall triage queue""
        }
      ],
      ""state"": ""open"",
      ""locked"": false,
      ""assignee"": {
        ""login"": ""rohan-varma"",
        ""id"": 8039770,
        ""node_id"": ""MDQ6VXNlcjgwMzk3NzA="",
        ""avatar_url"": ""https://avatars.githubusercontent.com/u/8039770?v=4"",
        ""gravatar_id"": """",
        ""url"": ""https://api.github.com/users/rohan-varma"",
        ""html_url"": ""https://github.com/rohan-varma"",
        ""followers_url"": ""https://api.github.com/users/rohan-varma/followers"",
        ""following_url"": ""https://api.github.com/users/rohan-varma/following{/other_user}"",
        ""gists_url"": ""https://api.github.com/users/rohan-varma/gists{/gist_id}"",
        ""starred_url"": ""https://api.github.com/users/rohan-varma/starred{/owner}{/repo}"",
        ""subscriptions_url"": ""https://api.github.com/users/rohan-varma/subscriptions"",
        ""organizations_url"": ""https://api.github.com/users/rohan-varma/orgs"",
        ""repos_url"": ""https://api.github.com/users/rohan-varma/repos"",
        ""events_url"": ""https://api.github.com/users/rohan-varma/events{/privacy}"",
        ""received_events_url"": ""https://api.github.com/users/rohan-varma/received_events"",
        ""type"": ""User"",
        ""site_admin"": false
      },
      ""assignees"": [
        {
          ""login"": ""rohan-varma"",
          ""id"": 8039770,
          ""node_id"": ""MDQ6VXNlcjgwMzk3NzA="",
          ""avatar_url"": ""https://avatars.githubusercontent.com/u/8039770?v=4"",
          ""gravatar_id"": """",
          ""url"": ""https://api.github.com/users/rohan-varma"",
          ""html_url"": ""https://github.com/rohan-varma"",
          ""followers_url"": ""https://api.github.com/users/rohan-varma/followers"",
          ""following_url"": ""https://api.github.com/users/rohan-varma/following{/other_user}"",
          ""gists_url"": ""https://api.github.com/users/rohan-varma/gists{/gist_id}"",
          ""starred_url"": ""https://api.github.com/users/rohan-varma/starred{/owner}{/repo}"",
          ""subscriptions_url"": ""https://api.github.com/users/rohan-varma/subscriptions"",
          ""organizations_url"": ""https://api.github.com/users/rohan-varma/orgs"",
          ""repos_url"": ""https://api.github.com/users/rohan-varma/repos"",
          ""events_url"": ""https://api.github.com/users/rohan-varma/events{/privacy}"",
          ""received_events_url"": ""https://api.github.com/users/rohan-varma/received_events"",
          ""type"": ""User"",
          ""site_admin"": false
        }
      ],
      ""milestone"": null,
      ""comments"": 0,
      ""created_at"": ""2021-06-24T14:28:33Z"",
      ""updated_at"": ""2021-06-24T16:40:42Z"",
      ""closed_at"": null,
      ""author_association"": ""CONTRIBUTOR"",
      ""active_lock_reason"": null,
      ""body"": ""Platforms:Mac, windows, Linux\r\n```\r\nJun 24 00:59:14 ======================================================================\r\nJun 24 00:59:14 ERROR [0.477s]: test_async_script (__main__.ProcessGroupGlooWrapperTest)\r\nJun 24 00:59:14 ----------------------------------------------------------------------\r\nJun 24 00:59:14 Traceback (most recent call last):\r\nJun 24 00:59:14   File \""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py\"", line 398, in wrapper\r\nJun 24 00:59:14     self._join_processes(fn)\r\nJun 24 00:59:14   File \""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py\"", line 590, in _join_processes\r\nJun 24 00:59:14     self._check_return_codes(elapsed_time)\r\nJun 24 00:59:14   File \""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py\"", line 633, in _check_return_codes\r\nJun 24 00:59:14     raise RuntimeError(error)\r\nJun 24 00:59:14 RuntimeError: Process 0 exited with error code 10 and exception:\r\nJun 24 00:59:14 RuntimeError: [/var/lib/jenkins/workspace/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [172.17.0.2]:21400\r\nJun 24 00:59:14 \r\nJun 24 00:59:14 During handling of the above exception, another exception occurred:\r\nJun 24 00:59:14 \r\nJun 24 00:59:14 Traceback (most recent call last):\r\nJun 24 00:59:14   File \""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py\"", line 516, in run_test\r\nJun 24 00:59:14     getattr(self, test_name)()\r\nJun 24 00:59:14   File \""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py\"", line 400, in wrapper\r\nJun 24 00:59:14     fn()\r\nJun 24 00:59:14   File \""distributed/test_pg_wrapper.py\"", line 270, in test_collective_hang\r\nJun 24 00:59:14     self._test_collective_hang(pg)\r\nJun 24 00:59:14   File \""distributed/test_pg_wrapper.py\"", line 52, in _test_collective_hang\r\nJun 24 00:59:14     wrapper_pg.allreduce([tensor])\r\nJun 24 00:59:14   File \""/opt/conda/lib/python3.6/unittest/case.py\"", line 217, in __exit__\r\nJun 24 00:59:14     expected_regex.pattern, str(exc_value)))\r\nJun 24 00:59:14   File \""/opt/conda/lib/python3.6/unittest/case.py\"", line 135, in _raiseFailure\r\nJun 24 00:59:14     raise self.test_case.failureException(msg)\r\nJun 24 00:59:14 AssertionError: \""Ranks 1 failed to pass monitoredBarrier\"" does not match \""[/var/lib/jenkins/workspace/third_party/gloo/gloo/transport/tcp/pair.cc:598] Connection closed by peer [172.17.0.2]:21400\""\r\n```\r\n\r\nhttps://www.internalfb.com/intern/opensource/ci/job/log/225221175921058/\n\ncc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan-varma gqchen aazzolini osalpekar jiayisuse agolynski SciPioneer H-Huang mrzzd cbalioglu gcramer23"",
      ""performed_via_github_app"": null,
      ""score"": 0.0
    }
  ]
}
```

Reviewed By: iramazanli

Differential Revision: D29627799

Pulled By: janeyx99

fbshipit-source-id: 5ef79127cbe0055c4f41766048e66f98cf80d2c4",23.0,5.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2542.0,1.0,147656.0,13701.0,30958.5,0.0,Feature Addition,0.0,1
pytorch,734a97a7c828d6bfe82764c9cae399c349828091,2bc2adf2ba99eff404443e5b230f5a4ac426a21c,Ramin Azarmehr,razarmehr@apple.com,Fri May 20 12:46:19 2022 +0000,1653050779.0,"MPS: Fix some memory leak issues in release pools (#77934)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77934
Approved by: https://github.com/albanD",12.0,12.0,"aten/src/ATen/mps/MPSDevice.mm,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Convolution.mm",3.0,7,1,1.28067213,1.0,1005.0,2.0,217474.0,3483.0,8285.0,0.0,Corrective,1.0,1
pytorch,69dd0bab90cdaf174cfab235374eb6071d228308,2bede78a05505ef9e01bb7d47305415028a5ccc9,Lucas Roberts,rlucas7@vt.edu,Mon Aug 31 18:44:47 2020 -0700,1598899487.0,"add qr_backward functionality for wide case (#42216)

Summary:
Unblocks implementation of https://github.com/pytorch/pytorch/issues/27036. Note that this PR ***does not*** fix #{27036}.
Currently QR decomposition only has support for square and tall (a.k.a. skinny) case.
This PR adds functionality for wide A matrix/tensors, includes 3 unit tests for the new case
and restructures the `qr_backward` method to use the same Walther method as a helper.

cc albanD t-vi

I don't have a gpu machine so haven't tested on cuda but everything passes on my local machine in cpu.

The basic idea of the PR is noted in the comments in the `Functions.cpp` file but I'll note here too for clarity:

let <img src=""https://render.githubusercontent.com/render/math?math=A_{m,n}""> be a matrix and <img src=""https://render.githubusercontent.com/render/math?math=m < n""> then partition <img src=""https://render.githubusercontent.com/render/math?math=A_{m, n}""> as  <img src=""https://render.githubusercontent.com/render/math?math=A_{m,n} = [ X_{m,m} |\ Y_{m, n-m} ]"">
and take QR of <img src=""https://render.githubusercontent.com/render/math?math=X""> and call that one
<img src=""https://render.githubusercontent.com/render/math?math=X=QU""> the <img src=""https://render.githubusercontent.com/render/math?math=Q""> here from <img src=""https://render.githubusercontent.com/render/math?math=X""> is the same as the <img src=""https://render.githubusercontent.com/render/math?math=Q""> from <img src=""https://render.githubusercontent.com/render/math?math=QR""> on entire <img src=""https://render.githubusercontent.com/render/math?math=A""> matrix. Then transform <img src=""https://render.githubusercontent.com/render/math?math=Y""> with the <img src=""https://render.githubusercontent.com/render/math?math=Q""> rotation got from <img src=""https://render.githubusercontent.com/render/math?math=X""> to get <img src=""https://render.githubusercontent.com/render/math?math=V=Q^{T}Y""> now <img src=""https://render.githubusercontent.com/render/math?math=R= [U |\ V] ""> and similarly for the grads of each piece, e.g. if <img src=""https://render.githubusercontent.com/render/math?math=\bar{A}""> is  `grad_A` then
<img src=""https://render.githubusercontent.com/render/math?math=\bar{A} = [ \bar{X} |\ \bar{Y}]""> and <img src=""https://render.githubusercontent.com/render/math?math=\bar{R} = [ \bar{U} |\ \bar{V}]""> and then
<img src=""https://render.githubusercontent.com/render/math?math=\bar{Y} =  Q\bar{V}""> and
<img src=""https://render.githubusercontent.com/render/math?math=\bar{V}""> is the `narrow()` of `grad_R`.
<img src=""https://render.githubusercontent.com/render/math?math=\bar{X}""> is calculated very similar to the original Walther formula (exactly the same in the tall and square cases) but is slightly modified here for wide case matrices.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42216

Reviewed By: glaringlee

Differential Revision: D23373118

Pulled By: albanD

fbshipit-source-id: 3702ba7e7e23923868c02cdb7e10a96036052344",115.0,53.0,"tools/autograd/templates/Functions.cpp,torch/testing/_internal/common_methods_invocations.py",2.0,6,2,0.129233775,13.0,4297.0,1.0,24717.0,4683.0,10920.5,0.0,Corrective,1.0,1
pytorch,11c412a8ec07b9d38c42c94b098c99f051ec0140,2bfa0184623c4bdc6caea0332cc1ef6d019892d1,Mikayla Gawarecki,mikaylagawarecki@gmail.com,Fri Apr 01 05:52:12 2022 -0700,1648792332.0,"[BC-breaking] Use ScatterGatherKernel for scatter_reduce (CPU-only) (#74226)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74226

Update signature of `scatter_reduce_` to match `scatter_/scatter_add_`

`Tensor.scatter_reduce_(int64 dim, Tensor index, Tensor src, str reduce)`

- Add new reduction options in ScatterGatherKernel.cpp and update `scatter_reduce` to call into the cpu kernel for `scatter.reduce`
- `scatter_reduce` now has the same shape constraints as `scatter_` and `scatter_add_`
- Migrate `test/test_torch.py:test_scatter_reduce` to `test/test_scatter_gather_ops.py`

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D35222842

Pulled By: mikaylagawarecki

fbshipit-source-id: 84930add2ad30baf872c495251373313cb7428bd
(cherry picked from commit 1b45139482e22eb0dc8b6aec2a7b25a4b58e31df)",564.0,249.0,"aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/TensorAdvancedIndexing.h,aten/src/ATen/native/cpu/ScatterGatherKernel.cpp,aten/src/ATen/native/cuda/ScatterGatherKernel.cu,aten/src/ATen/native/native_functions.yaml,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/test_scatter_gather_ops.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",15.0,15,4,2.592779487,47.0,66279.0,12.0,1785495.8666666667,1883.0,4532.0,0.0,Feature Addition,0.0,1
pytorch,b3e500c52296024c81f1a640bb0855803b96ce9c,2c038f20748c588c9e98f3d549036237ab280ff7,Sam Gross,colesbury@gmail.com,Fri Jun 30 19:41:40 2017 -0400,1498851700.0,"Add weight normalization implementation (#1945)

* Add weight normalization implementation

This adds forward ""pre-hooks"" which get called before the module's
forward() method. Weight norm is implemented as a hook which calculates
the weight variable from the weight_g and weight_v every iteration.

Based on @rtqichen implementation.

* Specify return type",178.0,0.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/__init__.py,torch/nn/modules/module.py,torch/nn/utils/__init__.py,torch/nn/utils/weight_norm.py",6.0,7,3,1.436127661,30.0,5011.0,3.0,467699.8,1062.0,12586.54911,0.0,Feature Addition,0.0,1
pytorch,61e4723132ba30d3841c648b5d1ba0c6d6632bbe,2c07f88ea396f2ce2a5e03b1ac5234af6826045e,Zhou Mo,cdluminate@gmail.com,Sun Jul 30 12:38:00 2017 +0000,1501418280.0,Fix typos.,31.0,31.0,"test/test_nn.py,test/test_torch.py,torch/_thnn/__init__.py,torch/_torch_docs.py,torch/autograd/__init__.py,torch/autograd/_functions/reduce.py,torch/autograd/variable.py,torch/csrc/cuda/expand_utils.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/methods/TensorSerialization.cwrap,torch/cuda/comm.py,torch/distributed/__init__.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/SpatialDropout.py,torch/legacy/nn/Sum.py,torch/legacy/nn/VolumetricDropout.py,torch/legacy/optim/asgd.py,torch/lib/THD/base/data_channels/Store.cpp,torch/lib/THD/base/init_methods/InitMethodTCP.cpp,torch/lib/gloo/docs/readme.md,torch/lib/gloo/docs/rendezvous.md,torch/lib/libshm/socket.h,torch/nn/modules/module.py,torch/nn/parallel/scatter_gather.py",24.0,25,2,4.438067278,37.0,19026.0,2.0,865.5416666666666,1388.0,14378.32126,0.0,Corrective,1.0,1
pytorch,6f3f302d9fb99c8724c5f52006d5441006b487e7,2c0fe338da6a0e375d86575bb00dc58ac0c8f877,Nikita Shulga,nshulga@fb.com,Fri Oct 22 20:36:41 2021 -0700,1634935001.0,"[ONNX] Modify softplus symbolic to support beta!=1 (#65001) (#66146)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66146

* Modify softplus symbolic to support beta!=1

* Remove parse args

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D31424096

fbshipit-source-id: 971af54a28141737ccb17510ada03b0651be2a63",25.0,3.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.749595257,3.0,13518.0,2.0,7.0,16505.0,38694.5,0.0,,0.0,1
pytorch,1aabb8f98c42ff262473ff1362e166b3fb2c9f25,2c17b6a0fe254f18f3c969cb0e68920341ec600b,BowenBao,bowbao@microsoft.com,Thu May 27 19:03:59 2021 -0700,1622142239.0,"[ONNX] Enable support for roll() op. (#58389) (#58697)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58697

1. Add a symbolic function for aten::roll() op in symbolic_opset9.py.
2. Add a test with multiple scenarios as well.

Test Plan: Imported from OSS

Reviewed By: driazati, bhosmer

Differential Revision: D28714807

Pulled By: SplitInfinity

fbshipit-source-id: eae85f2dcf02737c9256a180f6905a935ca3f57e

Co-authored-by: fatcat-z <jiz@microsoft.com>",41.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.958711883,3.0,12339.0,1.0,3.0,12534.0,28432.5,0.0,Feature Addition,0.0,1
pytorch,993b4651fd4eb6a858b35e660c6d623a6ab39bb5,2c1b215b488c4c9cdffcd86ba25f5bfd549162b2,Zachary DeVito,zdevito@fb.com,Tue Sep 15 22:49:55 2020 -0700,1600210195.0,"[fx] remove delegate, replace with tracer (#44566)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/44566

The Delegate objects were confusing. They were suppose to be a way to
configure how tracing works, but in some cases they appeared necessary
for consturcting graphs, which was not true. This makes the organization
clearer by removing Delgate and moving its functionality into a Tracer class,
similar to how pickle has a Pickler class.

Test Plan: Imported from OSS

Reviewed By: jamesr66a

Differential Revision: D23683177

Pulled By: zdevito

fbshipit-source-id: 7605a34e65dfac9a487c0bada39a23ca1327ab00",190.0,164.0,"test/fx/quantization.py,test/test_fx.py,torch/fx/__init__.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/fx/proxy.py,torch/fx/symbolic_trace.py",7.0,4,2,1.832476549,1.0,1691.0,7.0,987067.1428571428,5145.0,11755.5,0.0,,0.0,1
pytorch,f7a04f310b76438448df758a3c9c2bf91b704a11,2c1efe7472079fbeeb1ee9415db83851d8276c93,Bin Bao,binbao@fb.com,Wed Oct 26 16:13:20 2022 +0000,1666800800.0,"Enable some PyTorch core tests with inductor (#87490)

Summary:
1) Graph break on torch.random.set_rng_state since it blocks running
inductor core tests;
2) Add several inductor-specific skips;
3) Enable several core tests for inductor CI;

cc @jansel @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87490
Approved by: https://github.com/eellison",24.0,10.0,".jenkins/pytorch/test.sh,test/dynamo/test_repros.py,test/test_modules.py,test/test_ops.py,test/test_ops_gradients.py,torch/_dynamo/variables/torch.py",6.0,7,3,2.401052244,7.0,6303.0,5.0,455727.8333333333,8789.0,20805.0,0.0,Feature Addition,0.0,1
pytorch,4e3b1c46d99bd0673b07725de81206bba28fdb3e,2c2648ea385e68389ad6a38ac2c3d0a442915042,Zach DeVito,zdevito@fb.com,Tue Jun 20 23:24:50 2017 -0700,1498001090.0,split Local.cwrap from Declarations.cwrap so local ones can be modified without regenerating declarations from pytorch,4331.0,4.0,"aten/CMakeLists.txt,aten/src/aten/extract_cwrap.py,aten/tools/Declarations.cwrap",3.0,4,1,0.020423056,1.0,108.0,2.0,0.0,144.0,1144.0,0.0,,0.0,1
pytorch,0ff3059ad087ae7e6b306db82694a0ffb430de2e,2c337dd934b8deca45abdc6f09b2707f9039b063,kshitij12345,kshitijkalambarkar@gmail.com,Thu Apr 13 08:17:15 2023 +0000,1681373835.0,"[fix] update the condition for aliveness of TensorWrapper (#98748)

Fixes https://github.com/pytorch/pytorch/issues/95561
Fixes https://github.com/pytorch/pytorch/issues/98021

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98748
Approved by: https://github.com/zou3519",26.0,6.0,"aten/src/ATen/functorch/TensorWrapper.cpp,test/functorch/test_eager_transforms.py,test/functorch/test_ops.py",3.0,6,2,1.494784946,1.0,7199.0,3.0,834077.3333333334,14507.0,33130.5,0.0,Corrective,1.0,1
pytorch,b01d2d1d3ee2c590ae122ef0e440e0bd0a6cea8e,2c351c76e0980fede8020b25adec467635f1c2a0,kshitij12345,kshitijkalambarkar@gmail.com,Tue Sep 07 22:24:08 2021 -0700,1631053448.0,"[special] Alias igamma, igammac to special.gammaninc, special.gammaincc (#61902)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50345

Also added relevant OpInfo

TODO:
* [x] Check rendered docs gammainc : https://docs-preview.pytorch.org/61902/special.html#torch.special.gammainc
* [x] Check rendered docs gammaincc: https://docs-preview.pytorch.org/61902/special.html#torch.special.gammaincc

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61902

Reviewed By: ngimel

Differential Revision: D30761428

Pulled By: mruberry

fbshipit-source-id: 06a16432873357958d53364f12a4e91c29779d26",247.0,102.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_autograd.py,test/test_fx.py,test/test_fx_experimental.py,torch/_torch_docs.py,torch/csrc/api/include/torch/special.h,torch/csrc/jit/passes/normalize_ops.cpp,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",14.0,18,4,2.794041111,44.0,52440.0,9.0,787042.0714285715,15245.0,34940.5,0.0,Feature Addition,0.0,1
pytorch,0257f5d19f0585f9a82bc06e0c4987e2136332c9,2c71b679d2902c960fb5512efd4a982f339d2017,Sam Gross,colesbury@gmail.com,Mon Dec 18 07:03:28 2017 -0500,1513580608.0,"Implement pin_memory() as a NativeFunction (#4094)

* Implement pin_memory() as a NativeFunction

This adds allocators as a concept in ATen that extends deleters. An
allocator is a subclass of at::Allocator that implements the virtual
methods:

  virtual void* allocate(size_t n);
  virutal void deallocate(void* ptr);

A tensor created with a custom allocator can be resized, unlike a tensor
with a custom deleter.

* Rename AllocatorContext to AllocatorRetainable",215.0,30.0,"aten/src/ATen/ATen.h,aten/src/ATen/Allocator.h,aten/src/ATen/PinnedMemoryAllocator.cpp,aten/src/ATen/PinnedMemoryAllocator.h,aten/src/ATen/Retainable.h,aten/src/ATen/TensorImpl.h,aten/src/ATen/UndefinedType.cpp,aten/src/ATen/UndefinedType.h,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/templates/StorageDerived.cpp,aten/src/ATen/templates/StorageDerived.h,aten/src/ATen/templates/Type.cpp,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeDerived.cpp,aten/src/ATen/templates/TypeDerived.h,test/test_autograd.py,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h",19.0,9,3,3.547410334,38.0,4480.0,8.0,443616.4,2198.0,24222.85823,0.0,Feature Addition,0.0,1
pytorch,c3239442a3dd1040b251ff33bef40589cba40e1c,2c87ef3dbfe1e5bd9ee4b6a9e4aa5030dc714284,Jason Ansel,jansel@meta.com,Tue Aug 29 19:57:13 2023 -0700,1693339033.0,"[inductor] Fix inputs with existing offsets (#108168)

This cherrypicks the reinterpret_tensor change from #102625 in order to fix a subtle correctness bug when the graph inputs already have a storage_offset set.

The view change also fixes some issues with quantized models in torchbench.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108168
Approved by: https://github.com/desertfire",50.0,9.0,"test/inductor/test_cpp_wrapper.py,test/inductor/test_torchinductor.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/graph.py,torch/_inductor/ir.py,torch/csrc/inductor/inductor_ops.cpp,torch/csrc/inductor/inductor_ops.h",7.0,7,2,2.504168573,2.0,16503.0,5.0,1307271.5714285714,19173.0,43522.0,0.0,Corrective,1.0,1
pytorch,f99f5ee0880c239a96ead37b1b7eb8bbf7de6fb2,2cab77f8107086e9ac8e3e2bddece61f02a41119,Pearu Peterson,pearu.peterson@gmail.com,Fri Nov 19 20:39:11 2021 -0800,1637354351.0,"Masked normalization infrastructure and strided masked softmax (#68333)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68333

Test Plan: Imported from OSS

Reviewed By: dagitses, ZolotukhinM

Differential Revision: D32564435

Pulled By: cpuhrsch

fbshipit-source-id: 4d4662323ceffd12c210b7e931682d0442578157",237.0,40.0,"test/test_masked.py,torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.305250299,2.0,13875.0,2.0,776805.0,17217.0,40539.5,0.0,,0.0,1
pytorch,f54745a6ffc6420d423c7f7d97ee8f535ce2d908,2cb385dd6e880b6339dd337e1b987adb547c862f,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Wed Dec 08 16:50:52 2021 -0800,1638982252.0,"OpInfo for `nn.functional.dropout2d`, revise sample inputs for `dropout` (#67891)

Summary:
Earlier, we were only testing for inputs with the shape of `(5,)` for `nn.functional.dropout`, but since it's used a lot - I feel it's a good idea to test for a few more shapes including scalars. This PR:

1. Revises sample inputs for `nn.functional.dropout`
2. Adds an OpInfo for `nn.functional.dropout2d`.

A note regarding the documentation:

Looks like `nn.functional.dropout2d` also supports inputs of shape `(H, W)` apart from `(N, C, H, W) / (C, H, W)` but the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d) doesn't mention that (`H, W` case). Should that be revised or am I missing anything here? (Filed an issue here: https://github.com/pytorch/pytorch/issues/67892)

```python
# A 2D tensor is a valid input for Dropout2d
In [11]: tensor = torch.randn((3, 4), device='cpu', dtype=torch.float32)
In [12]: dropout2d = torch.nn.Dropout2d(p=0.5)

In [13]: dropout2d(tensor)
Out[13]:
tensor([[-0.1026, -0.0000, -0.0000, -0.0000],
        [-1.5647,  0.0000, -0.0000, -0.5820],
        [-0.0000, -3.2080,  0.1164, -3.6780]])
```

Issue Tracker: https://github.com/pytorch/pytorch/issues/54261

cc: mruberry zou3519

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67891

Reviewed By: mrshenli

Differential Revision: D32628527

Pulled By: mruberry

fbshipit-source-id: 4c9b89550f1d49526e294378ce107eba9f29cabb",67.0,9.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.101066964,2.0,16772.0,2.0,23657.5,17609.0,41443.0,0.0,Feature Addition,0.0,1
pytorch,f9786ad351b91269a1d1ed063f975eebd8ca99bb,2ce39de3fc6cb3e2dec93ea3deb04916f91e1b1f,Xiaomeng Yang,yangxm@fb.com,Sat Apr 27 00:16:59 2019 -0700,1556324219.0,"Add elementwise_affine for layer_norm_op (#19713)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19713

Add elementwise_affine for layer_norm_op

Reviewed By: houseroad

Differential Revision: D15075454

fbshipit-source-id: e8a7d3da1c81e49fa55323f5e74a68bc4ef8d83f",407.0,189.0,"caffe2/core/operator_c10wrapper.h,caffe2/operators/layer_norm_op.cc,caffe2/operators/layer_norm_op.cu,caffe2/operators/layer_norm_op.h,caffe2/python/operator_test/layer_norm_op_test.py,caffe2/utils/math/reduce.cc,caffe2/utils/math/reduce.cu,test/test_torch.py",8.0,8,2,2.099876996,42.0,13877.0,7.0,2398714.0,8373.0,25007.33333,0.0,Feature Addition,0.0,1
pytorch,ab8af155455bc9148234b4a595a6e3280478478d,2cf4d8128dd054bee22b4898be5473caadf9aa13,Philip Meier,github.pmeier@posteo.de,Tue Aug 03 15:59:39 2021 -0700,1628006379.0,"add `OpInfo` for `torch.nn.functional.mse_loss` (#62254)

Summary:
Addresses facebookresearch/functorch#78.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62254

Reviewed By: malfet

Differential Revision: D30013331

Pulled By: zou3519

fbshipit-source-id: e3242cb97d1f061b932e3e0ed589f1ee6a291512",43.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8398.0,1.0,3429.0,14364.0,32844.0,0.0,Feature Addition,0.0,1
pytorch,82cdd3abae173985c21889522cbe9f7d6dc43dc5,2cf576e9ea225e195f7bd49e4545ceb02abe04c3,ettiee,ettie-eyre@cookpad.com,Wed Mar 11 17:58:25 2020 -0700,1583949505.0,"small typos (#34589)

Summary:
Spotted a couple of small typos ð
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34589

Differential Revision: D20387653

Pulled By: ngimel

fbshipit-source-id: 3089fe606ccb8c8ee57cf7a900aba714fd0ce567",2.0,2.0,"test/test_jit.py,torch/onnx/symbolic_opset9.py",2.0,3,2,1,14.0,21258.0,2.0,141104.0,10.0,53.5,0.0,,0.0,1
pytorch,9df1b98bab07bca46cd7166f451af43c291c7e64,2d36b30a8c35cc5640e1c735583c105b7037f9ca,Mike Ruberry,mruberry@devfair044.maas,Tue Mar 09 16:15:19 2021 -0800,1615306519.0,"Expands OpInfo out= testing (#53259)

Summary:
Addresses several of the challenges described in https://github.com/pytorch/pytorch/issues/49468.

This PR builds on https://github.com/pytorch/pytorch/pull/50741 and https://github.com/pytorch/pytorch/issues/53105 to extend OpInfo out= testing. It covers the following cases for ops that produce a single tensor:

- out= values don't affect computation
- out= noncontiguous produces the correct output and preserves strides
- out= with the wrong shape throws a warning
- out= with an empty tensor throws no warning
- out= with the wrong device throws an error
- out= with a dtype the computation's result can't be ""safely"" cast to throws an error

It works with operations that produce a single tensor and operations that produce an iterable of tensors (the latter is tested with operations like torch.svd).

In addition to the new out= test, the OpInfos have been updated. ""supports_tensor_out"" is replaced with the more general and straightforward ""supports_out"" metadata, and many operations which previously had to skip out= testing with an explicit SkipInfo no longer need to. A couple redundant tests in test_unary_ufuncs.py have been removed, too.

One other perk of these tests is that once all operations have OpInfos this will allow us to validate that we've universally deprecated incorrectly sized tensors passed to out=, and give us the option to actually disable the behavior.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53259

Reviewed By: mrshenli

Differential Revision: D26894723

Pulled By: mruberry

fbshipit-source-id: 2b536e9baf126f36386a35f2f806dd88c58690b3",246.0,91.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.983415783,2.0,4364.0,1.0,59499.0,9566.0,21233.5,0.0,Corrective,0.0,1
pytorch,9943cf2378a9919c576420a743ff96306512efe1,2d485ffb17ebdec54b399df591e1da031e101d46,Edward Yang,ezyang@fb.com,Wed Dec 12 19:19:03 2018 -0800,1544642343.0,"Move CUDAGuard, CUDAStream and CUDAGuardImpl to c10/cuda (#14248)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14248

This diff also introduces a horrifying hack to override CUDA's DeviceGuardImpl
with a HIPGuardImplMasqueradingAsCUDA, to accommodate PyTorch's current
behavior of pretending CUDA is HIP when you build with ROCm enabled.

Reviewed By: bddppq

Differential Revision: D13145293

fbshipit-source-id: ee0e207b6fd132f0d435512957424a002d588f02",1019.0,873.0,"aten/src/ATen/CMakeLists.txt,aten/src/ATen/cuda/CUDAContext.cpp,aten/src/ATen/cuda/CUDAContext.h,aten/src/ATen/cuda/CUDAEvent.h,aten/src/ATen/cuda/CUDAGuard.h,aten/src/ATen/cuda/CUDAMultiStreamGuard.h,aten/src/ATen/cuda/CUDAStream.cpp,aten/src/ATen/cuda/CUDAStream.h,aten/src/ATen/cuda/detail/CUDAGuardImpl.cpp,aten/src/ATen/cuda/detail/CUDAGuardImpl.h,aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.cpp,aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h,aten/src/ATen/miopen/Utils.h,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/Resize.cuh,aten/src/ATen/test/cuda_stream_test.cpp,aten/src/THC/THCCachingAllocator.cpp,aten/src/THC/THCCachingAllocator.h,aten/src/THC/THCCachingHostAllocator.h,aten/src/THC/THCGeneral.cpp,aten/src/THC/THCGeneral.h.in,aten/src/THC/THCStream.cpp,c10/cuda/CMakeLists.txt,c10/cuda/CUDAGuard.h,c10/cuda/CUDAStream.cpp,c10/cuda/CUDAStream.h,c10/cuda/impl/CUDAGuardImpl.cpp,c10/cuda/impl/CUDAGuardImpl.h,c10/impl/InlineDeviceGuard.h,docs/cpp/source/Doxyfile,tools/amd_build/pyHIPIFY/cuda_to_hip_mappings.py,tools/amd_build/pyHIPIFY/hipify_python.py,tools/cwrap/plugins/AutoGPU.py,tools/cwrap/plugins/NNExtension.py,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/profiler.cpp,torch/csrc/cuda/Stream.cpp,torch/csrc/cuda/comm.cpp,torch/csrc/cuda/nccl.cpp,torch/csrc/cuda/python_nccl.cpp,torch/csrc/distributed/c10d/ddp.cpp,torch/csrc/generic/StorageSharing.cpp,torch/csrc/jit/fuser/cuda/fused_kernel.cpp,torch/csrc/utils.h,torch/lib/THD/base/data_channels/DataChannelNccl.cpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/ProcessGroupGloo.hpp,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/test/CUDATest.hpp,torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,torch/lib/c10d/test/ProcessGroupNCCLTest.cpp",52.0,40,5,3.613205243,44.0,14919.0,18.0,766874.9555555555,6018.0,18707.83333,0.0,Feature Addition,0.0,1
pytorch,0ef10385b2d82b87d89124a6dc3e0d95dfb97a51,2d5fbe6e0de1e5dfa292afedec34b277c14d7c10,Peter Goldsborough,peter@goldsborough.me,Tue Feb 13 04:26:26 2018 -0800,1518495986.0,"Improve Variable interface (#5127)

* Improve Variable interface

* Address comments from @apaszke and @colesbury

* string ::operator= is not noexcept

* Remove ir.h from tracer_state.h to improve build times

* Make Variable a struct and pack SavedVariable fields

* Implement as_variable_ref

* grad_fn_ptr() -> grad_fn_unsafe()

* Reduce hackiness of set_type hack

* Include variable.h and edge.h in tracer_state.h because it uses them

* class Variable -> struct Variable because Windows cant even

* Make Variable::output_nr uint32_t instead of int

* Add comment about tracing state

* Replaced more static_cast<Variable&> and improve docs

* Remove SavedVariable destructor and construct members in init list

* Clarify docs for Variable

* Variable::set_version -> set_version_counter",934.0,616.0,".clang-format,aten/src/ATen/TensorImpl.h,setup.py,tools/autograd/gen_autograd_functions.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_torch_functions.cpp,torch/csrc/autograd/edge.h,torch/csrc/autograd/function_hook.h,torch/csrc/autograd/functions/basic_ops.cpp,torch/csrc/autograd/functions/special.cpp,torch/csrc/autograd/functions/utils.cpp,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/autograd/saved_variable.cpp,torch/csrc/autograd/saved_variable.h,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/autograd/variable_version.h,torch/csrc/jit/graph_executor.cpp,torch/csrc/jit/interpreter.cpp,torch/csrc/jit/interpreter_autograd_function.cpp,torch/csrc/jit/ir.h,torch/csrc/jit/pybind.h,torch/csrc/jit/python_compiled_function.cpp,torch/csrc/jit/test_jit.cpp,torch/csrc/jit/tracer.cpp,torch/csrc/jit/tracer.h,torch/csrc/jit/tracer_state.cpp,torch/csrc/jit/tracer_state.h,torch/csrc/jit/variable_flags.cpp,torch/csrc/utils/hash.h,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tuple_parser.cpp,torch/csrc/utils/variadic.h",38.0,12,3,3.240243281,39.0,11344.0,23.0,1794419.189189189,2367.0,24606.85823,0.0,Feature Addition,0.0,1
pytorch,5dcbcc6de83b552f43415ef60bb2e771bdbc0026,2da43ec01a6e8c6dbb06e176e632a6c1fe7188de,Mike Ruberry,mruberry@devfair044.h1.fair,Fri Apr 15 23:07:53 2022 +0000,1650064073.0,"Adds margin_ranking_loss opinfo

per title
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75887
Approved by: https://github.com/ngimel",31.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,16944.0,1.0,1889.0,2340.0,5493.0,0.0,Feature Addition,0.0,1
pytorch,1071e92335d8a69107f4694f56bebbb9655371aa,2db742fc95b00c310a3122826022869eb7f65cb3,bddppq,bai@in.tum.de,Thu Dec 20 05:29:41 2018 -0800,1545283781.0,"Do not use fork to invoke test scripts in pytorch rocm CI

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/14600

Differential Revision: D13523937

Pulled By: bddppq

fbshipit-source-id: 1493fdd051283650081d7944bb2bd7f0c4c44990",728.0,691.0,"test/common_methods_invocations.py,test/common_utils.py,test/run_test.py,test/test_autograd.py,test/test_distributions.py,test/test_jit.py",6.0,1,1,0.244358375,42.0,20999.0,6.0,1146286.8333333333,6155.0,19128.33333,0.0,,0.0,1
pytorch,db298618e4cf27b8bb92e166d1018481b45be517,2dcaa404259bf4e05bdf1e6079c9d6509192fc84,Edward Z. Yang,ezyang@fb.com,Thu Sep 21 16:18:32 2017 -0700,1506010712.0,"Add get_rng_state_all and set_rng_state_all.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",53.0,5.0,"test/test_cuda.py,torch/cuda/__init__.py,torch/cuda/random.py",3.0,3,2,0.906478631,37.0,1502.0,3.0,652417.0,1865.0,24929.55562,0.0,Feature Addition,0.0,1
pytorch,21d48be2dcab9ffcb51beef18d63d1d427f10612,2dd7039b6badcc362fb6da62a33c49418acd5c5d,peterjc123,peter_jiachen@163.com,Sat Jan 06 16:41:36 2018 +0800,1515256896.0,Fix multiprocessing and dataloader tests on Windows (#4453),30.0,30.0,"test/test_dataloader.py,test/test_multiprocessing.py",2.0,1,1,0.519702787,36.0,957.0,2.0,855233.0,899.0,6698.672317,0.0,Corrective,1.0,1
pytorch,dc2c4d02f95c8020cb1a8e3896a51920567f7aa8,2dd867f30fef46fc85fd4e2e08dc32a60f171da0,Pavel Belevich,pbelevich@fb.com,Thu Mar 26 02:48:28 2020 -0700,1585190908.0,"Move normal() to DistributionTemplates (#35167)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35167

The purpose of this PR is to move `normal`/`normal_`/`normal_out` to `native/DistributionTemplates.h`, `native/cpu/DistributionTemplates.h` and `native/cuda/DistributionTemplates.h` to make it reusable for custom RNG, see cpu_rng_test.cpp as an example of custom RNG.

Test Plan: Imported from OSS

Differential Revision: D20588248

Pulled By: pbelevich

fbshipit-source-id: 7ee60be97f81522cd68894ff1389007c05130a60",618.0,374.0,"aten/src/ATen/native/ComplexHelper.h,aten/src/ATen/native/DistributionTemplates.h,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/cpu/DistributionTemplates.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/DistributionNormal.cu,aten/src/ATen/native/cuda/DistributionTemplates.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/cpu_rng_test.cpp,test/test_torch.py",11.0,8,2,3.104618042,42.0,25665.0,6.0,332293.0,467.0,1271.0,0.0,,0.0,1
pytorch,8013dac43d2acb592cab75317f17d4f9c5b9eb6a,2e0dd8690320fb1a7ecd548730824c1610207179,Peter Goldsborough,psag@fb.com,Thu Aug 16 04:09:33 2018 -0700,1534392573.0,"Make torch::Tensor -> at::Tensor (#10516)

Summary:
This PR removes the `using Tensor = autograd::Variable;` alias from `torch/tensor.h`, which means `torch::Tensor` is now `at::Tensor`. This PR fixes up some last uses of `.data()` and tidies up the resulting code. For example, I was able to remove `TensorListView` such that code like

```
auto loss = torch::stack(torch::TensorListView(policy_loss)).sum() +
    torch::stack(torch::TensorListView(value_loss)).sum();
```

is now

```
auto loss = torch::stack(policy_loss).sum() + torch::stack(value_loss).sum();
```

CC jgehring

ebetica
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10516

Differential Revision: D9324691

Pulled By: goldsborough

fbshipit-source-id: a7c1cb779c9c829f89cea55f07ac539b00c78449",151.0,260.0,"test/cpp/api/any.cpp,test/cpp/api/integration.cpp,test/cpp/api/module.cpp,test/cpp/api/modules.cpp,test/cpp/api/parallel.cpp,test/cpp/api/rnn.cpp,test/cpp/api/sequential.cpp,test/cpp/api/serialization.cpp,test/cpp/api/tensor.cpp,test/cpp/api/util.h,tools/autograd/gen_variable_factories.py,tools/autograd/templates/variable_factories.h,torch/csrc/api/include/torch/nn/cloneable.h,torch/csrc/api/include/torch/nn/module.h,torch/csrc/api/include/torch/nn/modules/any.h,torch/csrc/api/include/torch/optim/adagrad.h,torch/csrc/api/include/torch/optim/adam.h,torch/csrc/api/include/torch/optim/lbfgs.h,torch/csrc/api/include/torch/optim/optimizer.h,torch/csrc/api/include/torch/optim/rmsprop.h,torch/csrc/api/include/torch/optim/sgd.h,torch/csrc/api/include/torch/serialization.h,torch/csrc/api/include/torch/tensor.h,torch/csrc/api/include/torch/tensor_list_view.h,torch/csrc/api/include/torch/torch.h,torch/csrc/api/src/nn/cursor.cpp,torch/csrc/api/src/nn/modules/conv.cpp,torch/csrc/api/src/nn/modules/linear.cpp,torch/csrc/api/src/nn/modules/rnn.cpp,torch/csrc/api/src/optim/adagrad.cpp,torch/csrc/api/src/optim/adam.cpp,torch/csrc/api/src/optim/lbfgs.cpp,torch/csrc/api/src/optim/rmsprop.cpp,torch/csrc/api/src/optim/sgd.cpp,torch/csrc/autograd/variable.cpp",35.0,19,3,4.33709959,38.0,6033.0,15.0,1334214.0571428572,3501.0,9521.333333,0.0,Corrective,1.0,1
pytorch,a840afbeb4e4e39640bbe2886ce9c3b8b2fc3aff,2e359ef86d595e25616c7436757398b898db6398,Natalia Gimelshein,ngimel@fb.com,Thu Jan 30 00:31:47 2020 -0800,1580344307.0,"enable empty batch for all flavor of convolutions (#32709)

Summary:
resubmitting https://github.com/pytorch/pytorch/issues/32612 after a merge gone wrong. Enables convolution with an empty batch or number of channels for all flavors of convolution (grouped convolution, convTranspose). Would make https://github.com/pytorch/pytorch/issues/31658 unnecessary. Also returns zero gradients for the parameters, that's necessary for correct DDP operation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32709

Differential Revision: D19627968

Pulled By: ngimel

fbshipit-source-id: 7359759bd05ff0df0eb658cac55651c607f1b59f",52.0,20.0,"aten/src/ATen/native/Convolution.cpp,test/test_nn.py",2.0,5,2,0.903776288,42.0,11812.0,2.0,522716.0,14484.0,39252.83333,0.0,Corrective,0.0,1
pytorch,61cc03fb8d61a8da7e34715f9680ef0342284903,2e37ab85afa1b3b7b05f1ebe24c220809a05de9b,Iurii Zdebskyi,iuriiz@fb.com,Thu Jun 06 18:56:26 2019 -0700,1559847386.0,"Enable bool support for several index methods (#21435)

Summary:
Enable bool tensors for these index methods:
- index_select
- index_copy
- put
- take
- index_fill

Tested via unit tests

TODO:
Enable index_add in a separate PR as it requires more ""side"" changes.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21435

Differential Revision: D15684964

Pulled By: izdeby

fbshipit-source-id: 48440e4d44873d70c4577e017dd0d8977e0fa15a",358.0,304.0,"aten/src/ATen/Declarations.cwrap,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/THC/THCTensorIndex.cu,aten/src/THC/THCTensorMath.h,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorIndex.h,test/test_torch.py",8.0,8,2,1.781556183,41.0,16993.0,8.0,4804944.875,9208.0,26843.83333,0.0,Feature Addition,0.0,1
pytorch,cb626da145cd7bd015f6cfc387ba7cbcd4f1b0a5,2e4f566d305e9e2fc4688d32cd7226cdd8fcb6bc,Philip Meier,github.pmeier@posteo.de,Mon Aug 02 16:43:04 2021 -0700,1627922584.0,"add `OpInfo` for `torch.nn.functional.softplus` (#62317)

Summary:
Addresses facebookresearch/functorch#78.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62317

Reviewed By: malfet

Differential Revision: D30013322

Pulled By: zou3519

fbshipit-source-id: e80affd10b81534234694c9e4326cc68c7efc7fe",31.0,1.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8313.0,1.0,268844.0,14326.0,32767.5,0.0,Feature Addition,0.0,1
pytorch,1046593509d4d5f1df97cdbd41188624943c2cb0,2e5a8cee82b21b917d3d5cf6b77577f798b596a5,Xiang Gao,qasdfgtyuiop@gmail.com,Thu Feb 28 20:59:34 2019 -0800,1551387574.0,"Customize the printing of namedtuple return (#17136)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/17112
```python
print(""good"", torch.randn(5,5,5).max(1))
print(""terrible"", torch.randn(5,5,10).max(1))
print(""not as good"", torch.randn(5,5,500).max(1))
print (""old behaviour = gold standard"")
print(tuple(torch.randn(5,5,5).max(1)))
print(tuple(torch.randn(5,5,10).max(1)))
print(tuple(torch.randn(5,5,500).max(1)))
```
now gives
```
>>> import torch
>>> print(""good"", torch.randn(5,5,5).max(1))
good torch.return_types.max(
values=tensor([[ 1.2821,  1.8063,  1.8075,  1.3082, -0.1267],
        [ 0.3437,  0.7353,  1.2619,  0.7557,  1.6662],
        [ 0.8583,  1.8906,  1.0246,  1.7598,  1.1184],
        [ 1.7821,  0.0230,  0.9452,  1.0318,  1.0823],
        [ 0.4116, -0.0379, -0.1843,  1.4129,  1.8796]]),
indices=tensor([[4, 4, 3, 2, 1],
        [1, 2, 4, 1, 1],
        [2, 4, 0, 2, 1],
        [0, 2, 0, 3, 1],
        [0, 4, 4, 4, 4]]))
>>> print(""terrible"", torch.randn(5,5,10).max(1))
terrible torch.return_types.max(
values=tensor([[ 2.1272,  1.3664,  2.2067,  1.3974, -0.0883,  1.2505,  1.0074,  1.1217,
          0.3849,  0.6936],
        [ 0.6288, -0.4560,  1.2748,  1.5482,  1.2777,  1.6874,  0.7151,  0.6041,
          1.3572,  1.6232],
        [ 1.6703,  1.0075,  1.6480,  2.2839,  1.3390,  0.4938,  1.6449,  1.7628,
          0.8141,  2.5714],
        [ 0.7079,  1.8677,  3.2478,  1.5591,  2.4870,  0.8635, -0.1450,  1.6923,
          1.4924,  1.6298],
        [ 2.4056,  0.8002,  0.9317,  0.7455,  0.7866,  2.1191,  0.3492,  1.2095,
          1.8637,  1.7470]]),
indices=tensor([[1, 1, 0, 0, 0, 0, 3, 4, 4, 4],
        [4, 2, 2, 1, 2, 2, 3, 1, 1, 3],
        [0, 3, 3, 0, 2, 1, 4, 1, 0, 1],
        [4, 1, 3, 0, 3, 2, 0, 1, 4, 3],
        [1, 0, 3, 2, 1, 0, 0, 1, 0, 1]]))
>>> print(""not as good"", torch.randn(5,5,500).max(1))
not as good torch.return_types.max(
values=tensor([[ 0.3877,  0.7873,  1.8701,  ...,  0.5971,  1.6103, -0.3435],
        [ 1.1300,  2.2418,  1.4239,  ...,  1.3943,  0.3872,  1.6475],
        [ 2.0656,  1.3136,  0.9896,  ...,  2.3918,  0.8226,  1.0517],
        [ 1.1054,  0.9945,  1.0561,  ...,  2.1039,  1.1524,  3.0304],
        [ 1.5041,  2.2809,  1.0883,  ...,  0.8504,  2.4774,  1.1041]]),
indices=tensor([[4, 3, 1,  ..., 1, 4, 0],
        [4, 4, 4,  ..., 3, 0, 3],
        [3, 0, 1,  ..., 2, 2, 4],
        [0, 1, 1,  ..., 4, 2, 2],
        [1, 0, 4,  ..., 2, 0, 2]]))
>>> print (""old behaviour = gold standard"")
old behaviour = gold standard
>>> print(tuple(torch.randn(5,5,5).max(1)))
(tensor([[ 1.1908,  1.1807,  1.3151,  1.7184,  0.3556],
        [ 0.3798,  0.9213,  0.3001,  1.3087,  2.2419],
        [ 1.4233,  1.4814,  1.9900,  1.7744,  1.3059],
        [ 1.0026, -0.0330,  1.3061,  1.8730,  2.0685],
        [ 1.3041,  1.6458,  1.3449,  1.8948,  3.6206]]), tensor([[0, 4, 3, 4, 0],
        [1, 1, 4, 0, 4],
        [4, 1, 0, 3, 3],
        [1, 2, 1, 4, 0],
        [3, 3, 0, 3, 3]]))
>>> print(tuple(torch.randn(5,5,10).max(1)))
(tensor([[-0.1232,  0.8275,  0.6732,  1.1223,  0.8247,  1.2851,  1.6009,  1.9979,
          1.9109,  0.7313],
        [ 0.2260,  0.5922,  1.6928,  0.6024,  2.1158,  3.0619,  0.5653,  0.7426,
          0.8316,  0.6346],
        [ 0.4319,  0.2231,  0.5255,  1.7620,  1.1657,  0.8875,  0.5782,  0.6506,
          0.5032,  1.7097],
        [ 0.4137,  1.7265,  1.4260,  2.0301,  1.2244,  0.7128,  2.6345,  0.7230,
          1.3553,  1.6508],
        [ 1.0684,  1.7195,  1.4068,  0.7076, -0.0242,  0.8474,  0.8754,  1.7108,
          0.2188,  1.1584]]), tensor([[0, 1, 3, 4, 2, 3, 4, 2, 1, 0],
        [1, 4, 0, 0, 3, 2, 0, 0, 3, 3],
        [2, 3, 1, 1, 4, 0, 1, 4, 4, 4],
        [0, 4, 1, 3, 2, 0, 2, 0, 3, 1],
        [1, 0, 0, 0, 0, 3, 3, 3, 2, 0]]))
>>> print(tuple(torch.randn(5,5,500).max(1)))
(tensor([[0.9395, 1.5572, 1.8797,  ..., 2.0494, 0.8202, 0.9623],
        [1.7937, 0.7225, 1.8836,  ..., 0.7927, 1.4976, 1.1813],
        [0.8558, 1.6943, 1.4192,  ..., 0.8327, 1.9661, 0.4197],
        [1.2993, 1.4995, 0.9357,  ..., 0.7810, 1.3030, 2.6216],
        [1.4206, 1.8315, 1.0338,  ..., 1.4312, 1.3198, 1.5233]]), tensor([[0, 4, 3,  ..., 3, 0, 2],
        [0, 1, 0,  ..., 0, 4, 3],
        [3, 4, 3,  ..., 3, 0, 0],
        [3, 2, 3,  ..., 1, 2, 1],
        [1, 2, 4,  ..., 3, 1, 3]]))
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17136

Differential Revision: D14250021

Pulled By: VitalyFedyunin

fbshipit-source-id: aae72f03b35980063b1ac1f07b8353eddb0c8b93",154.0,0.0,"test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_nn_functions.cpp,tools/autograd/templates/python_torch_functions.cpp,tools/autograd/templates/python_variable_methods.cpp,torch/CMakeLists.txt,torch/csrc/utils/six.h,torch/csrc/utils/structseq.cpp,torch/csrc/utils/structseq.h",9.0,7,3,1.539844319,41.0,13346.0,7.0,2548748.4285714286,7267.0,22211.83333,0.0,Corrective,1.0,1
pytorch,195ab5e864ebc5e380b05a30bb5081afbf5666d0,2e600feda9bb9359700ea21dac4c23c5aa552887,kshitij12345,kshitijkalambarkar@gmail.com,Sun Dec 06 06:03:09 2020 -0800,1607234589.0,"[numpy] `torch.sinh`: promote integer inputs to float (#48644)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48644

Reviewed By: heitorschueroff

Differential Revision: D25298436

Pulled By: mruberry

fbshipit-source-id: 675ad8e3c34e61fbbab77eca15048df09b09c1ed",41.0,9.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/UnaryGeometricKernels.cu,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py",4.0,11,2,1.08904965,9.0,4900.0,3.0,351415.75,7248.0,16370.0,0.0,,0.0,1
pytorch,e9953c45951685449c22d3f4b1335c9f0017cc62,2e7635b9299026145cd66700b536f26f31724a34,andrew giessel,andrew@giessel.com,Wed May 03 12:46:28 2017 -0400,1493815588.0,Add flexible bilinear upsampling aspect ratio redux (#1317),69.0,11.0,"test/test_nn.py,torch/nn/_functions/thnn/upsampling.py,torch/nn/functional.py,torch/nn/modules/upsampling.py",4.0,6,2,1.894488361,28.0,3948.0,3.0,258332.75,707.0,8750.317468,0.0,Feature Addition,0.0,1
pytorch,3027e783b1cba83546dab50610a9d244bc2632b2,2e97c82470966df6942f364102690460ea58403e,Igor Fedan,ifedan@fb.com,Tue Apr 02 20:18:20 2019 -0700,1554236300.0,"torch.cross' dim default changed to c10::optional instead of int=-1 (#17582)

Summary:
Argument dim=-1 doesn't work for torch.cross. The signature of the torch.cross has been changed to c10::optional<int64_t> dim instead of int64_t. So based on document ""If dim is not given, it defaults to the first dimension found with the size 3."" and if dim is specified (even negative) it will use the correspondent dim.

Fixes #17229
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17582

Differential Revision: D14483063

Pulled By: ifedan

fbshipit-source-id: f9699093ec401cb185fd33ca4563c8a46cdcd746",203.0,85.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/Cross.cpp,aten/src/ATen/native/Cross.h,aten/src/ATen/native/LegacyDefinitions.cpp,aten/src/ATen/native/cpu/CrossKernel.cpp,aten/src/ATen/native/cuda/CrossKernel.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathPointwise.h,test/test_torch.py,tools/autograd/derivatives.yaml",16.0,14,3,3.068103544,42.0,26659.0,7.0,523964.8333333333,7839.0,23732.83333,0.0,Corrective,1.0,1
pytorch,b9a02128bc8fd7c4ba2f5f6199bbf4ef78b6592f,2e9eb5afa21a005a9be7b534597daadebf807859,Jane Xu,janeyx@fb.com,Wed Apr 07 15:02:58 2021 -0700,1617807778.0,"Use slow tests stats in common_utils (#55190)

Summary:
This is a step in adding automatic slowTest detection to our testing infrastructure. This uses stats (updated daily) in https://github.com/pytorch/test-infra/blob/master/stats/.pytorch-slow-tests to determine whether more tests need to be marked as slow as they are run.

More details in previous PR draft/proposal [here](https://github.com/pytorch/pytorch/pull/54456#issue-598388491), though I no longer think we need the third step as using a raw git file does not require much processing.

Upon looking at [logs](https://circleci.com/api/v1.1/project/github/pytorch/pytorch/12060292/output/107/0?file=true&allocation-id=606660dbd8e5857bcc2b2e0f-0-build%2F60DCA8CD) for the coverage tests as of the first commit [when I had not skipped the tests so we could see their actual times], here are some slow tests that weren't marked as slow before:
```
test_fn_gradgrad_unfold_cpu_complex128 (__main__.TestGradientsCPU) (172.554s)
test_matmul_4d_4d_complex_cpu (__main__.TestAutogradDeviceTypeCPU) (180.057s)
test_conv1d_basic (__main__.TestXNNPACKConv1dTransformPass) (94.737s)
```

And here is a test that wasn't actually slow but was still marked as slow based on stats:
```
test_trunc_normal (__main__.TestNNInit) ... ok (1.208s)
```

The new logs show the above tests as skipped (as they should be):
[Coverage Test 1](https://app.circleci.com/pipelines/github/pytorch/pytorch/296224/workflows/ba6c2917-51f8-4fb8-be57-90151c2e5c25/jobs/12126156) and [Coverage Test 2](https://app.circleci.com/pipelines/github/pytorch/pytorch/296224/workflows/ba6c2917-51f8-4fb8-be57-90151c2e5c25/jobs/12126155)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55190

Reviewed By: samestep

Differential Revision: D27566663

Pulled By: janeyx99

fbshipit-source-id: c13f8c676bb8eb15d9d697d224dbaef7df98aef3",22.0,1.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2129.0,1.0,136741.0,10513.0,23297.5,0.0,Feature Addition,0.0,1
pytorch,d183f2305fa6359275b521d33fc971954f79c3b5,2ebd7f17ebe90738c0f9c30c89bd9af88ba5deee,gchanan,gregchanan@gmail.com,Thu Feb 01 18:57:26 2018 -0500,1517511446.0,Support stack_out as a native function. (#4977),51.0,11.0,"aten/src/ATen/function_wrapper.py,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/ATen/preprocess_declarations.py,test/test_torch.py",6.0,5,2,2.071372372,38.0,7597.0,6.0,1419486.0,491.0,1451.405869,0.0,,0.0,1
pytorch,c2c32340a4a8a258c97665a5cf2d2054f06cf0f6,2ed99fee0dc4fdf040d5371ab1e34d06bb02a7e6,Edward Yang,ezyang@fb.com,Fri Mar 01 22:14:02 2019 -0800,1551478442.0,"Revert D13935403: Call c10 cuda op from test_torch

Differential Revision:
D13935403

Original commit changeset: b2915ec8a366

fbshipit-source-id: 0f3409d5c102d719bc1f0483695aee93e7d613c9",1.0,13.0,test/test_torch.py,1.0,1,1,0,40.0,10448.0,1.0,11947.0,7299.0,22346.33333,0.0,,0.0,1
pytorch,1e2b2ee5ffd5a60866d2ac23a7637aa1ea06761c,2f099c7555ab41e55ddfd12ba3ed804e52894820,Xue Li,xueli@fb.com,Fri Oct 15 22:19:28 2021 -0700,1634336368.0,"Revert D30652629: use irange for loops

Test Plan: revert-hammer

Differential Revision:
D30652629 (https://github.com/pytorch/pytorch/commit/687c2267d4dfb69138483f90deb4d4f5921a2965)

Original commit changeset: 0ae6c4bbbb55

fbshipit-source-id: 5c4f067b584a021c8c9656454d1ee60999600fb3",21930.0,22184.0,"android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp,android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp,aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Context.h,aten/src/ATen/ExpandUtils.cpp,aten/src/ATen/ExpandUtils.h,aten/src/ATen/MemoryOverlap.cpp,aten/src/ATen/NamedTensorUtils.cpp,aten/src/ATen/ParallelNative.cpp,aten/src/ATen/SparseTensorImpl.h,aten/src/ATen/SparseTensorUtils.cpp,aten/src/ATen/TensorIndexing.cpp,aten/src/ATen/TensorIndexing.h,aten/src/ATen/TensorIterator.cpp,aten/src/ATen/TensorIterator.h,aten/src/ATen/TensorIteratorInternal.h,aten/src/ATen/TensorNames.cpp,aten/src/ATen/TensorUtils.cpp,aten/src/ATen/VmapTransforms.cpp,aten/src/ATen/WrapDimUtils.h,aten/src/ATen/WrapDimUtilsMulti.h,aten/src/ATen/benchmarks/stateful_conv1d.cpp,aten/src/ATen/core/Array.h,aten/src/ATen/core/Formatting.cpp,aten/src/ATen/core/MT19937RNGEngine.h,aten/src/ATen/core/TensorAccessor.h,aten/src/ATen/core/boxing/impl/test_helpers.h,aten/src/ATen/core/dispatch/DispatchKeyExtractor.h,aten/src/ATen/core/dispatch/backend_fallback_test.cpp,aten/src/ATen/core/function_schema.h,aten/src/ATen/core/function_schema_inl.h,aten/src/ATen/core/ivalue_inl.h,aten/src/ATen/core/op_registration/infer_schema.cpp,aten/src/ATen/core/qualified_name.h,aten/src/ATen/core/stack.h,aten/src/ATen/cpu/vec/functional_base.h,aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec/vec256/vec256_double.h,aten/src/ATen/cpu/vec/vec256/vec256_float.h,aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vec/vec256/vec256_int.h,aten/src/ATen/cpu/vec/vec256/vec256_qint.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_quint8_vsx.h,aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h,aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h,aten/src/ATen/cpu/vec/vec512/vec512_double.h,aten/src/ATen/cpu/vec/vec512/vec512_float.h,aten/src/ATen/cpu/vec/vec512/vec512_int.h,aten/src/ATen/cpu/vec/vec512/vec512_qint.h,aten/src/ATen/cpu/vec/vec_base.h,aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cudnn/Descriptors.cpp,aten/src/ATen/miopen/Descriptors.cpp,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/AdaptiveAveragePooling.cpp,aten/src/ATen/native/AdaptiveAveragePooling3d.cpp,aten/src/ATen/native/AdaptiveMaxPooling2d.cpp,aten/src/ATen/native/AdaptiveMaxPooling3d.cpp,aten/src/ATen/native/AveragePool3d.cpp,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/BlasKernel.cpp,aten/src/ATen/native/Bucketization.cpp,aten/src/ATen/native/Col2Im.cpp,aten/src/ATen/native/ComplexHelper.h,aten/src/ATen/native/ConstantPadNd.cpp,aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/ConvolutionMM2d.cpp,aten/src/ATen/native/ConvolutionMM3d.cpp,aten/src/ATen/native/ConvolutionTBC.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Cross.cpp,aten/src/ATen/native/DilatedConvolutionUtils.h,aten/src/ATen/native/DilatedMaxPool3d.cpp,aten/src/ATen/native/Dropout.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/Fill.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/Im2Col.cpp,aten/src/ATen/native/IndexingUtils.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/LossMultiLabelMargin.cpp,aten/src/ATen/native/LossMultiMargin.cpp,aten/src/ATen/native/LossNLL.cpp,aten/src/ATen/native/LossNLL2d.cpp,aten/src/ATen/native/NNPACK.cpp,aten/src/ATen/native/NamedTensor.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/PackedSequence.cpp,aten/src/ATen/native/Pool.h,aten/src/ATen/native/QuantizedLinear.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/RangeFactories.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOpsUtils.h,aten/src/ATen/native/ReflectionPad.cpp,aten/src/ATen/native/Repeat.cpp,aten/src/ATen/native/ReplicationPadding.cpp,aten/src/ATen/native/ResizeCommon.h,aten/src/ATen/native/RowwisePrune.cpp,aten/src/ATen/native/ScatterGatherChecks.h,aten/src/ATen/native/SegmentReduce.cpp,aten/src/ATen/native/SobolEngineOps.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/SortingUtils.h,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/TensorDimApply.h,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorProperties.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/TensorTransformations.cpp,aten/src/ATen/native/TriangularOps.cpp,aten/src/ATen/native/Unfold3d.cpp,aten/src/ATen/native/Unique.cpp,aten/src/ATen/native/UpSample.cpp,aten/src/ATen/native/UpSampleBicubic2d.cpp,aten/src/ATen/native/UpSampleBilinear2d.cpp,aten/src/ATen/native/UpSampleNearest2d.cpp,aten/src/ATen/native/UpSampleNearest3d.cpp,aten/src/ATen/native/UpSampleTrilinear3d.cpp,aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp,aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp,aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp,aten/src/ATen/native/cpu/AvgPoolKernel.cpp,aten/src/ATen/native/cpu/BlasKernel.cpp,aten/src/ATen/native/cpu/CatKernel.cpp,aten/src/ATen/native/cpu/CrossKernel.cpp,aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp,aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,aten/src/ATen/native/cpu/DistributionTemplates.h,aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.cpp,aten/src/ATen/native/cpu/HistogramKernel.cpp,aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp,aten/src/ATen/native/cpu/Loops.h,aten/src/ATen/native/cpu/MaxPoolKernel.cpp,aten/src/ATen/native/cpu/MaxPooling.cpp,aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp,aten/src/ATen/native/cpu/MultinomialKernel.cpp,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/ScatterGatherKernel.cpp,aten/src/ATen/native/cpu/SoftMaxKernel.cpp,aten/src/ATen/native/cpu/SortingKernel.cpp,aten/src/ATen/native/cpu/StackKernel.cpp,aten/src/ATen/native/cpu/SumKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/Unfold2d.cpp,aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp,aten/src/ATen/native/cpu/UpSampleKernel.cpp,aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp,aten/src/ATen/native/cpu/batch_norm_kernel.cpp,aten/src/ATen/native/cpu/group_norm_kernel.cpp,aten/src/ATen/native/cpu/layer_norm_kernel.cpp,aten/src/ATen/native/cpu/moments_utils.h,aten/src/ATen/native/cuda/CuFFTPlanCache.h,aten/src/ATen/native/cuda/SpectralOps.cpp,aten/src/ATen/native/cudnn/Conv_v7.cpp,aten/src/ATen/native/cudnn/GridSampler.cpp,aten/src/ATen/native/cudnn/LossCTC.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/im2col.h,aten/src/ATen/native/metal/MetalShaders.h,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/ATen/native/miopen/RNN_miopen.cpp,aten/src/ATen/native/mkl/LinearAlgebra.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/mkldnn/Pooling.cpp,aten/src/ATen/native/mkldnn/Utils.cpp,aten/src/ATen/native/quantized/Copy.cpp,aten/src/ATen/native/quantized/affine_quantizer_base.cpp,aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp,aten/src/ATen/native/quantized/cpu/fbgemm_utils.h,aten/src/ATen/native/quantized/cpu/int_repr_quant.cpp,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool3d.cpp,aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp,aten/src/ATen/native/quantized/cpu/qclamp.cpp,aten/src/ATen/native/quantized/cpu/qconv.cpp,aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp,aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp,aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp,aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp,aten/src/ATen/native/quantized/cpu/qlinear.cpp,aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp,aten/src/ATen/native/quantized/cpu/qpool.cpp,aten/src/ATen/native/quantized/cpu/qrelu.cpp,aten/src/ATen/native/quantized/cpu/qsigmoid.cpp,aten/src/ATen/native/quantized/cpu/qtanh.cpp,aten/src/ATen/native/quantized/cpu/quant_utils.h,aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp,aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp,aten/src/ATen/native/quantized/cpu/qupsample_nearest3d.cpp,aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp,aten/src/ATen/native/sparse/SoftMax.cpp,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp,aten/src/ATen/native/sparse/SparseMatMul.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,aten/src/ATen/native/utils/ParamsHash.h,aten/src/ATen/native/vulkan/Vulkan.cpp,aten/src/ATen/native/vulkan/VulkanAten.cpp,aten/src/ATen/native/vulkan/VulkanOps.cpp,aten/src/ATen/native/vulkan/api/Runtime.cpp,aten/src/ATen/native/vulkan/api/vk_mem_alloc.h,aten/src/ATen/native/vulkan/ops/Convolution.cpp,aten/src/ATen/native/vulkan/ops/Mm.cpp,aten/src/ATen/native/vulkan/ops/Padding.cpp,aten/src/ATen/native/xnnpack/Convolution.cpp,aten/src/ATen/nnapi/nnapi_bind.cpp,aten/src/ATen/test/apply_utils_test.cpp,aten/src/ATen/test/atest.cpp,aten/src/ATen/test/basic.cpp,aten/src/ATen/test/cpu_generator_test.cpp,aten/src/ATen/test/cuda_tensor_interop_test.cpp,aten/src/ATen/test/ivalue_test.cpp,aten/src/ATen/test/math_kernel_test.cpp,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/packedtensoraccessor_test.cpp,aten/src/ATen/test/pow_test.cpp,aten/src/ATen/test/quantized_test.cpp,aten/src/ATen/test/tensor_interop_test.cpp,aten/src/ATen/test/thread_init_test.cpp,aten/src/ATen/test/vec_test_all_types.cpp,aten/src/ATen/test/vec_test_all_types.h,aten/src/ATen/test/vitals.cpp,aten/src/ATen/test/vmap_test.cpp,aten/src/ATen/test/vulkan_test.cpp,aten/src/THC/THCGeneral.cpp,aten/src/THC/THCTensor.cpp,aten/src/THC/generic/THCTensor.cpp,benchmarks/cpp/tensorexpr/bench_concat.cpp,benchmarks/cpp/tensorexpr/bench_fuser_overhead.cpp,benchmarks/cpp/tensorexpr/bench_parallel.cpp,benchmarks/cpp/tensorexpr/bench_reduce.cpp,binaries/benchmark_helper.h,c10/core/CPUAllocator.cpp,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,c10/core/impl/InlineStreamGuard.h,c10/test/core/impl/SizesAndStrides_test.cpp,c10/test/util/Bitset_test.cpp,c10/test/util/bfloat16_test.cpp,c10/test/util/ordered_preserving_dict_test.cpp,c10/util/Backtrace.cpp,c10/util/typeid.h,caffe2/contrib/aten/aten_op_template.h,caffe2/contrib/fakelowp/fp16_fc_acc_op.h,caffe2/contrib/fakelowp/int8_dequantize_op_nnpi.h,caffe2/contrib/fakelowp/int8_quantize_op_nnpi.h,caffe2/contrib/fakelowp/int8_swish_op_nnpi.h,caffe2/contrib/fakelowp/layernorm_fp16_fake_op.h,caffe2/contrib/fakelowp/lengths_reducer_fused_4bit_rowwise_fp16_fake_op.h,caffe2/contrib/fakelowp/lengths_reducer_fused_8bit_rowwise_fp16_fake_op.h,caffe2/contrib/fakelowp/lengths_reducer_ops.h,caffe2/contrib/fakelowp/quant_lut_fp16_fake_op.h,caffe2/contrib/fakelowp/spatial_batch_norm_fp16_fake_op.h,caffe2/contrib/fakelowp/sum_fp16_fake_op.h,caffe2/contrib/gloo/allgather_ops.h,caffe2/contrib/gloo/allreduce_ops.h,caffe2/contrib/gloo/broadcast_ops.h,caffe2/contrib/gloo/reduce_scatter_ops.h,caffe2/contrib/opencl/OpenCL/cl.hpp,caffe2/core/blob_serialization.h,caffe2/core/context.h,caffe2/core/db.h,caffe2/core/export_c10_op_to_caffe2.h,caffe2/core/export_caffe2_op_to_c10.h,caffe2/core/nomnigraph/include/nomnigraph/Converters/Dot.h,caffe2/core/nomnigraph/include/nomnigraph/Transformations/SubgraphMatcher.h,caffe2/core/operator_schema.h,caffe2/core/qtensor.h,caffe2/core/qtensor_serialization.h,caffe2/core/stats.h,caffe2/core/test_utils.h,caffe2/cuda_rtc/common_rtc.h,caffe2/experiments/operators/fully_connected_op_prune.h,caffe2/experiments/operators/fully_connected_op_sparse.h,caffe2/experiments/operators/funhash_op.h,caffe2/experiments/operators/sparse_funhash_op.h,caffe2/experiments/operators/sparse_matrix_reshape_op.h,caffe2/ideep/operators/conv_pool_base_op.h,caffe2/ideep/operators/conv_transpose_unpool_base_op.h,caffe2/ideep/operators/operator_fallback_ideep.h,caffe2/ideep/utils/ideep_context.h,caffe2/image/image_input_op.h,caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h,caffe2/mobile/contrib/libopencl-stub/include/CL/cl.hpp,caffe2/operators/arg_ops.h,caffe2/operators/assert_op.h,caffe2/operators/batch_gather_ops.h,caffe2/operators/bisect_percentile_op.h,caffe2/operators/byte_weight_dequant_op.h,caffe2/operators/cast_op.h,caffe2/operators/cc_bmm_bg_op.h,caffe2/operators/ceil_op.h,caffe2/operators/concat_split_op.h,caffe2/operators/conv_op_impl.h,caffe2/operators/conv_pool_op_base.h,caffe2/operators/conv_transpose_op_impl.h,caffe2/operators/conv_transpose_op_mobile_impl.h,caffe2/operators/conv_transpose_unpool_op_base.h,caffe2/operators/deform_conv_op_impl.h,caffe2/operators/dense_vector_to_id_list_op.h,caffe2/operators/distance_op.h,caffe2/operators/do_op.h,caffe2/operators/elementwise_logical_ops.h,caffe2/operators/elementwise_op_test.h,caffe2/operators/enforce_finite_op.h,caffe2/operators/expand_op.h,caffe2/operators/expand_squeeze_dims_op.h,caffe2/operators/feature_maps_ops.h,caffe2/operators/filler_op.h,caffe2/operators/find_duplicate_elements_op.h,caffe2/operators/find_op.h,caffe2/operators/floor_op.h,caffe2/operators/fused_rowwise_8bit_conversion_ops.h,caffe2/operators/fused_rowwise_nbit_conversion_ops.h,caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h,caffe2/operators/gather_fused_8bit_rowwise_op.h,caffe2/operators/gather_op.h,caffe2/operators/gather_ranges_to_dense_op.h,caffe2/operators/generate_proposals_op_util_boxes.h,caffe2/operators/generate_proposals_op_util_nms.h,caffe2/operators/given_tensor_byte_string_to_uint8_fill_op.h,caffe2/operators/given_tensor_fill_op.h,caffe2/operators/gru_unit_op.h,caffe2/operators/h_softmax_op.h,caffe2/operators/histogram_op.h,caffe2/operators/im2col_op.h,caffe2/operators/index_hash_ops.h,caffe2/operators/index_ops.h,caffe2/operators/inference_lstm_op.h,caffe2/operators/key_split_ops.h,caffe2/operators/length_split_op.h,caffe2/operators/lengths_pad_op.h,caffe2/operators/lengths_reducer_fused_8bit_rowwise_ops.h,caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.h,caffe2/operators/lengths_reducer_ops.h,caffe2/operators/lengths_reducer_rowwise_8bit_ops.h,caffe2/operators/load_save_op.h,caffe2/operators/locally_connected_op_impl.h,caffe2/operators/lstm_utils.h,caffe2/operators/map_ops.h,caffe2/operators/mean_op.h,caffe2/operators/merge_id_lists_op.h,caffe2/operators/minmax_ops.h,caffe2/operators/moments_op.h,caffe2/operators/ngram_ops.h,caffe2/operators/normalize_op.h,caffe2/operators/numpy_tile_op.h,caffe2/operators/onnx_while_op.h,caffe2/operators/op_utils_cudnn.h,caffe2/operators/operator_fallback_gpu.h,caffe2/operators/order_switch_ops.h,caffe2/operators/pack_rnn_sequence_op.h,caffe2/operators/partition_ops.h,caffe2/operators/piecewise_linear_transform_op.h,caffe2/operators/pool_op.h,caffe2/operators/prepend_dim_op.h,caffe2/operators/quant_decode_op.h,caffe2/operators/quantile_op.h,caffe2/operators/quantized/int8_concat_op.h,caffe2/operators/quantized/int8_dequantize_op.h,caffe2/operators/quantized/int8_given_tensor_fill_op.h,caffe2/operators/quantized/int8_resize_nearest_op.h,caffe2/operators/quantized/int8_roi_align_op.h,caffe2/operators/quantized/int8_test_utils.h,caffe2/operators/reduce_front_back_max_ops.h,caffe2/operators/reduce_front_back_sum_mean_ops.h,caffe2/operators/reduce_ops.h,caffe2/operators/reducer_functors.h,caffe2/operators/reduction_ops.h,caffe2/operators/remove_data_blocks_op.h,caffe2/operators/reshape_op.h,caffe2/operators/reverse_packed_segs_op.h,caffe2/operators/rnn/recurrent_network_blob_fetcher_op.h,caffe2/operators/rnn/recurrent_network_executor.h,caffe2/operators/rnn/recurrent_network_op.h,caffe2/operators/rowmul_op.h,caffe2/operators/scale_blobs_op.h,caffe2/operators/segment_reduction_op.h,caffe2/operators/self_binning_histogram_op.h,caffe2/operators/shape_op.h,caffe2/operators/sinusoid_position_encoding_op.h,caffe2/operators/slice_op.h,caffe2/operators/space_batch_op.h,caffe2/operators/sparse_to_dense_mask_op.h,caffe2/operators/sparse_to_dense_op.h,caffe2/operators/square_root_divide_op.h,caffe2/operators/string_ops.h,caffe2/operators/tensor_protos_db_input.h,caffe2/operators/tile_op.h,caffe2/operators/transpose_op.h,caffe2/operators/tt_linear_op.h,caffe2/operators/unsafe_coalesce.h,caffe2/operators/utility_ops.h,caffe2/operators/variable_length_sequence_padding.h,caffe2/opt/custom/cc_amrc.h,caffe2/opt/nql/ast.h,caffe2/opt/onnxifi_op.h,caffe2/perfkernels/adagrad.h,caffe2/perfkernels/lstm_unit_cpu-impl.h,caffe2/predictor/emulator/data_filler.h,caffe2/python/pybind_state.h,caffe2/quantization/server/elementwise_dnnlowp_op.h,caffe2/quantization/server/im2col_dnnlowp.h,caffe2/quantization/server/mmio.h,caffe2/quantization/server/utility_dnnlowp_ops.h,caffe2/queue/queue_ops.h,caffe2/queue/rebatching_queue_ops.h,caffe2/sgd/adadelta_op.h,caffe2/sgd/adagrad_fused.h,caffe2/sgd/adagrad_op.h,caffe2/sgd/adam_op.h,caffe2/sgd/learning_rate_adaption_op.h,caffe2/sgd/learning_rate_op.h,caffe2/sgd/momentum_sgd_op.h,caffe2/sgd/rowwise_adagrad_fused.h,caffe2/sgd/rowwise_counter.h,caffe2/sgd/storm_op.h,caffe2/sgd/wngrad_op.h,caffe2/sgd/yellowfin_op.h,caffe2/transforms/pattern_net_transform.h,caffe2/utils/proto_utils.h,caffe2/utils/threadpool/WorkersPool.h,caffe2/video/video_input_op.h,test/cpp/api/dataloader.cpp,test/cpp/api/dispatch.cpp,test/cpp/api/expanding-array.cpp,test/cpp/api/fft.cpp,test/cpp/api/functional.cpp,test/cpp/api/init.cpp,test/cpp/api/integration.cpp,test/cpp/api/module.cpp,test/cpp/api/modulelist.cpp,test/cpp/api/modules.cpp,test/cpp/api/nn_utils.cpp,test/cpp/api/operations.cpp,test/cpp/api/optim.cpp,test/cpp/api/parallel.cpp,test/cpp/api/parameterlist.cpp,test/cpp/api/sequential.cpp,test/cpp/api/serialize.cpp,test/cpp/api/static.cpp,test/cpp/api/tensor.cpp,test/cpp/c10d/ProcessGroupGlooTest.cpp,test/cpp/c10d/ProcessGroupNCCLTest.cpp,test/cpp/rpc/test_tensorpipe_serialization.cpp,test/cpp/rpc/test_wire_serialization.cpp,test/cpp/tensorexpr/padded_buffer.cpp,test/cpp/tensorexpr/padded_buffer.h,test/cpp/tensorexpr/test_aten.cpp,test/cpp/tensorexpr/test_boundsinference.cpp,test/cpp/tensorexpr/test_cpp_codegen.cpp,test/cpp/tensorexpr/test_cuda.cpp,test/cpp/tensorexpr/test_expr.cpp,test/cpp/tensorexpr/test_kernel.cpp,test/cpp/tensorexpr/test_llvm.cpp,test/cpp/tensorexpr/test_reductions.cpp,test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tutorial.cpp,test/custom_operator/op.cpp,test/custom_operator/test_custom_ops.cpp,test/mobile/custom_build/predictor.cpp,torch/csrc/autograd/functions/basic_ops.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/deploy/loader.cpp",487.0,118,8,1.55735933,48.0,262355.0,5.0,5546.897330595482,16300.0,38161.0,0.0,,0.0,1
pytorch,4b0098f3ae1a445760f1bf743e1accbf7ae70b56,2f5c0c30cd9a6f3477bf892f753a1fd49e5e718c,Gregory Chanan,gchanan@fb.com,Wed Jul 25 20:32:49 2018 -0700,1532550769.0,"Make logsumexp work with empty tensors again. (#9825)

Summary:
https://github.com/pytorch/pytorch/pull/9755 broke this, but it was only tested if size zero dims were turned on (it can still happen even if that isn't turned on, because we support size [0] tensors).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9825

Differential Revision: D8997303

Pulled By: gchanan

fbshipit-source-id: 911dce112f73fad0f3980a7f4f9423df0f2d923d",10.0,6.0,aten/src/ATen/native/ReduceOps.cpp,1.0,4,1,0,5.0,685.0,1.0,90036.0,3107.0,7552.333333,0.0,,0.0,1
pytorch,61a2d47ec684735960264d6f0b71eefb4e04228d,2f82a0682687552c27f86312de012c95cbb754f5,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Fri Nov 02 14:44:58 2018 -0700,1541169898.0,"Fix half_tensor.bernoulli_(double) (#13474)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/12431
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13474

Differential Revision: D12897834

Pulled By: SsnL

fbshipit-source-id: 598250fd7b9f1d2509ec0e5012724d7895a62daf",15.0,7.0,"aten/src/ATen/native/cuda/Distributions.cu,test/test_cuda.py,test/test_torch.py",3.0,6,2,1.342019218,41.0,11651.0,2.0,1150556.0,5108.0,15209.33333,0.0,Corrective,1.0,1
pytorch,68f847c4c6dc7af1bf32a34957212657bcab0587,2fa17dedaca8d45ef20917c7617f5868c6cb77b2,Xing Liu,xingl@fb.com,Mon Apr 20 18:36:28 2020 -0700,1587407788.0,"add a fast path for EmbeddingBag calling FBGEMM (#36679)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36679

Test Plan:
Imported from OSS

Unit tests:
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_failures_cpu
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_offsets_cpu
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu
python test/run_test.py -i test_nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_no_offsets_cpu
python test/test_nn.py TestNN.test_embeddingbag_from_pretrained
python test/test_nn.py TestNN.test_embeddingbag_from_pretrained_options

Finally run: python test/test_nn.py

Reviewed By: supriyar

Differential Revision: D21058034

Pulled By: xing-liu

fbshipit-source-id: 8fef39078132f63c406976d6b76c51f9ce573f90",50.0,0.0,aten/src/ATen/native/EmbeddingBag.cpp,1.0,4,1,0,7.0,780.0,1.0,4798902.0,1211.0,3147.5,0.0,Feature Addition,0.0,1
pytorch,281463ba0b27480e62d2aa0d54a7510ef0397792,2fa91fa3050bf48780cb757c74db3d0b70a5e458,Nick Gibson,nickg@fb.com,Fri Oct 09 22:13:20 2020 -0700,1602281600.0,"[NNC] Fix crash when simplifying certain subtractions (#46108)

Summary:
Fixes a crash bug in the IRSimplifier when the LHS is a Term (e.g. 2x) and the RHS is a Polynomial (e.g. 2x+1).

This case crashes 100% of the time so I guess it's not very common in models we've been benchmarking.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46108

Reviewed By: agolynski

Differential Revision: D24226593

Pulled By: nickgg

fbshipit-source-id: ef454c855ff472febaeba16ec34891df932723c0",43.0,2.0,"test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",2.0,7,2,0.432750159,1.0,6035.0,2.0,502852.5,5876.0,13635.0,0.0,Corrective,1.0,1
pytorch,0639387ff1aecb69c628e1922bf1e2838c984d34,2fbd70d3366f0e223d263a874f09fa6fc77869c3,Peter Bell,peterbell10@live.co.uk,Wed Nov 18 23:37:22 2020 -0800,1605742642.0,"fft: Generalize fill with conjugate symmetry and use complex dtypes (#46908)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/46908

Generalize to non-contiguous dimensions.

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D25048504

Pulled By: mruberry

fbshipit-source-id: a82545de17fc207fefea7fbd88d03042a3ca41fe",342.0,189.0,"aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/SpectralOpsUtils.h,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/mkl/SpectralOps.cpp",6.0,6,1,1.785363269,8.0,3675.0,3.0,3269921.0,6897.0,15643.0,0.0,,0.0,1
pytorch,1fd7ea1ba8f9b70f1986b91021dae10044fc4188,301a28bf8ccc5936ab73062fc1592ca1f315d708,Khushi Agrawal,khushiagrawal411@gmail.com,Mon Mar 06 17:59:47 2023 +0000,1678125587.0,"[primTorch] move diagonal & add linalg.diagonal refs (#95774)

Fixes #85419

Also, add `_refs.linalg.diagonal`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95774
Approved by: https://github.com/lezcano",121.0,77.0,"test/functorch/test_vmap.py,torch/_refs/linalg/__init__.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/linalg.py",4.0,9,2,1.276614342,7.0,27901.0,4.0,1855275.0,13057.0,30413.5,0.0,Corrective,1.0,1
pytorch,4dc7d874218e84a65948e176310dfbd3b90876c9,3049d990270d0e8e6981a439ffd32b16a987ffac,Richard Zou,zou3519@gmail.com,Mon Dec 12 23:10:35 2022 -0800,1670886635.0,"autograd.Function supports vmap staticmethod (#90037)

This PR adds a `vmap` staticmethod to autograd.Function and a
corresponding vmap kernel for custom_function_call. These two items mean
that autograd.Function with a vmap staticmethod can be used with vmap.

```py
class NumpyMul(torch.autograd.Function)
    staticmethod
    def forward(x, y):
        return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)

    staticmethod
    def setup_context(ctx, outputs, x, y):
        ctx.save_for_backward(x, y)

    staticmethod
    def backward(ctx, grad_output):
        x, y = ctx.saved_tensors
        gx = None
        if isinstance(x, torch.Tensor) and x.requires_grad:
            gx = NumpyMul.apply(grad_output, y)
        gy = None
        if isinstance(y, torch.Tensor) and y.requires_grad:
            gy = NumpyMul.apply(grad_output, x)
        return gx, gy

    staticmethod
    def vmap(info, in_dims, x, y):
        x_bdim, y_bdim = in_dims
        x = x.movedim(x_bdim, -1) if x_bdim else x.unsqueeze(-1)
        y = y.movedim(y_bdim, -1) if y_bdim else y.unsqueeze(-1)
        result = NumpyMul.apply(x, y)
        result = result.movedim(-1, 0)
        return result, 0
```

API Spec
- the staticmethod takes two arguments (info, in_dims) as well as the
unexpanded inputs (x, y).
- If we think about it as `vmap(info, in_dims, *args)`, `in_dims` is a
pytree with the same tree structure as args. It has None if the arg is
not being vmapped over and an integer vmapped dimension index if it is.
- `info` is an object with metadata about the vmap. It currently has one
field, `info.batch_size`. In the future we can extend this by adding
things like the randomness information.
- If there is a single vmap going on, (x, y) are NOT BatchedTensors,
they've already been unpacked.
- We expect the user to return a `(outputs, out_dims)` tuple. `out_dims`
must ""broadcast"" to the same pytree structure as `outputs`.

Semantics
- vmap(NumpyMul.apply)(x) will apply the vmap staticmethod if there is
one and will never actually run NumpyMul.forward.
- In order for the autograd.Function to support nested vmap (e.g.,
`vmap(vmap(NumpyMul.apply))(x)`, then the vmap staticmethod must call
into operations that vmap understands (i.e. PyTorch operators or more
autograd.Function).

At a high level, this PR:
- adds a vmap rule for custom_function_call

Testing
- Added some tests for in_dims and info
- Added vmap staticmethod to most of the autograd.Function in
autograd_function_db and sent them through functorch's vmap-related
OpInfo tests

Future
- Better error messages if the user gets the return contract wrong. I
didn't include them in this PR because it might involve a refactor of
some of the existing code in functorch/_src/vmap.py that will add
~200LOC to the PR, but LMK if you'd prefer it here.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90037
Approved by: https://github.com/samdow, https://github.com/soulitzer",281.0,18.0,"aten/src/ATen/functorch/ADInterpreters.cpp,aten/src/ATen/functorch/PlumbingHelper.cpp,aten/src/ATen/functorch/PlumbingHelper.h,test/functorch/test_eager_transforms.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,torch/_C/_functorch.pyi,torch/_functorch/autograd_function.py,torch/_functorch/pyfunctorch.py,torch/csrc/functorch/init.cpp,torch/testing/_internal/autograd_function_db.py",11.0,13,3,2.432780663,1.0,11903.0,6.0,565354.0909090909,10483.0,23964.5,0.0,Feature Addition,0.0,1
pytorch,a9459bf7b544919b8e6255603dcf0f14908bc591,30521a37ad7cb3a13b73a2fb950e941939691c6d,Roy Li,royboy@fb.com,Fri Sep 21 01:51:31 2018 -0700,1537494691.0,"codemod: caffe::float16 -> at::Half (#11785)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11785

Replace each instead of float16 with Half.

Reviewed By: Yangqing

Differential Revision: D9892158

fbshipit-source-id: b9225ca7bd5c84fd1c04a9d24b026c8b6cbff120",589.0,591.0,"caffe2/contrib/aten/aten_op_template.h,caffe2/contrib/gloo/allgather_ops.cc,caffe2/contrib/gloo/allreduce_ops.cc,caffe2/contrib/gloo/allreduce_ops_gpu.cc,caffe2/contrib/gloo/broadcast_ops.cc,caffe2/contrib/gloo/broadcast_ops_gpu.cc,caffe2/contrib/gloo/reduce_scatter_ops.cc,caffe2/contrib/nccl/cuda_nccl_gpu.cc,caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,caffe2/core/blob_serialization.cc,caffe2/core/blob_test.cc,caffe2/core/common_cudnn.h,caffe2/core/hip/common_miopen.h,caffe2/core/typeid_test.cc,caffe2/core/types.cc,caffe2/core/types.h,caffe2/image/image_input_op.h,caffe2/image/transform_gpu.cu,caffe2/operators/activation_ops_cudnn.h,caffe2/operators/batch_matmul_op.cu,caffe2/operators/boolean_mask_ops.cu,caffe2/operators/cast_op.cc,caffe2/operators/cast_op.cu,caffe2/operators/conv_op_cudnn.cc,caffe2/operators/dropout_op_cudnn.cc,caffe2/operators/elementwise_ops.cu,caffe2/operators/elu_op_cudnn.cc,caffe2/operators/experimental/c10/cpu/cast_cpu.cc,caffe2/operators/experimental/c10/cpu/sparse_lengths_sum_cpu.cc,caffe2/operators/filler_op.cc,caffe2/operators/fully_connected_op_gpu.cc,caffe2/operators/half_float_ops.cu,caffe2/operators/hip/softmax_op_miopen.cc,caffe2/operators/lengths_reducer_ops.cc,caffe2/operators/lengths_reducer_ops.h,caffe2/operators/local_response_normalization_op_cudnn.cc,caffe2/operators/lstm_unit_op_gpu.cu,caffe2/operators/max_pool_with_index.cu,caffe2/operators/order_switch_ops_cudnn.cc,caffe2/operators/pool_op_cudnn.cu,caffe2/operators/reduction_ops.cu,caffe2/operators/relu_op.cu,caffe2/operators/rnn/recurrent_network_op_gpu.cu,caffe2/operators/scale_op_gpu.cc,caffe2/operators/softmax_op_cudnn.cc,caffe2/operators/spatial_batch_norm_op_cudnn.cu,caffe2/operators/utility_ops.cu,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_cudnn.cc,caffe2/perfkernels/embedding_lookup.cc,caffe2/perfkernels/embedding_lookup_avx2.cc,caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,caffe2/perfkernels/hp_emblookup_codegen.py,caffe2/perfkernels/typed_axpy.cc,caffe2/perfkernels/typed_axpy_avx.cc,caffe2/perfkernels/typed_axpy_avx2.cc,caffe2/proto/caffe2.proto,caffe2/proto/torch.proto,caffe2/python/pybind_state.cc,caffe2/python/pybind_state_dlpack.cc,caffe2/sgd/adadelta_op_gpu.cu,caffe2/sgd/adagrad_op_gpu.cu,caffe2/sgd/fp16_momentum_sgd_op.cu,caffe2/sgd/fp16_momentum_sgd_op.h,caffe2/utils/hip/math_hip.cc,caffe2/utils/math_gpu.cu",66.0,20,1,4.507932832,19.0,31950.0,34.0,6005057.848484849,4249.0,12002.33333,0.0,,0.0,1
pytorch,1b479416b7e77807259a642ecfc992acecd63b14,30675d0921a6f503cdf0d2130a7f7ab3d3932f39,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Jan 29 18:28:43 2021 -0800,1611944923.0,"Added OpInfo-based testing of triangular_solve (#50948)

Summary:
Added OpInfo-based testing of `torch.triangular_solve`.

These tests helped to discover that CPU `triangular_solve` wasn't working for empty matrices and for CUDA inputs a warning was printed to the terminal. It is fixed now.

CUDA gradgrad checks are skipped.
```
11.44s call     test/test_ops.py::TestGradientsCUDA::test_fn_gradgrad_triangular_solve_cuda_complex128
2.97s call     test/test_ops.py::TestGradientsCUDA::test_fn_gradgrad_triangular_solve_cuda_float64
1.60s call     test/test_ops.py::TestGradientsCPU::test_fn_gradgrad_triangular_solve_cpu_complex128
1.36s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_triangular_solve_cuda_complex128
1.20s call     test/test_ops.py::TestGradientsCUDA::test_fn_grad_triangular_solve_cuda_complex128
0.86s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_triangular_solve_cuda_complex64
0.85s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_triangular_solve_cuda_complex128
0.81s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_triangular_solve_cuda_float64
0.77s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_triangular_solve_cuda_float32
0.46s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_triangular_solve_cpu_complex128
0.44s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_triangular_solve_cpu_complex64
0.44s call     test/test_ops.py::TestGradientsCUDA::test_fn_grad_triangular_solve_cuda_float64
0.42s call     test/test_ops.py::TestGradientsCPU::test_fn_gradgrad_triangular_solve_cpu_float64
0.40s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_triangular_solve_cpu_float32
0.40s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_triangular_solve_cpu_float64
0.17s call     test/test_ops.py::TestGradientsCPU::test_fn_grad_triangular_solve_cpu_complex128
```

Ref. https://github.com/pytorch/pytorch/issues/50006

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50948

Reviewed By: ailzhang

Differential Revision: D26123998

Pulled By: mruberry

fbshipit-source-id: 54136e8fc8a71f107dddb692c5be298c6d5ed168",73.0,31.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,test/test_autograd.py,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,1.664697051,42.0,21891.0,2.0,40971.6,8478.0,19123.0,0.0,Corrective,1.0,1
pytorch,639dd0e3244ef27dddb3f82c0fef913fd1d42407,30849eb6680e83ef9ed914a963f14fc46c3181cb,gchanan,gregchanan@gmail.com,Tue Apr 17 13:54:49 2018 -0400,1523973289.0,"Bind 0-dim variables without requires grad to int64/double similar to how we do with Scalar. (#6637)

Note:
- Only integral scalar types bind to int64
- Both integral and floating point scalar types bind to double (same rules as python numbers).",36.0,3.0,"test/test_torch.py,torch/csrc/utils/python_arg_parser.cpp",2.0,4,2,0.961236605,39.0,7210.0,2.0,20490.0,2570.0,24926.35823,0.0,,0.0,1
pytorch,60382de455ea5f860d5787fac72ea100b7a405bb,30e48bbeae545c3292c2ab3fed0cb2dba4a92fed,Anjali Chourdia,chourdiaanjali@fb.com,Tue Jul 13 20:49:22 2021 -0700,1626209362.0,"Add neg bit (#56058)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56058

User facing changes:
1. Adds a negative bit and corresponding new API (`is_neg()`,`resolve_neg()`)
2. `tensor.conj().imag` now returns a floating point tensor with neg bit set to 1 instead of a tensor with no notion of negative bit. Note that imag is still a view and all the view properties still hold for imag.

Non user facing changes:
1. Added a new Negative dispatch key and a backend fallback to handle it
2. Updated copy kernel to handle negative bit
3. Merged conjugate and negative bit fallback kernel
4. fixed https://github.com/pytorch/pytorch/issues/60478 (caused due to https://github.com/pytorch/pytorch/pull/54987)

Testing:
1. Added a new OpInfo based test `test_neg_view` (verifies that out-of-place and in-place operations work correctly for all operations when the input is a neg view tensor by checking the result against an actually negated tensor, verifies that autograd returns the same output for both neg view and actually negated tensors as well as it works fine when grad_out is a neg view).
2. Added a new test class containing `test_conj_view`, `test_neg_view`.

Test Plan: Imported from OSS

Reviewed By: soulitzer

Differential Revision: D29636403

fbshipit-source-id: 12214c9dc4806c51850f4a72a109db9527c0ca63",626.0,306.0,"aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/ConjugateFallback.cpp,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/ComplexHelper.h,aten/src/ATen/native/MathBitsFallback.h,aten/src/ATen/native/NegateFallback.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/UnarySignKernels.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/templates/Functions.h,aten/src/ATen/templates/TensorBody.h,c10/core/DispatchKey.cpp,c10/core/DispatchKey.h,c10/core/TensorImpl.h,docs/source/tensors.rst,docs/source/torch.rst,test/backward_compatibility/check_backward_compatibility.py,test/test_ops.py,test/test_view_ops.py,tools/autograd/derivatives.yaml,tools/autograd/gen_inplace_or_view_type.py,tools/autograd/gen_variable_type.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/autograd/gradcheck.py,torch/csrc/autograd/FunctionsManual.cpp,torch/overrides.py,torch/testing/_asserts.py,torch/testing/_core.py,torch/testing/_internal/common_methods_invocations.py",39.0,23,6,3.813120168,44.0,65044.0,26.0,1458092.945945946,13811.0,31134.5,0.0,Corrective,1.0,1
pytorch,7a36c132cef1c3256a26048251ef022f60e22f3e,30ec06c140b0428d591e2f5007bc8046d1bdf7c4,Sam Gross,colesbury@gmail.com,Fri Feb 23 23:03:31 2018 -0500,1519427011.0,"Merge Variable and Tensor classes (#5225)

This replaces the torch.Tensor constructors with factories that produce
Variables. Similarly, functions on the torch module (e.g. torch.randn)
now return Variables.

To keep the PR to a reasonable size, I've left most of the unused tensor
code. Subsequent PRs will remove the dead code, clean-up calls to
torch.autograd.Variable, and rename Variable to Tensor everywhere.

There are some breaking changes because Variable and Tensors had
slightly different semantics. There's a list of those changes here:

 https://github.com/pytorch/pytorch/wiki/Breaking-Changes-from-Variable-and-Tensor-merge",1264.0,999.0,".jenkins/perf_test/test_cpu_speed_mini_sequence_labeler.sh,aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/TensorFactories.cpp,setup.py,test/common.py,test/common_nn.py,test/data/test_cuda_ignores.txt,test/test_autograd.py,test/test_cuda.py,test/test_distributions.py,test/test_indexing.py,test/test_jit.py,test/test_legacy_nn.py,test/test_multiprocessing.py,test/test_nn.py,test/test_optim.py,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp,tools/autograd/templates/python_torch_functions_dispatch.h,torch/__init__.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/_utils.py,torch/autograd/gradcheck.py,torch/autograd/variable.py,torch/csrc/Generator.cpp,torch/csrc/Module.cpp,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/ModuleSparse.cpp,torch/csrc/distributed/Module.cpp,torch/csrc/generic/Storage.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/tensor/python_tensor.h,torch/csrc/utils/pybind.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_apply.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_types.cpp,torch/cuda/__init__.py,torch/cuda/comm.py,torch/cuda/random.py,torch/cuda/sparse.py,torch/distributions/categorical.py,torch/functional.py,torch/legacy/nn/AbsCriterion.py,torch/legacy/nn/BCECriterion.py,torch/legacy/nn/CDivTable.py,torch/legacy/nn/ClassNLLCriterion.py,torch/legacy/nn/ClassSimplexCriterion.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/DistKLDivCriterion.py,torch/legacy/nn/HingeEmbeddingCriterion.py,torch/legacy/nn/L1Cost.py,torch/legacy/nn/L1HingeEmbeddingCriterion.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/MSECriterion.py,torch/legacy/nn/MarginCriterion.py,torch/legacy/nn/MarginRankingCriterion.py,torch/legacy/nn/Max.py,torch/legacy/nn/Min.py,torch/legacy/nn/MultiLabelMarginCriterion.py,torch/legacy/nn/MultiMarginCriterion.py,torch/legacy/nn/SmoothL1Criterion.py,torch/legacy/nn/SoftMarginCriterion.py,torch/legacy/nn/SpatialClassNLLCriterion.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/legacy/nn/WeightedMSECriterion.py,torch/legacy/nn/utils.py,torch/legacy/optim/lbfgs.py,torch/nn/functional.py,torch/nn/modules/module.py,torch/onnx/__init__.py,torch/onnx/symbolic.py,torch/optim/lbfgs.py,torch/optim/optimizer.py,torch/random.py,torch/sparse/__init__.py,torch/utils/data/sampler.py",85.0,33,5,4.757264098,40.0,55206.0,45.0,4469368.256097561,547.0,1642.405869,0.0,,0.0,1
pytorch,1ec30a6647be35d123a741a39cab8b4253c1cbe0,30fb2c4abaaaa966999eab11674f25b18460e609,Michael Suo,suo@fb.com,Sat Jun 11 17:22:58 2022 -0700,1654968178.0,"[lint] autoformat test/cpp and torch/csrc

Let's have some fun.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78828

Approved by: https://github.com/ezyang",42007.0,27159.0,".lintrunner.toml,aten/src/ATen/autocast_mode.h,test/cpp/api/any.cpp,test/cpp/api/autograd.cpp,test/cpp/api/dataloader.cpp,test/cpp/api/dispatch.cpp,test/cpp/api/enum.cpp,test/cpp/api/fft.cpp,test/cpp/api/functional.cpp,test/cpp/api/grad_mode.cpp,test/cpp/api/imethod.cpp,test/cpp/api/inference_mode.cpp,test/cpp/api/init.cpp,test/cpp/api/init_baseline.h,test/cpp/api/jit.cpp,test/cpp/api/meta_tensor.cpp,test/cpp/api/misc.cpp,test/cpp/api/module.cpp,test/cpp/api/moduledict.cpp,test/cpp/api/modulelist.cpp,test/cpp/api/modules.cpp,test/cpp/api/namespace.cpp,test/cpp/api/nn_utils.cpp,test/cpp/api/optim.cpp,test/cpp/api/optim_baseline.h,test/cpp/api/parallel.cpp,test/cpp/api/parameterdict.cpp,test/cpp/api/rnn.cpp,test/cpp/api/sequential.cpp,test/cpp/api/serialize.cpp,test/cpp/api/special.cpp,test/cpp/api/static.cpp,test/cpp/api/support.h,test/cpp/api/tensor.cpp,test/cpp/api/tensor_cuda.cpp,test/cpp/api/tensor_flatten.cpp,test/cpp/api/tensor_indexing.cpp,test/cpp/api/tensor_options.cpp,test/cpp/api/tensor_options_cuda.cpp,test/cpp/api/transformer.cpp,test/cpp/c10d/ProcessGroupGlooAsyncTest.cpp,test/cpp/c10d/ProcessGroupGlooTest.cpp,test/cpp/c10d/ProcessGroupMPITest.cpp,test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp,test/cpp/c10d/ProcessGroupNCCLTest.cpp,test/cpp/c10d/TCPStoreTest.cpp,test/cpp/dist_autograd/test_dist_autograd.cpp,test/cpp/lazy/test_backend_device.cpp,test/cpp/lazy/test_cache.cpp,test/cpp/lazy/test_ir.cpp,test/cpp/lazy/test_ir_util.cpp,test/cpp/lazy/test_lazy_ops.cpp,test/cpp/lazy/test_lazy_ops_util.cpp,test/cpp/lazy/test_lazy_ops_util.h,test/cpp/lazy/test_misc.cpp,test/cpp/lazy/test_shape.cpp,test/cpp/lazy/test_symbolic_shape.cpp,test/cpp/lazy/test_tensor_impl.cpp,test/cpp/lazy/test_trie_cache.cpp,test/cpp/lazy/test_util.cpp,test/cpp/lite_interpreter_runtime/main.cpp,test/cpp/lite_interpreter_runtime/test_mobile_profiler.cpp,test/cpp/profiler/containers.cpp,test/cpp/profiler/record_function.cpp,test/cpp/rpc/e2e_test_base.h,test/cpp/rpc/test_tensorpipe_serialization.cpp,torch/csrc/CudaIPCTypes.cpp,torch/csrc/CudaIPCTypes.h,torch/csrc/DataLoader.cpp,torch/csrc/Device.cpp,torch/csrc/Device.h,torch/csrc/Dtype.cpp,torch/csrc/Dtype.h,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Exceptions.cpp,torch/csrc/Exceptions.h,torch/csrc/Generator.cpp,torch/csrc/Generator.h,torch/csrc/Layout.cpp,torch/csrc/Layout.h,torch/csrc/MemoryFormat.cpp,torch/csrc/MemoryFormat.h,torch/csrc/Module.cpp,torch/csrc/PythonTypes.h,torch/csrc/QScheme.cpp,torch/csrc/QScheme.h,torch/csrc/Size.cpp,torch/csrc/Size.h,torch/csrc/Storage.cpp,torch/csrc/Storage.h,torch/csrc/StorageMethods.cpp,torch/csrc/StorageSharing.cpp,torch/csrc/Stream.cpp,torch/csrc/Stream.h,torch/csrc/THConcat.h,torch/csrc/THP.h,torch/csrc/TypeInfo.cpp,torch/csrc/Types.h,torch/csrc/api/include/torch/all.h,torch/csrc/api/include/torch/arg.h,torch/csrc/api/include/torch/autograd.h,torch/csrc/api/include/torch/cuda.h,torch/csrc/api/include/torch/data/dataloader.h,torch/csrc/api/include/torch/data/dataloader/stateful.h,torch/csrc/api/include/torch/data/dataloader/stateless.h,torch/csrc/api/include/torch/data/datasets/base.h,torch/csrc/api/include/torch/data/datasets/chunk.h,torch/csrc/api/include/torch/data/datasets/stateful.h,torch/csrc/api/include/torch/data/detail/sequencers.h,torch/csrc/api/include/torch/data/iterator.h,torch/csrc/api/include/torch/data/samplers/base.h,torch/csrc/api/include/torch/data/samplers/custom_batch_request.h,torch/csrc/api/include/torch/data/samplers/random.h,torch/csrc/api/include/torch/detail/TensorDataContainer.h,torch/csrc/api/include/torch/enum.h,torch/csrc/api/include/torch/expanding_array.h,torch/csrc/api/include/torch/fft.h,torch/csrc/api/include/torch/imethod.h,torch/csrc/api/include/torch/jit.h,torch/csrc/api/include/torch/linalg.h,torch/csrc/api/include/torch/nn/cloneable.h,torch/csrc/api/include/torch/nn/functional.h,torch/csrc/api/include/torch/nn/functional/activation.h,torch/csrc/api/include/torch/nn/functional/batchnorm.h,torch/csrc/api/include/torch/nn/functional/conv.h,torch/csrc/api/include/torch/nn/functional/distance.h,torch/csrc/api/include/torch/nn/functional/dropout.h,torch/csrc/api/include/torch/nn/functional/embedding.h,torch/csrc/api/include/torch/nn/functional/fold.h,torch/csrc/api/include/torch/nn/functional/instancenorm.h,torch/csrc/api/include/torch/nn/functional/linear.h,torch/csrc/api/include/torch/nn/functional/loss.h,torch/csrc/api/include/torch/nn/functional/normalization.h,torch/csrc/api/include/torch/nn/functional/padding.h,torch/csrc/api/include/torch/nn/functional/pixelshuffle.h,torch/csrc/api/include/torch/nn/functional/pooling.h,torch/csrc/api/include/torch/nn/functional/upsampling.h,torch/csrc/api/include/torch/nn/functional/vision.h,torch/csrc/api/include/torch/nn/init.h,torch/csrc/api/include/torch/nn/module.h,torch/csrc/api/include/torch/nn/modules.h,torch/csrc/api/include/torch/nn/modules/_functions.h,torch/csrc/api/include/torch/nn/modules/activation.h,torch/csrc/api/include/torch/nn/modules/adaptive.h,torch/csrc/api/include/torch/nn/modules/batchnorm.h,torch/csrc/api/include/torch/nn/modules/common.h,torch/csrc/api/include/torch/nn/modules/container/any.h,torch/csrc/api/include/torch/nn/modules/container/any_module_holder.h,torch/csrc/api/include/torch/nn/modules/container/any_value.h,torch/csrc/api/include/torch/nn/modules/container/functional.h,torch/csrc/api/include/torch/nn/modules/container/moduledict.h,torch/csrc/api/include/torch/nn/modules/container/named_any.h,torch/csrc/api/include/torch/nn/modules/container/sequential.h,torch/csrc/api/include/torch/nn/modules/conv.h,torch/csrc/api/include/torch/nn/modules/distance.h,torch/csrc/api/include/torch/nn/modules/dropout.h,torch/csrc/api/include/torch/nn/modules/embedding.h,torch/csrc/api/include/torch/nn/modules/fold.h,torch/csrc/api/include/torch/nn/modules/instancenorm.h,torch/csrc/api/include/torch/nn/modules/linear.h,torch/csrc/api/include/torch/nn/modules/loss.h,torch/csrc/api/include/torch/nn/modules/normalization.h,torch/csrc/api/include/torch/nn/modules/padding.h,torch/csrc/api/include/torch/nn/modules/pixelshuffle.h,torch/csrc/api/include/torch/nn/modules/pooling.h,torch/csrc/api/include/torch/nn/modules/rnn.h,torch/csrc/api/include/torch/nn/modules/transformer.h,torch/csrc/api/include/torch/nn/modules/transformercoder.h,torch/csrc/api/include/torch/nn/modules/transformerlayer.h,torch/csrc/api/include/torch/nn/modules/upsampling.h,torch/csrc/api/include/torch/nn/modules/utils.h,torch/csrc/api/include/torch/nn/options.h,torch/csrc/api/include/torch/nn/options/activation.h,torch/csrc/api/include/torch/nn/options/adaptive.h,torch/csrc/api/include/torch/nn/options/batchnorm.h,torch/csrc/api/include/torch/nn/options/conv.h,torch/csrc/api/include/torch/nn/options/distance.h,torch/csrc/api/include/torch/nn/options/dropout.h,torch/csrc/api/include/torch/nn/options/embedding.h,torch/csrc/api/include/torch/nn/options/fold.h,torch/csrc/api/include/torch/nn/options/instancenorm.h,torch/csrc/api/include/torch/nn/options/linear.h,torch/csrc/api/include/torch/nn/options/loss.h,torch/csrc/api/include/torch/nn/options/normalization.h,torch/csrc/api/include/torch/nn/options/padding.h,torch/csrc/api/include/torch/nn/options/pixelshuffle.h,torch/csrc/api/include/torch/nn/options/pooling.h,torch/csrc/api/include/torch/nn/options/rnn.h,torch/csrc/api/include/torch/nn/options/transformer.h,torch/csrc/api/include/torch/nn/options/transformercoder.h,torch/csrc/api/include/torch/nn/options/transformerlayer.h,torch/csrc/api/include/torch/nn/options/upsampling.h,torch/csrc/api/include/torch/nn/options/vision.h,torch/csrc/api/include/torch/nn/parallel/data_parallel.h,torch/csrc/api/include/torch/nn/utils/clip_grad.h,torch/csrc/api/include/torch/nn/utils/convert_parameters.h,torch/csrc/api/include/torch/nn/utils/rnn.h,torch/csrc/api/include/torch/optim/adagrad.h,torch/csrc/api/include/torch/optim/adam.h,torch/csrc/api/include/torch/optim/adamw.h,torch/csrc/api/include/torch/optim/lbfgs.h,torch/csrc/api/include/torch/optim/optimizer.h,torch/csrc/api/include/torch/optim/rmsprop.h,torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h,torch/csrc/api/include/torch/optim/schedulers/step_lr.h,torch/csrc/api/include/torch/optim/serialize.h,torch/csrc/api/include/torch/optim/sgd.h,torch/csrc/api/include/torch/ordered_dict.h,torch/csrc/api/include/torch/serialize.h,torch/csrc/api/include/torch/serialize/input-archive.h,torch/csrc/api/include/torch/serialize/output-archive.h,torch/csrc/api/include/torch/sparse.h,torch/csrc/api/include/torch/special.h,torch/csrc/api/include/torch/types.h,torch/csrc/api/include/torch/utils.h,torch/csrc/api/src/cuda.cpp,torch/csrc/api/src/imethod.cpp,torch/csrc/api/src/jit.cpp,torch/csrc/api/src/nn/init.cpp,torch/csrc/api/src/nn/module.cpp,torch/csrc/api/src/nn/modules/_functions.cpp,torch/csrc/api/src/nn/modules/activation.cpp,torch/csrc/api/src/nn/modules/adaptive.cpp,torch/csrc/api/src/nn/modules/batchnorm.cpp,torch/csrc/api/src/nn/modules/conv.cpp,torch/csrc/api/src/nn/modules/distance.cpp,torch/csrc/api/src/nn/modules/dropout.cpp,torch/csrc/api/src/nn/modules/embedding.cpp,torch/csrc/api/src/nn/modules/fold.cpp,torch/csrc/api/src/nn/modules/instancenorm.cpp,torch/csrc/api/src/nn/modules/linear.cpp,torch/csrc/api/src/nn/modules/loss.cpp,torch/csrc/api/src/nn/modules/normalization.cpp,torch/csrc/api/src/nn/modules/padding.cpp,torch/csrc/api/src/nn/modules/pixelshuffle.cpp,torch/csrc/api/src/nn/modules/pooling.cpp,torch/csrc/api/src/nn/modules/rnn.cpp,torch/csrc/api/src/nn/modules/transformer.cpp,torch/csrc/api/src/nn/modules/upsampling.cpp,torch/csrc/api/src/nn/options/activation.cpp,torch/csrc/api/src/nn/options/adaptive.cpp,torch/csrc/api/src/nn/options/batchnorm.cpp,torch/csrc/api/src/nn/options/embedding.cpp,torch/csrc/api/src/nn/options/instancenorm.cpp,torch/csrc/api/src/nn/options/linear.cpp,torch/csrc/api/src/nn/options/normalization.cpp,torch/csrc/api/src/nn/options/padding.cpp,torch/csrc/api/src/nn/options/rnn.cpp,torch/csrc/api/src/nn/options/transformer.cpp,torch/csrc/api/src/optim/adagrad.cpp,torch/csrc/api/src/optim/adam.cpp,torch/csrc/api/src/optim/adamw.cpp,torch/csrc/api/src/optim/lbfgs.cpp,torch/csrc/api/src/optim/optimizer.cpp,torch/csrc/api/src/optim/rmsprop.cpp,torch/csrc/api/src/optim/schedulers/lr_scheduler.cpp,torch/csrc/api/src/optim/schedulers/step_lr.cpp,torch/csrc/api/src/optim/sgd.cpp,torch/csrc/api/src/python/init.cpp,torch/csrc/api/src/serialize.cpp,torch/csrc/api/src/serialize/input-archive.cpp,torch/csrc/api/src/serialize/output-archive.cpp,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/csrc/autograd/InferenceMode.h,torch/csrc/autograd/TraceTypeManual.cpp,torch/csrc/autograd/VariableTypeManual.cpp,torch/csrc/autograd/VariableTypeUtils.h,torch/csrc/autograd/anomaly_mode.h,torch/csrc/autograd/autograd.cpp,torch/csrc/autograd/autograd.h,torch/csrc/autograd/autograd_meta.cpp,torch/csrc/autograd/autograd_not_implemented_fallback.cpp,torch/csrc/autograd/autograd_not_implemented_fallback.h,torch/csrc/autograd/cpp_hook.cpp,torch/csrc/autograd/cpp_hook.h,torch/csrc/autograd/custom_function.cpp,torch/csrc/autograd/custom_function.h,torch/csrc/autograd/edge.h,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/engine.h,torch/csrc/autograd/forward_grad.cpp,torch/csrc/autograd/forward_grad.h,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/function_hook.cpp,torch/csrc/autograd/function_hook.h,torch/csrc/autograd/functions/accumulate_grad.cpp,torch/csrc/autograd/functions/accumulate_grad.h,torch/csrc/autograd/functions/basic_ops.cpp,torch/csrc/autograd/functions/basic_ops.h,torch/csrc/autograd/functions/comm.cpp,torch/csrc/autograd/functions/comm.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/pybind.h,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/functions/tensor.h,torch/csrc/autograd/functions/utils.cpp,torch/csrc/autograd/functions/utils.h,torch/csrc/autograd/grad_mode.h,torch/csrc/autograd/init.cpp,torch/csrc/autograd/input_buffer.cpp,torch/csrc/autograd/input_buffer.h,torch/csrc/autograd/input_metadata.h,torch/csrc/autograd/profiler.h,torch/csrc/autograd/profiler_kineto.cpp,torch/csrc/autograd/profiler_kineto.h,torch/csrc/autograd/profiler_legacy.cpp,torch/csrc/autograd/profiler_legacy.h,torch/csrc/autograd/profiler_python.cpp,torch/csrc/autograd/profiler_python.h,torch/csrc/autograd/python_anomaly_mode.cpp,torch/csrc/autograd/python_anomaly_mode.h,torch/csrc/autograd/python_autograd.h,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_cpp_function.h,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_engine.h,torch/csrc/autograd/python_enum_tag.h,torch/csrc/autograd/python_fft_functions.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_hook.cpp,torch/csrc/autograd/python_hook.h,torch/csrc/autograd/python_legacy_variable.cpp,torch/csrc/autograd/python_legacy_variable.h,torch/csrc/autograd/python_linalg_functions.h,torch/csrc/autograd/python_nn_functions.h,torch/csrc/autograd/python_return_types.h,torch/csrc/autograd/python_saved_variable_hooks.cpp,torch/csrc/autograd/python_saved_variable_hooks.h,torch/csrc/autograd/python_sparse_functions.h,torch/csrc/autograd/python_special_functions.h,torch/csrc/autograd/python_torch_functions.h,torch/csrc/autograd/python_torch_functions_manual.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/autograd/python_variable_indexing.h,torch/csrc/autograd/record_function_ops.cpp,torch/csrc/autograd/record_function_ops.h,torch/csrc/autograd/saved_variable.cpp,torch/csrc/autograd/saved_variable.h,torch/csrc/autograd/saved_variable_hooks.h,torch/csrc/autograd/symbolic.h,torch/csrc/autograd/utils/error_messages.h,torch/csrc/autograd/utils/grad_layout_contract.h,torch/csrc/autograd/utils/python_arg_parsing.h,torch/csrc/autograd/utils/warnings.cpp,torch/csrc/autograd/utils/warnings.h,torch/csrc/autograd/utils/wrap_outputs.h,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/copy_utils.h,torch/csrc/cuda/Event.cpp,torch/csrc/cuda/Event.h,torch/csrc/cuda/Graph.cpp,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Module.h,torch/csrc/cuda/Stream.cpp,torch/csrc/cuda/Stream.h,torch/csrc/cuda/THCP.h,torch/csrc/cuda/Tensor.cpp,torch/csrc/cuda/comm.cpp,torch/csrc/cuda/comm.h,torch/csrc/cuda/device_set.h,torch/csrc/cuda/nccl.cpp,torch/csrc/cuda/nccl.h,torch/csrc/cuda/override_macros.h,torch/csrc/cuda/python_comm.cpp,torch/csrc/cuda/python_comm.h,torch/csrc/cuda/python_nccl.cpp,torch/csrc/cuda/python_nccl.h,torch/csrc/cuda/restore_macros.h,torch/csrc/cuda/shared/cudart.cpp,torch/csrc/cuda/shared/cudnn.cpp,torch/csrc/cuda/shared/nvtx.cpp,torch/csrc/cuda/utils.cpp,torch/csrc/cuda/utils.h,torch/csrc/distributed/autograd/autograd.cpp,torch/csrc/distributed/autograd/context/container.cpp,torch/csrc/distributed/autograd/context/context.h,torch/csrc/distributed/autograd/engine/dist_engine.cpp,torch/csrc/distributed/autograd/engine/dist_engine.h,torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp,torch/csrc/distributed/autograd/init.cpp,torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_resp.cpp,torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp,torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp,torch/csrc/distributed/autograd/utils.cpp,torch/csrc/distributed/autograd/utils.h,torch/csrc/distributed/c10d/FileStore.cpp,torch/csrc/distributed/c10d/GlooDeviceFactory.cpp,torch/csrc/distributed/c10d/NCCLUtils.cpp,torch/csrc/distributed/c10d/ParamCommsUtils.cpp,torch/csrc/distributed/c10d/ProcessGroup.cpp,torch/csrc/distributed/c10d/ProcessGroupGloo.cpp,torch/csrc/distributed/c10d/ProcessGroupMPI.cpp,torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,torch/csrc/distributed/c10d/ProcessGroupRoundRobin.cpp,torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp,torch/csrc/distributed/c10d/Store.cpp,torch/csrc/distributed/c10d/TCPStore.cpp,torch/csrc/distributed/c10d/comm.cpp,torch/csrc/distributed/c10d/debug.cpp,torch/csrc/distributed/c10d/debug.h,torch/csrc/distributed/c10d/default_comm_hooks.cpp,torch/csrc/distributed/c10d/error.h,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/logger.cpp,torch/csrc/distributed/c10d/logging.h,torch/csrc/distributed/c10d/python_comm_hook.h,torch/csrc/distributed/c10d/quantization/quantization.cpp,torch/csrc/distributed/c10d/quantization/quantization.h,torch/csrc/distributed/c10d/quantization/quantization_gpu.h,torch/csrc/distributed/c10d/quantization/quantization_utils.h,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer_cuda.cpp,torch/csrc/distributed/c10d/sequence_num.cpp,torch/csrc/distributed/c10d/socket.cpp,torch/csrc/distributed/c10d/socket.h,torch/csrc/distributed/rpc/rpc_agent.cpp,torch/csrc/generic/utils.h,torch/csrc/lazy/backend/backend_data.h,torch/csrc/lazy/backend/backend_device.cpp,torch/csrc/lazy/backend/backend_device.h,torch/csrc/lazy/backend/backend_interface.cpp,torch/csrc/lazy/backend/backend_interface.h,torch/csrc/lazy/backend/lowering_context.cpp,torch/csrc/lazy/backend/lowering_context.h,torch/csrc/lazy/core/config.cpp,torch/csrc/lazy/core/config.h,torch/csrc/lazy/core/debug_util.cpp,torch/csrc/lazy/core/debug_util.h,torch/csrc/lazy/core/hash.cpp,torch/csrc/lazy/core/hash.h,torch/csrc/lazy/core/ir.cpp,torch/csrc/lazy/core/ir.h,torch/csrc/lazy/core/ir_builder.h,torch/csrc/lazy/core/ir_metadata.cpp,torch/csrc/lazy/core/lazy_graph_executor.cpp,torch/csrc/lazy/core/lazy_view.cpp,torch/csrc/lazy/core/metrics.h,torch/csrc/lazy/core/ops/arithmetic_ir_ops.cpp,torch/csrc/lazy/core/ops/utils.cpp,torch/csrc/lazy/core/ops/utils.h,torch/csrc/lazy/core/permutation_util.h,torch/csrc/lazy/core/shape.cpp,torch/csrc/lazy/core/shape_inference.cpp,torch/csrc/lazy/core/shape_inference.h,torch/csrc/lazy/core/tensor.cpp,torch/csrc/lazy/core/tensor.h,torch/csrc/lazy/core/tensor_impl.cpp,torch/csrc/lazy/core/tensor_impl.h,torch/csrc/lazy/core/tensor_util.cpp,torch/csrc/lazy/core/thread_pool.cpp,torch/csrc/lazy/core/trie.cpp,torch/csrc/lazy/core/trie.h,torch/csrc/lazy/core/util.h,torch/csrc/lazy/python/init.cpp,torch/csrc/lazy/python/init.h,torch/csrc/lazy/python/python_util.cpp,torch/csrc/lazy/python/python_util.h,torch/csrc/lazy/ts_backend/dynamic_ir.cpp,torch/csrc/lazy/ts_backend/dynamic_ir.h,torch/csrc/lazy/ts_backend/ir_builder.h,torch/csrc/lazy/ts_backend/ops/batch_norm_ops.cpp,torch/csrc/lazy/ts_backend/ops/batch_norm_ops.h,torch/csrc/lazy/ts_backend/ops/device_data.h,torch/csrc/lazy/ts_backend/ops/random_ops.cpp,torch/csrc/lazy/ts_backend/ops/random_ops.h,torch/csrc/lazy/ts_backend/ops/to_copy.h,torch/csrc/lazy/ts_backend/tensor_aten_ops.cpp,torch/csrc/lazy/ts_backend/tensor_aten_ops.h,torch/csrc/lazy/ts_backend/ts_autograd_functions.cpp,torch/csrc/lazy/ts_backend/ts_autograd_functions.h,torch/csrc/lazy/ts_backend/ts_backend_impl.cpp,torch/csrc/lazy/ts_backend/ts_eager_fallback.cpp,torch/csrc/lazy/ts_backend/ts_eager_fallback.h,torch/csrc/lazy/ts_backend/ts_lowering_context.cpp,torch/csrc/lazy/ts_backend/ts_lowering_context.h,torch/csrc/lazy/ts_backend/ts_native_functions.cpp,torch/csrc/lazy/ts_backend/ts_node.cpp,torch/csrc/lazy/ts_backend/ts_node.h,torch/csrc/lazy/ts_backend/ts_node_lowering.cpp,torch/csrc/lazy/ts_backend/ts_node_lowering.h,torch/csrc/monitor/events.h,torch/csrc/multiprocessing/init.cpp,torch/csrc/onnx/init.h,torch/csrc/profiler/api.cpp,torch/csrc/profiler/api.h,torch/csrc/profiler/collection.cpp,torch/csrc/profiler/collection.h,torch/csrc/profiler/containers.h,torch/csrc/profiler/cuda.cpp,torch/csrc/profiler/kineto_client_interface.cpp,torch/csrc/profiler/kineto_shim.cpp,torch/csrc/profiler/kineto_shim.h,torch/csrc/profiler/nvtx_observer.cpp,torch/csrc/profiler/util.cpp,torch/csrc/profiler/util.h,torch/csrc/python_dimname.cpp,torch/csrc/python_dimname.h,torch/csrc/serialization.cpp,torch/csrc/serialization.h,torch/csrc/tensor/python_tensor.cpp,torch/csrc/tensor/python_tensor.h,torch/csrc/utils.cpp,torch/csrc/utils.h,torch/csrc/utils/auto_gil.h,torch/csrc/utils/byte_order.cpp,torch/csrc/utils/byte_order.h,torch/csrc/utils/cpp_stacktraces.cpp,torch/csrc/utils/cuda_enabled.h,torch/csrc/utils/cuda_lazy_init.cpp,torch/csrc/utils/cuda_lazy_init.h,torch/csrc/utils/disable_torch_function.cpp,torch/csrc/utils/disable_torch_function.h,torch/csrc/utils/init.cpp,torch/csrc/utils/invalid_arguments.cpp,torch/csrc/utils/object_ptr.cpp,torch/csrc/utils/object_ptr.h,torch/csrc/utils/out_types.cpp,torch/csrc/utils/out_types.h,torch/csrc/utils/pybind.h,torch/csrc/utils/pycfunction_helpers.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/python_compat.h,torch/csrc/utils/python_dispatch.cpp,torch/csrc/utils/python_dispatch.h,torch/csrc/utils/python_numbers.h,torch/csrc/utils/python_scalars.h,torch/csrc/utils/python_strings.h,torch/csrc/utils/python_torch_function_mode.h,torch/csrc/utils/python_tuples.h,torch/csrc/utils/six.h,torch/csrc/utils/structseq.cpp,torch/csrc/utils/structseq.h,torch/csrc/utils/tensor_apply.cpp,torch/csrc/utils/tensor_apply.h,torch/csrc/utils/tensor_dtypes.cpp,torch/csrc/utils/tensor_dtypes.h,torch/csrc/utils/tensor_flatten.cpp,torch/csrc/utils/tensor_flatten.h,torch/csrc/utils/tensor_layouts.cpp,torch/csrc/utils/tensor_layouts.h,torch/csrc/utils/tensor_list.cpp,torch/csrc/utils/tensor_list.h,torch/csrc/utils/tensor_memoryformats.cpp,torch/csrc/utils/tensor_memoryformats.h,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h,torch/csrc/utils/tensor_numpy.cpp,torch/csrc/utils/tensor_numpy.h,torch/csrc/utils/tensor_qschemes.cpp,torch/csrc/utils/tensor_qschemes.h,torch/csrc/utils/tensor_types.cpp,torch/csrc/utils/tensor_types.h,torch/csrc/utils/throughput_benchmark.cpp,torch/csrc/utils/throughput_benchmark.h,torch/csrc/utils/torch_dispatch_mode.h,torch/csrc/utils/variadic.h",564.0,69,3,7.06760255,48.0,148972.0,264.0,22481888.66134752,4243.0,10246.5,0.0,,0.0,1
pytorch,21ba320cd55bc6eecc280e18c1d8d1212a9737f0,31139b5f9a0218724086cf85f5ba6554d867cddc,Mike Ruberry,mruberry@fb.com,Sun Sep 15 20:40:43 2019 -0700,1568580043.0,"Back out ""[pytorch][PR] Refines test_torch.py generic device testing"" (#26252)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26252

Original commit changeset: 1375774f24c2

Testing to see if this is somehow the source of hangs on ROCm builds.

Test Plan: Change is to tests themselves. This diff is for testing the ROCm hang, however.

Differential Revision: D17390575

fbshipit-source-id: a6ffd5eb1df3971b99b6d42271a8d3d501ac79c6",819.0,712.0,"test/common_device_type.py,test/common_utils.py,test/test_cuda.py,test/test_torch.py",4.0,1,1,0.402838787,41.0,17334.0,1.0,36422.0,11419.0,32080.33333,0.0,,0.0,1
pytorch,6c70cbedb6102da08fe91186d40a41b50991681d,3113a1de4ac75bb397911ab1ae0bd3e98de89e03,Akifumi Imanishi,imanishi@preferred.jp,Wed May 19 20:08:37 2021 -0700,1621454917.0,"Fix some tensor operators to return `NotImplemented` for invalid inputs (#58216)

Summary:
Same as https://github.com/pytorch/pytorch/issues/57934. (cc/ albanD)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58216

Reviewed By: ailzhang

Differential Revision: D28494886

Pulled By: albanD

fbshipit-source-id: 380205867ee1cde90e1c6fcfe2a31749e1243530",46.0,16.0,"test/onnx/expect/TestOperators.test_rsub.expect,test/test_binary_ufuncs.py,test/test_fx.py,test/test_fx_experimental.py,torch/_tensor.py,torch/testing/_internal/common_methods_invocations.py",6.0,6,2,2.150538149,2.0,15156.0,5.0,337565.8333333333,12242.0,27672.0,0.0,Corrective,1.0,1
pytorch,6925576e8841b369e6211c918b71ed6c37019f39,3116d87024e3b06a340f234473db9013dc64959f,soulitzer,soulitzer@gmail.com,Thu Dec 23 23:49:35 2021 -0800,1640303375.0,"Add forward AD formulas for `{adaptive_,fractional_,}max_pool{2,3}d_{backward,}` (#69884)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69884

Also fixes: https://github.com/pytorch/pytorch/issues/69322, https://github.com/pytorch/pytorch/issues/69325

Test Plan: Imported from OSS

Reviewed By: bdhirsh

Differential Revision: D33093039

Pulled By: soulitzer

fbshipit-source-id: b9a522a00f4e9e85974888de5058de07280f8f66",66.0,20.0,"test/test_nn.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",3.0,6,3,1.185986674,44.0,38183.0,3.0,197770.3333333333,17984.0,42540.5,0.0,Corrective,1.0,1
pytorch,3a07228509b51b82322659129e9cd1858c3ec74f,3152be5fb365187e92ea615fa8ae34f4b2e165ac,Adam Lerer,adam.lerer@gmail.com,Fri Jan 13 20:53:52 2017 -0500,1484340832.0,Add repr to RNNs and Embedding (#428),53.0,11.0,"torch/nn/modules/rnn.py,torch/nn/modules/sparse.py",2.0,3,1,0.912999214,20.0,564.0,2.0,766406.5,322.0,6410.224559,0.0,Feature Addition,0.0,1
pytorch,d962dba0c44341b0e22d090be883a25fdd428dd9,316c0d3e6b21fcc8da4b2bd7e557d3424162eb33,Jiong Gong,jiong.gong@intel.com,Tue Jul 23 14:58:04 2024 -0400,1721746684.0,"[inductor][cpp][gemm] support k slicing for static shapes (#130821)

This PR provides the initial support for k-slicing (i.e. parallel reduction along k-dim) of CPP GEMM template. Only static shapes are supported now. When k-slicing is enabled, there would be extra temporary buffers allocated to hold the intermediate results and an extra barrier after initial GEMM compute by each thread, i.e. each thread first stores the GEMM result to temporary accumulation buffers (pointed by `local_buf_ptrs` which is an array of pointers pointing to accumulation buffers), followed by a reduction along k-slices, epilogue computes and store to the final output `Y`. In each k-slicing thread group, the reduction along k-slices and epilogue computes are conducted in parallel along M-dim. The algorithm is designed to reduce the synchronization overhead as much as possible.

The k-slicing is enabled when blocking on M and N is unable to occupy all threads. Since k-slicing doesn't always bring benefit, an extra configuration is added to enable it (disable by default). We need to identify a good heuristics in the future to enable k-slicing by default.

Performance numbers with 64x4096x64, 64x10000x64, 64x20000x64 as examples on 60-core SPR as examples. As you can see, the perf of k-slicing is only better than non-k-slicing when K is large enough.

Without k-slicing
AUTOTUNE linear_unary(64x4096, 64x4096, 64)
  cpp_packed_gemm_0 0.0108 ms 100.0%
  _linear_pointwise 0.0431 ms 25.1%

AUTOTUNE linear_unary(64x10000, 64x10000, 64)
  cpp_packed_gemm_0 0.0272 ms 100.0%
  _linear_pointwise 0.0892 ms 30.5%

AUTOTUNE linear_unary(64x20000, 64x20000, 64)
  cpp_packed_gemm_0 0.0781 ms 100.0%
  _linear_pointwise 0.1693 ms 46.1%

With k-slicing:
AUTOTUNE linear_unary(64x4096, 64x4096, 64)
  cpp_packed_gemm_0 0.0260 ms 100.0%
  _linear_pointwise 0.0444 ms 58.5%

AUTOTUNE linear_unary(64x10000, 64x10000, 64)
  cpp_packed_gemm_0 0.0275 ms 100.0%
  _linear_pointwise 0.0893 ms 30.8%

AUTOTUNE linear_unary(64x20000, 64x20000, 64)
  cpp_packed_gemm_0 0.0284 ms 100.0%
  _linear_pointwise 0.1686 ms 16.8%

Pull Request resolved: https://github.com/pytorch/pytorch/pull/130821
Approved by: https://github.com/leslie-fang-intel, https://github.com/jansel
ghstack dependencies: #131024",207.0,41.0,"test/inductor/test_cpu_select_algorithm.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_template_kernel.py,torch/_inductor/config.py",4.0,5,2,1.098275197,1.0,2818.0,4.0,260163.0,31692.0,79801.5,0.0,Feature Addition,0.0,1
pytorch,5f13cc861c3101fda67452e65dd8d0daaf2a36e9,316f0b89c3aa51329f40a13c00de587da60faa66,kshitij12345,kshitijkalambarkar@gmail.com,Tue Jan 19 14:00:08 2021 -0800,1611064808.0,"[testing] Port `torch.{repeat, tile}` tests to use OpInfo machinery (#50199)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50013

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50199

Reviewed By: ngimel

Differential Revision: D25949791

Pulled By: mruberry

fbshipit-source-id: 10eaf2d749fac8c08847f50461e72ad1c75c61e3",85.0,76.0,"test/test_shape_ops.py,test/test_tensor_creation_ops.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,1.411484724,43.0,13681.0,4.0,245875.75,8167.0,18471.0,0.0,,0.0,1
pytorch,94262efc7d381ace82aa74ed2f5f5ec76f8fca95,31981d01399d5cd45ec6f2ae93269d226dffe075,Richard Zou,zou3519@gmail.com,Tue Dec 20 20:36:12 2022 -0800,1671568572.0,"[generate_vmap_rule] add restore_vmap helper function (#90963)

As seen in
https://docs.google.com/document/d/1bIQkWXy3J35_20c_a5kchikabBW5M8_uRAhl0BIMwU4/edit

`restore_vmap` is a private helper function. It is vmap but has the
following
differences:
- instead of returning outputs, it returns an (outputs, out_dims) tuple.
  out_dims is a pytree of shape shape as outputs and contains Optional[int]
  specifying where the vmapped dimension, if it exists, is in the
  corresponding output.
- does no validation on in_dims or inputs (vmap expects at least one
  Tensor to be vmapped).
  restore_vmap allows for no inputs to have the vmap dimension
- does no validation on outputs (vmap expects only Tensor outputs)
  restore_vmap allows for return of arbitrary outputs (not just
  Tensors)

Test Plan:
- added some simple test to test restore_vmap
- I am OK with restore_vmap not being a part of vmap right now -- the
implementation of vmap rarely changes and it is a bit difficult to
refactor vmap in a way that restore_vmap is a subroutine.

Other questions:
- Bikeshedding the `restore_vmap` name
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90963
Approved by: https://github.com/samdow, https://github.com/soulitzer",97.0,23.0,"test/functorch/test_vmap.py,torch/_functorch/autograd_function.py,torch/_functorch/vmap.py",3.0,4,2,1.538949039,1.0,5436.0,3.0,358508.6666666667,10778.0,24575.5,0.0,Feature Addition,0.0,1
pytorch,0308910c58cb2c1765aad1dc64b613ba7b527f5d,31b72b90048a091e32d6d34d0dd3b0779b99d961,Zhengping Zuo,zzuo@fb.com,Sat Mar 11 00:17:40 2017 -0800,1489191460.0,"move reshape out of utility_ops

Summary: move reshape as individual op

Reviewed By: ajtulloch

Differential Revision: D4690919

fbshipit-source-id: a84859d738039125a4f4122365619b69d5990427",239.0,156.0,"caffe2/operators/reshape_op.cc,caffe2/operators/reshape_op.h,caffe2/operators/reshape_op_gpu.cc,caffe2/operators/reshape_op_gpu_test.cc,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_gpu.cc",7.0,2,1,2.323717415,8.0,2642.0,2.0,189346.3333333333,552.0,1862.833333,0.0,,0.0,1
pytorch,b5f71375f514aef956851f097928bdf004e9b3a5,31c7e5d62936a41db2758d2786b6e77dff35327c,wushirong,sw782@cornell.edu,Wed Dec 22 16:49:14 2021 -0800,1640191754.0,"Install TensorRT lib on oss docker and enable fx2trt unit test (#70203)

Summary:
CI

Lib installed and unit test run on https://github.com/pytorch/pytorch/actions/runs/1604076060

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70203

Reviewed By: malfet

Differential Revision: D33264641

Pulled By: wushirong

fbshipit-source-id: ba30010bbd06e70d31415d8c52086d1779371bcf",74.0,4.0,".circleci/docker/build.sh,.circleci/docker/common/install_tensorrt.sh,.circleci/docker/ubuntu-cuda/Dockerfile,.github/scripts/generate_ci_workflows.py,.github/scripts/generate_pytorch_test_matrix.py,.github/templates/linux_ci_workflow.yml.j2,.github/workflows/generated-linux-bionic-cuda10.2-py3.9-gcc7.yml,.github/workflows/generated-linux-bionic-py3.6-clang9.yml,.github/workflows/generated-linux-vulkan-bionic-py3.6-clang9.yml,.github/workflows/generated-linux-xenial-cuda11.3-py3.6-gcc7.yml,.github/workflows/generated-linux-xenial-py3.6-clang7-asan.yml,.github/workflows/generated-linux-xenial-py3.6-clang7-onnx.yml,.github/workflows/generated-linux-xenial-py3.6-gcc5.4.yml,.github/workflows/generated-linux-xenial-py3.6-gcc7.yml,.github/workflows/generated-parallelnative-linux-xenial-py3.6-gcc5.4.yml,.github/workflows/generated-periodic-linux-bionic-cuda11.5-py3.6-gcc7.yml,.github/workflows/generated-periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck.yml,.github/workflows/generated-periodic-linux-xenial-cuda11.1-py3.6-gcc7-debug.yml,.jenkins/pytorch/test.sh,test/run_test.py",20.0,11,4,3.145693215,9.0,10031.0,4.0,113046.6,17940.0,42439.5,0.0,Non Functional,0.0,1
pytorch,2e9b7c5fe11403b26090f8e45081734d1285f7b5,323b0e0a0f8a7531bb58780dbf8eacbf5ed882e6,lixinyu,lixinyu@devgpu175.prn2.facebook.com,Wed Feb 12 20:41:29 2020 -0800,1581540089.0,"fix #30480 torch.normal shape checking is broken (#32243) (#33050)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33050

Following what gchanan proposed in #30480
- If the (logical) shapes of mean and std are broadcastable, we broadcast them for the output
  Done in tensor iterator already.
- If the (logical) shapes of mean and std are not broadcastable and they have the same number of elements, we fall back to the old behavior (pick the shape of mean)
  Done by reshape std to the same shape of mean.
- If the (logical) shapes of mean and std are not broadcastable and don't have the same number of elements, we error out.
  Done by tensor iterator already.

Test Plan: Imported from OSS

Differential Revision: D19771186

Pulled By: glaringlee

fbshipit-source-id: a0b71063c7f5fdda2d4ceb84e06384414d7b4262",124.0,4.0,"aten/src/ATen/ExpandUtils.cpp,aten/src/ATen/ExpandUtils.h,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/cuda/Distributions.cu,test/test_torch.py",6.0,6,2,2.062213469,40.0,16868.0,4.0,1239654.1666666667,14704.0,39548.33333,0.0,Corrective,1.0,1
pytorch,7a1ee75ff32ee6751d2a5b0b71f563e488de73c3,324f4d5e516574b882827144eb7f42f279a1832d,Richard Zou,zou3519@gmail.com,Wed Jun 02 13:05:08 2021 -0700,1622639108.0,"[functorch] Implement batch norm batch rule for one case (where everything is batched)

It's not clear how to write the other cases",163.0,1.0,"functorch/functorch/csrc/BatchRulesNorm.cpp,functorch/test/test_vmap.py",2.0,4,1,0.901170196,1.0,2909.0,1.0,0.0,129.0,259.5,0.0,,0.0,1
pytorch,f65793507d7831c001010d4a9fbd7f90f89603cc,3288c9d304574de82d01e3f8200197c3080220f4,kshitij12345,kshitijkalambarkar@gmail.com,Thu Jun 17 00:43:11 2021 -0700,1623890591.0,"[numpy] mvlgamma: int -> float promotion (#59934)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Last int->float promotion as per the tracker!

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59934

Reviewed By: H-Huang

Differential Revision: D29160008

Pulled By: mruberry

fbshipit-source-id: 389a5a7683e0c00d474da913012768bf2a212ef0",19.0,8.0,"aten/src/ATen/native/UnaryOps.cpp,torch/testing/_internal/common_methods_invocations.py",2.0,7,2,0.918295834,9.0,8334.0,2.0,113591.5,13078.0,29608.0,0.0,,0.0,1
pytorch,4bde9efbd74dcce712dcc4ec8b69e30964d646e7,328b4160688ca9d5611b5a1b3b84eda8184363c0,Martin Raison,raison@fb.com,Sun Mar 12 17:20:49 2017 -0700,1489339249.0,THCS contiguous + to_dense,422.0,205.0,"test/test_sparse.py,torch/lib/THC/CMakeLists.txt,torch/lib/THCS/THCSTensor.cu,torch/lib/THCS/THCSTensor.h,torch/lib/THCS/generic/THCSTensor.c,torch/lib/THCS/generic/THCSTensor.cu,torch/lib/THCS/generic/THCSTensorMath.cu",7.0,6,2,1.762015846,28.0,1216.0,1.0,1450.0,327.0,14999.107,0.0,,0.0,1
pytorch,4fb28e5df913e742d60b120e0e31c4d6b4a2d595,329757a9074157b41a4176a4966a5b7582db383c,CamiWilliams,cwillycs@gmail.com,Fri Aug 30 15:51:39 2019 -0700,1567180299.0,"Torch.flatten() returns a 1-dim tensor on a 0-dim tensor (#25406)

Summary:
PR for `torch.flatten()` to return a 1-dim tensor on a 0-dim tensor

> torch.tensor(123).shape -> torch.Size([])
> torch.tensor(123).flatten() -> torch.tensor([123])
> torch.tensor(123).flatten().shape -> torch.Size([1])

resolve https://github.com/pytorch/pytorch/issues/22963
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25406

Differential Revision: D17120464

Pulled By: CamiWilliams

fbshipit-source-id: efbecd61f0aefd82f2ab417ca6bb467488ff99de",19.0,1.0,"aten/src/ATen/native/TensorShape.cpp,test/test_torch.py",2.0,5,2,0.881290899,40.0,14285.0,2.0,165312.0,11063.0,31207.33333,0.0,,0.0,1
pytorch,e37da05bd5b0c0b30aa021f8ddc906b282a42fdf,32b23a4bfc64a4b824a89a6491590576622d15ed,Seth Hendrickson,sethah@users.noreply.github.com,Fri May 18 17:14:42 2018 -0700,1526663682.0,"Throw error on tensor creation when sequence shape cannot be determined (#7583)

* first commit

* unit test

* minor style edits",24.0,0.0,"test/test_torch.py,torch/csrc/utils/tensor_new.cpp",2.0,4,2,0.543564443,39.0,7829.0,2.0,739790.5,2645.0,25028.35823,0.0,,0.0,1
pytorch,c16478fe3fb8842119438b8fd79d98c8f50ca688,32b38415535a7a0311cb644b3fddad47b438c401,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Thu Mar 08 18:21:12 2018 +0500,1520533272.0,"[ready] General documentation improvements (#5450)

* Improvize documentation
1. Add formula for erf, erfinv
2. Make exp, expm1 similar to log, log1p
3. Symbol change in ge, le, ne, isnan

* Fix minor nit in the docstring

* More doc improvements
1. Added some formulae
2. Complete scanning till ""Other Operations"" in Tensor docs

* Add more changes
1. Modify all torch.Tensor wherever required

* Fix Conv docs
1. Fix minor nits in the references for LAPACK routines

* Improve Pooling docs
1. Fix lint error

* Improve docs for RNN, Normalization and Padding
1. Fix flake8 error for pooling

* Final fixes for torch.nn.* docs.
1. Improve Loss Function documentation
2. Improve Vision Layers documentation

* Fix lint error

* Improve docstrings in torch.nn.init

* Fix lint error

* Fix minor error in torch.nn.init.sparse

* Fix Activation and Utils Docs
1. Fix Math Errors
2. Add explicit clean to Makefile in docs to prevent running graph generation script
while cleaning
3. Fix utils docs

* Make PYCMD a Makefile argument, clear up prints in the build_activation_images.py

* Fix batch norm doc error",945.0,610.0,"docs/Makefile,docs/source/scripts/build_activation_images.py,torch/__init__.py,torch/_storage_docs.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/functional.py,torch/nn/init.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/distance.py,torch/nn/modules/dropout.py,torch/nn/modules/instancenorm.py,torch/nn/modules/linear.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/modules/normalization.py,torch/nn/modules/padding.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/upsampling.py,torch/nn/utils/rnn.py,torch/random.py,torch/serialization.py",27.0,7,2,3.678612518,39.0,16328.0,13.0,927702.8888888888,2426.0,24735.35823,0.0,Corrective,1.0,1
pytorch,d6d286f651980a5faaa5631c8f8f8934a49978fd,32c5da8cd213ccab382d8c145aa256e856af1906,Philip Meier,github.pmeier@posteo.de,Wed Sep 15 14:16:29 2021 -0700,1631715389.0,"add `OpInfo` for `torch.nn.functional.dropout` (#62315)

Summary:
Addresses facebookresearch/functorch#78.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62315

Reviewed By: mruberry

Differential Revision: D30932765

Pulled By: zou3519

fbshipit-source-id: 481c67b59a966b4d640973d252b3e392d8db728e",43.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.159350063,2.0,11503.0,2.0,351351.5,15449.0,35329.5,0.0,Feature Addition,0.0,1
pytorch,1ef1dd9cadb00ac91f3b922978fb3e0d2ddf3a09,3314d51dcc1535dc2d00d357be889807d1bb8c57,Sasank Chilamkurthy,sasankchilamkurthy@gmail.com,Tue Jul 11 14:13:22 2017 +0500,1499782402.0,Add __repr__ to Avgpool and maxunpool layers (#2047),39.0,0.0,torch/nn/modules/pooling.py,1.0,3,1,0,31.0,852.0,1.0,85056.0,1114.0,17765.9346,0.0,Feature Addition,0.0,1
pytorch,1f4828002d6db5bfd183ef20486e5508eb15264b,332086c08dd48eb74d7ca0cca4eb6e00bb22053b,Beilei Zheng,beilei.zheng@intel.com,Thu Apr 14 15:42:18 2022 +0000,1649950938.0,"Add BFloat16 support for multinomial and poisson on CPU

Add BFloat16 support for multinomial and poisson on CPU

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63215
Approved by: https://github.com/frank-wei, https://github.com/bigfootjon",163.0,21.0,"aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/cpu/MultinomialKernel.cpp,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",4.0,9,3,0.704979614,45.0,25774.0,4.0,1968516.5,2275.0,5370.0,0.0,Feature Addition,0.0,1
pytorch,12ab4f08b77e1f2685ff15e7cd2a07d15ed80a43,3326c14e86ffb8db72219be24302f0804fcbfb57,Li-Huai (Allan) Lin,qqaatw@gmail.com,Tue Mar 07 08:35:59 2023 +0000,1678178159.0,"Add a sample for index_fill to test framework (#91534)

Currently the index_fill test doesn't include a sample with tensor `value` input.

This PR adds one.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91534
Approved by: https://github.com/ngimel",21.0,5.0,"test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,0.468166641,7.0,25136.0,3.0,370746.0,13093.0,30476.0,0.0,Feature Addition,0.0,1
pytorch,9d20d6d5ec5c0ac5ff00e4967f480f07ba0bb2bf,333540a458d40603feea84d30e4ad9b96b07318d,Edward Z. Yang,ezyang@fb.com,Mon Jan 09 14:57:21 2023 -0500,1673276241.0,"Reland ""Add torch.utils.device_mode"" (#91796)

Original PR https://github.com/pytorch/pytorch/pull/91525

Signed-off-by: Edward Z. Yang <ezyangfb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91796
Approved by: https://github.com/albanD",422.0,94.0,"docs/source/tensor_attributes.rst,docs/source/torch.rst,test/test_autograd.py,test/test_overrides.py,test/test_utils.py,torch/_C/__init__.pyi.in,torch/__init__.py,torch/autograd/grad_mode.py,torch/csrc/Device.cpp,torch/overrides.py,torch/signal/windows/windows.py,torch/utils/_contextlib.py,torch/utils/_device.py",13.0,10,3,2.860448973,46.0,19624.0,1.0,58285.0,11115.0,25352.0,0.0,Feature Addition,0.0,1
pytorch,19d6e32e9ab3fc81097e198872d66ee0bd2be0ad,333e29c45f756c6d4ec6045477206d78447817df,neginraoof,neginmr@utexas.edu,Mon May 11 22:43:47 2020 -0700,1589237027.0,"[ONNX] Fix pow op export (#38065)

Summary:
Fix pow type cast for opset 9 and update opset 12
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38065

Differential Revision: D21485353

Pulled By: malfet

fbshipit-source-id: 3993e835ffad07b2e6585eb5cf1cb7c8474de2ec",36.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.374262713,3.0,5836.0,3.0,681368.3333333334,1928.0,4970.5,0.0,Corrective,1.0,1
pytorch,6ebcb4606f079b9152cb242b36e03b8eddcb6173,333e8c9b227057635d365af96430cc6f4a1bab86,Soumith Chintala,soumith@gmail.com,Wed Apr 25 18:05:29 2018 -0400,1524679529.0,"any/all returns LongTensor, make test expect that (#6957)",2.0,2.0,test/test_torch.py,1.0,1,1,0,39.0,6859.0,1.0,34791.0,966.0,2365.805292,0.0,,0.0,1
pytorch,50a8f8531b0928b92ffec34e3a6df496ae431bea,3365d74df90c02fb43085c888847199ffb5cb0df,albanD,alban@robots.ox.ac.uk,Mon Oct 29 22:51:52 2018 -0700,1540853512.0,"Fix refcounting in anomaly metadata

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/13249

Differential Revision: D12823875

Pulled By: soumith

fbshipit-source-id: a0857a7cc8a4888aff99991fbae6bdd7a49d1ac4",3.0,2.0,torch/csrc/autograd/python_anomaly_mode.cpp,1.0,3,1,0,2.0,60.0,1.0,10693829.0,4983.0,14833.83333,0.0,Corrective,1.0,1
pytorch,5a979fcb990113b19f9744dcdd7b85ddd68d7fb5,336e1ec592b2e7c89377c4c7f2662f6bc684348d,Hong Xu,hong@topbug.net,Thu May 14 03:17:29 2020 -0700,1589426249.0,"Clean up error handling in is_nonzero and where in TensorCompare.cpp (#38150)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/38150

Differential Revision: D21539736

Pulled By: ezyang

fbshipit-source-id: e390c12f5948192a552d66dcd1bb89b2cb45f170",17.0,22.0,"aten/src/ATen/native/TensorCompare.cpp,test/expect/TestTorch.test_is_nonzero-empty.expect,test/expect/TestTorch.test_is_nonzero-multiple.expect,test/jit/test_list_dict.py,test/test_jit.py,test/test_sparse.py,test/test_torch.py",7.0,7,2,2.112365273,43.0,40563.0,6.0,14910808.285714284,2040.0,5177.0,0.0,,0.0,1
pytorch,c328bb6d79b6580a9a00379ec1ac82443c2860d9,33eea146eec170d6f8cecbef5d99c38cc3237a1e,Peter Bell,peterbell10@live.co.uk,Mon May 03 19:51:16 2021 -0700,1620071476.0,"torch.clamp with tensor min and max (#52695)

Summary:
Fixes gh-2793

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52695

Reviewed By: mruberry

Differential Revision: D27395977

Pulled By: ezyang

fbshipit-source-id: f86aa240feb034d42e4c45447e72218f6a773c24",647.0,210.0,"aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorCompare.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/TensorCompare.cu,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_autograd_functions.py,tools/pyi/gen_pyi.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/csrc/jit/runtime/symbolic_script.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/testing/_internal/common_methods_invocations.py",24.0,20,4,3.923518356,46.0,64906.0,17.0,1256627.25,11605.0,26252.0,0.0,Corrective,1.0,1
pytorch,2b3b205f18e8b46ee1cb7056f1460ae70407d0d6,33fabe9a2ebceb27e42cb089e025342676df3452,Nikita Vedeneev,nik@quansight.com,Thu May 05 10:13:51 2022 +0000,1651745631.0,"`functional.max_unpool`: OpInfo tests + simpler backward + forward ad + fwad over backward ad

Resolves https://github.com/pytorch/pytorch/issues/67657, https://github.com/pytorch/pytorch/issues/67658, https://github.com/pytorch/pytorch/issues/67660.

These are not necessarily bugs because we cannot produce arbitrary samples coming from `max_pool` to the gradcheck's eternal satisfaction.

This PR also replaces low-level complicated backward kernels with much simpler high-level and well-tested counterparts. The replacement is also faster (before: parallel for loop, after: memory layout optimized TensorIterator's parallelization coming from `gather`).

cc @albanD @mruberry @jbschlosser @walterddr
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68625
Approved by: https://github.com/albanD",50.0,270.0,"aten/src/ATen/native/MaxUnpooling.cpp,aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp,aten/src/ATen/native/cpu/MaxUnpoolKernel.h,aten/src/ATen/native/cuda/MaxUnpooling.cu,aten/src/ATen/native/native_functions.yaml,test/forward_backward_compatibility/check_forward_backward_compatibility.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py",9.0,15,4,2.259924023,19.0,39782.0,5.0,2264335.888888889,2890.0,6950.5,0.0,Corrective,1.0,1
pytorch,22e60d77e7fd17d4b5f53851bea4553b8a37ac6b,34075e2c8b0c251b8effc8e09b04587de2c5b2b6,BowenBao,bowbao@microsoft.com,Wed Jul 21 22:00:36 2021 -0700,1626904836.0,"[ONNX] Fix the issue of converting empty list to sequence. (#58651) (#61558)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61558

When we construct an empty list by python list comprehension, we need to avoid converting the node without inputs to onnx::Concat in shape_type_inference.cpp and peephole.cpp because it will create an invalid Concat node which doesn't have inputs.

In addition, update the code to avoid passing a Sequence input to an onnx::Cast node which doesn't accept Sequence data type as an input.

Add tests for the validation as well.

Test Plan: Imported from OSS

Reviewed By: nikithamalgifb

Differential Revision: D29767989

Pulled By: SplitInfinity

fbshipit-source-id: f97f172ff20eebda4c3744c7a934df36716f12a2

Co-authored-by: fatcat-z <jiz@microsoft.com>",99.0,40.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/helper.cpp,torch/csrc/jit/passes/onnx/helper.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",7.0,8,2,2.544307091,9.0,16462.0,6.0,3139663.4285714286,14024.0,31566.0,0.0,Corrective,1.0,1
pytorch,8961ad8c5ba644cb8c3d33f7e9a9536c3528eee3,34382e428fb94ae121dde78ae6a07e1814e75525,James Reed,jamesreed@fb.com,Sun Apr 07 00:44:53 2019 -0700,1554597893.0,"Emit math functions specific to output type (#18815)

Summary:
Stacked on https://github.com/pytorch/pytorch/pull/18811

This makes it so that we only emit the *f variants of math functions if the output value's type is FloatTensor, otherwise we call the double variants to prevent loss of precision. This fixes more numerical issues
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18815

Differential Revision: D14816965

Pulled By: jamesr66a

fbshipit-source-id: 464be644168875ede987142281fb2168f4041e81",141.0,51.0,"test/test_jit.py,torch/csrc/jit/fuser/codegen.cpp,torch/csrc/jit/fuser/cpu/resource_strings.h",3.0,6,2,1.244164349,12.0,14348.0,3.0,2746719.0,7954.0,24011.83333,0.0,Corrective,1.0,1
pytorch,8f1445c406723b602530d08aa812c92105a3e0b0,3445020ca3242e689e0e577409e427b27d799ee4,Junjie Bai,jbai@fb.com,Fri Apr 26 20:27:16 2019 -0700,1556310436.0,"Add aten mkldnn conv2d operator

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/19204

Reviewed By: dzhulgakov

Differential Revision: D14857513

fbshipit-source-id: 1172c9785e5a17a7d7360474551bdc7a511b3f2f",317.0,91.0,"aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/mkldnn/Conv.cpp,aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp,aten/src/ATen/native/mkldnn/MKLDNNCommon.h,aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp,aten/src/ATen/native/mkldnn/Utils.cpp,aten/src/ATen/native/mkldnn/Utils.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/utils/ParamUtils.h,test/test_mkldnn.py,torch/utils/mkldnn.py",11.0,9,3,3.019623188,10.0,5698.0,4.0,2415410.571428572,8355.0,24973.83333,0.0,Feature Addition,0.0,1
pytorch,5d80f634784d21151f66927c050474f90f377769,34561dadcddf6ce3c76daf16f09a09adc9c7b73b,"Gao, Xiang",qasdfgtyuiop@gmail.com,Tue Jan 07 00:46:01 2020 -0800,1578357961.0,"Don't handle bias inside cudnn_convolution* (#31524)

Summary:
Compared to cuDNN bias, PyTorch add has the following advantage:
- faster, especially for backward (see: https://github.com/zasdfgbnm/things/blob/master/2019/conv-backward-profile.md)
- handles 64bit indexing automatically
- has less code, less maintenance effort

ngimel I submit this PR early so the CI could start building it. But I have not tested it locally yet (still waiting for compiling).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31524

Differential Revision: D19264244

Pulled By: ngimel

fbshipit-source-id: cb483d378a6d8bce0a05c3643a796e544bd8e8f0",93.0,130.0,"aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py,tools/autograd/derivatives.yaml",6.0,9,3,1.574092843,14.0,10646.0,5.0,714083.3333333334,14080.0,38248.83333,0.0,Feature Addition,0.0,1
pytorch,0722ce35f5fe9815bfa651e7bb419346e8978b05,3477a2ee03f62c377321a99fff353853f7ec261a,rzou,zou3519@gmail.com,Thu Dec 14 18:59:59 2023 -0800,1702580399.0,"unMarkDynamoStrictTest on OpInfo-based tests (#115856)

These take too long to run under strict mode. We'll worry about them
later. Note that these decorators don't do anything yet (unless we flip
the default from non-strict to strict).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/115856
Approved by: https://github.com/voznesenskym
ghstack dependencies: #115845, #115855",21.0,1.0,"test/functorch/test_ops.py,test/functorch/test_vmap.py,test/test_decomp.py,test/test_meta.py,test/test_ops.py,test/test_ops_fwd_gradients.py,test/test_ops_gradients.py,test/test_ops_jit.py",8.0,2,1,2.80459653,4.0,13067.0,6.0,3115801.625,23116.0,52369.0,0.0,,0.0,1
pytorch,d3acbc821e4a3a29bf252f990a817b2103658a4c,347b036350e64fde2e68be20f1bfc2872624a1ec,Huy Do,huydhn@gmail.com,Wed Jul 13 07:59:20 2022 +0000,1657699160.0,"Apply ufmt linter to all py files under tools (#81285)

With ufmt in place https://github.com/pytorch/pytorch/pull/81157, we can now use it to gradually format all files. I'm breaking this down into multiple smaller batches to avoid too many merge conflicts later on.

This batch (as copied from the current BLACK linter config):
* `tools/**/*.py`

Upcoming batchs:
* `torchgen/**/*.py`
* `torch/package/**/*.py`
* `torch/onnx/**/*.py`
* `torch/_refs/**/*.py`
* `torch/_prims/**/*.py`
* `torch/_meta_registrations.py`
* `torch/_decomp/**/*.py`
* `test/onnx/**/*.py`

Once they are all formatted, BLACK linter will be removed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81285
Approved by: https://github.com/suo",344.0,317.0,".lintrunner.toml,tools/amd_build/build_amd.py,tools/autograd/context.py,tools/autograd/gen_annotated_fn_args.py,tools/autograd/gen_autograd.py,tools/autograd/gen_autograd_functions.py,tools/autograd/gen_inplace_or_view_type.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_trace_type.py,tools/autograd/gen_variable_factories.py,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py,tools/build_libtorch.py,tools/build_pytorch_libs.py,tools/code_analyzer/gen_op_registration_allowlist.py,tools/code_analyzer/gen_operators_yaml.py,tools/code_analyzer/gen_oplist.py,tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py,tools/download_mnist.py,tools/fast_nvcc/fast_nvcc.py,tools/gdb/pytorch-gdb.py,tools/generate_torch_version.py,tools/iwyu/fixup.py,tools/jit/gen_unboxing.py,tools/jit/test/test_gen_unboxing.py,tools/lite_interpreter/gen_selected_mobile_ops_header.py,tools/nightly.py,tools/nvcc_fix_deps.py,tools/render_junit.py,tools/setup_helpers/cmake.py,tools/setup_helpers/cmake_utils.py,tools/setup_helpers/env.py,tools/setup_helpers/gen_version_header.py,tools/setup_helpers/generate_code.py,tools/shared/__init__.py,tools/stats/import_test_stats.py,tools/stats/print_test_stats.py,tools/stats/s3_stat_parser.py,tools/stats/scribe.py,tools/stats/test_history.py,tools/stats/upload_sccache_stats.py,tools/stats/upload_test_stats.py,tools/test/test_cmake.py,tools/test/test_codegen.py,tools/test/test_codegen_model.py,tools/test/test_gen_backend_stubs.py,tools/test/test_import_test_stats.py,tools/test/test_test_selections.py,tools/test/test_upload_test_stats.py,tools/testing/explicit_ci_jobs.py,tools/testing/modulefinder_determinator.py,tools/testing/test_selections.py",52.0,18,1,4.63056131,38.0,14811.0,31.0,6591813.784313725,5275.0,12458.0,0.0,Preventative,0.0,1
pytorch,1499a604cff1ddaf4dd68453acad0e79880c99e9,3497f0207c84f41a03bf508a0e357bd0c4837518,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Wed Apr 04 11:19:47 2018 +0500,1522840787.0,[distributions] KL-Divergence for Multivariate Normal (#6172),71.0,9.0,"test/test_distributions.py,torch/distributions/kl.py,torch/distributions/multivariate_normal.py",3.0,3,2,1.313809969,9.0,4239.0,3.0,314268.0,1028.0,6946.172317,0.0,,0.0,1
pytorch,44128e09f009caf82143e12d888b6a0fb65d988b,34aee933f9f38f80adaa38b52f4cd5a59cb47e48,Lara,lahaidar@microsoft.com,Wed Jun 19 20:36:57 2019 -0700,1560976617.0,"ONNX Export Interpolate (Resize) for opset version 10

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/21434

Reviewed By: zrphercule

Differential Revision: D15777197

Pulled By: houseroad

fbshipit-source-id: 517b06a54a234ffdb762401e83f5a732023ed259",211.0,394.0,"test/onnx/expect/TestOperators.test_upsample_bilinear.expect,test/onnx/expect/TestOperators.test_upsample_nearest.expect,test/onnx/test_onnx_opset.py,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_caffe2.py,torch/nn/functional.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py",9.0,6,2,2.250488313,34.0,8788.0,5.0,986779.4444444444,9506.0,27604.83333,0.0,,0.0,1
pytorch,50326e94b183c05ce6217fc030cf2ba1e418727d,34bcd4c2376aca476174c5259816feea7c47a083,Adam Paszke,adam.paszke@gmail.com,Sun Oct 09 19:05:24 2016 -0700,1476039924.0,Rename FullConv to ConvTranspose and allow specifying output size,73.0,15.0,"test/test_nn.py,torch/nn/functions/thnn/auto.py,torch/nn/modules/__init__.py,torch/nn/modules/conv.py",4.0,6,2,1.156734338,8.0,1369.0,3.0,450920.75,240.0,2288.64599,0.0,,0.0,1
pytorch,ba5d33bedeec57bb35cf27a9d6023c8b3676dda5,34c7c56c735c0321a419af94cbc71bdefcf86756,Gregory Chanan,gchanan@fb.com,Tue Jul 31 23:30:38 2018 -0700,1533079838.0,"Re-enable empty n-dimensional empty tensor and fix parallel CPU on empty tensors (#10077)

Summary:
This is a combination of https://github.com/pytorch/pytorch/pull/9947 (this was reverted) and https://github.com/pytorch/pytorch/pull/10076.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10077

Differential Revision: D9087491

Pulled By: gchanan

fbshipit-source-id: 9fe9905628000f2ff3e47df32533cd7d1f25a354",20.0,273.0,"aten/src/ATen/Parallel.h,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/test/scalar_tensor_test.cpp,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/THCTensor.cpp,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMath.cu,test/common.py,test/test_autograd.py,test/test_indexing.py,test/test_torch.py,torch/csrc/Module.cpp,torch/csrc/autograd/python_variable_indexing.cpp",19.0,14,3,3.889385468,43.0,22906.0,2.0,50277.0,3223.0,8527.333333,0.0,Corrective,1.0,1
pytorch,5450614cf6d9b588d9ab59e9ce39c520f5415677,34cc77a8116718ad22936e571d18b54b793bf7c0,Guilherme Leobas,gleobas@quansight.com,Wed Dec 09 03:40:01 2020 -0800,1607485201.0,"Torch onnx (#48980)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/45215

This is a follow up PR of https://github.com/pytorch/pytorch/issues/45258 and https://github.com/pytorch/pytorch/issues/48782

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48980

Reviewed By: zhangguanheng66

Differential Revision: D25399823

Pulled By: ezyang

fbshipit-source-id: 798055f4abbbffecdfab0325884193c81addecec",110.0,52.0,"mypy.ini,torch/_C/__init__.pyi.in,torch/_C/_onnx.pyi,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",8.0,3,1,2.262392161,8.0,6119.0,1.0,114221.0,7310.0,16459.5,0.0,Corrective,1.0,1
pytorch,2f5c352162a6b7e1ca72e978d44db53c011fe06f,34d0bd5b1dfbbbb4e7420b3af7983f2b11567f82,Sam Estep,sam@samestep.com,Mon Apr 19 22:25:48 2021 -0700,1618871148.0,"Fix TestTypeHints.test_doc_examples (#56388)

Summary:
https://github.com/pytorch/pytorch/issues/54268 removed `test_run_mypy` since now we're running `mypy` as its own job in GitHub Actions, but previously we used this `set_cwd` context manager in that test to ensure that we picked up the `mypy` config correctly. However, for some reason, we have not been doing that in `test_doc_examples`, which has been succeeding in CI for a while despite being broken.

Specifically, [`run_test.py` changes the working directory to `test/` before running test files](https://github.com/pytorch/pytorch/blob/48aaea3359cffe7982ce24b8742c7a1f4b456a92/test/run_test.py#L534-L535), which is contrary to [what `CONTRIBUTING.md` instructs developers to do](https://github.com/pytorch/pytorch/blob/48aaea3359cffe7982ce24b8742c7a1f4b456a92/CONTRIBUTING.md#python-unit-testing). As a result, in CI, `test/test_type_hints.py` has been passing in CI, but if you run it locally from the root of the repo, this you get this error:
```
F
======================================================================
FAIL: test_doc_examples (__main__.TestTypeHints)
Run documentation examples through mypy.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test/test_type_hints.py"", line 127, in test_doc_examples
    self.fail(f""mypy failed:\n{stdout}"")
AssertionError: mypy failed:
test/generated_type_hints_smoketest.py:851: error: Name 'tensor' is not defined  [name-defined]
test/generated_type_hints_smoketest.py:853: error: Name 'tensor' is not defined  [name-defined]
Found 2 errors in 1 file (checked 1 source file)

----------------------------------------------------------------------
Ran 1 test in 1.416s

FAILED (failures=1)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56388

Test Plan:
Before this PR, the first of the following two commands should fail (since that is essentially what is run in CI), but the second should fail:
```
python test/run_test.py -i test_type_hints
python test/test_type_hints.py
```
After this PR, both commands should succeed.

Reviewed By: driazati

Differential Revision: D27860173

Pulled By: samestep

fbshipit-source-id: efb82fffd7ccb04d0331824b40bdef7bbc319c98",15.0,10.0,"test/test_type_hints.py,torch/_torch_docs.py",2.0,2,2,0.634309555,31.0,10818.0,2.0,1517491.0,10997.0,24293.0,0.0,Corrective,1.0,1
pytorch,5e23cc20af0c361eddb3ee69d8d6db81490b0f51,34d7bf6865bed674888cbecaaf232b5b23e88d8f,Richard Zou,zou3519@users.noreply.github.com,Thu Mar 31 21:04:58 2022 -0400,1648760698.0,[functorch] Beef up jvpvjp testing (pytorch/functorch#648),172.0,82.0,"functorch/test/discover_coverage.py,functorch/test/test_ops.py,functorch/test/xfail_suggester.py",3.0,2,1,1.034954963,1.0,2396.0,3.0,0.0,923.0,1284.0,0.0,,0.0,1
pytorch,ece878e5b8372fce1b4b8cc314470193a8a81a4c,34ef473d92afd1ff7e89bd9749e2c3cd6361ffc6,Omkar Salpekar,osalpekar@fb.com,Mon May 18 18:50:28 2020 -0700,1589827828.0,"[Tensorpipe Agent] Timeouts for RPC requests (#38448)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38448

This PR implements timeout support for RPCs, and respects the new per-RPC timeout functionality.

A map containing RPC futures, keyed by an expiration time, is populated by the send function for each RPC.

A separate watchdog thread polls this map and sets all incomplete futures with errors.
Note: we cannot set errors to a future with the lock held (this will trigger callbacks immediately and, if one of the callback functions tries to acquire the lock that we held when setting the error, we have a lock order cycle). Thus we add all incomplete futures to a list, and then iterate through the list outside the lock to set errors on those futures if necessary.
ghstack-source-id: 104227075

Test Plan: Will patch the testing diff on top of this to run tests.

Differential Revision: D21468526

fbshipit-source-id: 4514484ece6fb6be673427d44c7f3164ab3d9d7c",109.0,2.0,"torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h",2.0,4,1,0.814828383,1.0,794.0,1.0,485421.0,2153.0,5347.5,0.0,Feature Addition,1.0,1
pytorch,8f079b895bf96fe2c9298a4fe34453f9d7ddbb59,351d73b97f7c2affd93f452fcb293835ce76ad15,Ram Rachum,ram@rachum.com,Wed Dec 07 04:28:56 2022 +0000,1670387336.0,"Fix exception causes all over the codebase (#90271)

This is the continuation to #90134 and hopefully the final PR in this series.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90271
Approved by: https://github.com/kit1980",150.0,147.0,"benchmarks/dynamo/common.py,benchmarks/functional_autograd_benchmark/torchaudio_models.py,caffe2/python/caffe_translator.py,caffe2/python/core.py,caffe2/python/model_helper.py,caffe2/python/models/download.py,caffe2/python/operator_test/roi_align_rotated_op_test.py,caffe2/python/operator_test/video_input_op_test.py,caffe2/python/schema.py,caffe2/python/trt/transform.py,setup.py,test/distributed/fsdp/test_fsdp_state_dict.py,test/distributed/test_c10d_nccl.py,test/distributed/test_dynamo_distributed.py,test/dynamo/test_repros.py,test/inductor/test_torchinductor.py,test/jit/test_dtype_analysis.py,test/lazy/test_extract_compiled_graph.py,test/onnx/test_pytorch_onnx_onnxruntime_cuda.py,test/test_autograd.py,test/test_fx.py,test/test_jit_fuser_te.py,test/test_meta.py,tools/gen_vulkan_glsl.py,tools/render_junit.py,torch/_dynamo/debug_utils.py,torch/_dynamo/symbolic_convert.py,torch/_dynamo/utils.py,torch/_dynamo/variables/constant.py,torch/_dynamo/variables/nn_module.py,torch/_functorch/partitioners.py,torch/_inductor/codecache.py,torch/_inductor/triton_ops/autotune.py,torch/_subclasses/fake_utils.py,torch/autograd/gradcheck.py,torch/cuda/__init__.py,torch/distributed/_composable/_ddp.py,torch/distributed/elastic/rendezvous/api.py,torch/distributed/elastic/rendezvous/etcd_rendezvous.py,torch/distributed/fsdp/_init_utils.py,torch/distributed/fsdp/_optim_utils.py,torch/distributed/optim/utils.py,torch/distributed/optim/zero_redundancy_optimizer.py,torch/distributed/pipeline/sync/skip/skippable.py,torch/distributed/rendezvous.py,torch/distributed/rpc/internal.py,torch/distributed/run.py,torch/jit/_dataclass_impls.py,torch/jit/_recursive.py,torch/profiler/profiler.py,torch/testing/_comparison.py,torch/testing/_internal/common_fsdp.py,torch/testing/_internal/common_utils.py,torch/testing/_internal/distributed/_tensor/common_dtensor.py,torch/utils/data/datapipes/datapipe.py,torch/utils/data/datapipes/iter/callable.py,torch/utils/data/datapipes/map/combining.py,torch/utils/data/datapipes/map/grouping.py,torch/utils/data/datapipes/utils/decoder.py,torch/utils/data/datapipes/utils/snapshot.py,torchgen/gen_backend_stubs.py,torchgen/gen_lazy_tensor.py,torchgen/model.py",63.0,49,6,5.643439444,56.0,74671.0,58.0,11463438.777777778,10257.0,23489.0,0.0,Corrective,1.0,1
pytorch,d7b79983705a6a6877e2e9682723030514bdbfb4,3526627f46411b5352f4d03c622b01a7e26e02fa,Wanchao Liang,wanchaol.home@gmail.com,Mon Apr 13 22:50:39 2020 -0700,1586818239.0,"Use unittest assertWarns instead (#36411)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36411

This PR remove pytorch specific defined assertwarns and use the unit
test one, also format some tests

Test Plan: Imported from OSS

Differential Revision: D20998159

Pulled By: wanchaol

fbshipit-source-id: 1280ecff2dd293b95a639d13cc7417fc819c2201",94.0,117.0,"test/distributed/test_data_parallel.py,test/test_dataloader.py,test/test_jit.py,test/test_mkldnn.py,test/test_nn.py,test/test_optim.py,test/test_torch.py,torch/testing/_internal/common_utils.py",8.0,5,2,2.29173762,46.0,52120.0,8.0,847317.0,1020.0,2696.0,0.0,,0.0,1
pytorch,a3d0abf729e6622352011e962e50f3d875e5a3ea,3556bea5aab6690629bb2b06024472e46a04f6e7,Pieter Noordhuis,pietern@fb.com,Thu Sep 05 14:08:12 2019 -0700,1567692492.0,"Build torch.distributed with Gloo backend on macOS (#25260)

Summary:
In facebookincubator/gloo#212, a libuv based Gloo transport was introduced,
which allows us to use Gloo on macOS (and later perhaps also Windows). This
commit updates CMake code to enable building with USE_DISTRIBUTED=1 on macOS.

A few notes:
* The Caffe2 ops are not compiled, for they depend on `gloo::transport::tcp`.
* The process group implementation uses `gloo::transport::tcp` on Linux (because of `epoll(2)` on Linux and `gloo::transport::uv` on macOS).
* The TCP store works but sometimes crashes on process termination.
* The distributed tests are not yet run.
* The nightly builds don't use `USE_DISTRIBUTED=1`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/25260

Reviewed By: mrshenli

Differential Revision: D17202381

Pulled By: pietern

fbshipit-source-id: ca80a82e78a05b4154271d2fb0ed31c8d9f26a7c",197.0,91.0,".jenkins/pytorch/macos-build.sh,CMakeLists.txt,caffe2/CMakeLists.txt,caffe2/contrib/CMakeLists.txt,caffe2/ideep/operators/operator_fallback_ideep.cc,cmake/Dependencies.cmake,setup.py,test/test_c10d.py,test/test_c10d_spawn.py,third_party/gloo,torch/CMakeLists.txt,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/ProcessGroupGloo.hpp,torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,torch/lib/c10d/test/ProcessGroupGlooTest.cpp,torch/lib/c10d/test/TestUtils.hpp",17.0,16,6,2.950239074,74.0,11841.0,15.0,4509141.764705882,11167.0,31463.83333,0.0,Feature Addition,0.0,1
pytorch,6ea422dd0be544140e3cc2bb8546159d103a4b0d,355d5da24b7dfd50e6600e06a5407526a402c510,Kulin Seth,kulin_seth@apple.com,Mon Jul 25 23:02:13 2022 +0000,1658790133.0,"[MPS]  Perf fixes. (#81951)

Fixes https://github.com/pytorch/pytorch/issues/81610
* Use fillBuffer() for zero_mps()
Fix minor bug in add_sub_template() with value=0.0
Change default value of use_scalar_value to false in getTensorsStringKey()

* Fallback to fill_scalar_mps() if buffer isn't contiguous.

* Fix high memory consumption in view ops

* Change commitAndWait to Commit in View Ops
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81951
Approved by: https://github.com/razarmehr, https://github.com/albanD",90.0,63.0,"aten/src/ATen/mps/MPSStream.h,aten/src/ATen/mps/MPSStream.mm,aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/ConstantOps.mm,aten/src/ATen/native/mps/operations/View.mm",6.0,7,1,1.98956876,1.0,1373.0,4.0,2046414.8333333333,5737.0,13365.5,0.0,Corrective,1.0,1
pytorch,7dd28b885dcdee7494287fefa8245f3e1181d29c,3564b775537f3a49fe8693f502cca87b14319d48,BTNC,guorui.xt@gmail.com,Sat Oct 01 19:27:30 2016 +0800,1475350050.0,"a couple of changes for win32 (#779)

* windows timer with milliseconds",62.0,47.0,"CMakeLists.txt,THAllocator.c,THAtomic.c,THDiskFile.c,THGeneral.c",5.0,0,0,1.739623646,48.0,2167.0,3.0,740576.2,173.0,1091.950488,0.0,,0.0,1
pytorch,28ed04c620f019352d0ba4ccb50a8ac6a6407149,35693e9b4b09de02a18f0840ec491726f2e61fe2,Michael Carilli,mcarilli@nvidia.com,Thu May 07 01:16:33 2020 -0700,1588814193.0,"Give at::cuda::blas::gemv<at::Half> parity with <float> and <double>. Nature is healing. (#37569)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/37157 on my machine.

This was annoying to track down.  The essence is that cublas expects column major inputs and Pytorch tensors are usually row major.  Cublas lets you request that it act on transposed data, and the erroring `gemv` calls in https://github.com/pytorch/pytorch/issues/37157 make that request.  The problem is, [cublasSgemv and cublasDgemv](https://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemv) (called by [`gemv<float>`](https://github.com/pytorch/pytorch/blob/091a1192d7c1013915b100dd1a4d00eecf6abe5e/aten/src/ATen/cuda/CUDABlas.cpp#L318) and `gemv<double>`) regard their `m, n` arguments values as _pre_-transpose sizes, while [cublasGemmEx](https://docs.nvidia.com/cuda/cublas/index.html#cublas-GemmEx) (called by `gemv<at::Half>`, see [here](https://github.com/pytorch/pytorch/blob/091a1192d7c1013915b100dd1a4d00eecf6abe5e/aten/src/ATen/cuda/CUDABlas.cpp#L342) and [here](https://github.com/pytorch/pytorch/blob/091a1192d7c1013915b100dd1a4d00eecf6abe5e/aten/src/ATen/cuda/CUDABlas.cpp#L229)) regards its `m, k` argument values as _post_-transpose sizes.  This is inconsistent.  It turns out the `gemv<float>/<double>` calls are configured correctly and the `gemv<at::Half>` calls aren't.

Strikethrough text below is no longer accurate, ngimel suggested a better way to handle gemv->gemm forwarding.  [Comments in code](https://github.com/pytorch/pytorch/pull/37569/files#diff-686aa86335f96b4ecb9b37f562feed12R323-R348) provide an up-to-date explanation.

Keeping out-of-date strikethrough text because I don't have the heart to delete it all and because it captures an intermediate state of my brain that will help orient me if i ever have to fix this again.

~~To convince myself this PR keeps `at::cuda::blas::gemv`'s external API consistent across dtypes, I need to think through what happens when a pytorch tensor input of size `(a,b)` multiples a vector of size `(b,)` for 4 cases:~~

### ~~1. input is row-major (needs cublas internal transpose)~~
#### ~~1a. input is float or double~~
~~`gemv<float>/<double>` call `cublasS/Dgemv`, forwarding `trans`,** `m`, and `n` directly.~~

~~`cublasS/Ggemv` expects ""a m Ã n matrix stored in column-major format"" (so m is the input's fast dim).  Input has size `(a, b)` in row-major format.  We can reinterpret it as a column-major matrix with size `(b, a)` without any memory movement.  So the gemv call should supply `m=b`, `n=a`.  However, we're not trying to multiply a matrix `(b, a)` x a vector `(b,)`, we're trying to sum across `b` for matrix and vector.  So we also request that cublas transpose the matrix internally by supplying `trans='t'` to `blas::gemv`, which becomes `trans=CUBLAS_OP_T` to the `cublasS/Ggemv`.~~

~~As long as the code calling `blas::gemv` thinks carefully and passes `trans='t'`, `m=b`, `n=a`, cublas carries out `(a, b) x (b,)` and all is well.~~

#### ~~1b. input is half or bfloat16~~
~~`blas::gemv<at::Half>` takes a different code path, calling `gemm<at::Half>` which calls `cublasGemmEx`.  The job of this PR is to make sure the exterior `blas::gemv` caller's carefully thought-out argument choices (`trans='t'`, `m=b`, `n=a`) remain correct.~~

~~`cublasGemmEx` takes args `transa, transb, m, n, k, ....others we don't care about` and carries out~~
```
 C = Î± op ( A ) op ( B ) + Î² C
where Î± and Î² are scalars, and A , B and C are matrices stored in column-major format with
dimensions op ( A ) m Ã k , op ( B ) k Ã n and C m Ã n Also, for matrix A
           A if  transa == CUBLAS_OP_N
op ( A ) = A^T if  transa == CUBLAS_OP_T ...
```
~~`gemv<at::Half>` hacks a gemv by calling gemm such that the raw gemm's `m` is the output dim, `k` is the summed dim, and `n=1`, .  Reasonable, as long as we get the values right, given that we also need to transpose the input.~~

~~To conform with cublas docs we interpret input as column-major with size `(b, a)`.  As for the `<float>/<double>` gemv we want cublas to carry out input (interpreted as column major), internally transposed, times vector of size `(b,)`.  In other words we want cublas to apply `op(A) x B`, where op is transpose and `A` is input interpreted as column major.  Docs define `m` and `k` by saying `op(A)` has dims `m x k` **(`m` and `k` are _post_-`op` sizes)**.  `A` was `(b, a)`, `op(A)` is `(a, b)`, so the correct thing is to supply `m=a`, `k=b` to the underlying gemm.  **For the `<float>/<double>` gemv, we passed `m=b`, not `m=a`, to the raw `cublasS/Dgemv`.**~~

~~The exterior `blas::gemv` must have been called with `trans='t'`, `m=b`, `n=a` (as required by the `<float>/<double>` versions).  So when gemv is about to call gemm, **we [swap](https://github.com/pytorch/pytorch/pull/37569/files#diff-686aa86335f96b4ecb9b37f562feed12R330) the local values of `m` and `n` so that `m=a`, `n=b`,** then put `m (=a)` in the gemm's `m` spot, 1 in the gemm's `n` spot, and `n (=b)` in the gemm's `k` spot.  All is well (we made the right gemm call after ingesting the same arg values as `blas::gemv<float>/<double>`).~~

### ~~2. input is column-major (doesn't need cublas transpose)~~
#### ~~2a. input is float or double~~
~~input is `(a,b)`, already column-major with strides `(1,a)`.  Code calling `blas::gemv` supplies `trans='n'` (which becomes `CUBLAS_OP_N`, no internal transpose), `m=a`, `n=b`.~~

#### ~~2b. input is half or bfloat16~~
~~`blas::gemv` should pass `transa='n'`, `m=a`, `n=1`, `k=b` to the underlying gemm. The exterior `blas::gemv` must have been called with `trans='t'`, `m=a`, `n=b` (as required by the `<float>/<double>` versions). So **in this case we _don't_ swap `blas::gemv`'s local values of `m` and `n`.** We directly put `m (=a)` in the gemm's `m` spot, 1 in the gemm's `n` spot, and `n (=b)` in the gemm's `k` spot. All is well (we made the right gemm call after ingesting the same arg values as `blas::gemv<float>/<double>`).~~

~~** `trans` is a string `t` or `n` in the `at::cuda::blas::gemv` API, which gets [converted](https://github.com/pytorch/pytorch/blob/091a1192d7c1013915b100dd1a4d00eecf6abe5e/aten/src/ATen/cuda/CUDABlas.cpp#L314) to a corresponding cublas enum value `CUBLAS_OP_T` (do transpose internally) or `CUBLAS_OP_N` (don't transpose internally) just before the raw cublas call.~~
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37569

Differential Revision: D21405955

Pulled By: ngimel

fbshipit-source-id: e831414bbf54860fb7a4dd8d5666ef8081acd3ee",80.0,31.0,"aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/native/Blas.cpp,aten/src/ATen/native/cuda/Blas.cu,test/test_torch.py",4.0,7,2,1.601560616,41.0,18573.0,3.0,2043304.0,1781.0,4618.5,0.0,Corrective,1.0,1
pytorch,cb9fd0ce58d830375a4be67f7c64c51f90ecee49,3569a1c6ddb756a28fc3aafe083f1303eb269b73,Lara,lahaidar@microsoft.com,Tue Sep 24 00:06:26 2019 -0700,1569283586.0,"Fix Exporting RNN/LSTM's Initial State (h0/c0) to ONNX

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22813

Reviewed By: hl475

Differential Revision: D16275791

Pulled By: houseroad

fbshipit-source-id: 6e2259e84e1f5a674daabcbe0df99b1360ed2b35",139.0,54.0,"aten/src/ATen/core/interned_strings.h,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/init.cpp,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/peephole.h,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",8.0,12,3,2.103867083,12.0,7879.0,6.0,1417615.75,11647.0,32793.83333,0.0,Corrective,1.0,1
pytorch,8ab3d214d5a93f66a425652b1de3b3f08eb01893,35757af6f7632e299c1457d5098fba834949e3dc,Alykhan Tejani,alykhan.tejani@gmail.com,Fri Jul 21 20:02:07 2017 +0100,1500667327.0,"Add broadcasting of weights to bce/bce_with_logits (#2161)

* added tests + removed explicit expand of weight in bce with logits

* add auto broadcasting of weight to BCELoss

* remove the need for _BCELoss

* formatting of warning

* remove TODO

* move across assert from _functions/thnn/loss.py

* flake8 fixes",50.0,46.0,"test/test_nn.py,torch/nn/_functions/thnn/__init__.py,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/loss.py,torch/nn/functional.py",5.0,5,2,1.610417647,32.0,5267.0,1.0,224.0,1234.0,14733.02883,0.0,Corrective,1.0,1
pytorch,fe5424d0f8604f6e66d827ae9f94b05cb7119d55,35a197defa179c921505b59796d9913cd2d17565,leslie-fang-intel,leslie.fang@intel.com,Fri Jun 28 01:28:42 2024 -0700,1719538122.0,"[Inductor][CPP] Enable Quantized Linear GEMM Template with FP32 output (#128825)

**Summary**
Support int8 GEMM Template with refer MicroInt8GEMM kernel for case:

- Activation dtype: uint8
- Weight dtype: int8
- Output dtype: float32/bfloat16
- Post Op Fusion: without unary post operator fusion

**Test Plan**
```
clear && python -u -m pytest -s -v test/inductor/test_cpu_select_algorithm.py -k test_quantized_linear_with_pointwise
```

**Next Step**
- [ ] Unary post op fusion
- [ ] Int8 output
- [ ] Binary Fusion
- [ ] AMX int8 MicroGEMM Kernel

Pull Request resolved: https://github.com/pytorch/pytorch/pull/128825
Approved by: https://github.com/jgong5, https://github.com/jansel",391.0,76.0,"test/inductor/test_cpu_select_algorithm.py,test/inductor/test_mkldnn_pattern_matcher.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_micro_gemm.py,torch/_inductor/mkldnn_lowerings.py,torch/_inductor/utils.py,torch/testing/_internal/common_quantization.py",7.0,7,2,2.17611298,3.0,10116.0,6.0,476407.5714285714,30643.0,76669.5,0.0,,0.0,1
pytorch,6b4ed52f10daa3781a726014a2cde56c4d2119d0,35ba948dde18ec51759aded423386ca1dab8dafe,Soumith Chintala,soumith@fb.com,Mon Jan 02 09:03:22 2017 -0500,1483347802.0,"add doc for *mm* functions, *mv* functions and addcmul, addcdiv",215.0,2.0,torch/docs.py,1.0,1,1,0,8.0,1129.0,1.0,0.0,292.0,6376.224559,0.0,Feature Addition,0.0,1
pytorch,fe70823f8e3146ec0753f40391f5378c3952b275,35c4d73bdb22bdcdd68f50955135a27a600b8079,Richard Zou,zou3519@users.noreply.github.com,Thu Jan 04 17:38:04 2018 -0500,1515087484.0,"Deprecate nn.NLLLoss2d (#4238)

* Deprecate nn.NLLLoss2d

* Fix legacy tests

* Fix tests

* Remove NLLLoss2d from docs, add deprecation warning instead of error

* fix lint

* Add more to docs",76.0,91.0,"docs/source/nn.rst,test/common_nn.py,test/test_legacy_nn.py,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/loss.py",6.0,6,3,1.954075575,38.0,11398.0,5.0,1468279.5,2251.0,24334.85823,0.0,Corrective,1.0,1
pytorch,cd62bbf756d4988958540621af509e20e2100611,35d4a805ebc3b6eca1bafb2d332dffa8d0c1fc54,Peter Bell,peterbell10@live.co.uk,Thu Apr 07 22:48:27 2022 -0700,1649371707.0,"CUDA Kernels: Use per-operator headers (4/4) (#71215)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71215

Splitting this into multiple PRs to keep the diffs more managable.

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D33949902

Pulled By: malfet

fbshipit-source-id: e737245fb9ebba3c301ee644fba447ea5ddfdfba
(cherry picked from commit a3102cc0d6795e25d3132f8750e092ee2fac59e7)",308.0,63.0,"aten/src/ATen/native/TensorTransformations.cpp,aten/src/ATen/native/TensorTransformations.h,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/Sort.cpp,aten/src/ATen/native/cuda/SortImpl.cu,aten/src/ATen/native/cuda/Sorting.cpp,aten/src/ATen/native/cuda/SparseMM.cu,aten/src/ATen/native/cuda/SpectralOps.cpp,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/TensorCompare.cpp,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cuda/TensorModeKernel.cpp,aten/src/ATen/native/cuda/TensorShapeCUDA.cpp,aten/src/ATen/native/cuda/TensorTopK.cpp,aten/src/ATen/native/cuda/TensorTransformations.cu,aten/src/ATen/native/cuda/TriangularOps.cu,aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu,aten/src/ATen/native/cuda/Unique.cu,aten/src/ATen/native/cuda/UniqueCub.cu,aten/src/ATen/native/cuda/UniqueCub.cuh,aten/src/ATen/native/cuda/UpSampleBicubic2d.cu,aten/src/ATen/native/cuda/UpSampleBilinear2d.cu,aten/src/ATen/native/cuda/UpSampleLinear1d.cu,aten/src/ATen/native/cuda/UpSampleNearest1d.cu,aten/src/ATen/native/cuda/UpSampleNearest2d.cu,aten/src/ATen/native/cuda/UpSampleNearest3d.cu,aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu,aten/src/ATen/native/cuda/WeightNorm.cu,aten/src/ATen/native/cuda/group_norm_kernel.cu,aten/src/ATen/native/cuda/jit_utils.cpp,aten/src/ATen/native/cuda/layer_norm_kernel.cu",32.0,5,1,4.69585789,10.0,10968.0,26.0,10184421.46875,2092.0,5054.5,0.0,,0.0,1
pytorch,19985d6f8432fa2dad254319fb7a8f7691b5e70d,3607478ecd42ee93da1f05e4bd75fc1cfdaf6335,anjali411,chourdiaanjali123@gmail.com,Fri Jun 04 21:11:23 2021 -0700,1622841083.0,"Conjugate View (#54987)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54987

Based off of ezyang (https://github.com/pytorch/pytorch/pull/44799) and bdhirsh (https://github.com/pytorch/pytorch/pull/43702) 's prototype:

Here's a summary of the changes in this PR:
This PR adds a new dispatch key called Conjugate. This enables us to make conjugate operation a view and leverage the specialized library functions that fast path with the hermitian operation (conj + transpose).

1. Conjugate operation will now return a view with conj bit (1) for complex tensors and returns self for non-complex tensors as before. This also means `torch.view_as_real` will no longer be a view on conjugated complex tensors and is hence disabled. To fill the gap, we have added `torch.view_as_real_physical` which would return the real tensor agnostic of the conjugate bit on the input complex tensor. The information about conjugation on the old tensor can be obtained by calling `.is_conj()` on the new tensor.
2. NEW API:
    a) `.conj()` -- now returning a view.
    b) `.conj_physical()` -- does the physical conjugate operation. If the conj bit for input was set, you'd get `self.clone()`, else you'll get a new tensor with conjugated value in its memory.
    c) `.conj_physical_()`, and `out=` variant
    d) `.resolve_conj()`  -- materializes the conjugation. returns self if the conj bit is unset, else returns a new tensor with conjugated values and conj bit set to 0.
    e) `.resolve_conj_()` in-place version of (d)
    f) `view_as_real_physical` -- as described in (1), it's functionally same as `view_as_real`, just that it doesn't error out on conjugated tensors.
    g) `view_as_real` -- existing function, but now errors out on conjugated tensors.
3. Conjugate Fallback
    a) Vast majority of PyTorch functions would currently use this fallback when they are called on a conjugated tensor.
    b) This fallback is well equipped to handle the following cases:
        - functional operation e.g., `torch.sin(input)`
        - Mutable inputs and in-place operations e.g., `tensor.add_(2)`
        - out-of-place operation e.g., `torch.sin(input, out=out)`
        - Tensorlist input args
        - NOTE: Meta tensors don't work with conjugate fallback.
4. Autograd
    a) `resolve_conj()` is an identity function w.r.t. autograd
    b) Everything else works as expected.
5. Testing:
    a) All method_tests run with conjugate view tensors.
    b) OpInfo tests that run with conjugate views
        - test_variant_consistency_eager/jit
        - gradcheck, gradgradcheck
        - test_conj_views (that only run for `torch.cfloat` dtype)

NOTE: functions like `empty_like`, `zero_like`, `randn_like`, `clone` don't propagate the conjugate bit.

Follow up work:
1. conjugate view RFC
2. Add neg bit to re-enable view operation on conjugated tensors
3. Update linalg functions to call into specialized functions that fast path with the hermitian operation.

Test Plan: Imported from OSS

Reviewed By: VitalyFedyunin

Differential Revision: D28227315

Pulled By: anjali411

fbshipit-source-id: acab9402b9d6a970c6d512809b627a290c8def5f",781.0,178.0,"aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/ConjugateFallback.cpp,aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/ComplexHelper.h,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/UnaryComplexKernels.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/templates/Functions.h,aten/src/ATen/templates/TensorBody.h,c10/core/DispatchKey.cpp,c10/core/DispatchKey.h,c10/core/TensorImpl.h,docs/source/tensor_view.rst,docs/source/tensors.rst,docs/source/torch.rst,test/backward_compatibility/check_backward_compatibility.py,test/test_autograd.py,test/test_jit_fuser_te.py,test/test_ops.py,test/test_view_ops.py,tools/autograd/derivatives.yaml,tools/autograd/gen_inplace_or_view_type.py,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/autograd/gradcheck.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/jit/tensorexpr/external_functions_codegen.cpp,torch/overrides.py,torch/testing/_core.py,torch/testing/_internal/common_methods_invocations.py",43.0,25,6,4.100728804,45.0,76846.0,28.0,2124964.8095238097,12738.0,28941.5,0.0,Feature Addition,1.0,1
pytorch,6885b3fd62a9de2fd425988aeef0bd11356cbb64,360c1bbd5b36fd84670292b2ea93170c31cbd5cf,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Tue Jul 24 18:58:22 2018 -0700,1532458702.0,"Add multivariate log-gamma (mvlgamma) (#9451)

Summary:
1. Add tests in test_cuda, test_torch
2. Add doc strings

Closes https://github.com/pytorch/pytorch/issues/9378 .

Differential Revision: D8859746

Pulled By: ezyang

fbshipit-source-id: 939c309d90940a7aa08f53004c9e7b3b1c9cf54e",107.0,0.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_cuda.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_tensor_docs.py,torch/_torch_docs.py",10.0,11,5,2.683976081,42.0,24846.0,9.0,455778.7,3079.0,7483.833333,0.0,Feature Addition,0.0,1
pytorch,0c737dff6333c859957c3b26f7a0114f73be12a6,361648a4a77e8222c5f8e884890f34267628294c,gchanan,gregchanan@gmail.com,Fri Apr 27 22:12:33 2018 -0400,1524867153.0,"Fix torch.tensor(...) device-type calculation when used with numpy anâ¦ (#6995)

* Fix torch.tensor(...) device-type calculation when used with numpy and type inference.

* Fix tensor device type inference as well.

* Better variable type inference: infer cuda-ness only if device is not specified.",61.0,24.0,"test/test_sparse.py,test/test_torch.py,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h",6.0,5,2,1.787233851,39.0,9313.0,5.0,289615.0,990.0,2414.305292,0.0,Corrective,1.0,1
pytorch,04461fa28953f17f16791f615aa77eb5d10d5abc,363de58a8b6786ca7300ce19511464f20f715dd9,li-roy,8813817+li-roy@users.noreply.github.com,Thu Mar 08 11:15:08 2018 -0800,1520507708.0,"implement double backwards for MaxPool3d (#5328)

* implement double backwards for MaxPool3d

* change MaxUnpool3d to use same indices as MaxPool3d

* fix nits",164.0,232.0,"aten/src/THCUNN/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THNN/generic/VolumetricDilatedMaxPooling.c,aten/src/THNN/generic/VolumetricMaxUnpooling.c,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp",9.0,10,3,2.562487739,37.0,10697.0,5.0,5505756.0,2424.0,24721.35823,0.0,Corrective,1.0,1
pytorch,55db156229928235abb8c01d63e0392c6e0d7fc9,364639041f38165d8a7365c024106052d885e6ac,Kimish Patel,kimishpatel@fb.com,Thu Apr 08 22:29:15 2021 -0700,1617920955.0,"Revert D27121170: [torch] Add cuda support for segment reduction 'max'

Test Plan: revert-hammer

Differential Revision:
D27121170 (https://github.com/pytorch/pytorch/commit/eb5e1fc7137aee8810ff14819e98fa2a4fd7419e)

Original commit changeset: 1c2565f42e29

fbshipit-source-id: 3dd394edcf5ef53c27098b4d0a1dd6fbbabdd506",43.0,189.0,"BUILD.bazel,aten/src/ATen/native/SegmentReduce.cpp,aten/src/ATen/native/SegmentReduce.h,aten/src/ATen/native/cuda/SegmentReduce.cu,aten/src/ATen/native/native_functions.yaml,test/test_segment_reductions.py",6.0,6,2,1.584886381,12.0,11281.0,1.0,7503.0,10580.0,23393.5,0.0,Feature Addition,0.0,1
pytorch,7391edb591ae3c9675063ad0fa8d2c5c62e24be1,3649a2c170c45653d2aa1267d48beb867914b039,kshitij12345,kshitijkalambarkar@gmail.com,Fri Nov 13 00:14:11 2020 -0800,1605226451.0,"[numpy] `torch.sqrt` : promote integer inputs to float (#47293)

Summary:
Reference https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47293

Reviewed By: malfet

Differential Revision: D24855994

Pulled By: mruberry

fbshipit-source-id: 1e6752f2eeba6d638dea0bdea0c650cf722718c9",39.0,9.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,test/test_torch.py,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py",5.0,12,3,1.577368168,44.0,26009.0,4.0,459143.4,6727.0,15301.5,0.0,,0.0,1
pytorch,f166b934ee40f47ead884a2f7db638cc72b12b69,36607c85eedd5c7e6d0686fd8a65218556b2e024,Nick Gibson,nickg@fb.com,Wed Jun 03 01:20:55 2020 -0700,1591147255.0,"[TensorExpr] eliminate zero length Allocations in IRSimplifier (#38794)

Summary:
If the size of a temporary buffer is reduced to zero via binding of a dynamic variable we still run the alloc, even though it is a no op. It's easy to strip these out during simplification, so the expr:
```
{
  Allocate(x, int, {0});
  // Stuff...
  Free(x);
}
```
becomes
```
{
  // Stuff...
}
```

I am assuming here that if the allocation size is zero then any usage of the buffer is also eliminated since theres no safe way to refer to a zero size buffer.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38794

Differential Revision: D21723656

Pulled By: nickgg

fbshipit-source-id: 3eaa8bd8974a13b0a351be04abe2348498b31b02",162.0,17.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",4.0,7,2,1.604636863,2.0,4435.0,4.0,1231370.5,2531.0,6260.0,0.0,,0.0,1
pytorch,2f7b6db4290994007373daa383d38a8ffdeea397,366299f9f3334bbe361f25b7b14963088e95afd6,Luca Antiga,luca.antiga@orobix.com,Thu Jul 13 20:37:27 2017 +0200,1499978247.0,"Wrap unbiased flag in var, std, varall, stdall",124.0,80.0,"test/test_autograd.py,test/test_torch.py,tools/cwrap/plugins/BoolOption.py,torch/autograd/variable.py,torch/csrc/generic/methods/TensorMath.cwrap,torch/lib/THD/master_worker/master/generic/THDTensorMath.cpp,torch/lib/THD/master_worker/master/generic/THDTensorMath.h,torch/lib/THD/master_worker/worker/dispatch/TensorMath.cpp,torch/lib/THPP/Tensor.hpp,torch/lib/THPP/tensors/THCSTensor.hpp,torch/lib/THPP/tensors/THCTensor.hpp,torch/lib/THPP/tensors/THSTensor.hpp,torch/lib/THPP/tensors/THTensor.hpp,torch/lib/THPP/tensors/generic/THCSTensor.cpp,torch/lib/THPP/tensors/generic/THCTensor.cpp,torch/lib/THPP/tensors/generic/THSTensor.cpp,torch/lib/THPP/tensors/generic/THTensor.cpp",17.0,19,3,3.896492413,34.0,18802.0,1.0,470.0,1180.0,14592.21058,0.0,,0.0,1
pytorch,a1299a28021f063a0a7b81936230f2395dad480c,36828aa0fff1bf57679ac62693b683ecb796e1c4,Nikita Shulga,nshulga@fb.com,Thu Apr 22 04:09:43 2021 -0700,1619064583.0,"Revert D27866138: [ONNX] Redesign inplace conversion (#55033)

Test Plan: revert-hammer

Differential Revision:
D27866138 (https://github.com/pytorch/pytorch/commit/24ff92f76d9998455258257334fca490b558cbd8)

Original commit changeset: ab5c9188740c

fbshipit-source-id: b99bf5b12e109089ebd5748c1dc152c6af1cebdb",592.0,874.0,".jenkins/caffe2/test.sh,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp,torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",6.0,10,3,0.821893049,8.0,14041.0,1.0,11515.0,11134.0,24641.5,0.0,,0.0,1
pytorch,4d3930fe8a06b3fc3c5a8738aa9b6fde2499c0ec,369d9f4137a8bfc20e6a4e1d6ab35eeae4e9b345,lezcano,lezcano-93@hotmail.com,Tue May 17 12:24:46 2022 +0000,1652790286.0,"A few forward AD formulas

It includes all-time favourites like:
- `put`
- `nn.functional.embedding`
- `prelu`
- `nn.functional.bilinear`
- `nn.functional.rrelu`
- `nn.functional.logsigmoid`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77421

Approved by: https://github.com/soulitzer",48.0,11.0,"aten/src/ATen/native/Activation.cpp,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",5.0,11,3,2.03393791,16.0,28796.0,5.0,228940.4,3312.0,7942.0,0.0,,0.0,1
pytorch,7772d26cb01096c170e6a692afe47035ec901930,36abf023bdaeb3be8d6b009539914a51b04c0ffd,Soumith Chintala,soumith@gmail.com,Thu Mar 01 00:32:15 2018 -0500,1519864335.0,"Added 3d grid sampler (for volumetric transformer networks) (#5453)

* add 3d grid_sample

* add cuda implementation, more testing",1079.0,14.0,"aten/src/TH/THTensorMacros.h,aten/src/THCUNN/CMakeLists.txt,aten/src/THCUNN/VolumetricGridSamplerBilinear.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THCUNN/generic/VolumetricGridSamplerBilinear.cu,aten/src/THNN/generic/THNN.h,aten/src/THNN/generic/VolumetricGridSamplerBilinear.c,aten/src/THNN/init.cpp,test/test_nn.py,torch/nn/_functions/vision.py,torch/nn/functional.py",11.0,11,3,2.113311812,37.0,12645.0,6.0,2495579.625,570.0,1741.405869,0.0,Feature Addition,0.0,1
pytorch,85126629a59ac51dcebca08b0ef2da5dd3afd827,36b476ccdd0ebaadf12d889801f8646a871fbaa7,Winston Smith,76181208+imaginary-person@users.noreply.github.com,Sun Apr 18 05:50:55 2021 -0700,1618725055.0,"Added OpInfos for eq, ne, ge, gt, le, and lt (#55709)

Summary:
A https://github.com/pytorch/pytorch/issues/54261 task
Added OpInfos for `eq`, `ne`, `ge`, `gt`, `le`, and `lt`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55709

Reviewed By: jbschlosser

Differential Revision: D27760382

Pulled By: mruberry

fbshipit-source-id: 30d8c9633c69a097c1e4a9daf4178c617c0a9093",86.0,76.0,"test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.650022422,43.0,12996.0,2.0,157994.5,10961.0,24177.0,0.0,Feature Addition,0.0,1
pytorch,272f4db043ec2c63ecfe6d2759e7893cb842a3c3,36c87f1243f582ae761ec6d69f5965bd91c32d3c,Mike Ruberry,mruberry@devfair044.maas,Sun Nov 29 04:09:52 2020 -0800,1606622992.0,"Refactors test_torch.py to be fewer than 10k lines (#47356)

Summary:
Creates multiple new test suites to have fewer tests in test_torch.py, consistent with previous test suite creation like test_unary_ufuncs.py and test_linalg.py.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47356

Reviewed By: ngimel

Differential Revision: D25202268

Pulled By: mruberry

fbshipit-source-id: 75fde3ca76545d1b32b86d432a5cb7a5ba8f5bb6",15261.0,14651.0,"test/run_test.py,test/test_autograd.py,test/test_binary_ufuncs.py,test/test_indexing.py,test/test_linalg.py,test/test_numpy_interop.py,test/test_reductions.py,test/test_shape_ops.py,test/test_sort_and_select.py,test/test_tensor_creation_ops.py,test/test_testing.py,test/test_torch.py,test/test_unary_ufuncs.py,test/test_view_ops.py,torch/testing/_internal/jit_metaprogramming_utils.py",15.0,4,2,2.611773892,45.0,34967.0,8.0,2810821.875,7017.0,15877.0,0.0,Perfective,0.0,1
pytorch,468375824b4eabc32da0ae03c809f14fa2d9f06a,36ce1b34ee2d70583ca6177d6c446c660b2430e6,Samantha Andow,samdow@fb.com,Thu Apr 21 12:51:08 2022 -0400,1650545468.0,[functorch] fix tests (pytorch/functorch#721),51.0,37.0,"functorch/test/test_ops.py,functorch/test/test_vmap.py",2.0,2,1,0.976020648,1.0,5699.0,2.0,0.0,981.0,1363.0,0.0,Corrective,1.0,1
pytorch,291e56eda43a7846974eee41eedd702a4576d93f,370d0afc1b2bbba5163733e6ca2be9fabaaed7cf,Pearu Peterson,pearu.peterson@gmail.com,Thu Dec 02 03:17:33 2021 -0800,1638415053.0,"Strided masked var. (#68738)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68738

Test Plan: Imported from OSS

Reviewed By: davidberard98

Differential Revision: D32767155

Pulled By: cpuhrsch

fbshipit-source-id: a5c095103405fbfc28b9f4fd624bdbbc45e7f715",126.0,3.0,"test/test_masked.py,torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.249792438,2.0,15123.0,1.0,30861.0,17433.0,40925.5,0.0,,0.0,1
pytorch,756cf2913d90d0b41484376a3731c5dd577e825c,3713103db48183fe323ea15e6e0fdb0603e495b7,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Wed Feb 14 22:44:21 2024 +0000,1707950661.0,"Revert ""[Inductor] Setting kernel launch and exit callbacks for inductor generated triton kernels (#119450)""

This reverts commit 4e93b00b692118b8531f3807ec95eb4c538ea419.

Reverted https://github.com/pytorch/pytorch/pull/119450 on behalf of https://github.com/soulitzer due to Regressed perf on the dashboard ([comment](https://github.com/pytorch/pytorch/pull/119450#issuecomment-1944876761))",1.0,32.0,"test/inductor/test_profiler.py,torch/_inductor/triton_heuristics.py",2.0,4,2,0.61361902,1.0,1608.0,2.0,430079.5,25197.0,56903.5,0.0,Non Functional,0.0,1
pytorch,eea9c6a04847ea1c8f4406b5d5a8d99e67df8386,375ddb01b5a50d3913c064ee2289a8724a669e27,anjali411,chourdiaanjali123@gmail.com,Tue May 12 02:57:31 2020 -0700,1589252251.0,"Fix tensor printing (#38031)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/38031

Test Plan: Imported from OSS

Differential Revision: D21502915

Pulled By: anjali411

fbshipit-source-id: 0cc3017a390da55af47ba81f651a883cd52b10da",16.0,9.0,"test/test_torch.py,torch/_tensor_str.py",2.0,2,2,0.634309555,41.0,18506.0,2.0,1391573.0,1939.0,5011.0,0.0,Corrective,1.0,1
pytorch,e0d5d1b7c92f20b99691518bc1c8d5d967e03d49,3796ce925586de418072a5de40a4d76d14f9245c,Tongzhou Wang,SsnL@users.noreply.github.com,Mon Dec 18 07:11:01 2017 -0500,1513581061.0,assert (#4056),98.0,40.0,"aten/src/TH/generic/THBlas.c,aten/src/TH/generic/THTensorMath.c,aten/src/THNN/generic/SpatialDilatedConvolution.c,aten/src/THNN/generic/SpatialFullDilatedConvolution.c,aten/src/THNN/generic/VolumetricDilatedConvolution.c",5.0,6,1,2.095114222,4.0,5244.0,3.0,2972090.8,2201.0,24230.35823,0.0,,0.0,1
pytorch,d2e0c628e9e1b5eaaa226f7b9198289582736f3e,379e4d9cad2b040f09022014f26f34f24ca9aaa6,Max Balandat,balandat@fb.com,Mon Apr 13 14:48:05 2020 -0700,1586789285.0,"[pytorch] Make behavior of SobolEngine consistent w/ other RNG functions (#36427)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36427

Addresses https://github.com/pytorch/pytorch/issues/36341

Test Plan: unit tests

Reviewed By: ldworkin

Differential Revision: D20952703

fbshipit-source-id: 28055f4c4c0f8012c2d96e473b822fa455dd833c",14.0,2.0,"test/test_torch.py,torch/quasirandom.py",2.0,2,2,0.811278124,41.0,17100.0,2.0,3255135.5,1005.0,2672.0,0.0,Feature Addition,0.0,1
pytorch,902bf0bbbe8fb1560bc24d7dd27c0bba33ff35db,37d1b39413fc3382b1fc2124a623edce7352c43b,kshitij12345,kshitijkalambarkar@gmail.com,Thu Apr 08 08:20:42 2021 -0700,1617870042.0,"OpInfo: `atan2` (#55132)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55132

Reviewed By: mrshenli

Differential Revision: D27615135

Pulled By: mruberry

fbshipit-source-id: 22fa1a225b9a75eb478797316e4462d4af4e8826",25.0,5.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,4978.0,1.0,1487.0,10558.0,23356.0,0.0,,0.0,1
pytorch,b6379591a967bca7f55ce5c8e348555a97674c56,37e3c60897d16dd39ebcb0249d70e2340ff4899d,leslie-fang-intel,leslie.fang@intel.com,Fri Jun 28 04:41:47 2024 -0700,1719549707.0,"[Inductor][CPP] Remove redundant INT8-specific logic in the INT8 GEMM template (#129470)

**Summary**
Remove redundant INT8-specific logic in the INT8 GEMM template to unify the code structure with FP32/BF16/FP16 GEMM Template.

**Test Plan**
```
numactl -C 56-111 -m 1 python -u -m pytest -s -v test/inductor/test_cpu_select_algorithm.py -k test_quantized_linear
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/129470
Approved by: https://github.com/jgong5
ghstack dependencies: #128825, #129048, #129049, #129103, #129220, #129221",39.0,56.0,"torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/mkldnn_lowerings.py",2.0,3,1,0.452086199,1.0,1822.0,2.0,1164.0,30706.0,76783.0,0.0,,0.0,1
pytorch,fc6985ecebb9f2f1cc7f5c1a6224c159aad252ca,381b3d8f4b25487499f17fc99b2b8413a82b3ccf,Jeffrey Wan,jw3468@fb.com,Tue Apr 13 17:02:07 2021 -0700,1618333327.0,"Refactor get numerical jacobian to calculate wrt all outputs at once (#54378)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54378

### For release notes
`torch.autograd.gradcheck.get_numerical_jacobian` (not part of the public api) is being deprecated.

In the future, user code relying on this function will break because, among other changes, `get_numerical_jacobian` now returns `List[Tuple[torch.Tensor]]` instead of `List[torch.Tensor]`.

(more details if necessary)
For a `fn` that takes in M inputs and N outputs we now return a list of M N-tuples of jacobians where `output[i][j]` would represent the numerical jacobian w.r.t. to the ith input and the jth output. Previously `get_numerical_jacobian` returned a list of tensors where each tensor represents the jacobian w.r.t. to each of the M inputs and a specific output. Finally, the function passed in as the parameter `fn` should expect to handle individual parameters, where previously `fn` is required to expect its parameters wrapped in a tuple.

 --- end --

This PR addresses the comment here https://github.com/pytorch/pytorch/pull/53857#discussion_r595429639, to reduce the run-time of old gradcheck's get numerical jacobian by a factor of num_outputs. However, because very few ops actually return multiple outputs, there is not too much real speed up here.

The main benefit of doing this change as part of the refactor is that it helps us isolate the possible bugs that are specific to switching `get numerical jacobian` to run in a per output way vs all outputs at once. Much of the logic implemented here will be the same for the fast gradcheck case, so knowing for certain that everything should pass after this stage will make the next step much simpler.

The get_numerical_jacobian api is also being used in common_nn. So we update the callsite there as well.

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D27728720

Pulled By: soulitzer

fbshipit-source-id: ee0f90b4f26ddc5fdbe949c4965eaa91c9ed0bb8",341.0,93.0,"test/test_autograd.py,test/test_overrides.py,torch/autograd/gradcheck.py,torch/testing/_internal/common_nn.py",4.0,5,2,1.193563797,42.0,15507.0,4.0,663090.75,10710.0,23688.0,0.0,Corrective,1.0,1
pytorch,57e78c6654c9a657a4c7547b55b9b8c1e60f4259,3823b4b19761430fb2533aeab6c6e9e4fcf10a5e,Samantha Andow,samdow@fb.com,Thu Mar 03 18:39:40 2022 -0500,1646332780.0,[functorch] error on any use of autograd function (pytorch/functorch#558),37.0,2.0,"functorch/functorch/csrc/DynamicLayer.cpp,functorch/test/test_vmap.py",2.0,4,1,0.477071306,1.0,4357.0,2.0,0.5,849.0,1184.5,0.0,,0.0,1
pytorch,564456ac4472d44bc848681aa30c79132689ee82,382a47b493443c923f3cc2549e0540329bbb0e90,Kurt Mohler,kmohler@quansight.com,Thu Mar 18 13:39:27 2021 -0700,1616074767.0,"Add torch.linalg.vector_norm function (#51099)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/50214

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51099

Reviewed By: agolynski

Differential Revision: D27147360

Pulled By: mruberry

fbshipit-source-id: 1056f840e7027ad81971c9d1a9f952ab9648f1b5",626.0,69.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/SharedReduceOps.h,aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/api/include/torch/linalg.h,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",18.0,21,5,3.009478465,16.0,35168.0,4.0,364917.6666666667,9884.0,21888.5,0.0,Corrective,1.0,1
pytorch,5a1aa0e21e5c7316c001141ee8d8f5a2447345f3,3830998ac387c4ca60d3e5f17077b0116bbbf0c6,Michael Suo,suo@fb.com,Thu Aug 27 17:41:26 2020 -0700,1598550086.0,"[fx] When generating names, avoid shadowing builtins (#43653)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/43653

When nodes are created without an explicit name, a name is generated for
it based on the target. In these cases, we need to avoid shadowing
builtin names. Otherwise, code like:
```
a.foo.bar
```
results in pretty-printed code like:
```
getattr = a.foo
getattr_1 = getattr.bar
```

While this is technically allowed in Python, it's probably a bad idea,
and more importantly is not supported by TorchScript (where `getattr` is
hardcoded).

This PR changes the name generation logic to avoid shadowing all
builtins and langauge keywords. We already do this for PyTorch
built-ins, so just extend that logic. So now the generated code will
look like:

```
getattr_1 = a.foo
getattr_2 = getattr_1.bar
```
Fixes #43522

Test Plan: Imported from OSS

Reviewed By: jamesr66a

Differential Revision: D23357420

Pulled By: suo

fbshipit-source-id: 91e9974adc22987eca6007a2af4fb4fe67f192a8",31.0,8.0,"test/test_fx.py,torch/fx/graph.py",2.0,3,2,0.89049164,1.0,563.0,2.0,183278.0,4582.0,10607.5,0.0,Corrective,1.0,1
pytorch,94016b153a4130814d95181014f635b33eb117df,38340f59fddb90b1c94eb6feff3f38981b71daef,SsnL,tongzhou.wang.1994@gmail.com,Mon Nov 18 16:05:23 2019 -0800,1574093123.0,"randint accept generator=None (#29748)

Summary:
This PR fixes the inconsistent behavior of `randint`'s `generator=` kwarg. It does not accept `None`, which is inconsistent with how other random functions behave:
```
In [12]: torch.randint(0, 4, size=(2,3), generator=torch.Generator())
Out[12]:
tensor([[2, 0, 1],
        [0, 1, 3]])

In [13]: torch.randint(0, 4, size=(2,3), generator=None)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-13-a6bc6525a1e1> in <module>
----> 1 torch.randint(0, 4, size=(2,3), generator=None)

TypeError: randint() received an invalid combination of arguments - got (int, int, generator=NoneType, size=tuple), but expected one of:
 * (int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int high, tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int low, int high, tuple of ints size, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
 * (int low, int high, tuple of ints size, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)
```

Other random functions work fine:
```
In [9]: torch.bernoulli(torch.ones(3))
Out[9]: tensor([1., 1., 1.])

In [10]: torch.bernoulli(torch.ones(3), generator=None)
Out[10]: tensor([1., 1., 1.])
```

This PR also documents the `generator=` kwarg, and fixes https://github.com/pytorch/pytorch/issues/29683 since it's a related easy fix.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29748

Differential Revision: D18529951

Pulled By: ezyang

fbshipit-source-id: e956cc989decc94e9483fd4a30f9255240d7c07e",36.0,68.0,"test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp,torch/_torch_docs.py",3.0,5,3,1.380580707,41.0,22358.0,3.0,310579.3333333333,13236.0,36296.83333,0.0,Corrective,1.0,1
pytorch,0cced57cb8ff14ac04ddf624f4a51c33c864f170,38362fa9f3e96731fa96ecd85c77f5eb43e45530,gchanan,gregchanan@gmail.com,Tue Jun 12 01:18:02 2018 -0400,1528766282.0,"Prepare for moving 0-sized dimensions in TH/THC. (#8337)

This does the following:
1) makes nDimension an int64_t (to match ATen)
2) changes the dimension value to dim_ (so we catch direct usages)
3) provide an _dim() that provides access to the ""old"" view (so we can migrate functions one at a time)
4) have code call ->-_dim() instead of ->nDimension.",821.0,801.0,"aten/src/ATen/gen.py,aten/src/ATen/templates/TensorDense.cpp,aten/src/ATen/templates/TensorDerived.cpp,aten/src/TH/THTensor.hpp,aten/src/TH/THTensorApply.h,aten/src/TH/THTensorDimApply.h,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensorConv.cpp,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorRandom.cpp,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensor.hpp,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMathBlas.cu,aten/src/THC/generic/THCTensorMathMagma.cu,aten/src/THC/generic/THCTensorMathPairwise.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THCS/generic/THCSTensor.cpp,aten/src/THCS/generic/THCSTensor.cu,aten/src/THCS/generic/THCSTensor.hpp,aten/src/THCS/generic/THCSTensorMath.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/Im2Col.cu,aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/MultiMarginCriterion.cu,aten/src/THCUNN/generic/SparseLinear.cu,aten/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/SpatialAveragePooling.cu,aten/src/THCUNN/generic/SpatialConvolutionLocal.cu,aten/src/THCUNN/generic/SpatialConvolutionMM.cu,aten/src/THCUNN/generic/SpatialCrossMapLRN.cu,aten/src/THCUNN/generic/SpatialDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu,aten/src/THCUNN/generic/SpatialFullDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialMaxUnpooling.cu,aten/src/THCUNN/generic/SpatialSubSampling.cu,aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu,aten/src/THCUNN/generic/SpatialUpSamplingNearest.cu,aten/src/THCUNN/generic/TemporalConvolution.cu,aten/src/THCUNN/generic/TemporalMaxPooling.cu,aten/src/THCUNN/generic/TemporalRowConvolution.cu,aten/src/THCUNN/generic/TemporalUpSamplingLinear.cu,aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFullDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricUpSamplingTrilinear.cu,aten/src/THNN/generic/Col2Im.c,aten/src/THNN/generic/HardTanh.c,aten/src/THNN/generic/Im2Col.c,aten/src/THNN/generic/MultiLabelMarginCriterion.c,aten/src/THNN/generic/MultiMarginCriterion.c,aten/src/THNN/generic/SparseLinear.c,aten/src/THNN/generic/SpatialAdaptiveAveragePooling.c,aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c,aten/src/THNN/generic/SpatialAveragePooling.c,aten/src/THNN/generic/SpatialConvolutionLocal.c,aten/src/THNN/generic/SpatialConvolutionMM.c,aten/src/THNN/generic/SpatialConvolutionMap.c,aten/src/THNN/generic/SpatialDilatedConvolution.c,aten/src/THNN/generic/SpatialDilatedMaxPooling.c,aten/src/THNN/generic/SpatialFullConvolutionMap.c,aten/src/THNN/generic/SpatialFullDilatedConvolution.c,aten/src/THNN/generic/SpatialGridSamplerBilinear.c,aten/src/THNN/generic/SpatialMaxUnpooling.c,aten/src/THNN/generic/SpatialReflectionPadding.c,aten/src/THNN/generic/SpatialReplicationPadding.c,aten/src/THNN/generic/SpatialSubSampling.c,aten/src/THNN/generic/SpatialUpSamplingBilinear.c,aten/src/THNN/generic/SpatialUpSamplingNearest.c,aten/src/THNN/generic/Sqrt.c,aten/src/THNN/generic/Square.c,aten/src/THNN/generic/Tanh.c,aten/src/THNN/generic/TemporalConvolution.c,aten/src/THNN/generic/TemporalMaxPooling.c,aten/src/THNN/generic/TemporalReflectionPadding.c,aten/src/THNN/generic/TemporalReplicationPadding.c,aten/src/THNN/generic/TemporalRowConvolution.c,aten/src/THNN/generic/TemporalSubSampling.c,aten/src/THNN/generic/TemporalUpSamplingLinear.c,aten/src/THNN/generic/TemporalUpSamplingNearest.c,aten/src/THNN/generic/VolumetricAdaptiveAveragePooling.c,aten/src/THNN/generic/VolumetricAdaptiveMaxPooling.c,aten/src/THNN/generic/VolumetricAveragePooling.c,aten/src/THNN/generic/VolumetricConvolution.c,aten/src/THNN/generic/VolumetricConvolutionMM.c,aten/src/THNN/generic/VolumetricDilatedConvolution.c,aten/src/THNN/generic/VolumetricDilatedMaxPooling.c,aten/src/THNN/generic/VolumetricFullDilatedConvolution.c,aten/src/THNN/generic/VolumetricGridSamplerBilinear.c,aten/src/THNN/generic/VolumetricMaxUnpooling.c,aten/src/THNN/generic/VolumetricReplicationPadding.c,aten/src/THNN/generic/VolumetricUpSamplingNearest.c,aten/src/THNN/generic/VolumetricUpSamplingTrilinear.c,aten/src/THS/generic/THSTensor.cpp,aten/src/THS/generic/THSTensor.hpp,aten/src/THS/generic/THSTensorMath.c",107.0,16,1,5.968331536,9.0,41281.0,20.0,4047209.476635514,700.0,3772.5,0.0,,0.0,1
pytorch,53c0d91db9fe1bebb6d8fadff9dc7f26f16b6c7c,383c0a385886aa0af02b943ccbd495c1bcc955ac,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Oct 01 14:29:17 2021 -0700,1633098557.0,"Fix internal assert failure for torch.all and torch.any with requires_grad=True (#65714)

Summary:
This PR fixes https://github.com/pytorch/pytorch/issues/58547.
I added an OpInfo-based test that fails on master and passes with the
proposed changes.

cc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 mruberry

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65714

Reviewed By: saketh-are, mruberry

Differential Revision: D31248307

Pulled By: albanD

fbshipit-source-id: 041eaa9b744c3043f78dd8ae5f457f67c311df4f",30.0,4.0,"test/test_ops.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",3.0,6,3,1.522189872,14.0,13823.0,3.0,50663.0,15890.0,36717.5,0.0,Corrective,1.0,1
pytorch,77abb6938e6cee510a003e7316e2a1429fcc9d64,385165ec674b764eb42ffe396f98fadd08a513eb,Jerry Zhang,jerryzh168@gmail.com,Fri Apr 24 17:19:05 2020 -0700,1587748745.0,"[reland][quant] QuantizedCUDA implementation (#36936) (#37081)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37081

Closes https://github.com/pytorch/pytorch/issues/30813

Relanding of https://github.com/pytorch/pytorch/pull/35463

1. Tensor quantization logic(quantize_*) is moved to the aten/native/quantized. Previously all logic for tensor quantization lived in the aten/quantized/Quantizer.cpp file, and started to become complicated and hard to read. This problem should be addressed in refactoring PR. Still, I reworked this partially because I had to add tensor quantization logic for CUDA, and it was native to move everything to the aten/native/quantized.
2. Requirements to run CUDA_tensor_apply* was eased to process any tenser that lives on the CUDA device(QuantizedCUDA included).
3. All quantized data types now have a default constructor. NVCC refuses to compile any gpu_kernel or CUDA_tensor_apply* without them.
4. Minor changes in many files to register QuantizedCUDA backend.
5. test_quantized_tensor is extended to process QuantizedCUDA backend where possible.

Test Plan: Imported from OSS

Differential Revision: D21206694

Pulled By: jerryzh168

fbshipit-source-id: c7433aad9c095a34c57e6dddd128b5c5d9292373",1899.0,1274.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/cuda/CUDAApplyUtils.cuh,aten/src/ATen/gen.py,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/Copy.cpp,aten/src/ATen/native/quantized/Copy.h,aten/src/ATen/native/quantized/QTensor.cpp,aten/src/ATen/native/quantized/TensorFactories.cpp,aten/src/ATen/native/quantized/affine_quantizer.cpp,aten/src/ATen/native/quantized/affine_quantizer.h,aten/src/ATen/native/quantized/cpu/int_repr_quant.cpp,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/make_per_tensor_quantized_tensor.cpp,aten/src/ATen/native/quantized/cpu/qrelu.cpp,aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp,aten/src/ATen/native/quantized/cuda/affine_quantizer.cu,aten/src/ATen/native/quantized/cuda/int_repr_quant.cu,aten/src/ATen/native/quantized/cuda/make_per_tensor_quantized_tensor.cu,aten/src/ATen/preprocess_declarations.py,aten/src/ATen/quantized/Quantizer.cpp,aten/src/ATen/quantized/Quantizer.h,aten/src/ATen/test/quantized_test.cpp,c10/core/Backend.h,c10/core/DispatchKey.h,c10/core/TensorImpl.h,c10/core/TensorOptions.h,c10/util/qint32.h,c10/util/qint8.h,c10/util/quint8.h,test/quantization/test_quantized_tensor.py,torch/csrc/utils/tensor_layouts.cpp",34.0,22,4,3.325181735,12.0,17716.0,5.0,207351.9411764706,1362.0,3657.0,0.0,Feature Addition,0.0,1
pytorch,bdeee47d33d7f0e74753c02e118f8ce3db1ef80a,3853d5da97ead031d7d77f39fe7ce2de243ce369,Richard Zou,zou3519@users.noreply.github.com,Thu Oct 26 17:54:19 2017 -0400,1509040459.0,"Add reduce keyword to NLLLoss and NLLLoss2d (#3080)

* API changes

* Implement reduce for THNN ClassNLLCriterion

* Implement reduce keyword for THCUNN ClassNLLCriterion

* Implement reduce for THNN SpatialClassNLLCriterion

* Implement reduce for THCUNN SpatialClassNLLCriterion

* Make legacy NLLLoss work

* Docs for NLLLoss reduce

* reduce keyword for double backwards NLLLoss

* reduce=False tests

* Addressed comments

* Fix trailing whitespace

* Fix test failures in legacy nn

* Rebase: add reduce keyword to aten declarations of NLLLoss

* Add reference functions for all NLLLoss and NLLLoss2d test cases

* Replaced slow get/set fns. Don't use int64_t in kernels.

* Use TH_INDEX_BASE in NLLLoss for consistency

* Fix legacy ClassNLLCriterion tests",674.0,58.0,"test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/legacy/nn/ClassNLLCriterion.py,torch/legacy/nn/SpatialClassNLLCriterion.py,torch/lib/ATen/nn.yaml,torch/lib/THCUNN/ClassNLLCriterion.cu,torch/lib/THCUNN/SpatialClassNLLCriterion.cu,torch/lib/THCUNN/generic/ClassNLLCriterion.cu,torch/lib/THCUNN/generic/SpatialClassNLLCriterion.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THNN/generic/ClassNLLCriterion.c,torch/lib/THNN/generic/SpatialClassNLLCriterion.c,torch/lib/THNN/generic/THNN.h,torch/nn/_functions/thnn/auto_double_backwards.py,torch/nn/functional.py,torch/nn/modules/loss.py",17.0,16,3,3.478635219,39.0,12951.0,1.0,91190.0,42.0,880.0,0.0,Corrective,1.0,1
pytorch,de902b5d02ef0b264d4ce1406b8e16c7ba1e2680,385773cb7784ecef0d37df241d7182f390138627,mingfeima,mingfei.ma@intel.com,Wed Jan 12 22:18:38 2022 -0800,1642025918.0,"add BFloat16 support for MaxPool2d on CPU (#56903)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56903

Test Plan: Imported from OSS

Reviewed By: mikaylagawarecki

Differential Revision: D28836791

Pulled By: VitalyFedyunin

fbshipit-source-id: e03d55cc30dfa3628f096938fbad34b1031948af",186.0,10.0,"aten/src/ATen/native/cpu/MaxPoolKernel.cpp,test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,0.644705968,43.0,36127.0,3.0,2507066.0,18313.0,43391.5,0.0,Feature Addition,0.0,1
pytorch,6aaa14f5fe267f9179dad8f76f26652d1507ffe2,385913be1ca193974142f002f2fb80d7a2e58f5b,Alfredo Canziani,alfredo.canziani@gmail.com,Wed Feb 15 05:07:20 2017 -0500,1487135240.0,"Fix class torch.nn.ConvTransposeNd documentation (#739)

There is no `dilation`
`output_padding` doc was missing",17.0,19.0,torch/nn/modules/conv.py,1.0,3,1,0,21.0,621.0,1.0,462922.0,416.0,3889.575745,0.0,Corrective,1.0,1
pytorch,8a21cac3c3dfc3aa646517dfd0822fc94fda316f,3859aace20dbc0af5fc2358670b6060c63883548,Nikita Shulga,nshulga@meta.com,Wed Dec 14 19:51:00 2022 +0000,1671047460.0,"[MPS] Skip tests broken on Ventura (#90843)

Also add `torch.backends.mps.is_macos13_or_newer`
See https://github.com/pytorch/pytorch/issues/85758

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90843
Approved by: https://github.com/kulinseth, https://github.com/albanD",29.0,0.0,"test/test_mps.py,torch/_C/__init__.pyi.in,torch/backends/mps/__init__.py,torch/csrc/Module.cpp",4.0,6,2,1.662017606,43.0,11276.0,4.0,4622162.75,10547.0,24077.0,0.0,Feature Addition,0.0,1
pytorch,a24f6c13a368c407b408045e0f82c8b18f991036,3861520603e212bbbda57b2a555ffada0c189c0d,Jerry Zhang,jerryzh@fb.com,Tue Jul 23 01:30:37 2019 -0700,1563845437.0,"Verify flatten works for quantized Tensor (#23121)

Summary:
Added a test in `test_torch.py`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/23121
ghstack-source-id: 86983227

Differential Revision: D16391409

fbshipit-source-id: 04e72b2f753a0a6ddbf58d55b794e443b18a2156",41.0,35.0,test/test_torch.py,1.0,1,1,0,40.0,12419.0,1.0,38695.0,10088.0,29143.83333,0.0,Feature Addition,0.0,1
pytorch,ed09704899253be4c3810f96202655d4ed3bfb04,388cfdf2ac58c79fe5427fe545e181fa601cd671,Mike Ruberry,38511765+mruberry@users.noreply.github.com,Wed Sep 18 17:22:43 2019 -0700,1568827363.0,"Removes torchtest, expands generic device testing (#26374)

Summary:
- Removes torchtest
- <s>Moves test_torch tests skipped on ROCm to generic device test class</s>
- Creates test_nn generic device test class

Next: adding dtypes to generic device testing framework.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26374

Test Plan: Change is to tests themselves.

Differential Revision: D17442218

Pulled By: mruberry

fbshipit-source-id: d7e4451d09fc9049478b35a7efb8bb580071e8c8",715.0,822.0,"test/common_utils.py,test/test_nn.py,test/test_torch.py",3.0,1,1,0.220012841,43.0,24469.0,3.0,102448.0,11494.0,32357.33333,0.0,Feature Addition,0.0,1
pytorch,3ada9da808a16ba712242793d3b09c3205f78f41,38b9598685386fc6dd8e982165e930ca3ce838f1,Sam Gross,sgross@fb.com,Tue Jun 06 21:15:31 2017 -0700,1496783731.0,"Added GLU (gated linear unit)

From https://arxiv.org/abs/1612.08083",46.0,2.0,"test/test_nn.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py",4.0,4,2,1.492754192,30.0,4793.0,1.0,15391.0,945.0,11129.78627,0.0,Feature Addition,0.0,1
pytorch,f2ba3c1621f798f7c91ae7604ca24704fff2a815,38ed398580208a79e92368096d87674793c0cd4b,Jordan Fix,jfix@fb.com,Mon Dec 14 02:04:13 2020 -0800,1607911453.0,"[fx] Add constant folding pass (#48443)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48443

Add a constant folding pass in FX:
- Iterate over an input graph and tag what nodes are fully constant, i.e. either `get_attr` nodes, or nodes with all inputs that are either `get_attr` or constant
- Use `model_transform.split_by_tags()` to split the graph into two
- Look for the `output` node in the constant graph to get names of attrs that will be folded
- Iterate over the non-constant graph and replace placeholders that are using the same name as the attrs with a `get_attr` as well as a dummy attr on the module
- Return these two graphs in a new `FoldedGraphModule`, which is a normal GraphModule but also stores the constant graph on the side along with a `run_folding()` method that will run const folding and update the dummy parameters with the actual folded parameters

Test Plan: Added a couple tests

Reviewed By: 842974287

Differential Revision: D25033996

fbshipit-source-id: 589c036751ea91bb8155d9be98af7dbc0552ea19",544.0,1.0,"test/fx/test_fx_const_fold.py,torch/fx/experimental/const_fold.py,torch/fx/graph.py",3.0,5,2,1.03124239,1.0,848.0,1.0,181227.0,7465.0,16729.0,0.0,Feature Addition,0.0,1
pytorch,f11fb319bd8b3e4387ee0ee13ea1e8d70fbf40bd,38f87cc9c42838cac976ec52ea89b4c5e99acaf4,SsnL,SsnL@users.noreply.github.com,Sat Oct 14 06:52:01 2017 -0400,1507963921.0,"Limit print scale by sys.float_info (#3113)

* limit print scale by sys.float_info

* test print tiny/huge values in test_print

* fix lint",9.0,1.0,"test/test_torch.py,torch/_tensor_str.py",2.0,2,2,0.970950594,37.0,4843.0,1.0,6464.0,1987.0,23825.85823,0.0,Corrective,1.0,1
pytorch,8b8d1c970f0703ebe8a9830978cee21d2cd3d590,39228e7d1da61b054e983f6e4f35485b09e59393,Sam Andow,samdow@fb.com,Mon Jan 10 20:06:15 2022 +0000,1641845175.0,[functorch] instance norm,22.0,15.0,"functorch/functorch/csrc/BatchRulesDecompositions.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,1.040333845,1.0,5190.0,4.0,1.25,785.0,1075.5,0.0,,0.0,1
pytorch,ad1863dd3b78c0ab5ccb69518a03dc52d73c350d,392de819f806a96ae4e020d55217febd88d8d34a,Jeff Johnson,jhj@fb.com,Sat Jun 11 00:37:24 2016 -0700,1465605444.0,reduce and BLAS work,2398.0,1438.0,"CMakeLists.txt,THCBlas.cu,THCBlas.h,THCGenerateAllTypes.h,THCHalf.cu,THCHalf.h,THCNumerics.cuh,THCReduce.cuh,THCReduceAll.cuh,THCTensorCopy.cu,THCTensorMasked.cu,THCTensorMath.cu,THCTensorMath.h,THCTensorMath2.cu,THCTensorMathBlas.cu,THCTensorMathCompare.cu,THCTensorMathCompareT.cu,THCTensorMathMagma.cu,THCTensorMathPairwise.cu,THCTensorMathPointwise.cu,THCTensorMathReduce.cu,THCTensorMathTransformReduce.cu,THCTensorTypeUtils.cu,THCTensorTypeUtils.cuh,generic/THCTensorMasked.cu,generic/THCTensorMasked.h,generic/THCTensorMathBlas.cu,generic/THCTensorMathBlas.h,generic/THCTensorMathCompare.cu,generic/THCTensorMathCompare.h,generic/THCTensorMathCompareT.cu,generic/THCTensorMathCompareT.h,generic/THCTensorMathReduce.cu,generic/THCTensorMathReduce.h",34.0,1,1,4.225000938,39.0,5481.0,6.0,5832364.590909091,122.0,115.1254274,0.0,,0.0,1
pytorch,c43896732e23d0312b54c5156d487470a6a48cf3,3940e7f0a71a58c187818f46ba1e9e990f63a078,Chenguang Xi,cxi@fb.com,Tue Mar 20 04:41:39 2018 -0700,1521520899.0,"Support computing averaged norm in blob magnitdue visualization

1. support the LpNorm operator to calculate the average LpNorm by adding one more boolean argument, i.e., LpNorm(average = true) = LpNorm(x) / size of (x)

2. integrate the average option into visualization framework",130.0,14.0,"caffe2/operators/lpnorm_op.cc,caffe2/operators/lpnorm_op.h,caffe2/python/modeling/compute_norm_for_blobs.py,caffe2/python/modeling/compute_norm_for_blobs_test.py,caffe2/python/operator_test/lpnorm_op_test.py",5.0,5,1,1.808497048,3.0,471.0,4.0,6340077.0,31.0,109.5,0.0,Feature Addition,0.0,1
pytorch,5deacb5bce8bfb41662b57a61ea635c44d2a347f,39434ee2e44fb2770db22dc83a3c9ca1a664a8da,Emanuel JÃ¶bstl,emanuel.joebstl@gmail.com,Wed Sep 20 13:19:29 2017 +0200,1505913569.0,Added LPPool1d. (#2783),71.0,3.0,"test/test_nn.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py",4.0,4,2,1.323020546,36.0,6409.0,3.0,184996.75,1759.0,24788.05562,0.0,Feature Addition,0.0,1
pytorch,138ee75a3b07537712d0df2c87d6fc21c0fbfa6c,3975a2676e7b7f8c6c4a27ba6e6505ba965f48c6,Adam Paszke,adam.paszke@gmail.com,Mon Jan 23 22:34:52 2017 +0100,1485210892.0,Fix invalid DECREF in torch.Size constructor,11.0,4.0,"test/test_torch.py,torch/csrc/Size.cpp",2.0,3,2,0.996791632,21.0,2919.0,1.0,85986.0,388.0,4909.976424,0.0,Corrective,1.0,1
pytorch,ef1c107be523c23f883a7f76963b70a47638b5f5,3995fb1840bfaeb4901cf0965b9098c86443d0bf,BowenBao,bowbao@microsoft.com,Thu Jun 17 22:47:49 2021 -0700,1623970069.0,"Add new_ones symbolic (#59255) (#59539)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59539

Add new_ones symbolic in PT-ONNX exporter

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D29046603

Pulled By: SplitInfinity

fbshipit-source-id: e7420c7b543c33e3640e62461d08ff4d5843eda7

Co-authored-by: Shubham Bhokare <shubhambhokare@gmail.com>",16.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.954434003,3.0,12440.0,1.0,185107.0,13121.0,29696.0,0.0,Feature Addition,0.0,1
pytorch,42f131c09f485873a64034da3e7587e0e30678e3,39ab5bcba86ddc84720b93564cac637952abd677,Soumith Chintala,soumith@fb.com,Wed Jan 04 08:11:42 2017 -0500,1483517502.0,"fix MaxPool1d,2d,3d docs for rst",131.0,52.0,"torch/nn/modules/conv.py,torch/nn/modules/pooling.py",2.0,3,1,0.668710135,20.0,950.0,2.0,71172.0,303.0,6387.224559,0.0,Corrective,1.0,1
pytorch,241a1e0f52bd0bff05b548674fa3e4d635195528,39d4814933470490768a75f4ac7768bcb06876f0,Tao He,sighingnow@gmail.com,Wed Apr 25 08:25:38 2018 +0800,1524644738.0,Make any and all on ByteTensor behave like sum/prod. (#4627),423.0,38.0,"aten/src/ATen/Declarations.cwrap,aten/src/TH/generic/THTensorMath.c,aten/src/TH/generic/THTensorMath.h,aten/src/THC/THCTensorMath.h,aten/src/THC/THCTensorMathReduce.cu,test/test_torch.py,torch/_tensor_docs.py,torch/lib/THD/master_worker/common/Functions.hpp,torch/lib/THD/master_worker/master/generic/THDTensorMath.cpp,torch/lib/THD/master_worker/master/generic/THDTensorMath.h,torch/lib/THD/master_worker/worker/Dispatch.cpp,torch/lib/THD/master_worker/worker/dispatch/TensorMath.cpp",12.0,16,3,2.496103833,39.0,20367.0,6.0,1319245.6666666667,1047.0,6972.172317,0.0,,0.0,1
pytorch,24a2f2e3a047963fd446c5995d6834f90e364bb0,3a07228509b51b82322659129e9cd1858c3ec74f,Sam Gross,colesbury@gmail.com,Fri Jan 13 20:22:57 2017 -0500,1484338977.0,Add ConvTranspose1d module (#449),213.0,137.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/conv.py",5.0,6,3,0.605045411,21.0,2827.0,2.0,166527.4,321.0,6409.224559,0.0,Feature Addition,0.0,1
pytorch,b3710a2e010c665e33782b682267a8abaf3c861f,3a335427b06c908fa7994398ac6ff28782591545,Fritz Obermeyer,fritz.obermeyer@gmail.com,Tue Jan 09 08:44:59 2018 -0800,1515487499.0,"Start framework for kl_divergence(-,-) in torch.distributions (#4525)",166.0,3.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/kl.py",4.0,5,3,1.074588395,8.0,1521.0,1.0,37412.0,903.0,6706.172317,0.0,,0.0,1
pytorch,375687839e7a8724cf294b3983a173110123d5ed,3a44d269ac1eae5fdfb6655bcaa8fa8be589dbb3,Nikita Shulga,nshulga@fb.com,Thu Apr 22 21:22:18 2021 -0700,1619126538.0,"Add periodic_ prefix to all jobs run by cron (#56695)

Summary:
To make them more easily distinguishable in the HUD

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56695

Reviewed By: walterddr, samestep

Differential Revision: D27939938

Pulled By: malfet

fbshipit-source-id: e0abd1a6bc931a89f2aa5c6e2d8ebb471c461051",24.0,24.0,".circleci/config.yml,.circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml",2.0,3,1,1,3.0,8415.0,2.0,39607.5,11173.0,24698.0,0.0,Corrective,1.0,1
pytorch,45bf3f6216a9f1321df3669976f7e96f9965984a,3a5427baf45e53f6424668b167fd58b126fd2e37,Edward Z. Yang,ezyang@meta.com,Tue Apr 25 18:50:54 2023 -0700,1682448654.0,"Add torch.utils._content_store (#99809)

Implements a simple content-addressable store for storages (with tensors implemented as cheap references on top), enabling incremental serialization of tensors to disk, which I intend to use in the accuracy repro extractor.  Check the comment at the top of torch/utils/_content_store.py for more details on the intended use case.

One major piece of this PR is implementing the content hash for tensors.  For our prospective use case, we may need to repeatedly hash up to 80 GB of tensor data every time we snapshot (and we may snapshot multiple times).  Using a conventional cryptographic hash and hashing each snapshot would likely take on order of minutes, which seemed too slow to me.  So instead, I implemented a crappy hash function that can be run on GPU.  It is at least somewhat theoretically grounded: using random parameters generated by Philox, we use the standard shift-multiply and xor sum universal hash family.  The hash function is a bit dorky though; instead of properly doing 160-bit math, it just runs 32-bit hash five times and cats them together.  By the way, this sets the first precedent for kernel in PyTorch library which MUST be torch.compile'd to be run (in fact, this kernel does not run in eager mode because of the use of xor_sum, which doesn't actually exist in ATen.)

I had to add a few more primitives to inductor, namely randint (over the entire int range) and xor_sum.  Fortunately, these primitives are natively supported by Triton/C++, and so they were very easy to plumb through.  xor_sum is exposed as a prim, while randint special cases on when low/high span the entire 32-bit signed integer range.

Thanks to Jeff Johnson for letting me bounce ideas of him on a Saturday morning lol.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99809
Approved by: https://github.com/voznesenskym",352.0,9.0,".lintrunner.toml,test/inductor/test_cpu_repro.py,test/test_content_store.py,tools/pyi/gen_pyi.py,torch/_C/__init__.pyi.in,torch/_dynamo/skipfiles.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/cpp_prefix.h,torch/_inductor/codegen/triton.py,torch/_inductor/ir.py,torch/_inductor/lowering.py,torch/_prims/__init__.py,torch/testing/_internal/common_cuda.py,torch/utils/_content_store.py,torchgen/api/python.py",15.0,15,4,2.323207001,5.0,23945.0,12.0,547323.6153846154,15111.0,34185.0,0.0,Feature Addition,0.0,1
pytorch,8f7ae770403e9a84551ae2504e6646e2322bae14,3a592730d5cf5d75c644a152269fc1b2044716bc,Hui Guo,huiguo@fb.com,Tue Aug 03 01:32:21 2021 -0700,1627954341.0,"[nnc] Simplify i%100 to i if i is less than 100; fixed #52580 (#60693)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/60693

Test Plan: Imported from OSS

Reviewed By: ZolotukhinM

Differential Revision: D29375938

Pulled By: huiguoo

fbshipit-source-id: 1388729c5b93805cb156efa53e8823d5462885bf",42.0,0.0,"test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",2.0,7,2,0.998363673,1.0,7940.0,1.0,75.0,14345.0,32808.5,0.0,Corrective,1.0,1
pytorch,8548a21c004fefc6576254b9f651a0635f824d06,3a63a939d4d2549fc970725e7d16d9c44a0314a8,Shen Li,shenli@fb.com,Wed Jul 15 15:12:53 2020 -0700,1594825973.0,"Revert D22517785: [pytorch][PR] Enable TF32 support for cuBLAS

Test Plan: revert-hammer

Differential Revision:
D22517785 (https://github.com/pytorch/pytorch/commit/288ece89e16fe915ae0cfb2d948447e90ab03b1a)

Original commit changeset: 87334c893561

fbshipit-source-id: 0a0674f49c1bcfc98f7f88af5a8c7de93b76e458",25.0,248.0,"aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cuda/CublasHandlePool.cpp,aten/src/ATen/native/cuda/MiscUtils.h,aten/src/THC/THCBlas.cu,docs/source/notes/cuda.rst,test/test_cuda.py,test/test_torch.py,torch/backends/cuda/__init__.py,torch/csrc/Module.cpp,torch/testing/_internal/common_cuda.py",12.0,17,4,3.008171762,45.0,25795.0,2.0,64526.16666666666,3604.0,8530.0,0.0,,0.0,1
pytorch,09d359dfd937ab0c9a0a355cfa4d23d1c422a668,3a72662d018b853636eb1802a559e236920a8584,Hong Xu,hong@topbug.net,Wed Nov 13 07:40:53 2019 -0800,1573630853.0,"Restructure comparison ops so as to better support XLA dispatch (#29591)

Summary:
Per ailzhang's suggestion in https://github.com/pytorch/pytorch/pull/28162#discussion_r344361926, this PR changes the implementation of binary comparison and logical ops
to those of unary ops in UnaryOps.cpp. The reason is that the call should eventually go through
at::op_out (e.g., at::logical_xor_out).

The check for Boolean output tensor is also removed, because:

- This check should only apply to _out functions but not on other variants. However, other variants
  must go through the _out variant eventually.
- It does not have a clear motivation and seems unnecessary.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29591

Differential Revision: D18460113

Pulled By: ailzhang

fbshipit-source-id: 58d501e59335186b3b8cc7d80ee9eed74efeeac8",60.0,65.0,"aten/src/ATen/native/BinaryOps.cpp,test/test_torch.py",2.0,5,2,0.242292189,40.0,14741.0,1.0,26906.0,13093.0,36041.33333,0.0,,1.0,1
pytorch,932ec8aa9f90a4d41583076ff3f5963c02cee27f,3a8d7463bd024dcbd00d04a323cdb10dfe867f18,Iurii Zdebskyi,iuriiz@fb.com,Wed Jul 10 04:47:47 2019 -0700,1562734067.0,"Enabled BFloat16 storage (#21523)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21523
ghimport-source-id: 698b3cbd6b21c09b9ff8bf8011980df8e35c33b0

Test Plan: Imported from OSS

Differential Revision: D15819368

Pulled By: izdeby

fbshipit-source-id: f6b3bba7b3ca8ee677bd80a231dbb3920c07d61c",265.0,27.0,"aten/src/ATen/DLConvertor.cpp,aten/src/ATen/function_wrapper.py,aten/src/TH/CMakeLists.txt,aten/src/TH/THGenerateBFloat16Type.h,aten/src/TH/THStorageFunctions.cpp,aten/src/TH/THStorageFunctions.h,aten/src/TH/THTensor.cpp,aten/src/TH/THTensor.h,aten/src/TH/THTensor.hpp,aten/src/TH/generic/THStorage.h,aten/src/TH/generic/THStorageCopy.cpp,aten/src/TH/generic/THStorageCopy.h,aten/src/TH/generic/THTensor.h,aten/src/THC/CMakeLists.txt,aten/src/THC/THCGenerateBFloat16Type.h,aten/src/THC/THCStorage.cpp,aten/src/THC/THCStorage.cu,aten/src/THC/THCStorage.h,aten/src/THC/THCStorageCopy.cpp,aten/src/THC/THCStorageCopy.cu,aten/src/THC/THCStorageCopy.h,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensor.cu,aten/src/THC/THCTensor.h,aten/src/THC/THCTensor.hpp,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorCopy.h,aten/src/THC/generic/THCStorage.h,aten/src/THC/generic/THCStorageCopy.cpp,aten/src/THC/generic/THCStorageCopy.cu,aten/src/THC/generic/THCStorageCopy.h,aten/src/THC/generic/THCTensor.h,c10/core/ScalarType.h,c10/test/util/bfloat16_test.cpp,c10/util/BFloat16.h,test/test_torch.py,torch/__init__.py,torch/_storage_docs.py,torch/csrc/Module.cpp,torch/csrc/Storage.cpp,torch/csrc/Storage.h,torch/csrc/byte_order.cpp,torch/csrc/byte_order.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Storage.cpp,torch/csrc/cuda/Storage.h,torch/csrc/cuda/serialization.cpp,torch/csrc/cuda/serialization.h,torch/csrc/cuda/utils.cpp,torch/csrc/cuda/utils.h,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/csrc/serialization.cpp,torch/csrc/serialization.h,torch/csrc/utils.cpp,torch/csrc/utils.h,torch/csrc/utils/tensor_dtypes.cpp,torch/cuda/__init__.py,torch/storage.py",59.0,19,4,5.271712375,43.0,21066.0,19.0,7494805.192982456,9848.0,28578.83333,0.0,,0.0,1
pytorch,b474c351dd6625641d430899a027023eff3dabbc,3ada2e0d64b40622e823b8135d2bbbc74e6526b9,Jianyu Huang,jianyuhuang@fb.com,Fri Jan 24 05:28:03 2020 -0800,1579843683.0,"[pytorch][embeddingbag] Parallelize the EmbeddingBag operator (#4049)

Summary:
Pull Request resolved: https://github.com/pytorch/glow/pull/4049

Pull Request resolved: https://github.com/pytorch/pytorch/pull/27477

We would like to add the intra-op parallelization support for the EmbeddingBag operator.

This should bring speedup for the DLRM benchmark:
https://github.com/pytorch/pytorch/pull/24385

Benchmark code:
```
from __future__ import absolute_import, division, print_function, unicode_literals

import torch
import time

eb = torch.nn.EmbeddingBag(1000000, 64, mode='sum')

input = torch.LongTensor(1500).random_(0, 1000000)
offsets = torch.zeros(64, dtype=torch.int64)

niter = 10000
s = time.time()
for _ in range(niter):
    out = eb(input, offsets)
time_per_iter = (time.time() - s) / niter
print('time_per_iter', time_per_iter)
print('GB/s', (input.numel() * 64 * 4 + out.numel() * 4) / time_per_iter / 1e9)
```

The following results are single core on Skylake T6:
- Before our change (with the original caffe2::EmbeddingLookup)
time_per_iter 6.313693523406982e-05
GB/s 6.341517821789133

- After our change using the EmbeddingLookupIdx API which takes the offsets instead of lengths.
time_per_iter 5.7627105712890626e-05
GB/s 6.947841559053659

- With Intel's PR: https://github.com/pytorch/pytorch/pull/24385
time_per_iter 7.393271923065185e-05
GB/s 5.415518381664018

For multi-core performance, because Clang doesn't work with OMP, I can only see the single-core performance on SKL T6.
ghstack-source-id: 97124557

Test Plan:
With D16990830:
```
buck run mode/dev //caffe2/caffe2/perfkernels:embedding_bench
```

With D17750961:
```
buck run mode/opt //experimental/jianyuhuang/embeddingbag:eb
buck run mode/opt-lto //experimental/jianyuhuang/embeddingbag:eb
```

OSS test
```
python run_test.py -i nn -- TestNNDeviceTypeCPU.test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu
```

Buck test
```
buck test mode/dev-nosan //caffe2/test:nn -- ""test_EmbeddingBag_per_sample_weights_and_new_offsets_cpu""

OMP_NUM_THREADS=3 buck test mode/opt -c pytorch.parallel_backend=tbb //caffe2/test:nn -- ""test_EmbeddingBag_per_sample_weights_and_new_offsets""  --print-passing-details
```

Generate the AVX2 code for embedding_lookup_idx_avx2.cc:
```
python hp_emblookup_codegen.py --use-offsets
```

Differential Revision: D17768404

fbshipit-source-id: 8dcd15a62d75b737fa97e0eff17f347052675700",395.0,255.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,caffe2/perfkernels/embedding_lookup_idx.cc,caffe2/perfkernels/embedding_lookup_idx.h,caffe2/perfkernels/embedding_lookup_idx_avx2.cc,caffe2/perfkernels/hp_emblookup_codegen.py,test/onnx/expect/TestOperators.test_embedding_bags.expect,test/test_nn.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/nn/functional/embedding.h,torch/csrc/api/include/torch/nn/options/embedding.h,torch/csrc/api/src/nn/modules/embedding.cpp,torch/nn/functional.py,torch/nn/modules/sparse.py,torch/nn/modules/sparse.pyi.in,torch/onnx/symbolic_opset9.py",17.0,26,5,2.642818557,44.0,30530.0,14.0,6638468.705882353,14371.0,39063.83333,0.0,Feature Addition,0.0,1
pytorch,fb9f89507ae763729e2c947efbf3200f967c679d,3adc8f8cf76844741f403ffa6f3f10653776a1fd,76181208+imaginary-person@users.noreply.github.com,76181208+imaginary-person@users.noreply.github.com,Fri Feb 19 06:58:02 2021 -0800,1613717882.0,"Enable min & max for Float16 & BFloat16 (#51244)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/50790.

Added `min()` & `max()` support for `Float16` & `BFloat16`.
CUDA already supported these ops on `Float16`, so the other three combinations had to be enabled.
`OpInfo`s for `min` & `max` were also added, and their sample inputs were removed from `method_tests()`.

### MORE INFO
The (slightly) long-term goal is to add dispatch for `min()` & `max()` related operations on CPU & CUDA for `Float16` & `BFloat16`,
wherever they aren't present already:
1. `amin()`
2. `argmax()`
3. `amax()`
4. `argmin()`
5. `torch._aminmax()`
6. `torch.clamp()` on CPU. Was already supported on CUDA
7. `min()` (in this PR)
8. `max()` (in this PR)
9. `minimum()`
10. `maximum()`

I'll submit separate PRs for the other ops.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51244

Reviewed By: jbschlosser

Differential Revision: D26503455

Pulled By: anjali411

fbshipit-source-id: c32247f214e9272ca2e4322a23337874e737b140",117.0,41.0,"aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu,test/test_reductions.py,torch/testing/_internal/common_methods_invocations.py",5.0,10,3,0.791272045,3.0,6502.0,5.0,3921782.6,9030.0,20233.0,0.0,Corrective,1.0,1
pytorch,cc07f968f8b40cef758ad1a09cdec415ad5bd1ba,3aeb78079bcd68282fe9117088e138b77318e288,Roy Li,royboy@fb.com,Sat Mar 09 00:39:04 2019 -0800,1552091944.0,"Change Dispatch.h to use ScalarType over Type

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17527

Reviewed By: zou3519

Differential Revision: D14235395

fbshipit-source-id: 3f53e33f6794f1f14c2edf79014b8ef8397822c5",423.0,432.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/detail/ScalarTypeConversions.h,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/AdaptiveAveragePooling.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/Lerp.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/Loss.cpp,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/RangeFactories.cpp,aten/src/ATen/native/ReflectionPad.cpp,aten/src/ATen/native/ReplicationPadding.cpp,aten/src/ATen/native/Scalar.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/TensorTransformations.cpp,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/Unique.cpp,aten/src/ATen/native/cpu/Activation.cpp,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.cpp,aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/SoftMaxKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/BinaryOpsKernel.cu,aten/src/ATen/native/cuda/CUDAScalar.cu,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/DistanceKernel.cu,aten/src/ATen/native/cuda/Distributions.cu,aten/src/ATen/native/cuda/Dropout.cu,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/FractionalMaxPool2d.cu,aten/src/ATen/native/cuda/FractionalMaxPool3d.cu,aten/src/ATen/native/cuda/GridSampler.cu,aten/src/ATen/native/cuda/IndexKernel.cu,aten/src/ATen/native/cuda/Lerp.cu,aten/src/ATen/native/cuda/Loss.cu,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/Normalization.cu,aten/src/ATen/native/cuda/RNN.cu,aten/src/ATen/native/cuda/RangeFactories.cu,aten/src/ATen/native/cuda/ReduceOpsKernel.cu,aten/src/ATen/native/cuda/ReflectionPad.cu,aten/src/ATen/native/cuda/ReplicationPadding.cu,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/SortingKthValue.cu,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/TensorCompare.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cuda/TensorTransformations.cu,aten/src/ATen/native/cuda/Unique.cu,aten/src/ATen/native/cuda/WeightNorm.cu,aten/src/ATen/native/mkl/LinearAlgebra.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,aten/src/ATen/test/apply_utils_test.cpp,aten/src/ATen/test/scalar_test.cpp,test/test_cpp_extensions.py,torch/csrc/TypeInfo.cpp",82.0,14,3,5.444134967,9.0,26500.0,26.0,2301446.12195122,7400.0,22743.33333,0.0,,1.0,1
pytorch,a655e6313e92c54102dcf1714998afb57720adfd,3b155fa3055ecd500eb4bb59d1b5b770d3d61e97,rluo,rluo@ttic.edu,Wed Aug 23 19:31:18 2017 -0700,1503516678.0,Not changing dimension size for expand when target size is -1,14.0,1.0,"test/test_torch.py,torch/_tensor_docs.py",2.0,2,2,0.836640742,36.0,6027.0,1.0,23121.0,1379.0,13792.35284,0.0,,0.0,1
pytorch,88cdc1683519917e3043580b80c3668ca6018ca0,3b700a43d57065a60d24b0355c673f6aaec5e3b8,Corentin Dancette,cdancette@users.noreply.github.com,Mon Jun 24 22:21:53 2019 -0700,1561414913.0,"Add missing whitespace in error message (#21904)

Summary:
The current error message displays as:
     `RuntimeError: index koccurs twice in output`
A whitespace is missing between the index and 'occurs'
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21904

Differential Revision: D15878941

Pulled By: colesbury

fbshipit-source-id: 163dda1829bf4956978cd01fd0e751673580722d",1.0,1.0,aten/src/ATen/native/Linear.cpp,1.0,4,1,0,7.0,523.0,1.0,3484316.0,9603.0,27925.33333,0.0,Feature Addition,0.0,1
pytorch,5fb5e7b01d3f63350b4c70e0183b76e1c24b7cc9,3b7fbc397e5802ae4948db817e71100a9c6b6551,gchanan,gregchanan@gmail.com,Sat Dec 23 02:23:27 2017 -0500,1513995807.0,Reorder native_functions.yaml by alphabetical order. (#4326),133.0,139.0,aten/src/ATen/native/native_functions.yaml,1.0,4,1,0,3.0,274.0,1.0,97292.0,400.0,1260.905869,0.0,,0.0,1
pytorch,96aafc3cdc58963591addb51a47e49aabc06dd6f,3bccd3fc0db8265bd26139069fdf1fcdac911b90,Pritam Damania,pritam.damania@fb.com,Sat Oct 12 16:44:40 2019 -0700,1570898680.0,"Distributed Autograd - FAST mode backward pass implementation. (#27022)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27022

This change implements the ""FAST"" mode distributed autograd backward
pass as described in https://github.com/pytorch/pytorch/issues/23110.

At a high level the backward pass works as follows:
1. We start by computing dependencies on the node that calls
`torch.distributed.backward`.
2. This node computes the dependencies starting from the root nodes provided in
the backward call and all the 'send' functions present in the current autograd
context. The ""FAST"" mode assumes all 'send' functions are part of the autograd
computation.
3. Once the dependency computation is done, the distributed autograd engine
calls the local autograd engine to execute the autograd graph. Note that the
autograd graph on a single node is not necessarily connected because of
inter-node communication. As a result, we have special handling to ensure the
local autograd engine ensures we execute the entire graph starting from the
provided roots and all 'send' functions on the node.
4. When the local autograd engine hits a 'recv' function, it performs an async
RPC to send the gradients over to the appropriate node and stores a future in
the autograd context to keep track of this RPC.
5. On the destination node, the appropriate 'send' function is looked up and
enqueued on the local autograd engine. If this is the first time the node is
hearing about this autograd context id on the backward pass, then the node
computes dependencies for the local autograd engine.
6. As part of compute dependencies, the distributed autograd engine discovers
all leaf nodes and ensures those are passed as 'outputs' to the local autograd
engine. This avoids running the 'AccumulateGrad' function.
7. The gradients computed for the leaf nodes are then actually accumulated in
`DistAutogradContext` for the appropriate autograd context id.
8. The distributed autograd engine waits for the local autograd engine
to complete and also waits for all the 'Futures' (stored in 4.) for respective
RPCs to finish.

We have made the following changes to the local autograd engine for this
purpose:

1. Expose GraphTask and NodeTask so that the distributed autograd engine can
use them.
2. Expose a `execute_with_graph_task` API which gives the distributed engine
to build a GraphTask and pass it to the local autograd engine.
3. Expose a `enqueue_on_cpu` API, which allows the distributed engine to build
a `NodeTask` for a 'send' function and enqueue it on the local autograd engine.

In addition to this a few general improvements:
1. Added a `PropagateGradients` RPC call for the 'recv' function to pass
gradients to the appropriate node during the backward pass.
2. Use IValues as much as possible in serialization for RpcWithAutograd.
3. If Future.wait(), contains a message type EXCEPTION, we throw an appropriate
exception instead of just returning the message. This is inline with what most
Future.wait() APIs do.
4. Added a `get_gradients(context_id)` API which allows users to retrieve a map
from Tensor to respective gradient for the provided context_id on the local
node.
ghstack-source-id: 91794926

Test Plan: unit tests.

Differential Revision: D17652615

fbshipit-source-id: 96f65c52adb2706ee29f4b49e1655afaa0a3bec3",1725.0,505.0,"caffe2/CMakeLists.txt,test/cpp/dist_autograd/test_dist_autograd.cpp,test/dist_autograd_test.py,test/dist_utils.py,test/rpc_test.py,tools/build_variables.py,torch/CMakeLists.txt,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/engine.h,torch/csrc/distributed/autograd/context/dist_autograd_container.cpp,torch/csrc/distributed/autograd/context/dist_autograd_context.cpp,torch/csrc/distributed/autograd/context/dist_autograd_context.h,torch/csrc/distributed/autograd/engine/dist_engine.cpp,torch/csrc/distributed/autograd/engine/dist_engine.h,torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp,torch/csrc/distributed/autograd/functions/recvrpc_backward.h,torch/csrc/distributed/autograd/functions/sendrpc_backward.cpp,torch/csrc/distributed/autograd/functions/sendrpc_backward.h,torch/csrc/distributed/autograd/init.cpp,torch/csrc/distributed/autograd/rpc_messages/autograd_metadata.cpp,torch/csrc/distributed/autograd/rpc_messages/autograd_metadata.h,torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp,torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.h,torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_resp.cpp,torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_resp.h,torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp,torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h,torch/csrc/distributed/autograd/utils.cpp,torch/csrc/distributed/autograd/utils.h,torch/csrc/distributed/rpc/future_message.cpp,torch/csrc/distributed/rpc/init.cpp,torch/csrc/distributed/rpc/message.cpp,torch/csrc/distributed/rpc/message.h,torch/csrc/distributed/rpc/py_rref.cpp,torch/csrc/distributed/rpc/python_functions.cpp,torch/csrc/distributed/rpc/request_callback.cpp,torch/csrc/distributed/rpc/request_callback_impl.cpp,torch/csrc/distributed/rpc/rpc_agent.cpp,torch/csrc/distributed/rpc/rpc_agent.h,torch/csrc/distributed/rpc/rpc_with_autograd.cpp,torch/csrc/distributed/rpc/rpc_with_autograd.h,torch/csrc/distributed/rpc/rref.cpp,torch/csrc/distributed/rpc/rref_context.cpp,torch/csrc/distributed/rpc/rref_context.h,torch/csrc/distributed/rpc/utils.cpp,torch/distributed/autograd/__init__.py,torch/distributed/rpc/api.py",47.0,18,4,4.573347592,45.0,7472.0,10.0,1140919.9189189188,12221.0,34164.33333,0.0,Feature Addition,0.0,1
pytorch,bf2b411730452aadebbce7f2b0da1f749c7e9cb2,3bdc4a37ed802f69a032179c1540aee6529eb2c1,peter,peterghost86@gmail.com,Mon Mar 30 18:32:05 2020 -0700,1585593125.0,"CMake script cleanup - mixed case for function names (#35589)

Summary:
Running the following code.
```bash
cmake --help-command-list |
grep -v ""cmake version"" |
while read c; do
    echo 's/\b'""$(echo $c | tr '[:lower:]' '[:upper:]')""'\(\s*\)(/'""$c""'\1(/g'
done >convert.sed &&
git ls-files -z -- bootstrap '*.cmake' '*.cmake.in' '*CMakeLists.txt' |
egrep -z -v '^(cmake/Modules/|cmake/Modules_CUDA_fix/)' |
xargs -0 sed -i -f convert.sed &&
rm convert.sed
```
cmake-lint is too sensitive about mixed case so I didn't switch the check on.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35589

Differential Revision: D20735648

Pulled By: ezyang

fbshipit-source-id: a09a60a7ce921bb198575a35335faa299bd10b66",848.0,848.0,"CMakeLists.txt,aten/CMakeLists.txt,aten/src/ATen/ATenConfig.cmake.in,aten/src/ATen/CMakeLists.txt,aten/src/ATen/core/CMakeLists.txt,aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt,aten/src/ATen/native/quantized/cpu/qnnpack/cmake/DownloadCpuinfo.cmake,aten/src/ATen/native/quantized/cpu/qnnpack/cmake/DownloadFP16.cmake,aten/src/ATen/native/quantized/cpu/qnnpack/cmake/DownloadFXdiv.cmake,aten/src/ATen/native/quantized/cpu/qnnpack/cmake/DownloadGoogleBenchmark.cmake,aten/src/ATen/native/quantized/cpu/qnnpack/cmake/DownloadGoogleTest.cmake,aten/src/ATen/native/quantized/cpu/qnnpack/cmake/DownloadPSimd.cmake,aten/src/ATen/native/quantized/cpu/qnnpack/cmake/DownloadPThreadPool.cmake,aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/CMakeLists.txt,aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/cmake/DownloadGoogleTest.cmake,aten/src/ATen/quantized/CMakeLists.txt,aten/src/ATen/test/CMakeLists.txt,aten/src/TH/CMakeLists.txt,aten/src/THC/CMakeLists.txt,aten/src/THCUNN/CMakeLists.txt,caffe2/CMakeLists.txt,cmake/Codegen.cmake,cmake/Dependencies.cmake,cmake/Utils.cmake,cmake/public/LoadHIP.cmake,torch/lib/libshm/CMakeLists.txt,torch/lib/libshm_windows/CMakeLists.txt",27.0,24,4,2.831475132,78.0,6472.0,11.0,10029808.74074074,601.0,1783.5,0.0,Corrective,1.0,1
pytorch,82dd01150c182b39d2560b61d3983d9f08242245,3be6a4db4de8293ef1a5b472e3756ede36ae03cf,Lingyi Liu,lingyiliu@fb.com,Wed Apr 08 21:25:08 2020 -0700,1586381108.0,"improve the quantized batch_norm performance (#35639)

Summary:
The original batch_norm performance is 2X slower than C2 for some shape, especially for the remaining channel size close to 32. For example, we have a total channel size 32*1 + 24. The 24 channel execution in original implementation will be slow.
Benchmark
```
import torch, time

for dtype in [torch.qint8, torch.quint8, torch.qint32]:
    print('****', str(dtype), '*****')
    x = torch.rand(1, 4, 56, 56, 24)

    q_x = torch.quantize_per_tensor(x, 0.5, 1, dtype)
    q_x = q_x.permute([0, 4, 1, 2, 3])
    c = 24
    mean = torch.rand(c).float()
    var = torch.rand(c).float()
    weight = torch.rand(c).float()
    bias = torch.rand(c).float()
    eps = 0.001

    x = x.permute([0, 4, 1, 2, 3])

    NITER = 10

    s = time.time()
    for i in range(NITER):
        float_out = torch.nn.functional.batch_norm(x, weight=weight, bias=bias, running_mean=mean, running_var=var, training=False, momentum=0, eps=eps)
        float_out = torch.nn.functional.relu(float_out)
    time_per_iter_float = (time.time() - s) / NITER

    s = time.time()
    for i in range(NITER):
        quant_out = torch.ops.quantized.batch_norm3d_relu(q_x, weight, bias, mean, var, eps, 0.5, 1)
    time_per_iter_quant = (time.time() - s) / NITER

    print('time/iter ms (float)', 'time/iter ms (quant)', 'quant/float', sep='\t')
    print(time_per_iter_float * 1000, time_per_iter_quant * 1000, time_per_iter_quant / time_per_iter_float, sep='\t')

```
```
**** torch.qint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
0.6527423858642578      1.649641990661621       2.5272481554532837
**** torch.quint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
0.5787134170532227      1.040959358215332       1.7987475796152104
**** torch.qint32 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
0.5466938018798828      2.262735366821289       4.138944614042739
```

//Before the change:
```
**** torch.qint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
0.7526159286499023      2.330636978149414      3.0967149238128426
**** torch.quint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
0.21767616271972656     1.3946294784545898      6.406900328587075
**** torch.qint32 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
0.24483203887939456     2.561521530151367       10.46236245009251
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35639

Differential Revision: D20723292

Pulled By: lly-zero-one

fbshipit-source-id: 66692eabaffb5030c2a37ec0f1322df3665411aa",81.0,26.0,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,1.0,7,1,0,2.0,1655.0,1.0,108117.0,882.0,2417.5,0.0,Perfective,0.0,1
pytorch,a4ebc61f15280f06a98be1da0d2d9acf955c9fc3,3bf922a6ce37b973f0a6bbb06847916265ab9a2c,Edward Z. Yang,ezyang@meta.com,Sat Jul 29 14:51:26 2023 -0400,1690642286.0,"Apply UFMT to low traffic torch modules (#106249)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106249
Approved by: https://github.com/Skylion007",8455.0,4395.0,".lintrunner.toml,torch/_dispatch/python.py,torch/_prims_common/__init__.py,torch/_prims_common/wrappers.py,torch/amp/__init__.py,torch/amp/autocast_mode.py,torch/autograd/__init__.py,torch/autograd/_functions/tensor.py,torch/autograd/_functions/utils.py,torch/autograd/anomaly_mode.py,torch/autograd/forward_ad.py,torch/autograd/function.py,torch/autograd/functional.py,torch/autograd/grad_mode.py,torch/autograd/gradcheck.py,torch/autograd/graph.py,torch/autograd/profiler.py,torch/autograd/profiler_legacy.py,torch/autograd/profiler_util.py,torch/backends/__init__.py,torch/backends/_coreml/preprocess.py,torch/backends/_nnapi/prepare.py,torch/backends/_nnapi/serializer.py,torch/backends/cpu/__init__.py,torch/backends/cuda/__init__.py,torch/backends/cudnn/__init__.py,torch/backends/cudnn/rnn.py,torch/backends/mkl/__init__.py,torch/backends/mkldnn/__init__.py,torch/backends/mps/__init__.py,torch/backends/opt_einsum/__init__.py,torch/backends/quantized/__init__.py,torch/backends/xeon/run_cpu.py,torch/backends/xnnpack/__init__.py,torch/cpu/__init__.py,torch/cpu/amp/autocast_mode.py,torch/csrc/jit/tensorexpr/codegen_external.py,torch/csrc/jit/tensorexpr/scripts/bisect.py,torch/csrc/lazy/test_mnist.py,torch/cuda/__init__.py,torch/cuda/_sanitizer.py,torch/cuda/_utils.py,torch/cuda/amp/__init__.py,torch/cuda/amp/autocast_mode.py,torch/cuda/amp/common.py,torch/cuda/amp/grad_scaler.py,torch/cuda/comm.py,torch/cuda/graphs.py,torch/cuda/jiterator.py,torch/cuda/memory.py,torch/cuda/nccl.py,torch/cuda/nvtx.py,torch/cuda/profiler.py,torch/cuda/random.py,torch/cuda/streams.py,torch/distributions/__init__.py,torch/distributions/bernoulli.py,torch/distributions/beta.py,torch/distributions/binomial.py,torch/distributions/categorical.py,torch/distributions/cauchy.py,torch/distributions/chi2.py,torch/distributions/constraint_registry.py,torch/distributions/constraints.py,torch/distributions/continuous_bernoulli.py,torch/distributions/dirichlet.py,torch/distributions/exp_family.py,torch/distributions/exponential.py,torch/distributions/fishersnedecor.py,torch/distributions/gamma.py,torch/distributions/geometric.py,torch/distributions/gumbel.py,torch/distributions/half_cauchy.py,torch/distributions/half_normal.py,torch/distributions/independent.py,torch/distributions/kl.py,torch/distributions/kumaraswamy.py,torch/distributions/laplace.py,torch/distributions/lkj_cholesky.py,torch/distributions/log_normal.py,torch/distributions/logistic_normal.py,torch/distributions/lowrank_multivariate_normal.py,torch/distributions/mixture_same_family.py,torch/distributions/multinomial.py,torch/distributions/multivariate_normal.py,torch/distributions/negative_binomial.py,torch/distributions/normal.py,torch/distributions/one_hot_categorical.py,torch/distributions/pareto.py,torch/distributions/poisson.py,torch/distributions/relaxed_bernoulli.py,torch/distributions/relaxed_categorical.py,torch/distributions/studentT.py,torch/distributions/transformed_distribution.py,torch/distributions/transforms.py,torch/distributions/uniform.py,torch/distributions/utils.py,torch/distributions/von_mises.py,torch/distributions/weibull.py,torch/distributions/wishart.py,torch/jit/__init__.py,torch/jit/_async.py,torch/jit/_await.py,torch/jit/_builtins.py,torch/jit/_check.py,torch/jit/_dataclass_impls.py,torch/jit/_decomposition_utils.py,torch/jit/_decompositions.py,torch/jit/_freeze.py,torch/jit/_fuser.py,torch/jit/_ir_utils.py,torch/jit/_monkeytype_config.py,torch/jit/_passes/_property_propagation.py,torch/jit/_pickle.py,torch/jit/_recursive.py,torch/jit/_shape_functions.py,torch/jit/_state.py,torch/jit/_trace.py,torch/jit/annotations.py,torch/jit/generate_bytecode.py,torch/jit/mobile/__init__.py,torch/jit/quantized.py,torch/jit/supported_ops.py,torch/jit/unsupported_tensor_ops.py,torch/mps/__init__.py,torch/mps/profiler.py,torch/multiprocessing/__init__.py,torch/multiprocessing/_atfork.py,torch/multiprocessing/pool.py,torch/multiprocessing/queue.py,torch/multiprocessing/reductions.py,torch/multiprocessing/spawn.py,torch/profiler/__init__.py,torch/profiler/_memory_profiler.py,torch/profiler/_pattern_matcher.py,torch/profiler/_utils.py,torch/profiler/itt.py,torch/profiler/profiler.py,torch/profiler/python_tracer.py,torch/quantization/__init__.py,torch/quantization/_numeric_suite.py,torch/quantization/_numeric_suite_fx.py,torch/quantization/fake_quantize.py,torch/quantization/fuse_modules.py,torch/quantization/fuser_method_mappings.py,torch/quantization/fx/__init__.py,torch/quantization/fx/_equalize.py,torch/quantization/fx/fusion_patterns.py,torch/quantization/fx/graph_module.py,torch/quantization/fx/match_utils.py,torch/quantization/fx/pattern_utils.py,torch/quantization/fx/prepare.py,torch/quantization/fx/quantization_patterns.py,torch/quantization/fx/quantization_types.py,torch/quantization/fx/utils.py,torch/quantization/observer.py,torch/quantization/qconfig.py,torch/quantization/quant_type.py,torch/quantization/quantization_mappings.py,torch/quantization/quantize.py,torch/quantization/quantize_fx.py,torch/quantization/quantize_jit.py,torch/quantization/stubs.py",163.0,37,1,6.138126323,48.0,41445.0,83.0,15854120.822085887,18173.0,41307.5,0.0,,0.0,1
pytorch,e5bbd23ca762d0cd597f1a68c05957b4ffd7fb47,3c042a6ab9c55197f2ffd7464103a8c1b0b77107,Jiakai Liu,liujiakai@fb.com,Wed Mar 04 03:22:17 2020 -0800,1583292137.0,"[pytorch][mobile] support for custom mobile build with dynamic dispatch (#34055)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34055

Enable custom mobile build with dynamic dispatch for OSS build.

It calls a python util script to calculate transitive dependencies from
the op dependency graph and the list of used root ops, then pass the
result as the op registration whitelist to aten codegen, so that only
these used ops are registered and kept at link time.

For custom build with dynamic dispatch to work correctly, it's critical
to have the accurate list of used ops. Current assumption is that only
those ops referenced by TorchScript model are used. It works well if
client code doesn't call libtorch API (e.g.  tensor methods) directly;
otherwise the extra used ops need to be added to the whitelist manually,
as shown by the HACK in prepare_model.py.

Also, if JIT starts calling extra ops independent of specific model,
then the extra ops need to be added to the whitelist as well.

Verified the correctness of the whole process with MobileNetV2:
```
TEST_CUSTOM_BUILD_DYNAMIC=1 test/mobile/custom_build/build.sh
```

Test Plan: Imported from OSS

Reviewed By: bhosmer

Differential Revision: D20193327

Pulled By: ljk53

fbshipit-source-id: 9d369b8864856b098342aea79e0ac8eec04149aa",108.0,1.0,"CMakeLists.txt,cmake/Codegen.cmake,test/mobile/custom_build/prepare_model.py,tools/code_analyzer/gen_transitive_deps.py",4.0,6,3,1.531320854,68.0,904.0,3.0,135963.66666666666,15190.0,40800.33333,0.0,Corrective,1.0,1
pytorch,dc7f8163a1bee98345140e9575fc1d9f1c9e8a0d,3c0d7f08c65cd6b30bbcc08a7fa7b03233d7df56,Richard Zou,zou3519@gmail.com,Fri Sep 24 15:12:54 2021 -0700,1632496374.0,[functorch] Move to PyTorch core's parametrize testing mechanism,63.0,203.0,"functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",4.0,2,1,1.133243851,1.0,4439.0,2.0,0.25,373.0,550.0,0.0,,0.0,1
pytorch,"9ac9809f27548466d9b26d23ad0979c8b0310e6f,10f78985e72fb6834b435ac3f8d0890fa6614365",3c26f7a20535261a3608f63d7c7eb744ccf321c0,soumith,soumith@fb.com,Sat Dec 31 03:24:00 2016 -0800,1483154640.0,Merge commit '10f78985e72fb6834b435ac3f8d0890fa6614365',10.0,1.0,torch/lib/THCUNN/CMakeLists.txt,1.0,3,1,0,19.0,58.0,1.0,211.0,212.0,13650.43299,0.0,,0.0,1
pytorch,16ecd6f99c468027e4a08feb273e4fb31bee50bb,3c39e857ca0e710cefb83292f471f26b9e471883,Teng Li,tengli@fb.com,Tue Aug 14 21:14:15 2018 -0700,1534281255.0,"Python binding for reduce,allgather,scatter,gather ops and python tests (#10159)

Summary:
Provided python binding for these four ops. Also provided nccl binding test.

Based on https://github.com/pytorch/pytorch/pull/10058

Please only review init.cpp, and test file.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10159

Reviewed By: yf225

Differential Revision: D9323192

Pulled By: teng-li

fbshipit-source-id: b03822009d3a785ec36fecce2fc3071d23f9994e",146.0,0.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp",2.0,5,2,0.913661787,2.0,742.0,2.0,1502554.5,3456.0,9317.333333,0.0,,0.0,1
pytorch,098d9975a7c6b33ada382bd51f322f7f5c694a17,3c4a90ce384ba74925aafb1a60ea941ff8c67285,albanD,desmaison.alban@gmail.com,Thu May 13 20:16:59 2021 -0700,1620937019.0,"Revert ""Revert D28387764: Codegen inplace forward AD formula from out of place one if needed"" (#58231)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58231

This reverts commit 066e7699eb8c375a441e6de168da3ba7a73c3f27.

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D28412480

Pulled By: albanD

fbshipit-source-id: 7a231aa81b9e89537e6dca19642c4f12cd4b5ea5",288.0,71.0,"test/test_autograd.py,test/test_overrides.py,tools/autograd/derivatives.yaml,torch/autograd/gradcheck.py",4.0,5,3,0.838267019,43.0,13183.0,4.0,28053.5,12042.0,27307.0,0.0,,0.0,1
pytorch,ec9b20ddc0f19f76c3809e38e066d51699752f2b,3c6b52ae62033341711b18a503f4be4d9750d867,David Riazati,davidriazati@fb.com,Mon Apr 12 16:15:48 2021 -0700,1618244148.0,"Cache slow/disabled test files (#55682)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55682

Fixes #55648

For now it downloads and writes the relevant files to the system's temp dir and marks it as valid for 3 hours.

Test Plan: Imported from OSS

Reviewed By: malfet, nikithamalgifb

Differential Revision: D27685616

Pulled By: driazati

fbshipit-source-id: 27469b85fe4b6b4addde6b22bf795bca3d4990ef",38.0,8.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2173.0,1.0,45471.0,10651.0,23552.0,0.0,Corrective,1.0,1
pytorch,3122a96ee45507e8d33f265410222e69cc66677a,3c870dadc3536b03bdcc5377ac85ef9e44cc1e87,Nikita Shulga,nshulga@fb.com,Wed Sep 21 03:53:25 2022 +0000,1663732405.0,"[BE] Mark unused range-loop vars with `C10_UNUSED` (#85383)

I.e. replace:
```
for(const auto i: c10::irange(lim)) {
  (void)i;
  ...
}
```
with
```
for(const auto i C10_UNUSED: c10::irange(lim)) {
  ...
}
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85383
Approved by: https://github.com/kit1980",38.0,76.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/UpSampleBicubic2d.cpp,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp,aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cpu/Loops.h,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/ScatterGatherKernel.cpp,aten/src/ATen/native/cpu/SortingKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/Unfold2d.cpp,aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp,aten/src/ATen/native/cpu/UpSampleKernel.cpp,aten/src/ATen/native/layer_norm.cpp",17.0,5,1,3.627300963,9.0,10261.0,14.0,8447483.764705881,7515.0,17596.0,0.0,,0.0,1
pytorch,a35162f1bc860a5fac026d86bfa625e0c48359be,3cb2470bb33305921b92ae4417d1364a5177a44e,Wei Yang,weiyang@fb.com,Tue Oct 30 19:54:37 2018 -0700,1540929277.0,"add __deepcopy__ back to Parameter (#12886)

Summary:
- fix https://github.com/pytorch/pytorch/issues/315
- add `__deepcopy__` back to Parameter class
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12886

Differential Revision: D12838771

Pulled By: weiyangfb

fbshipit-source-id: b2ce12244e36f981d89f6c7cdead63237dd820ea",19.0,0.0,"test/test_torch.py,torch/nn/parameter.py",2.0,3,2,0.981940787,40.0,9298.0,2.0,628671.5,5008.0,14898.33333,0.0,Corrective,1.0,1
pytorch,99de537a2ee2ba91488b2bf31ad25fe3c2ff3a4f,3cbe66ba8c96882a170a3d905d3a39dadce8772d,Adam Paszke,adam.paszke@gmail.com,Wed Oct 05 14:56:47 2016 -0700,1475679407.0,Change requires_grad default to False,43.0,36.0,"test/test_autograd.py,test/test_nn.py,test/test_utils.py,torch/autograd/variable.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/linear.py,torch/nn/modules/module.py,torch/nn/modules/sparse.py,torch/optim/optimizer.py",11.0,6,2,2.783264747,8.0,3461.0,6.0,139186.36363636365,235.0,2276.14599,0.0,,0.0,1
pytorch,638086950d3f339de49c6b5393733aea2fee6a55,3ce539881a12df901538d6cd93f752469583f65b,Edward Yang,ezyang@fb.com,Wed Jan 06 19:26:03 2021 -0800,1609961163.0,"Back out ""Revert D25757721: [pytorch][PR] Run mypy on more test files"" (#50142)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50142

Original commit changeset: 58437d719285

Test Plan: OSS CI

Reviewed By: walterddr, ngimel

Differential Revision: D25803866

fbshipit-source-id: d6b83a5211e430c0451994391876103f1ad96315",30.0,14.0,"mypy.ini,test/test_bundled_inputs.py,test/test_expecttest.py,test/test_numpy_interop.py,torch/testing/_internal/expecttest.py,torch/utils/bundled_inputs.py",6.0,5,2,2.235175134,5.0,1389.0,1.0,72562.0,7885.0,17849.5,0.0,,0.0,1
pytorch,1e8de64c661f8f053d5fd94c92cdbc7ff2fb4b1f,3ce67efea2e59ddfc7cbaeb100ffb85b87f64652,kshitij12345,kshitijkalambarkar@gmail.com,Mon Aug 16 20:26:46 2021 -0700,1629145606.0,"[opinfo] nn.functional.pad (#62814)

Summary:
Reference: https://github.com/facebookresearch/functorch/issues/78

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62814

Reviewed By: VitalyFedyunin

Differential Revision: D30307492

Pulled By: zou3519

fbshipit-source-id: 4f6062eb4a3c91ed1795df1f82846afa0abafcdc",130.0,18.0,"test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.534004273,43.0,27149.0,2.0,119621.0,14658.0,33632.0,0.0,,0.0,1
pytorch,8d33603901100dbdff43b1947867430a93432999,3cecdf84f15420318474feebec988f118b6d6c85,Leonid Vlasenkov,leo.vlasenkov@gmail.com,Fri Jun 16 22:34:20 2017 +0300,1497652460.0,Storage from_file method (#1821),89.0,2.0,"test/test_torch.py,torch/__init__.py,torch/_storage_docs.py,torch/csrc/generic/StorageMethods.cpp",4.0,4,2,1.693108795,32.0,4352.0,1.0,108035.0,663.0,5388.172317,0.0,,0.0,1
pytorch,4e5b25ed474e09f20c409dce8c8e63f58805bccb,3d06a1e075ef0e6f4bf862d13e83cdd4b02dbc32,Richard Zou,zou3519@users.noreply.github.com,Mon Nov 06 21:00:29 2017 -0500,1510002029.0,"Make THCTensor_varInnermostDim numerically stable using Welford's algorithm (#3425)

* Use Welford's algorithm when reducing along inner dimension for THCTensor's variance fn

* Use accreals in THCTensor's varInnermostDim

* Skip cuda tests if no cuda

* Variance testing",137.0,23.0,"aten/src/THC/THCTensorMathReduce.cuh,aten/src/THC/generic/THCTensorMathReduce.cu,test/test_cuda.py,test/test_torch.py",4.0,5,2,1.164310338,38.0,6948.0,2.0,170575.5,2074.0,23999.35823,0.0,,0.0,1
pytorch,26740f853e10e34a7e8d2b72b950875796350418,3d089de851836da8b4b21495a0e1d79e05c9282c,Adnan Akhundov,aakhundov@meta.com,Thu Mar 07 17:24:43 2024 -0800,1709832283.0,"Add torch.cond support to AOT Inductor (#121120)

Summary: In this PR, `torch.cond` support and the necessary codegening infrastructure is added to C++ wrapper (AOTInductor and friends).

Notable additions:

- A new mechanism in the Python wrapper codegen to precompile and save the Triton kernels (generated and user-defined) which haven't been covered by the active path through the control flow given the sample inputs. As we can't do the runtime autotuning of the kernels outside the active path, we precompile and save them with the `launchers[0]` (corresponding to the first config).

- Codegen infra for `torch.cond` in the C++ wrapper (ABI- and non-ABI-compatible). The `torch.cond` codegen has been slightly refactored to avoid duplication across the Python and C++ wrappers.

- More extensions of the caching sites in the wrapper code to cache per codegened graph (e.g., `codegen_int_array_var`) + some infra for tracking the current codegened graph in the wrapper (both during codegen-ing in the `Scheduler.codegen` and in the `WrapperCodeGen.generate` functions).

- New unit tests to cover the added AOT Inductor + `torch.cond` functionality.

Codegen examples from the new unit tests:

- [`test_cond_simple_abi_compatible_cpu`](https://gist.github.com/aakhundov/862d5de9aa460f5df399e1387f7b342e)
- [`test_cond_simple_abi_compatible_cuda`](https://gist.github.com/aakhundov/d70b81f95fa8cc768cedef9acacb25bb)
- [`test_cond_simple_non_abi_compatible_cpu`](https://gist.github.com/aakhundov/c0ae7a8cbb6fa311c838e1b580f9a3f6)
- [`test_cond_simple_non_abi_compatible_cuda`](https://gist.github.com/aakhundov/08b945d4e8a32c97b7f9ff6272f4a223)
- [`test_cond_nested_abi_compatible_cuda`](https://gist.github.com/aakhundov/ce664f433c53e010ce4c0d96a6c13711)
- [`test_cond_with_parameters_abi_compatible_cuda`](https://gist.github.com/aakhundov/77afbeb8eaab5c5b930a3f922a7baf12)
- [`test_cond_with_multiple_outputs_abi_compatible_cuda`](https://gist.github.com/aakhundov/8cc06105ec8a3fe88be09b3f6e32c690)

Test Plan:

```
$ python test/inductor/test_aot_inductor.py -k test_cond
...
----------------------------------------------------------------------
Ran 42 tests in 170.619s

OK
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/121120
Approved by: https://github.com/jansel, https://github.com/chenyang78",453.0,126.0,"test/inductor/test_aot_inductor.py,test/inductor/test_control_flow.py,torch/_inductor/codegen/cpp_wrapper_cpu.py,torch/_inductor/codegen/cpp_wrapper_cuda.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/graph.py,torch/_inductor/triton_heuristics.py,torch/csrc/inductor/aoti_torch/c/shim.h,torch/csrc/inductor/aoti_torch/shim_common.cpp",9.0,9,2,2.30033194,1.0,10301.0,7.0,280740.1111111111,25973.0,61658.5,0.0,Feature Addition,0.0,1
pytorch,0c3db1cb33cda1de61c754f14e7bfd2c53ea6d07,3d12ab452e18ce6146a7ddd77a7ac967f7cda5b8,BowenBao,bowbao@microsoft.com,Mon May 17 21:49:26 2021 -0700,1621288166.0,"[ONNX] Fix split export in opset13 (#56277) (#57605)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57605

Fix split export in opset13

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D28393522

Pulled By: SplitInfinity

fbshipit-source-id: 4de83345ec7bc9bafe778fe534d9a8760ce16ab3

Co-authored-by: Ksenija Stanojevic <ksenija.stanojevic@gmail.com>
Co-authored-by: BowenBao <bowbao@microsoft.com>",13.0,3.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset13.py",2.0,4,2,0.896038233,3.0,9050.0,2.0,3002571.5,12172.0,27563.0,0.0,Corrective,1.0,1
pytorch,3314d60a759fa16d142049bed9be454e69c535db,3d15ee1b34e8d9db4e1ddd53e8f8c5928fedc127,Nikolay Korovaiko,korovaikon@gmail.com,Fri Aug 02 04:12:47 2019 -0700,1564719167.0,"Remove more uses of `DimensionedTensorType`

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23060

Differential Revision: D16460391

Pulled By: Krovatkin

fbshipit-source-id: b50ee87d22ad18b8cbfff719b199ea876ef172f1",74.0,44.0,"aten/src/ATen/core/jit_type.h,torch/csrc/jit/fuser/codegen.cpp,torch/csrc/jit/fuser/compiler.cpp,torch/csrc/jit/passes/decompose_ops.cpp,torch/csrc/jit/passes/graph_fuser.cpp,torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,torch/csrc/jit/pybind_utils.h,torch/csrc/jit/python_ir.cpp,torch/jit/__init__.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",11.0,12,2,2.982167335,14.0,9810.0,10.0,862981.1818181818,10357.0,29626.33333,0.0,,0.0,1
pytorch,4da68227e91e47748b3bc77629bee206facfa79e,3d2c90131abb04dbd9ad039f96de7f43437c77f6,Negin Raoof,neginmr@utexas.edu,Fri Oct 11 23:17:15 2019 -0700,1570835835.0,"opset 11 updates (#27578)

Summary:
Opset 11 updates:
- Enabled ORT tests for updated ops in opset 11
- Updated index_copy and index_fill symbolic for opset 11 to modify onnx::Scatter -> onnx::ScatterElemets
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27578

Reviewed By: hl475

Differential Revision: D17852462

Pulled By: houseroad

fbshipit-source-id: c88747804054d0f3455f2c58fd1d8725e0b2f803",107.0,40.0,".jenkins/caffe2/test.sh,test/onnx/expect/TestOperators.test_cumsum.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",6.0,7,3,2.169633152,7.0,4289.0,5.0,769173.0,12202.0,34116.33333,0.0,,0.0,1
pytorch,87b6cbb6fdec3789a409b02665e731ba67b4377c,3d44305e9d3f0ecb9eb126dbf0125a0cdc513b2d,David Riazati,davidriazati@fb.com,Tue Mar 19 01:15:17 2019 -0700,1552958117.0,"Attribute serialization (#17423)

Summary:
Allows serialization/loading of attributes (`IValue`s of any type).
* metadata (attribute name, type) is stored in the `model.json`
* The binary format is a subset of the `pickle` module that supports the operations necessary for `IValue`s
    * Attributes are serialized in the order they are defined on a module to a list in a single `attributes` file, with submodule attributes coming first. This order directly matches the order attributes are listed in `model.json`
    * This can be inspected in Python with `pickle.load()` or with `pickletools` (PyTorch need not be installed for this to work)
        * A class is used to store a tensor's index into the tensor table of the model, so to unpickle the file you have to use a custom Unpickler:
        ```python
        class TensorID(object):
            def __setstate__(self, id):
                self.id = id

        class JitUnpickler(pickle.Unpickler):
            def find_class(self, module, name):
                if module == '__main__' and name == 'TensorID':
                    return TensorID

        JitUnpickler(open(""my_model/attributes.pkl"", ""rb"")).load()
        ```
    * pickle format: https://svn.python.org/projects/python/trunk/Lib/pickletools.py
* It currently does not support/guarantee that anything saved out with `pickle` (i.e. if you edit `attributes` with `pickle` directly) instead of our tools will be imported correctly

Also will fix #17683 and fix #16367

Followup Work:
* document format / choice of pickle: #17951
* create an example
* list specializations
* int size specializations, large binputs
* do a first pass over attributes to output only necessary `BINPUT` ops
* attribute reassignment (e.g `self.my_attribute = new_value`)
* `tensor.save(""some_checkpoint.pkl"")` support with tensors embedded in Pickle file
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17423

Differential Revision: D14470965

Pulled By: driazati

fbshipit-source-id: 6a21a9939efdbe59b4bc57fd31d6d630bab5297e",864.0,8.0,"caffe2/proto/torch.proto,test/test_jit.py,tools/build_variables.py,torch/CMakeLists.txt,torch/csrc/jit/export.cpp,torch/csrc/jit/import.cpp,torch/csrc/jit/import_source.cpp,torch/csrc/jit/pickler.cpp,torch/csrc/jit/pickler.h,torch/csrc/jit/script/parser.cpp,torch/csrc/jit/script/parser.h,torch/csrc/jit/script/script_type_parser.cpp,torch/csrc/jit/script/script_type_parser.h,torch/jit/__init__.py",14.0,9,4,1.965438898,13.0,19138.0,7.0,603650.25,7552.0,23055.83333,0.0,Corrective,1.0,1
pytorch,fb833aabac9fde268cab4b0569ef068019ec3b51,3d61d93ea7022e6e8d3b21f1d8cc45f4e3490002,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Wed Aug 10 16:31:28 2022 +0000,1660149088.0,"Revert ""merge_rules, person_of_interst and CODEOWNERS now better aligned  (#83127)""

This reverts commit fb833aabac9fde268cab4b0569ef068019ec3b51.

Reverted https://github.com/pytorch/pytorch/pull/83127 on behalf of https://github.com/malfet due to We should not have removed existing codeowners, nor spam Soumith and Ed with review requests",74.0,110.0,"CODEOWNERS,docs/source/community/persons_of_interest.rst",2.0,3,1,0.347816914,7.0,411.0,1.0,6303.0,6288.0,14557.0,0.0,,0.0,1
pytorch,62447a5aa3403312646b77228ea5e178b1853e42,3d6e956412d874c70837f69ce46583af4f389b60,David Riazati,davidriazati@fb.com,Wed Apr 24 23:36:24 2019 -0700,1556148984.0,"Add LONG_BINPUT to unpickler (#19696)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19696
ghimport-source-id: 8d711cd3ed2b2810b5b3d765564429882f96d1f1

Differential Revision: D15072658

Pulled By: driazati

fbshipit-source-id: a28a90218874e07cfbed4f8df3d6d23ae5e70933",12.0,0.0,torch/csrc/jit/pickler.cpp,1.0,3,1,0,1.0,501.0,1.0,1036.0,8303.0,24837.33333,0.0,Feature Addition,0.0,1
pytorch,daa30aa9921f02a2e08d7f1d6fcfcbdc55777850,3d6ebde756eeaf10236d51033a30f3fb3790c010,Soumith Chintala,soumith@fb.com,Wed Oct 12 16:46:32 2016 -0400,1476290792.0,qr and ormqr tests and bugfix,188.0,50.0,"test/test_cuda.py,test/test_torch.py,torch/csrc/Module.cpp,torch/csrc/generic/TensorMethods.cwrap",4.0,4,2,1.533454118,10.0,7449.0,4.0,284706.5,182.0,1098.950488,0.0,Corrective,1.0,1
pytorch,4bb59441339074515a4257704acca6ebd80826cc,3d7428d9ac2c96d347883711831fa6a4142c6b15,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Tue May 03 20:01:18 2022 +0000,1651608078.0,"Revert ""[lint] upgrade mypy to latest version""

This reverts commit 9bf18aab94943f5352604a39340ad57ad4d0c5a4.

Reverted https://github.com/pytorch/pytorch/pull/76753 on behalf of https://github.com/suo",61.0,70.0,".github/scripts/lint_native_functions.py,.lintrunner.toml,benchmarks/instruction_counts/core/expand.py,tools/setup_helpers/cmake.py,tools/shared/module_loader.py,tools/stats/s3_stat_parser.py,tools/test/test_stats.py,torch/_C/__init__.pyi.in,torch/_deploy.py,torch/ao/nn/sparse/quantized/linear.py,torch/autograd/functional.py,torch/distributed/distributed_c10d.py,torch/distributed/elastic/multiprocessing/api.py,torch/fx/experimental/unification/variable.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/hub.py,torch/jit/_script.py,torch/jit/frontend.py,torch/nn/modules/module.py,torch/nn/utils/prune.py,torch/package/_package_pickler.py,torch/package/_package_unpickler.py,torch/package/package_importer.py,torch/utils/bundled_inputs.py,torch/utils/cpp_extension.py,torch/utils/data/dataloader.py,torch/utils/data/datapipes/dataframe/dataframes.py,torch/utils/show_pickle.py",29.0,32,4,4.228970966,45.0,23013.0,1.0,1070.0,2818.0,6751.5,0.0,,0.0,1
pytorch,c60d2ef4eb4bae01aa8375c49ab3b1a6b85ddf27,3d83321b44b3f0c19315c3f646d5601f2a22e2fd,Kulin Seth,kulin_seth@apple.com,Fri May 20 03:18:09 2022 +0000,1653016689.0,"MPS Fixes: copy operations, addmm and baddmm (#77791)

Fixes for the copy operations and GEMM operations on MPS backend.

Fixes https://github.com/pytorch/pytorch/issues/77819
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77791
Approved by: https://github.com/albanD",534.0,330.0,"aten/src/ATen/Context.cpp,aten/src/ATen/mps/EmptyTensor.cpp,aten/src/ATen/mps/MPSDevice.mm,aten/src/ATen/mps/MPSFallback.mm,aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/mps/operations/LinearAlgebra.mm,test/test_mps.py",9.0,8,2,1.756110242,8.0,6250.0,3.0,469853.75,3472.0,8266.0,0.0,Corrective,1.0,1
pytorch,43c747859c70e7436a86bf33dd2c718fa9a113ef,3d878dee45c00905a7367c9d9c2e23cb8b11ef62,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Tue Apr 20 14:07:37 2021 -0700,1618927657.0,"Added out= variant for torch.linalg.lstsq (#54721)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54721

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D27874711

Pulled By: mruberry

fbshipit-source-id: 696ebb6eb0bad81988e9cb7a081388a3a5ab3e2c",400.0,185.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",6.0,9,3,0.824834224,12.0,28736.0,5.0,588338.0,11022.0,24340.5,0.0,Feature Addition,0.0,1
pytorch,e6fd28fb0560bccc267fc206a9b5052c255a4b34,3d8b6d336119a6ffc401a90f50f21ba4079272a6,ganler,jaway.liu@gmail.com,Wed Feb 16 22:28:08 2022 +0000,1645050488.0,"fix: onnx PReLU unidirectional broadcasting

Fixes https://github.com/pytorch/pytorch/issues/70570

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70571",19.0,3.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.684038436,4.0,13967.0,1.0,240456.0,795.0,1962.0,0.0,Corrective,1.0,1
pytorch,c10da636b5023931d126eab8382f645e21bef026,3d907ef78e3040422ed079b11eab9dc20f7b12f5,gchanan,gregchanan@gmail.com,Thu Apr 26 02:46:42 2018 -0400,1524710802.0,"Consistently check 'out' variants against specified dtype/layout/device parameters. (#6973)

We were previously doing this in the most common cases, but not consistently.",29.0,8.0,"test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp",3.0,4,2,1.507981653,39.0,7787.0,3.0,597946.0,978.0,2380.305292,0.0,,0.0,1
pytorch,ffcb0989e76798ddb893b9e156ae1113d2498bb5,3da4cea658e4b8aa27a4375e1c3fc4e17d7647fb,BowenBao,bowbao@microsoft.com,Thu Oct 01 04:54:15 2020 -0700,1601528055.0,"[ONNX] Add dim_param support in export with onnx shape inference (#44920)

Summary:
* Support propagating `dim_param` in ONNX by encoding as `ShapeSymbol` in `SymbolicShape` of outputs. If export is called with `dynamic_axes` provided, shape inference will start with these axes set as dynamic.
* Add new test file `test_pytorch_onnx_shape_inference.py`, reusing all test cases from `test_pytorch_onnx_onnxruntime.py`, but focus on validating shape for all nodes in graph. Currently this is not enabled in the CI, since there are still quite some existing issues and corner cases to fix. The test is default to run only at opset 12.
* Bug fixes, such as div, _len, and peephole.cpp passes for PackPadded, and LogSoftmaxCrossEntropy.
* This PR depends on existing PR such as 44332.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44920

Reviewed By: eellison

Differential Revision: D23958398

Pulled By: bzinodev

fbshipit-source-id: 00479d9bd19c867d526769a15ba97ec16d56e51d",393.0,124.0,"scripts/onnx/test.sh,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_pytorch_onnx_shape_inference.py,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.h,torch/csrc/jit/python/init.cpp,torch/csrc/jit/python/python_ir.cpp,torch/csrc/jit/python/script_init.cpp,torch/csrc/jit/serialization/export.cpp,torch/csrc/jit/serialization/export.h,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",15.0,12,3,3.002199515,10.0,16098.0,7.0,451875.9285714286,5634.0,13233.5,0.0,Corrective,1.0,1
pytorch,beceb8b92f030b19a0ea48ab9ae0def8f554e7f3,3dc402fd1eae13839ac5a5aa22aab3017c3fc961,Richard Zou,zou3519@gmail.com,Tue Aug 09 21:38:17 2022 -0700,1660081097.0,"[functorch] in-place jvp testing (#83077)

Testing code is starting to trend toward entropy.
Not sure if the in-place tests are actually ""necessary"" but better safe
than sorry.

Test Plan:
- run tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83077
Approved by: https://github.com/Chillee",63.0,29.0,"functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,2,1,1.166359857,2.0,6045.0,3.0,110857.33333333331,6340.0,14715.5,0.0,,0.0,1
pytorch,9ce95ce157dd23b1da969f168b1f64bcea12c78c,3e08988cd3bb05fe7372262be10c9a237d682991,FindHao,find@findhao.net,Wed May 24 23:56:53 2023 +0000,1684972613.0,"Fix redudant kernel generations (#102104)

## Issue description

The PR https://github.com/pytorch/pytorch/pull/100064 introduces a new RNG operation process. However, it causes every `randint` to load a separate random seed by default. TorchInductor generates a buffer to store all necessary random seeds and places the offsets as constant values in the subsequent compute buffers. In ir_pre_fusion generated by TorchInductor, some buffers only differ by one line, which is the load random seed with the corresponding offset. Subsequently, the codegen generates Triton kernels following the same rule. Finally, in the output_code.py, some Triton kernels only differ by one line, meaning that redundant kernels are being generated.

## Solution

This PR captures the seed offset and adds it to the existing `self.sizevars` structure. It generates variable names as placeholders, allowing the code wrapper to pass the offset as an argument to the kernels. I've also modified the divisible_by_16 check to exclude this argument.

This PR reduces the number of generated kernels from 50 to 17 for BertForMaskedLM forward.

According to tests on my own environment, the compilation time of attention_is_all_you_need_pytorch has been reduced from 94s to 66s. The speedup remains largely unchanged, at 1.37X.

The following is a comparison for a simple example.
Before:
```
triton_poi_fused_0 = async_compile.triton('triton_', '''
...
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    ...
    tmp0 = tl.load(in_ptr0 + 0)
    tmp1 = x0
    tmp2 = triton_helpers.randint64(tmp0, (tmp1).to(tl.uint32), 0, 10)

triton_poi_fused_1 = async_compile.triton('triton_', '''
...
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    ...
    tmp0 = tl.load(in_ptr0 + 1)
    tmp1 = x0
    tmp2 = triton_helpers.randint64(tmp0, (tmp1).to(tl.uint32), 0, 10)
...''')

def call(args):
        triton_poi_fused_0.run(buf0, buf1, 1024, grid=grid(1024), stream=stream0)
        triton_poi_fused_1.run(buf0, buf2, 1024, grid=grid(1024), stream=stream0)

```
After:
```
triton_poi_fused_0 = async_compile.triton('triton_', '''
...
def triton_(in_ptr0, out_ptr0, load_seed_offset, xnumel, XBLOCK : tl.constexpr):
    ...
    tmp0 = tl.load(in_ptr0 + load_seed_offset)
    tmp1 = x0
    tmp2 = triton_helpers.randint64(tmp0, (tmp1).to(tl.uint32), 0, 10)
    ....

def call(args):
        triton_poi_fused_0.run(buf0, buf1, 0, 1024, grid=grid(1024), stream=stream0)
        triton_poi_fused_0.run(buf0, buf2, 1, 1024, grid=grid(1024), stream=stream0)

```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102104
Approved by: https://github.com/jansel, https://github.com/ngimel",29.0,3.0,"test/inductor/test_torchinductor.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/triton.py",3.0,5,2,1.494784946,3.0,9717.0,3.0,60899.0,16197.0,36663.0,0.0,Corrective,1.0,1
pytorch,3823b4b19761430fb2533aeab6c6e9e4fcf10a5e,3e2f309177b4982eac7ac5f0842fba773285c954,Samantha Andow,samdow@fb.com,Thu Mar 03 19:45:16 2022 -0500,1646336716.0,"[functorch] bernoulli_ only taking in float, includes dropout (pytorch/functorch#523)

[ghstack-poisoned]",168.0,12.0,"functorch/functorch/csrc/BatchRulesDecompositions.cpp,functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/test/test_eager_transforms.py,functorch/test/test_vmap.py",5.0,4,1,1.859746635,1.0,6818.0,4.0,0.0,850.0,1187.0,0.0,,0.0,1
pytorch,5e6fcd02b5d5db9c2bb9e4bfce7e5774aae67825,3e3501c98d6c0b118fbaa665c72866ad6ee83af2,Mateusz Piotrowski,mpp302@gmail.com,Sun Nov 27 15:54:35 2016 +0100,1480262075.0,Integration tests of the THD Python interface (#28),312.0,1.0,"test/run_test.sh,test/test_distributed.py",2.0,1,1,0.434654314,21.0,64.0,1.0,257342.0,411.0,3962.696975,0.0,,0.0,1
pytorch,ce05b7a3244ae7a61e989c9cd4eabf6d668ecbb0,3e42da09dfac51a260a01acd65953e497d027059,Ilqar Ramazanli,iramazanli@fb.com,Mon Apr 19 01:47:55 2021 -0700,1618796875.0,"Porting logcumsumexp tests to OpInfo (#56135)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56135

Reviewed By: mruberry

Differential Revision: D27844398

Pulled By: iramazanli

fbshipit-source-id: e0191314dc4e248501ad25170da0b77c0b799781",26.0,3.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,5479.0,1.0,42712.0,10967.0,24185.5,0.0,,0.0,1
pytorch,30bb4e0071269518774105e71e4b6b54ed4260b4,3e6164449fe285b7c9c9e4f0df63b5f3ed8a3dc8,anjali411,chourdiaanjali123@gmail.com,Wed Dec 08 18:34:08 2021 -0800,1638988448.0,"Add efficient zero tensors (#64837)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64837

Test Plan: Imported from OSS

Reviewed By: gchanan

Differential Revision: D32834987

Pulled By: anjali411

fbshipit-source-id: 20ea08ade0db0044ca633d9c1a117a6a2e65d1fd",354.0,23.0,"BUILD.bazel,aten/src/ATen/ConjugateFallback.cpp,aten/src/ATen/ZeroTensorFallback.cpp,aten/src/ATen/core/TensorBase.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/MathBitFallThroughLists.h,aten/src/ATen/native/NegateFallback.cpp,aten/src/ATen/native/Resize.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/templates/Functions.h,c10/core/DispatchKey.cpp,c10/core/DispatchKey.h,c10/core/TensorImpl.h,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/codegen/gen.py,tools/codegen/model.py,torch/_tensor_str.py,torch/autograd/gradcheck.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",24.0,15,4,3.310860943,42.0,46447.0,7.0,651721.5833333334,17612.0,41457.5,0.0,Feature Addition,0.0,1
pytorch,4737b3361479f4104efaa3bfa2ea517eaacb60fb,3e6e0a1d1093992d35b2248fd5a54feab4b01984,Mario Lezcano,lezcano-93@hotmail.com,Wed Aug 24 10:53:25 2022 -0500,1661338405.0,"Support a stable double backward on linalg.det for real inputs (#80217)

The complex case still fails. I do not know why.

Fixes https://github.com/pytorch/pytorch/issues/62327
Fixes https://github.com/pytorch/pytorch/issues/53364
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80217
Approved by: https://github.com/nikitaved, https://github.com/albanD, https://github.com/malfet",128.0,57.0,"aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,test/test_proxy_tensor.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/opinfo/definitions/linalg.py",6.0,16,4,1.325439373,15.0,17074.0,2.0,175574.66666666666,6751.0,15740.5,0.0,Corrective,1.0,1
pytorch,d30fa4837e10505d172e0cc7a6849b1a07aeb573,3e6e2e9b7bd5b001fbd015b089cb058a874719a3,Richard Zou,zou3519@gmail.com,Tue Mar 10 14:49:14 2020 -0700,1583851754.0,"Print the current Node name in anomaly mode (#33875)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33875

Fixes #33675.

I added a `current_node_name` argument to AnomalyMetadata::print_stack.
This is a mandatory arg because I found only one callsite and making it
a default arg on a virtual function can be confusing.

Test Plan:
- Tested locally:
https://gist.github.com/zou3519/09937387c83efc76e1700374d5c9c9d9
- I don't know how to add a test for this: the message is printed to
stderr but it isn't an exception nor a warning. I considered capturing
the stderr of a subprocess but that seems like asking for flakiness.

Differential Revision: D20349399

Pulled By: zou3519

fbshipit-source-id: 7585ddffe2bf9e1081f4028a9c44de783978a052",9.0,6.0,"torch/csrc/autograd/anomaly_mode.h,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/python_anomaly_mode.cpp,torch/csrc/autograd/python_anomaly_mode.h",4.0,3,1,1.723231429,37.0,1145.0,3.0,11168298.5,15334.0,41065.33333,0.0,Corrective,1.0,1
pytorch,f9d25e8e724206218d276c3fccc6310c69b265b5,3eac7164f4cc1ca0512b9450eec23f37be6c201d,Adam Paszke,adam.paszke@gmail.com,Mon Sep 26 23:38:10 2016 -0700,1474933090.0,Add data parallel functions to nn,433.0,20.0,"test/test_cuda.py,test/test_nn.py,torch/_utils.py,torch/cuda/comm.py,torch/nn/parallel/__init__.py,torch/nn/parallel/functions.py,torch/nn/parallel/parallel_apply.py,torch/nn/parallel/replicate.py",8.0,5,2,2.813400953,7.0,911.0,1.0,78700.0,202.0,3365.992183,0.0,Feature Addition,0.0,1
pytorch,7f607e8cb5c933fda87149e64e3a74f125d8adaf,3ec71fce79f4e568c48796da4b18a3e6f2c6fc29,Peter Bell,peterbell10@live.co.uk,Tue Oct 04 20:12:22 2022 +0100,1664914342.0,"Improve make_tensor performance for float and complex types (#85473)

For floating types, `make_tensor` calls `rand` and then does a linear
interpolation from `low` to `high`. This instead calls `uniform_(low,
high)` to cut out the interpolation step.

For complex types, `make_tensor` does the `rand` + interpolation step
twice and calls `torch.complex(real, imag)` at the end. This instead
uses `view_as_real` and `uniform_(low, high)` to fuse it all into one
operation.

My benchmarks show significant speedups in all cases for float32 and
complex64.

| Device | dtype     | Size  | Master (us) | This PR (us) | Speedup |
|--------|-----------|-------|-------------|--------------|---------|
| CPU    | float32   | 8     | 19.4        | 6.34         | 3.1     |
|        |           | 4096  | 36.8        | 21.3         | 1.7     |
|        |           | 2**24 | 167,000     | 80,500       | 2.1     |
|        | complex32 | 8     | 37.0        | 7.57         | 4.9     |
|        |           | 4096  | 73.1        | 37.6         | 1.9     |
|        |           | 2**24 | 409,000     | 161,000      | 2.5     |
| CUDA   | float32   | 8     | 40.4        | 11.7         | 3.5     |
|        |           | 4096  | 38.7        | 11.7         | 3.3     |
|        |           | 2**24 | 2,300       | 238          | 9.7     |
|        | complex32 | 8     | 78.7        | 14           | 5.6     |
|        |           | 4096  | 82.7        | 13.8         | 6.0     |
|        |           | 2**24 | 5,520       | 489          | 11.3    |
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85473
Approved by: https://github.com/mruberry",195.0,39.0,"functorch/test/test_ops.py,test/test_decomp.py,torch/testing/_creation.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/_masked.py,torch/testing/_internal/opinfo/definitions/linalg.py",6.0,8,3,2.205691125,8.0,24134.0,2.0,344129.0,8039.0,18992.0,0.0,Perfective,0.0,1
pytorch,e7c1e6a8e39df0d206efe247f5eb0481eb8b8b6c,3ed720079e01c1b2e1c9f95e5bb43da56d6fc2c3,Luke Yeager,lukeyeager@users.noreply.github.com,Fri Jan 27 21:38:38 2017 -0800,1485553118.0,[pep8] Fix most remaining lint manually,179.0,154.0,"test/test_autograd.py,test/test_legacy_nn.py,tools/cwrap/cwrap.py,tools/cwrap/plugins/CuDNNPlugin.py,tools/cwrap/plugins/ReturnArguments.py,tools/cwrap/plugins/THPPlugin.py,tools/nnwrap/generate_wrappers.py,torch/__init__.py,torch/_torch_docs.py,torch/autograd/variable.py,torch/legacy/nn/BCECriterion.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/Max.py,torch/legacy/nn/Min.py,torch/legacy/nn/Padding.py,torch/legacy/nn/PartialLinear.py,torch/legacy/nn/SpatialConvolutionLocal.py,torch/legacy/nn/SpatialDivisiveNormalization.py,torch/legacy/nn/SpatialFractionalMaxPooling.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/legacy/optim/adadelta.py,torch/legacy/optim/adagrad.py,torch/legacy/optim/adam.py,torch/legacy/optim/nag.py,torch/legacy/optim/rmsprop.py,torch/legacy/optim/rprop.py,torch/legacy/optim/sgd.py,torch/multiprocessing/reductions.py,torch/nn/_functions/batchnorm.py,torch/nn/_functions/rnn.py,torch/nn/_functions/thnn/auto.py,torch/nn/functional.py,torch/nn/modules/conv.py,torch/nn/modules/loss.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/optim/optimizer.py,torch/tensor.py,torch/utils/serialization/read_lua_file.py,torch/utils/trainer/plugins/monitor.py",41.0,20,3,4.135472592,23.0,15278.0,1.0,0.0,383.0,3826.696975,0.0,Corrective,1.0,1
pytorch,4b8f4fc25902e3a325b06e2db415bba9fad7c0ef,3ef2e484bf8e04bbb9e3658587630f1411c19b33,Ailing,ailzhang@users.noreply.github.com,Wed Feb 21 13:35:29 2018 -0800,1519220129.0,Add fp16 testcases in test_cuda (#5122),143.0,21.0,"test/common.py,test/test_cuda.py",2.0,1,1,0.131744048,37.0,1849.0,2.0,528023.0,969.0,6832.672317,0.0,Feature Addition,0.0,1
pytorch,3022a395f37a3a82fe72a4e752a9aa7fe0243558,3f2ecf7755935bce603fdf8e0537d6a57e80864c,Peter Bell,peterbell10@live.co.uk,Tue Aug 22 16:18:56 2023 +0100,1692721136.0,"[inductor] Separate to_{dtype,device} from lowering to avoid copying (#107640)

These lowerings must copy even when they are no-ops in order to preserve
correctness in the presense of mutations. However, `to_dtype` and `to_device`
are also used in various lowerings as a helper function where it is okay to alias.

So, I've split these into two functions and allow the helper functions to alias
which saves some unnecessary copies.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107640
Approved by: https://github.com/lezcano",22.0,12.0,torch/_inductor/lowering.py,1.0,2,1,0,2.0,4613.0,1.0,217526.0,18907.0,42818.5,0.0,Corrective,0.0,1
pytorch,2d8c2972ae707ac68665fbd6dfdeaab1e8ca1e95,3f7ab95890ee9e9685cb61a1ce69b2f464badd90,Adam Paszke,adam.paszke@gmail.com,Thu Sep 29 16:39:00 2016 -0700,1475167140.0,Finish implementation of prng related functions,231.0,60.0,"test/test_cuda.py,test/test_torch.py,torch/__init__.py,torch/csrc/Generator.cpp,torch/csrc/Module.cpp,torch/csrc/cuda/Module.cpp,torch/cuda/__init__.py,torch/cuda/random.py",8.0,5,2,2.56999677,8.0,4157.0,4.0,168394.42857142858,210.0,3396.992183,0.0,,0.0,1
pytorch,229fce0dc245ac1fdf4cfbdecb720f99af0cd16a,3f85e1fd925e238ad6e676996d4af9c54ae11efe,Samantha Andow,samdow@fb.com,Fri May 20 20:01:04 2022 -0400,1653076864.0,"[functorch] Add extremal testing for forward over reverse using decompositions  (pytorch/functorch#818)

* add extremal testing

* make nll_loss and cross_entropy only test sane things

* make argnum implicit

* fix nits

* fix devices",183.0,22.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1297.0,1.0,0.0,1079.0,1467.0,0.0,Corrective,1.0,1
pytorch,54e73271c7e43f2519a4758ae2e4c379ef8369dc,3f88e3105fe09b8ff9ebee3f0870fbb6fd1fe2bb,Kurt Mohler,kmohler@quansight.com,Thu Sep 07 03:04:34 2023 +0000,1694055874.0,"Reland: Remove remaining global `set_default_dtype` calls from tests (#108088)

Fixes #68972

Relands #107246

To avoid causing Meta-internal CI failures, this PR avoids always asserting that the default dtype is float in the `TestCase.setUp/tearDown` methods. Instead, the assert is only done if `TestCase._default_dtype_check_enabled == True`. `_default_dtype_check_enabled` is set to True in the `if __name__ == ""__main__"":` blocks of all the relevant test files that have required changes for this issue

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108088
Approved by: https://github.com/ezyang",922.0,879.0,"test/distributed/test_data_parallel.py,test/distributions/test_distributions.py,test/jit/test_freezing.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/quantization/jit/test_quantize_jit.py,test/test_complex.py,test/test_cpp_api_parity.py,test/test_jit.py,test/test_linalg.py,test/test_matmul_cuda.py,test/test_nn.py,test/test_ops.py,test/test_ops_fwd_gradients.py,test/test_ops_gradients.py,test/test_ops_jit.py,test/test_tensor_creation_ops.py,test/test_torch.py,test/test_type_info.py,torch/testing/_internal/common_nn.py,torch/testing/_internal/common_utils.py",20.0,10,2,1.144327933,52.0,89692.0,9.0,967977.4,19420.0,44105.0,0.0,Corrective,1.0,1
pytorch,3024bcfff5c5e497b602063dc2d948a4d4c10155,3f9115dc7ae7cb4a2566a9785189dd3ca94c1978,Jane Xu,janeyx@fb.com,Thu Mar 24 17:07:08 2022 -0700,1648141628.0,"Decorate test_pdist_large for requiring large memory (#74574)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/74154

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74574

Reviewed By: george-qi

Differential Revision: D35100229

Pulled By: janeyx99

fbshipit-source-id: d7df377318e45c7f5447c034aa025b1422fcc06e
(cherry picked from commit 335a76d9f2a721b30e1b9e1c869bfbe431f01a2a)",8.0,5.0,test/test_torch.py,1.0,1,1,0,44.0,8391.0,1.0,70958.0,1656.0,3958.5,0.0,Corrective,1.0,1
pytorch,561037aef8134d81479fac6cdbd196d3652fd8e7,3f94fc4862d080fd1d765b948b6da93209cab990,Lara Haidar-Ahmad,lahaidar@microsoft.com,Thu Mar 07 17:59:28 2019 -0800,1551981568.0,"ONNX Export for Max and Average Pooling in CEIL_MODE

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/16769

Differential Revision: D14362175

Pulled By: houseroad

fbshipit-source-id: 65cfb1dfba6a43d39cc85374add368fe8e4e5645",206.0,17.0,"test/onnx/expect/TestOperators.test_avg_pool2d.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic.py",4.0,5,2,1.54213695,12.0,3413.0,1.0,41543.0,7365.0,22545.33333,0.0,Feature Addition,0.0,1
pytorch,f94c95a2dd5a6e7ade8b53f8cc57fc257ae1033a,3fe4718d164b4231684ff995ba13ca0d91d59c6d,Kurt Mohler,kmohler@quansight.com,Wed Apr 14 16:36:17 2021 -0700,1618418177.0,"Add `padding_idx` argument to EmbeddingBag (#49237)

Summary:
This PR adds a `padding_idx` parameter to `nn.EmbeddingBag` and `nn.functional.embedding_bag`. As with `nn.Embedding`'s `padding_idx` argument, if an embedding's index is equal to `padding_idx` it is ignored, so it is not included in the reduction.

This PR does not add support for `padding_idx` for quantized or ONNX `EmbeddingBag` for opset10/11 (opset9 is supported). In these cases, an error is thrown if `padding_idx` is provided.

Fixes https://github.com/pytorch/pytorch/issues/3194

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49237

Reviewed By: walterddr, VitalyFedyunin

Differential Revision: D26948258

Pulled By: jbschlosser

fbshipit-source-id: 3ca672f7e768941f3261ab405fc7597c97ce3dfc",927.0,293.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/EmbeddingBag.h,aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu,aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cuh,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py,test/cpp/api/functional.cpp,test/cpp/api/modules.cpp,test/quantization/test_quantize_jit.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/nn/functional/embedding.h,torch/csrc/api/include/torch/nn/modules/embedding.h,torch/csrc/api/include/torch/nn/options/embedding.h,torch/csrc/api/src/nn/modules/embedding.cpp,torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp,torch/csrc/jit/runtime/static/ops.cpp,torch/nn/functional.py,torch/nn/functional.pyi.in,torch/nn/modules/sparse.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/overrides.py,torch/testing/_internal/common_nn.py",26.0,34,4,2.897659276,46.0,61185.0,24.0,6499324.192307692,10780.0,23808.0,0.0,Corrective,1.0,1
pytorch,310c3735b9eb97f30cee743b773e5bb054989edc,4007dd76e20a4e222fa330a44d38d73aaa70734e,James Reed,jamesreed@fb.com,Mon Mar 12 19:39:39 2018 -0700,1520883579.0,"Add missing ONNX symbolics and fix fusible expand logic  (#5654)

This includes various fixes required to export the NMT decoder to ONNX

* Add missing ONNX symbolics and fix fusible expand logic

* Update comments and use of at::optional

* Use _unimplemented",65.0,10.0,"torch/csrc/jit/passes/onnx/peephole.cpp,torch/onnx/symbolic.py",2.0,6,1,0.772155144,9.0,1015.0,2.0,774328.5,482.0,2364.5,0.0,Corrective,1.0,1
pytorch,bc40fb5639c8d3b7bc01dfd8d6a30aa05c5e8457,401a6b682b920bb3f6ba9b74e8e46d388c9cc5eb,mingfeima,mingfei.ma@intel.com,Thu Dec 30 19:57:25 2021 -0800,1640894245.0,"add BFloat16 support for AdaptiveAvgPool2d on CPU (#56902)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56902

Test Plan: Imported from OSS

Reviewed By: mikaylagawarecki

Differential Revision: D28836789

Pulled By: VitalyFedyunin

fbshipit-source-id: caac5e5b15190b8010bbfbc6920aa44032208ee7",140.0,10.0,"aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp,test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,0.748647431,43.0,35962.0,3.0,2314960.6666666665,18034.0,42668.5,0.0,Feature Addition,0.0,1
pytorch,aae7b00f7c7b5e28d1fe8974cd02a538f5be0913,4048d4cdd2a579cf16bc86bc7e76f9ab40d80226,Mike Ruberry,mruberry@devfair044.h1.fair,Wed Apr 27 14:40:21 2022 +0000,1651070421.0,"[primTorch] Prototype tracer and elementwise unary reference opinfo class

Adds a prototype tracer with no caching support and the `ElementwiseUnaryPythonRefInfo` class. A reference for `floor` is added to test the latter, and the elementwise binary reference inputs are extended to also return noncontiguous inputs. The SampleInput transform operation has been updated to return an actual SampleInput instead of a tuple to facilitate uniform handling of (transformed) SampleInputs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76388
Approved by: https://github.com/ngimel",434.0,62.0,"test/test_binary_ufuncs.py,test/test_ops.py,torch/_prims/__init__.py,torch/_prims/context.py,torch/_prims/executor.py,torch/_prims/utils.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",9.0,6,2,2.415819856,5.0,27283.0,5.0,200837.57142857145,2659.0,6306.0,0.0,Feature Addition,0.0,1
pytorch,24b49314625e13f98cd761dec939db1d825420d9,40592f91b5f045c02443e9d390491bba7f5dcf46,Thomas Viehmann,tv.github@beamnet.de,Mon Apr 16 18:41:47 2018 +0200,1523904107.0,"Fix bilinear performance regression (#6110)

The current implementation of bilinar uses a matrix multiplication approach. This creates a large intermediate matrix (batch * output dimension * input dimension). Relative to the previous pure python approach, this caused severe performance regression (600ms vs. 18ms for 300x100x200 weights and a batch of 50 on CPU, and also quadratic memory).
The attached change restores the performance using the previous strategy of looping over output features. It implements forward, backward, and double backward as native ATen code.

Credits:

Martin Tutek reported the regression and pinpointed the problem
Adam Paszke patiently answered my questions about ATen
I would not have been able to prepare this without you, thank you!

I referenced the old python implementation, used a python version of the naive implementation, and coded manual functions etc.

The tests have gradgradcheck etc.

* fix memory use of native bilinear

* bilinear double backward

* Move bilinear_double_backward to Functions.cpp

Addresses review comment by Tongzhou Wang. Thank you!

* add WrapDimUtilsMulti.h

* start at generic trilinear

* move to generic trilinear

* catch up on dim_list_to_bitset

* switch bilinear to use _trilinear implement _trilinear_backward

* add comments to Linear.cpp, move _trilinear in yaml",209.0,6.0,"aten/src/ATen/WrapDimUtilsMulti.h,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/native_functions.yaml,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp",5.0,7,2,1.02482161,11.0,3224.0,4.0,669644.5,587.0,3477.0,0.0,Corrective,1.0,1
pytorch,e26139b7f77a9b2ba88c52310b8ae41f512e36dd,406040f6a93ab7356185a8500641bd154ece53b9,Ethan Luo,ethanluoyc@gmail.com,Sun Jul 02 14:13:44 2017 +0100,1499004824.0,fix torch.is_tensor not recognizing HalfTensor (#1934),7.0,5.0,"test/test_torch.py,torch/__init__.py",2.0,2,2,0.918295834,32.0,4510.0,2.0,369577.5,1065.0,12592.54911,0.0,Corrective,1.0,1
pytorch,f7520cb51e7208fd7c4c0d57d786c7b0207718bc,4068c5467d496cd3c09a841f40adacedf3ab41a0,Richard Zou,zou3519@gmail.com,Sat Dec 03 03:30:10 2022 -0800,1670038210.0,"[Reland] Move functorch/_src to torch/_functorch (#88756) (#90091)

This will be the last disruptive functorch internals change.

Why are we moving these files?
- As a part of rationalizing functorch we are moving the code in
functorch/_src to torch/_functorch
- This is so that we can offer the functorch APIs as native PyTorch APIs
(coming soon) and resolve some internal build issues.

Why are we moving all of these files at once?
- It's better to break developers all at once rather than many times

Test Plan:
- wait for tests

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90091
Approved by: https://github.com/anijain2305, https://github.com/ezyang",6961.0,6916.0,".lintrunner.toml,benchmarks/dynamo/common.py,functorch/__init__.py,functorch/_src/__init__.py,functorch/_src/aot_autograd.py,functorch/_src/aot_autograd/__init__.py,functorch/_src/benchmark_utils.py,functorch/_src/compile_utils.py,functorch/_src/compilers.py,functorch/_src/config.py,functorch/_src/eager_transforms.py,functorch/_src/eager_transforms/__init__.py,functorch/_src/fx_minifier.py,functorch/_src/make_functional.py,functorch/_src/make_functional/__init__.py,functorch/_src/named_members_polyfill.py,functorch/_src/partitioners.py,functorch/_src/python_key.py,functorch/_src/pytree_hacks.py,functorch/_src/top_operators_github_usage.py,functorch/_src/vmap.py,functorch/_src/vmap/__init__.py,functorch/benchmarks/chrome_trace_parser.py,functorch/benchmarks/cse.py,functorch/compile/__init__.py,functorch/experimental/__init__.py,test/dynamo/test_aot_cudagraphs.py,test/dynamo/test_repros.py,test/functorch/discover_coverage.py,test/functorch/test_aotdispatch.py,test/functorch/test_eager_transforms.py,test/functorch/test_memory_efficient_fusion.py,test/functorch/test_minifier.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor.py,test/test_functionalization.py,torch/_dynamo/debug_utils.py,torch/_dynamo/eval_frame.py,torch/_dynamo/optimizations/training.py,torch/_functorch/__init__.py,torch/_functorch/aot_autograd.py,torch/_functorch/benchmark_utils.py,torch/_functorch/compile_utils.py,torch/_functorch/compilers.py,torch/_functorch/config.py,torch/_functorch/eager_transforms.py,torch/_functorch/fx_minifier.py,torch/_functorch/make_functional.py,torch/_functorch/named_members_polyfill.py,torch/_functorch/partitioners.py,torch/_functorch/python_key.py,torch/_functorch/pytree_hacks.py,torch/_functorch/top_operators_github_usage.py,torch/_functorch/vmap.py,torch/_inductor/compile_fx.py",56.0,20,4,4.027669618,4.0,35877.0,10.0,297049.625,10148.0,23299.0,0.0,,0.0,1
pytorch,63c811b3a68d9c23addc219cffb691678b7407c2,407a9fee0c8fb172459ac9b3556419305e47f8a7,Wei Yang,weiyang@fb.com,Tue Sep 18 06:24:49 2018 -0700,1537251889.0,"make copy constructed tensor a leaf variable when using torch.tensor(sourceTensor) (#11061)

Summary:
- fix https://github.com/pytorch/pytorch/issues/10876
- the cause of the bug is because copy constructor cannot distinguish between default value of requires_grad and requires_grad=False, thus it makes a copy from source tensor along with its grad_fn if requires_grad=True at source
- with this fix, the behavior becomes
```
>>> source = torch.randn(2, 2, requires_grad=True)
>>> copy = torch.tensor(source, requires_grad=True)
>>> print(copy)
tensor([[-1.2001,  1.9869],
        [-1.0134,  1.3096]], grad_fn=<CopyBackwards>)

>>> source = torch.randn(2, 2, requires_grad=True)
>>> copy = torch.tensor(source, requires_grad=False)
>>> print(copy)
tensor([[-0.7402,  0.0467],
        [ 0.4344, -0.0420]])

>>> source = torch.randn(2, 2, requires_grad=True)
>>> copy = torch.tensor(source)
>>> print(copy)
tensor([[-0.7402,  0.0467],
        [ 0.4344, -0.0420]])
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11061

Differential Revision: D9569714

Pulled By: weiyangfb

fbshipit-source-id: ea368688bdc0f1ce5997870e164e42835b64b4a1",45.0,3.0,"test/test_torch.py,torch/_torch_docs.py,torch/csrc/utils/tensor_new.cpp",3.0,4,2,1.423548142,41.0,15665.0,3.0,123199.66666666669,4182.0,11663.33333,0.0,Corrective,1.0,1
pytorch,00b5bd536fc499d58d1e8414d63a4a550dd99f8f,40d138f7c19e8b4c89ad254aba4c9cdfed9bb7db,Iurii Zdebskyi,iuriiz@devfair004.maas,Tue Sep 08 23:58:32 2020 -0700,1599609512.0,"Added alpha overloads for add/sub ops with lists (#43413)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/43413

Test Plan: Imported from OSS

Reviewed By: cpuhrsch

Differential Revision: D23331896

Pulled By: izdeby

fbshipit-source-id: 2e7484339fec533e21224f18979fddbeca649d2c",112.0,37.0,"aten/src/ATen/native/ForeachOpsKernels.cpp,aten/src/ATen/native/ForeachUtils.h,aten/src/ATen/native/cuda/ForeachBinaryOpList.cu,aten/src/ATen/native/cuda/ForeachFunctors.cuh,aten/src/ATen/native/cuda/ForeachPointwiseOp.cu,aten/src/ATen/native/native_functions.yaml,test/test_foreach.py",7.0,6,2,2.407380688,12.0,8963.0,3.0,71855.57142857143,4932.0,11383.5,0.0,Feature Addition,0.0,1
pytorch,785ebb9d6d9fbca72e1916f7043be195ad4aa1d0,40de6b80eef78db2de831fcce1bc796f7b4719cc,BowenBao,bowbao@microsoft.com,Tue Feb 22 22:55:18 2022 -0800,1645570518.0,"[ONNX] Add infra for quantized model export and support quantized mobilenet v3 (#72215)

* Add infrastructure and helper functions to enable future work for other quantized operators and models.
* Add export for quantized operators needed by torchvision mobilenet v3 large.
    * ATen namespace: hardsigmoid, flatten, adaptive_avg_pool, quantize_per_tensor, dequantize.
    * Quantized namespace: conv2d, conv2d_relu, hardswish, add, mul.
* Numerous bug fixes, in unpack_quantized_weight.cpp, symbolic functions, and unit test.

Co-authored-by: BowenBao <bowbaomicrosoft.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/73102",399.0,137.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_utility_funs.py,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset14.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py",10.0,8,2,2.56476925,10.0,19896.0,6.0,1875259.9,917.0,2206.5,0.0,Corrective,1.0,1
pytorch,05b255c543f6b82725af6cf142216006e8c425d8,41099ef71caca95f99f300938657a696448d442b,kshitij12345,kshitijkalambarkar@gmail.com,Sun May 02 03:49:55 2021 -0700,1619927395.0,"OpInfo: mvlgamma (#56907)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56907

Reviewed By: astaff

Differential Revision: D28118669

Pulled By: mruberry

fbshipit-source-id: f54ad6dc64ddb6bcfca5c5c7fd8f395cd9761128",95.0,18.0,"test/test_torch.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.864626737,43.0,15857.0,3.0,130925.33333333331,11563.0,26178.5,0.0,,0.0,1
pytorch,9962b908570989ddf0607824b07f6010540dbd2a,41699bb9f519f01eab1afaa00efb9a4f2ff028d9,Richard Zou,zou3519@users.noreply.github.com,Fri Nov 12 23:24:30 2021 -0500,1636759470.0,[functorch] Update functorch lagging op db (pytorch/functorch#261),134.0,0.0,"functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",4.0,2,1,1.668722932,1.0,4761.0,4.0,1.0,524.0,727.5,0.0,,0.0,1
pytorch,3ab074b3c5515eaa6967f09cc2975240f61989e9,41705ce7d5e797c1924224357bd329cebb6744fb,andrew giessel,andrew@giessel.com,Tue Apr 25 14:58:51 2017 -0400,1493132331.0,Add zero padding module (#1326),39.0,3.0,"test/test_nn.py,torch/nn/modules/__init__.py,torch/nn/modules/padding.py",3.0,4,2,1.20600269,28.0,2926.0,2.0,214869.66666666663,647.0,6291.835419,0.0,Feature Addition,0.0,1
pytorch,36d9a74bc6c39a7d4d1b462d68270da0a6972bb1,417dc7f86cd741fbceb7d5a93a33a4124d7dad0d,Natalia Gimelshein,ngimel@fb.com,Tue Nov 09 05:15:00 2021 -0800,1636434900.0,"Revert D32007691: [pytorch][PR] Op info for activation functions 2 (softsign, tanh, tanhshrink, threshold, celu, sigmoid, mish, hardsigmoid)

Test Plan: revert-hammer

Differential Revision:
D32007691 (https://github.com/pytorch/pytorch/commit/ea60e7d5597e4bb4df0db8ab97cf7eca75969d56)

Original commit changeset: 6cb14dc56e29

fbshipit-source-id: 9ef599ef07302fb521b1f413b989786adfa3576c",2.0,133.0,"test/test_jit_fuser_te.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.15374218,2.0,14904.0,1.0,24363.0,16945.0,39847.5,0.0,,0.0,1
pytorch,0f1332b09eac6c1a8a9f518ce202dbec22f01041,41869d9acebe42c73bda400ab6de655d3e50a01c,Richard Zou,zou3519@gmail.com,Fri Apr 29 18:25:08 2022 -0700,1651256708.0,"[functorch] Update discover_coverage, add jvp support for linear",19.0,14.0,"functorch/functorch/csrc/PyTorchOperatorHacks.cpp,functorch/test/discover_coverage.py,functorch/test/test_ops.py",3.0,4,1,1.548736037,1.0,2824.0,2.0,1.3333333333333333,1029.0,1413.5,0.0,Feature Addition,0.0,1
pytorch,b87c7ab6d683737af7af14e166351b95a5c7b137,418a9fb9d855bf2ec28dec10c3fb3621d431463b,Shunting Zhang,shunting@fb.com,Thu Apr 20 04:28:15 2023 +0000,1681964895.0,"[reland][inductor] coordinate descent tuning upon max-autotune (#99594)

Reland https://github.com/pytorch/pytorch/pull/97203 .

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99594
Approved by: https://github.com/jansel",392.0,23.0,"test/inductor/test_coordinate_descent_tuner.py,test/inductor/test_cuda_repro.py,torch/_inductor/config.py,torch/_inductor/coordinate_descent_tuner.py,torch/_inductor/triton_heuristics.py,torch/_inductor/utils.py",6.0,4,2,1.718289018,1.0,2700.0,2.0,126291.66666666669,14881.0,33776.5,0.0,,0.0,1
pytorch,7a2930b357a4e62bb0bab53bb0d23c607b6ede38,419ef2cdcfe84442de5232739284c6a51a18632f,Horace He,chilli@fb.com,Fri Nov 18 21:39:11 2022 +0000,1668807551.0,"Added utility to count memory reads/written in Inductor (#89203)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89203
Approved by: https://github.com/jansel, https://github.com/ngimel",545.0,13.0,"test/inductor/test_perf.py,torch/_inductor/compile_fx.py,torch/_inductor/dependencies.py,torch/_inductor/graph.py,torch/_inductor/metrics.py,torch/_inductor/scheduler.py,torch/_inductor/utils.py,torch/_inductor/virtualized.py",8.0,4,2,1.232009261,1.0,2711.0,6.0,1127662.7142857143,9704.0,22503.5,0.0,Feature Addition,0.0,1
pytorch,a7dc893df445cdd7113e9fac46c8eebd1f0d5c55,41c59987d9513f2f3d40cad73d847166de60d198,Mike Ruberry,mruberry@devfair044.h1.fair,Fri Apr 15 22:36:24 2022 +0000,1650062184.0,"Adds OpInfos for max_unpool{1, 2, 3}d

per title
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75879
Approved by: https://github.com/ngimel",154.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,16790.0,1.0,908.0,2338.0,5491.5,0.0,Feature Addition,0.0,1
pytorch,edac323378efbb52ca2b153ed6967b446ccaa504,41ea7f2d861eb147282618def35dfe66580dd349,David Reiss,dreiss@fb.com,Fri Jun 26 20:25:10 2020 -0700,1593203110.0,"Add channels-last support to bundled_inputs (#36764)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36764

This allows bundling inputs that are large uniform buffers in
channels-last memory format.

Test Plan: Unit test.

Differential Revision: D21142660

Pulled By: dreiss

fbshipit-source-id: 31bbea6586d07c1fd0bcad4cb36ed2b8bb88a7e4",12.0,8.0,"test/test_bundled_inputs.py,torch/utils/bundled_inputs.py",2.0,3,2,0.881290899,1.0,292.0,1.0,6826841.0,3240.0,7808.5,0.0,Feature Addition,0.0,1
pytorch,aa66146974aac4733cc206e909d7d1a9135f2572,420b37f3c67950ed93cd8aa7a12e673fcfc5567b,Will Feng,willfeng@fb.com,Thu Aug 22 03:09:37 2019 -0700,1566443377.0,"Deprecate tensor.data<T>(), and codemod tensor.data<T>() to tensor.data_ptr<T>() (#24886)

Summary:
This PR adds deprecation message for `tensor.data<T>()` (https://github.com/pytorch/pytorch/commit/91d94e7d4145c462143587b19da0a0599c031da8), and changes all call sites of `tensor.data<T>()` to `tensor.data_ptr<T>()`  in PyTorch core.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24886

Differential Revision: D16924576

Pulled By: yf225

fbshipit-source-id: 0943d6be73245c7c549c78597b74c3b07fa24440",989.0,987.0,"aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/core/Formatting.cpp,aten/src/ATen/core/Tensor.h,aten/src/ATen/cuda/detail/IndexUtils.cuh,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/AdaptiveAveragePooling.cpp,aten/src/ATen/native/AdaptiveAveragePooling3d.cpp,aten/src/ATen/native/AdaptiveMaxPooling2d.cpp,aten/src/ATen/native/AdaptiveMaxPooling3d.cpp,aten/src/ATen/native/AveragePool2d.cpp,aten/src/ATen/native/AveragePool3d.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/Col2Im.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/DilatedMaxPool2d.cpp,aten/src/ATen/native/DilatedMaxPool3d.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/Im2Col.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/MaxUnpooling.cpp,aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp,aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp,aten/src/ATen/native/NaiveDilatedConvolution.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/PackedSequence.cpp,aten/src/ATen/native/QuantizedLinear.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/RangeFactories.cpp,aten/src/ATen/native/ReflectionPad.cpp,aten/src/ATen/native/Repeat.h,aten/src/ATen/native/ReplicationPadding.cpp,aten/src/ATen/native/Scalar.cpp,aten/src/ATen/native/SobolEngineOps.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorTransformations.cpp,aten/src/ATen/native/Unique.cpp,aten/src/ATen/native/UpSampleBicubic2d.cpp,aten/src/ATen/native/UpSampleBilinear2d.cpp,aten/src/ATen/native/UpSampleLinear1d.cpp,aten/src/ATen/native/UpSampleNearest1d.cpp,aten/src/ATen/native/UpSampleNearest2d.cpp,aten/src/ATen/native/UpSampleNearest3d.cpp,aten/src/ATen/native/UpSampleTrilinear3d.cpp,aten/src/ATen/native/cpu/Activation.cpp,aten/src/ATen/native/cpu/CrossKernel.cpp,aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,aten/src/ATen/native/cpu/SoftMaxKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/layer_norm_kernel.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu,aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu,aten/src/ATen/native/cuda/AveragePool2d.cu,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/CUDAScalar.cu,aten/src/ATen/native/cuda/Col2Im.cu,aten/src/ATen/native/cuda/DilatedMaxPool2d.cu,aten/src/ATen/native/cuda/DilatedMaxPool3d.cu,aten/src/ATen/native/cuda/DistanceKernel.cu,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/Im2Col.cu,aten/src/ATen/native/cuda/Indexing.cu,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/MaxUnpooling.cu,aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu,aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu,aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu,aten/src/ATen/native/cuda/RangeFactories.cu,aten/src/ATen/native/cuda/ReflectionPad.cu,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cuda/TensorTransformations.cu,aten/src/ATen/native/cuda/Unique.cu,aten/src/ATen/native/cuda/UpSampleBilinear2d.cu,aten/src/ATen/native/cuda/UpSampleNearest1d.cu,aten/src/ATen/native/cuda/UpSampleNearest2d.cu,aten/src/ATen/native/cuda/UpSampleNearest3d.cu,aten/src/ATen/native/cuda/WeightNorm.cu,aten/src/ATen/native/cudnn/LossCTC.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/miopen/RNN_miopen.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp,aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp,aten/src/ATen/native/quantized/Copy.cpp,aten/src/ATen/native/quantized/QTensor.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool.cpp,aten/src/ATen/native/quantized/cpu/qconv.cpp,aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp,aten/src/ATen/native/quantized/cpu/qconv_unpack.cpp,aten/src/ATen/native/quantized/cpu/qlinear.cpp,aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp,aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp,aten/src/ATen/native/quantized/cpu/qlinear_unpack.cpp,aten/src/ATen/native/quantized/cpu/qnnpack_fc.cpp,aten/src/ATen/native/quantized/cpu/qnnpack_relu.cpp,aten/src/ATen/native/quantized/cpu/qpool.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,aten/src/ATen/quantized/Quantizer.cpp,aten/src/ATen/templates/Tensor.h,aten/src/ATen/test/apply_utils_test.cpp,aten/src/ATen/test/basic.cpp,aten/src/ATen/test/cuda_tensor_interop_test.cpp,aten/src/ATen/test/quantized_test.cpp,aten/src/ATen/test/tensor_interop_test.cpp,test/cpp/api/module.cpp,test/cpp/api/parallel.cpp,test/cpp/api/support.h,test/cpp/api/tensor.cpp,test/cpp/api/tensor_cuda.cpp,test/cpp_extensions/cuda_extension.cpp,torch/csrc/api/src/data/samplers/random.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,torch/lib/c10d/test/ProcessGroupGlooTest.cpp,torch/lib/c10d/test/ProcessGroupNCCLTest.cpp",135.0,34,3,6.509282262,10.0,56913.0,84.0,4874937.948148148,10801.0,30692.83333,0.0,Feature Addition,0.0,1
pytorch,7bb0133b8cd2793e30341c0c8b9e5202921aece3,421f66a29f3a27424fc342c04370806943f95c55,Scott Wolchok,swolchok@fb.com,Mon Mar 28 19:32:46 2022 -0700,1648495966.0,"[PyTorch] Add fused addmm path in linear for contiguous 3D input (#72728)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72728

If the input is 3D and contiguous, we can get a fused addmm by reshaping.
ghstack-source-id: 152278479

Test Plan: existing tests?

Reviewed By: zrphercule

Differential Revision: D34176407

fbshipit-source-id: 899f216cadcd782c3b1b046025228df04228c740
(cherry picked from commit e601c5a512baac3791aa6d514d784234275a8f03)",6.0,0.0,aten/src/ATen/native/Linear.cpp,1.0,4,1,0,9.0,712.0,1.0,17426412.0,1739.0,4142.0,0.0,Feature Addition,0.0,1
pytorch,1a81ab3ba58d23566ba6cecd0d30eafe93dc7bc8,4247cc98a22760932d0b53236403e478e8612c2f,Denis Vieriu,dvieriu@apple.com,Wed Sep 14 17:24:24 2022 +0000,1663176264.0,"[MPS] Fix mps to cpu casting from a smaller dtype to a bigger dtype (#84928)

Fixes #82566 , #80800

- mps->cpu casts from a smaller dtype to a bigger dtype mps->mps cast from smaller/bigger dtype to another dtype in case of scatter
- For mps->cpu copies where we don't have a source/destination offset, we can save the cast result directly in the destTensor, so we can skip the additional overhead of the blit.
- In case we can return the data without doing the blit, we need to check if it's blocking call, case in which we'd need a synchronize(SyncType::COMMIT_AND_WAIT); call (previously this was done by the blit).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84928
Approved by: https://github.com/razarmehr",95.0,26.0,"aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/mps/operations/Normalization.mm,test/test_mps.py",3.0,7,2,1.035218566,1.0,8636.0,3.0,4437484.666666667,7334.0,17237.0,0.0,Corrective,1.0,1
pytorch,b0a2f6f2f5cc7077c5a4301f429e16e5fbe1acdc,428204dfa44839bfb6e08dbd619adf156a881948,Lingyi Liu,lingyiliu@fb.com,Fri Sep 27 17:18:32 2019 -0700,1569604712.0,"Fix the QuantizedAVX2 build issue (#26854)

Summary:
The QuantizedAVx2 does not support the int32 type. We switch to use at::quantize_vec function instead.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26854

Differential Revision: D17609872

Pulled By: llyfacebook

fbshipit-source-id: b4a77d93ce0ebfef696506b5cdbe3e91fe44bb36",28.0,23.0,"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,test/test_quantized.py",2.0,8,2,0.462749059,1.0,2843.0,2.0,111870.0,11832.0,33142.83333,0.0,Corrective,1.0,1
pytorch,fb23e6279735e9938ba75e2f44a0f7addb69a90b,42a68749bf3efda9d27bb3b594cf154f602c5b3a,Thomas Viehmann,tv.github@beamnet.de,Tue May 29 10:26:39 2018 +0200,1527589599.0,"einsum: don't inplace modify arguments (fixes: #7763) (#7765)

Thank you, Pierce Freeman, for the report and minimal example!",2.0,1.0,"aten/src/ATen/native/Linear.cpp,test/test_torch.py",2.0,5,2,0.918295834,39.0,7772.0,2.0,1050577.5,665.0,3647.5,0.0,Corrective,1.0,1
pytorch,b54ad0897872fb5f4cb7a90046b0bf33836ad559,42d2e31cd6d798fe887559465452613378e4b821,kshitij12345,kshitijkalambarkar@gmail.com,Wed Dec 30 18:31:50 2020 -0800,1609353110.0,"[numpy] `torch.rsqrt` : promote integer inputs to float (#47909)

Summary:
Reference https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47909

Reviewed By: ngimel

Differential Revision: D25730876

Pulled By: mruberry

fbshipit-source-id: c87a8f686e1dd64e511640e0278021c4a584ccf2",31.0,10.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,test/test_torch.py,test/test_unary_ufuncs.py,torch/csrc/jit/tensorexpr/eval.cpp,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py",7.0,12,3,2.408463971,44.0,16101.0,3.0,469720.8571428572,7780.0,17599.0,0.0,,0.0,1
pytorch,bef460a803fa1b8acda6c91bd5a6efaefda8e2f1,4316bf98f5a45063e37eef9f7a267b29dc35587b,James Reed,jamesreed@fb.com,Thu Nov 19 05:55:00 2020 -0800,1605765300.0,"[FX] Refactor unique name handling (#48205)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48205

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D25068934

Pulled By: jamesr66a

fbshipit-source-id: 04e02bbfd2cc9a8c3b963d9afdf40bac065c319b",56.0,43.0,"test/test_fx_experimental.py,torch/fx/graph.py,torch/fx/proxy.py",3.0,3,2,0.978530623,1.0,1515.0,3.0,341359.0,6912.0,15675.5,0.0,Perfective,0.0,1
pytorch,89aed1a933aa00b48e74bd99deb475d6ca0a9169,43406e218ab9efdacbefd51679814bc75db4feb9,BowenBao,bowbao@microsoft.com,Mon Sep 14 22:31:04 2020 -0700,1600122664.0,"[ONNX] Update ONNX shape inference (#43929)

Summary:
* Support sequence type (de)serialization, enables onnx shape inference on sequence nodes.
* Fix shape inference with block input/output: e.g. Loop and If nodes.
* Fix bugs in symbolic discovered by coverage of onnx shape inference.
* Improve debuggability: added more jit logs. For simplicity, the default log level, when jit log is enabled, will not dump ir graphs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43929

Reviewed By: albanD

Differential Revision: D23674604

Pulled By: bzinodev

fbshipit-source-id: ab6aacb16d0e3b9a4708845bce27c6d65e567ba7",389.0,105.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.h,torch/csrc/jit/serialization/export.cpp,torch/csrc/jit/serialization/onnx.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",12.0,9,2,2.033836183,14.0,12397.0,6.0,1843138.0833333333,5112.0,11676.0,0.0,Corrective,1.0,1
pytorch,514cba0661f4c63cb6a7464e10f2766867eb38d5,43c9cc7a9cb19ac40c4296a7a7c0e1788bf5d288,Vasiliy Kuznetsov,vasiliy@fb.com,Thu Mar 12 16:27:47 2020 -0700,1584030467.0,"add quantized ELU activation (#34267)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34267

Adds quantized ELU.

Test Plan:
```
python test/test_quantized.py TestQuantizedOps.test_qelu
```

still need to benchmark, saving that for after the review comments

Imported from OSS

Differential Revision: D20370953

fbshipit-source-id: fe941bf966f72dd9eee2c4b2ef45fe7afb50c866",189.0,0.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qelu.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,benchmarks/operator_benchmark/pt/qactivation_test.py,test/test_quantized.py,torch/nn/quantized/functional.py",7.0,14,4,2.288194537,11.0,11116.0,3.0,127727.0,32.0,181.0,0.0,Feature Addition,0.0,1
pytorch,d496f9b20cff72a0158f31cba858061c783f3d06,43d1405d0d0c3725923973be412649c35c5a0584,SsnL,SsnL@users.noreply.github.com,Fri Nov 10 00:43:29 2017 -0500,1510274609.0,Fix ld* conditions for gemv ger gemm (#3604),31.0,3.0,"aten/src/TH/generic/THBlas.c,test/test_torch.py",2.0,5,2,0.672294817,38.0,5280.0,2.0,102993.5,2103.0,24055.35823,0.0,Corrective,1.0,1
pytorch,320ff3ad6405bada6f3541ba7ba772d153f93f82,4424b3e352850d5e3f5020a98e3253882e498f8d,Zach DeVito,zdevito@fb.com,Thu Nov 02 22:45:34 2017 -0700,1509662734.0,Update CMakeLists.txt in TH* libraries to support static builds.,85.0,42.0,"aten/src/TH/CMakeLists.txt,aten/src/THC/CMakeLists.txt,aten/src/THCS/CMakeLists.txt,aten/src/THCUNN/CMakeLists.txt,aten/src/THNN/CMakeLists.txt,aten/src/THS/CMakeLists.txt",6.0,8,1,2.550705513,1.0,1311.0,1.0,0.0,330.0,2148.5,0.0,,0.0,1
pytorch,e81878e0a9445c2b7674fdfea7da3287ec24f343,444039c47bb997f698c2bd9481c8649188038c51,Iurii Zdebskyi,iuriiz@fb.com,Tue Feb 19 16:17:49 2019 -0800,1550593069.0,"Bool tensor. Part 0: Boolean storage implementation (#16810)

Summary:
This is the first commit from a series of planned changes in order to add boolean tensors to PyTorch. The whole plan looks like this:

0. Storage Implementation (this change)
1. Tensor Creation.
2. Tensor Conversions.
3. Tensor Indexing.
4. Tensor Operations.
5. Back compatibility related changes.

This feature was requested by the community:
https://github.com/pytorch/pytorch/issues/4764
https://github.com/pytorch/pytorch/issues/4219
https://github.com/pytorch/pytorch/issues/4288

**Change**:
Added boolean type to the Storage class for CPU and CUDA backends.

**Tested via**:
1. unit tests
2. running this:
-> import torch
-> torch.BoolStorage
<class 'torch.BoolStorage'>
-> torch.cuda.BoolStorage
<class 'torch.cuda.BoolStorage'>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16810

Reviewed By: gchanan

Differential Revision: D14087246

Pulled By: izdeby

fbshipit-source-id: 042642ced1cb0fd1bb6bff05f9ca871a5c54ee5e",323.0,20.0,"aten/src/ATen/DLConvertor.cpp,aten/src/ATen/core/Type.h,aten/src/ATen/gen.py,aten/src/TH/CMakeLists.txt,aten/src/TH/THFile.h,aten/src/TH/THGenerateBoolType.h,aten/src/TH/THStorageFunctions.cpp,aten/src/TH/THStorageFunctions.h,aten/src/TH/THTensor.cpp,aten/src/TH/THTensor.h,aten/src/TH/THTensor.hpp,aten/src/TH/generic/THStorage.h,aten/src/TH/generic/THStorageCopy.cpp,aten/src/TH/generic/THStorageCopy.h,aten/src/TH/generic/THTensor.h,aten/src/THC/CMakeLists.txt,aten/src/THC/THCGenerateBoolType.h,aten/src/THC/THCStorage.cpp,aten/src/THC/THCStorage.cu,aten/src/THC/THCStorage.h,aten/src/THC/THCStorageCopy.cpp,aten/src/THC/THCStorageCopy.cu,aten/src/THC/THCStorageCopy.h,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensor.cu,aten/src/THC/THCTensor.h,aten/src/THC/THCTensor.hpp,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorCopy.h,aten/src/THC/generic/THCStorage.h,aten/src/THC/generic/THCStorageCopy.cpp,aten/src/THC/generic/THCStorageCopy.cu,aten/src/THC/generic/THCStorageCopy.h,aten/src/THC/generic/THCTensor.h,aten/src/THC/generic/THCTensorCopy.cu,c10/core/ScalarType.h,test/test_torch.py,torch/__init__.py,torch/_storage_docs.py,torch/csrc/DynamicTypes.cpp,torch/csrc/Module.cpp,torch/csrc/Storage.cpp,torch/csrc/Storage.h,torch/csrc/byte_order.cpp,torch/csrc/byte_order.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Storage.cpp,torch/csrc/cuda/Storage.h,torch/csrc/cuda/serialization.cpp,torch/csrc/cuda/serialization.h,torch/csrc/cuda/utils.cpp,torch/csrc/cuda/utils.h,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/csrc/serialization.cpp,torch/csrc/serialization.h,torch/csrc/utils.cpp,torch/csrc/utils.h,torch/csrc/utils/tensor_dtypes.cpp,torch/cuda/__init__.py,torch/storage.py",61.0,17,4,4.847387753,42.0,18252.0,22.0,4951865.694915255,7095.0,21769.83333,0.0,Feature Addition,0.0,1
pytorch,a31aea8eaa99a5ff72b5d002c206cd68d5467a5e,445b31abff8bfdbd6e74acc420585089e95ef585,Joel Schlosser,jbschlosser@fb.com,Tue Nov 23 16:17:56 2021 -0800,1637684276.0,"Initial version of general convolution_backward (#65219)

Summary:
Towards [convolution consolidation](https://fb.quip.com/tpDsAYtO15PO).

Introduces the general `convolution_backward` function that uses the factored-out backend routing logic from the forward function.

Some notes:
* `finput` is now recomputed in the backward pass for the slow 2d / 3d kernels instead of being saved from the forward pass. The logic for is based on the forward computation and is present in `compute_finput2d` / `compute_finput3d` functions in `ConvUtils.h`.
* Using structured kernels for `convolution_backward` requires extra copying since the backend-specific backward functions return tensors. Porting to structured is left as future work.
* The tests that check the routing logic have been renamed from `test_conv_backend_selection` -> `test_conv_backend` and now also include gradcheck validation using an `autograd.Function` hooking up `convolution` to `convolution_backward`. This was done to ensure that gradcheck passes for the same set of inputs / backends.

The forward pass routing is done as shown in this flowchart (probably need to download it for it to be readable since it's ridiculous):
![conv_routing_graph md](https://user-images.githubusercontent.com/75754324/137186002-5bca75ca-f911-4e61-8245-ec07af841506.png)

![conv_nogroup_routing_graph md](https://user-images.githubusercontent.com/75754324/139731619-9d0d436e-cce3-4bc3-8eaf-d469f667f0d7.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65219

Reviewed By: mruberry

Differential Revision: D32611368

Pulled By: jbschlosser

fbshipit-source-id: 26d759b7c908ab8f19ecce627acea7bd3d5f59ba",586.0,59.0,"aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,tools/autograd/derivatives.yaml",5.0,7,3,1.526857335,45.0,34928.0,4.0,453537.4,17283.0,40647.5,0.0,Feature Addition,0.0,1
pytorch,737aba3fc5ed009cd91a48715bf7765b6cb8a1a1,445cc1f5b9a1647949c7b27b60e9800339f88db9,gchanan,gregchanan@gmail.com,Wed Nov 15 15:24:51 2017 -0500,1510759491.0,"NativeFunctions: support backend-specific dispatch and SpatialRoIPooling (#3672)

* Support [output] in native_parse.

* allow specifying [output] in NativeFunctions.

Limitation: doesn't work for method, functions; can only do one or the other.

* Sample native function with output.

* spatial roi pooling forward skeleton (note, build is broken after this commit)

* Support multiple variants in native functions with outputs.

* add roi pooling forward cpu

* Add support for tuple return in NativeFunctions.

* native functions cuda

* fix bug in roi pool cpu forward

* finish forward kernel minus invocation

* add option for getting current stream

* Support backend-specific native function dispatch.

* Move cuda stuff to native.

* Move native related files to /native.

* Get rid of NativeFucntionsCuda.h.

* launch forward kernel

* roipool backward kernel

* Rebase expand error message changes.

* Fix up header files.

* add backward kernel launch, write as native function

* Default to base dispatch.

* Re-arrnage native_parse.py.

* Get rid of tabs.

* Get rid of at:: in C++ code in native function decl.

* Parse name.

* Parse name and return.

* Parse arguments.

* Don't specify variants.

* Get rid of /NativeFunction.

* Infer dispatch level.

* Infer dispatch.

* Improve argument parser.

* Comment, simplify parsing.

* Allow single line comments.

* Parse 'const Tensor &foo' correctly.

* Add comment to native_get_return_types.

* Fix python2 build by removing kwarg to rsplit.

* tabs --> spaces in roi foward cpu

* rename to RoiPooling2d

* add _cpu to roi pooling functions on cpu

* fix name handling in native functions

* Fix lint.

* Simplify default handling.

* Get rid of dispatch_level; infer it from dispatch.

* Simplify multiple return type native parsing.

* Move naming of outputs to gen.py from gen_variable_type.

* Get rid of m_ for type methods; keep only method_prefix_derived for s_ functions.

* add derivatives.yaml entry for roi pool

* Native functions parsed from yaml.

* Add comment explaining native_functions.yaml.

* Fix runtime_error string format.",820.0,495.0,"aten/CMakeLists.txt,aten/README.md,aten/src/ATen/CMakeLists.txt,aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/NativeFunctions.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/gen.py,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/cuda/NativeFunctionsCuda.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/ATen/preprocess_declarations.py,aten/src/ATen/templates/NativeFunctions.h,aten/src/ATen/templates/TypeDerived.cpp,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp",18.0,9,2,2.77567605,10.0,5306.0,13.0,427980.28571428574,2126.0,24093.35823,0.0,Corrective,1.0,1
pytorch,21a9a93eb4bf8660e24721bb24f69ebb0340f2fb,446e477d4f9656afb4bf1a93f2fc56d588803f4c,kshitij12345,kshitijkalambarkar@gmail.com,Tue Mar 23 19:44:42 2021 -0700,1616528682.0,"[complex] torch.rsub(): complex autograd support (#53702)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/53643

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53702

Reviewed By: agolynski

Differential Revision: D27142807

Pulled By: anjali411

fbshipit-source-id: 053d0a0f9a478cf04efcb0d84aacf042abae1a4e",108.0,4.0,"tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,0.429180764,14.0,7309.0,2.0,44984.0,10003.0,22164.0,0.0,Corrective,1.0,1
pytorch,f7cdd3a7a0fae058d7a587cc17b933e12db57d19,44779d9bc624cd288c63e1d43ed80de89968ec86,Chien-Chin Huang,chienchin@fb.com,Wed Dec 07 17:10:24 2022 +0000,1670433024.0,"[FSDP][optim_state_dict][2/N] Add _get_fqn_to_fsdp_param_info to map from original FQN to flat_param (#89899)

**Motivation:**
Add a helper to map from the FQN to the corresponding flat_param. The helper will directly get flat_param from fsdp_state and flat_handler as flat_param is not registered to the module if `use_orig_params` is True.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89899
Approved by: https://github.com/awgu",83.0,23.0,torch/distributed/fsdp/_optim_utils.py,1.0,3,1,0,1.0,1274.0,1.0,54707.0,10274.0,23527.0,0.0,Feature Addition,0.0,1
pytorch,64594d83336c28ea5f07b8572c9bcb1f0e7abde4,447bcd341dec5a2772f888acafa40c65091c735c,George Gensure,ggensure@uber.com,Tue Apr 07 05:48:33 2020 -0700,1586238513.0,"Bazel build of pytorch with gating CI (#36011)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/36011

Differential Revision: D20873430

Pulled By: malfet

fbshipit-source-id: 8ffffd10ca0ff8bdab578a70a9b2b777aed985d0",4516.0,53.0,".bazelrc,.bazelversion,.circleci/cimodel/data/pytorch_build_definitions.py,.circleci/config.yml,.circleci/generate_config_yml.py,.circleci/verbatim-sources/job-specs-custom.yml,.circleci/verbatim-sources/workflows-pytorch-bazel-builds.yml,.gitignore,.jenkins/pytorch/build.sh,.jenkins/pytorch/common.sh,.jenkins/pytorch/test.sh,BUILD.bazel,WORKSPACE,aten.bzl,third_party/BUILD,third_party/asmjit.BUILD,third_party/cpuinfo.BUILD,third_party/eigen.BUILD,third_party/fbgemm.BUILD,third_party/foxi.BUILD,third_party/gloo.BUILD,third_party/ideep.BUILD,third_party/miniz-2.0.8/BUILD.bazel,third_party/mkl-dnn.BUILD,third_party/mkl.BUILD,third_party/mkl_headers.BUILD,third_party/onnx.BUILD,third_party/sleef.BUILD,third_party/sleef.bzl,third_party/substitution.bzl,third_party/tbb.BUILD,third_party/tbb.patch,tools/config/BUILD,tools/config/defs.bzl,tools/rules/BUILD,tools/rules/cu.bzl,tools/rules/workspace.bzl",37.0,11,4,2.849136326,45.0,8086.0,3.0,235018.38709677415,832.0,2326.0,0.0,,0.0,1
pytorch,c889ff6cf8c18017490c7ed0e99a524fc5ff7422,447d74a0747be7686dc44bb9db57ca45954fc865,Richard Zou,zou3519@gmail.com,Wed Apr 10 01:09:01 2019 -0700,1554858541.0,"EmbeddingBag w/ differentiable per_sample_weights (#18957)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18957
ghimport-source-id: 7396ca08b137ea40f04285764a9d9a6d4f19227e

Reviewed By: cpuhrsch

Differential Revision: D14856526

Pulled By: zou3519

fbshipit-source-id: 949faea219c7c02ad981b1db610a477194d3f5c9",201.0,11.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/THBlasUtils.h,test/test_nn.py,tools/autograd/derivatives.yaml",6.0,9,3,1.89006375,43.0,16203.0,4.0,89364.16666666667,8013.0,24168.83333,0.0,,0.0,1
pytorch,88b61d132c5e148459a3ccc427214e7a9d5bede9,44bbb247a6658cd3a97d952badb1bc0a46961695,Jeff Daily,jeff.daily@amd.com,Fri Apr 22 19:50:36 2022 +0000,1650657036.0,"[ROCm] enable fsdp tests

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75632
Approved by: https://github.com/kumpera, https://github.com/malfet",5.0,5.0,".github/workflows/_rocm-test.yml,test/distributed/fsdp/test_fsdp_checkpoint.py,test/run_test.py",3.0,5,2,1.521928095,9.0,1477.0,3.0,2264461.0,2541.0,5998.5,0.0,,0.0,1
pytorch,a3494bd56b9434aae62ffb0f71bbf8d7c74a7b43,45024e7a353c64d80f47f50a700feb0def9a91ae,Lara Haidar,haidar.lara@gmail.com,Tue Nov 19 17:23:18 2019 -0800,1574184198.0,"Support Exporting Bitshift to ONNX (#28210)

Summary:
Support exporting left/right bitshifts to ONNX for all opset versions.

ONNX has a bitshift operator in opset 11, but it only supports unsigned ints, so it can't be used in PyTorch (since only uint8 is the only uint type).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28210

Reviewed By: hl475

Differential Revision: D18575512

Pulled By: houseroad

fbshipit-source-id: 74161db67f599996a0614981edcc171af6780d21",271.0,1.0,".jenkins/caffe2/test.sh,test/onnx/expect/TestOperators.test_bitshift.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",6.0,7,3,1.739427135,7.0,5395.0,4.0,386680.6,13281.0,36385.83333,0.0,,0.0,1
pytorch,8ebf18b5b1d57ef16c24366649e720867f394a98,4518793aa2affdc473a89935378d5121e048bcb7,Sam Gross,colesbury@gmail.com,Tue Nov 21 18:19:00 2017 -0500,1511288340.0,"Implement indexing in ATen (#3725)

Implements basic and advanced indexing using ATen tensors/variables.
Basic indexing is translated at the Python-binding level
(python_variable_indexing.cpp) to slice/squeeze/unsqueeze/select calls.
Advanced indexing is implemented in ATen in terms of take() and put()
calls.",1065.0,334.0,"aten/src/ATen/ExpandUtils.h,aten/src/ATen/WrapDimUtils.h,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/native_functions.yaml,setup.py,test/test_autograd.py,test/test_indexing.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_variable_methods.cpp,tools/jit/gen_jit_dispatch.py,torch/autograd/_functions/tensor.py,torch/autograd/variable.py,torch/csrc/Exceptions.cpp,torch/csrc/Exceptions.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/autograd/python_variable_indexing.h,torch/csrc/generic/Tensor.cpp,torch/csrc/utils.h,torch/csrc/utils/python_compat.h,torch/nn/utils/rnn.py",23.0,18,4,2.980933275,40.0,9564.0,13.0,653615.2222222222,346.0,990.9058694,0.0,,0.0,1
pytorch,c45f1add9732c9e1fa38a24e2f6051600668a990,452ebd03a95463900c93cd5563d205a5c87c2e99,Mike Ruberry,mruberry@devfair044.h1.fair,Sat Apr 16 01:14:28 2022 +0000,1650071668.0,"OpInfos for triplet_margin_loss and triplet_margin_with_distance_loss

Per title.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75875
Approved by: https://github.com/ngimel",50.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,17025.0,1.0,5678.0,2347.0,5497.5,0.0,,0.0,1
pytorch,5d82cefa551c6b64ec9d10da0eaa54d1adc46c59,45391ccecbe90fcde0ca43071f0553a8eaa58d07,Supriya Rao,supriyar@fb.com,Tue Sep 24 00:55:18 2019 -0700,1569286518.0,"Update qengine flag in python to string (#26620)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26620

This change updates torch.backend.quantized.engine to accept string (""fbgemm""/""qnnpack""/""none"" for now).
set_qengine and get_qengine return an int which represents the at::QEngine enum

Test Plan:
python test/test_torch.py

Imported from OSS

Differential Revision: D17533582

fbshipit-source-id: 5103263d0d59ff37d43dec27243cb76ba8ba633f",47.0,175.0,"aten/src/ATen/Context.cpp,c10/core/QEngine.h,test/common_quantized.py,test/test_torch.py,tools/build_variables.py,torch/CMakeLists.txt,torch/backends/quantized/__init__.py,torch/csrc/Module.cpp,torch/csrc/QEngine.cpp,torch/csrc/QEngine.h,torch/csrc/utils/qengines.cpp,torch/csrc/utils/qengines.h",12.0,12,5,2.802087178,42.0,15140.0,7.0,486215.8333333333,11651.0,32805.83333,0.0,,0.0,1
pytorch,0705f759a326491a91af1f77874a5c11a60c716e,454bf21b367002556ce9a92a919eed884375d4c6,shihongzhi,shi65881583@gmail.com,Thu Sep 19 21:28:44 2019 -0700,1568928524.0,"port lgamma from TH to Aten (#25138)

Summary:
https://github.com/pytorch/pytorch/issues/24722
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25138

Differential Revision: D17171782

Pulled By: VitalyFedyunin

fbshipit-source-id: b0026f0ce5306debf19036f97b8624bf0a56f349",18.0,5.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/native_functions.yaml",9.0,7,1,2.968151629,12.0,11187.0,7.0,2660489.222222222,11548.0,32500.83333,0.0,,0.0,1
pytorch,da0fad8a7a3721eccdd60460914b62dda95daff9,457587088a382013c5906e4d2f99634637a02347,Alykhan Tejani,alykhan.tejani@gmail.com,Sun Jul 02 03:06:36 2017 +0100,1498964796.0,"Fix broadcasting issues in binary_cross_entropy_with_logits (#1944)

* done re-seed cuda device if in bad fork

* avoid broadcasting in binary_cross_entropy_with_logits

* assert input sizes for BCEWithLogitLoss

* added check that BCEWithLogitsLoss == Sigmoid + BCELoss

* fix flake8 issues

* rename test_bce_with_logits_gives_same_result_as_bce_and_sigmoid -> test_bce_with_logits_gives_same_result_as_sigmooid_and_bce_loss

* add warning in BCELoss about input shapes

* fix lint",34.0,0.0,"test/test_nn.py,torch/nn/_functions/thnn/loss.py,torch/nn/functional.py",3.0,5,2,1.260771794,30.0,4535.0,2.0,338598.0,1064.0,12591.54911,0.0,Corrective,1.0,1
pytorch,9594a94d80d29e119e686114439f0f595e89cea4,457ba1dd3e9dbe61c396f06e7fa7ad9612d56bc9,Brian Hirsh,hirsheybar@fb.com,Tue Dec 14 19:51:37 2021 -0800,1639511497.0,"Porting index_add to structured kernels, add an out variant (#65993)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65993

This PR attempts to port `index_add` to structured kernels, but does more than that:

* Adds an `out=` variant to `index_add`
* Revises `native_functions.yaml` registrations, to not have multiple entries and instead pass default value to `alpha`.
* Changes in `derivatives.yaml` file for autograd functioning
* Revises error messages, please see: https://github.com/pytorch/pytorch/pull/65993#issuecomment-945441615

Follow-up PRs in near future will attempt to refactor the OpInfo test, and will give another look at tests in `test/test_torch.py` for this function. (hence the use of ghstack for this)

~This is WIP because there are tests failing for `Dimname` variant on mobile/android builds, and I'm working on fixing them.~

Issue tracker: https://github.com/pytorch/pytorch/issues/55070

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D32646426

fbshipit-source-id: b035ecf843a9a27d4d1e18b202b035adc2a49ab5",115.0,86.0,"aten/src/ATen/native/NamedTensor.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/cuda/Indexing.cu,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/backward_compatibility/check_backward_compatibility.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/onnx/symbolic_opset9.py,torch/testing/_internal/common_methods_invocations.py",13.0,15,5,2.339358711,46.0,63228.0,11.0,1278418.076923077,17729.0,41889.5,0.0,Corrective,1.0,1
pytorch,60e8615a6dc4090ee4dd0d6b30c55edd7ef0b26c,45a504dd2d40780c56b04c9e8cbf07c372287d01,James Reed,jamesreed@fb.com,Sat Mar 07 17:59:11 2020 -0800,1583603951.0,"[JIT] Introduce BuiltinOpFunction and integrate into torchbind (#34098)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34098

* #33900 [JIT] Move stuff out of class_type.cpp

Test Plan: Imported from OSS

Differential Revision: D20229166

Pulled By: jamesr66a

fbshipit-source-id: d658a63a5d6e372e675f35b8456adc8de82b49f3",358.0,187.0,"aten/src/ATen/core/boxing/kernel_functor.h,aten/src/ATen/core/builtin_function.h,aten/src/ATen/core/custom_class.cpp,aten/src/ATen/core/function.h,aten/src/ATen/core/ivalue.cpp,aten/src/ATen/core/ivalue.h,aten/src/ATen/core/ivalue_inl.h,aten/src/ATen/core/jit_type.h,aten/src/ATen/core/op_registration/infer_schema.h,aten/src/ATen/core/type.cpp,caffe2/CMakeLists.txt,test/cpp/jit/test_custom_class.cpp,tools/build_variables.bzl,torch/csrc/jit/api/custom_class.cpp,torch/csrc/jit/api/custom_class.h,torch/csrc/jit/api/function_impl.h,torch/csrc/jit/frontend/schema_type_parser.cpp,torch/csrc/jit/ir/ir.cpp,torch/csrc/jit/passes/inliner.cpp,torch/csrc/jit/python/python_custom_class.cpp,torch/csrc/jit/python/script_init.cpp,torch/csrc/jit/runtime/interpreter.cpp,torch/csrc/jit/serialization/import_source.cpp,torch/csrc/jit/serialization/python_print.cpp,torch/custom_class.h,torch/custom_class_detail.h",26.0,21,5,3.686588207,13.0,15402.0,10.0,871391.24,15289.0,40984.83333,0.0,Feature Addition,0.0,1
pytorch,bbc71435b7bbaee310f488be766b1a37bb9a08ca,45b33c83f1434f4d1f4ec4e4499dbefd1d67a050,Heitor Schueroff,heitorschueroff@fb.com,Wed Dec 16 18:23:50 2020 -0800,1608143030.0,"Revert ""Revert D24923679: Fixed einsum compatibility/performance issues (#46398)"" (#49189)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/49189

This reverts commit d307601365c3b848072b8b8381208aedc1a0aca5 and fixes the bug with diagonals and ellipsis combined.

Test Plan: Imported from OSS

Reviewed By: glaringlee

Differential Revision: D25540722

Pulled By: heitorschueroff

fbshipit-source-id: 86d0c9a7dcfda600b546457dad102af2ff33e353",596.0,348.0,"aten/src/ATen/native/Linear.cpp,test/test_linalg.py,torch/functional.py",3.0,6,3,1.446918971,30.0,7860.0,3.0,603803.3333333334,7543.0,16855.0,0.0,Corrective,1.0,1
pytorch,04a33453358ec5aef96d481cfc759b3ebaaf6f10,45c9ed825a51c05395fd28b376271324e0315d0c,peter,peterghost86@gmail.com,Fri Mar 27 21:22:35 2020 -0700,1585344155.0,"Formatting cmake (to lowercase without space for if/elseif/else/endif) (#35521)

Summary:
Running commands:
```bash
shopt -s globstar

sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i caffe2/**/CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i torch/**/CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i c10/**/CMakeLists.txt
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i cmake/**/*.cmake
sed -e 's/IF (/if(/g' -e 's/IF(/if(/g' -e 's/if (/if(/g' -e 's/ELSE (/else(/g' -e 's/ELSE(/else(/g' -e 's/else (/else(/g' -e 's/ENDif(/endif(/g' -e 's/ELSEif(/elseif(/g' -i cmake/**/*.cmake.in
```
We may further convert all the commands into lowercase according to the following issue: https://gitlab.kitware.com/cmake/cmake/commit/77543bde41b0e52c3959016698b529835945d62d.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35521

Differential Revision: D20704382

Pulled By: malfet

fbshipit-source-id: 42186b9b1660c34428ab7ceb8d3f7a0ced5d2e80",470.0,470.0,"c10/CMakeLists.txt,c10/benchmark/CMakeLists.txt,c10/cuda/CMakeLists.txt,c10/cuda/test/CMakeLists.txt,c10/hip/CMakeLists.txt,c10/test/CMakeLists.txt,caffe2/contrib/CMakeLists.txt,caffe2/contrib/nccl/CMakeLists.txt,caffe2/contrib/prof/CMakeLists.txt,caffe2/core/CMakeLists.txt,caffe2/db/CMakeLists.txt,caffe2/distributed/CMakeLists.txt,caffe2/mobile/contrib/CMakeLists.txt,caffe2/mobile/contrib/ios/CMakeLists.txt,caffe2/mobile/contrib/nnapi/CMakeLists.txt,caffe2/operators/CMakeLists.txt,caffe2/operators/rnn/CMakeLists.txt,caffe2/perfkernels/CMakeLists.txt,caffe2/proto/CMakeLists.txt,caffe2/quantization/server/CMakeLists.txt,caffe2/share/CMakeLists.txt,caffe2/share/contrib/CMakeLists.txt,caffe2/utils/CMakeLists.txt,cmake/Caffe2Config.cmake.in,cmake/Caffe2ConfigVersion.cmake.in,cmake/Codegen.cmake,cmake/Dependencies.cmake,cmake/External/nccl.cmake,cmake/External/nnpack.cmake,cmake/External/rccl.cmake,cmake/MiscCheck.cmake,cmake/ProtoBuf.cmake,cmake/ProtoBufPatch.cmake,cmake/TorchConfig.cmake.in,cmake/TorchConfigVersion.cmake.in,cmake/Utils.cmake,cmake/Whitelist.cmake,cmake/cmake_uninstall.cmake.in,cmake/iOS.cmake,cmake/public/LoadHIP.cmake,cmake/public/cuda.cmake,cmake/public/gflags.cmake,cmake/public/glog.cmake,cmake/public/protobuf.cmake,cmake/public/utils.cmake,torch/lib/libshm/CMakeLists.txt,torch/lib/libshm_windows/CMakeLists.txt",47.0,33,4,3.824780796,52.0,5793.0,45.0,31693361.59574468,553.0,1624.0,0.0,,0.0,1
pytorch,89d6e8804267d467e39529d710e2da39982905fa,45cf33a731598df01d83f71cf69913a3419bcd4f,shihongzhi,shihzh@rd.netease.com,Thu Jul 11 16:16:59 2019 -0700,1562861819.0,"add fill_diagonal function (#21892)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/21796
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21892

Differential Revision: D16164678

Pulled By: colesbury

fbshipit-source-id: 85df8ae9b7a6a91b6023fe7295b3a8124e4526ea",139.0,0.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,test/test_torch.py,torch/_tensor_docs.py",8.0,9,4,1.98491289,42.0,26058.0,5.0,234338.25,9883.0,28665.83333,0.0,Corrective,1.0,1
pytorch,01581037dca1176508f4544b377a4c0f2d0dd165,45e5c17ecfbe00708e2c61488d688b3d651ab8ca,Yinghai Lu,yinghai@fb.com,Fri Jul 20 22:08:02 2018 -0700,1532124482.0,"ONNXIFI transform (#9569)

Summary:
Cut-off runnable subgraph and off-load to ONNXIFI backend
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9569

Reviewed By: Maratyszcza

Differential Revision: D8930408

Pulled By: yinghai

fbshipit-source-id: 2b494f7f8dc10c00e58cf0fed5c4a9434be6155b",689.0,10.0,"caffe2/operators/onnxifi_op.cc,caffe2/operators/onnxifi_op.h,caffe2/opt/onnxifi_transformer.cc,caffe2/opt/onnxifi_transformer.h,caffe2/python/onnx/onnxifi.py,caffe2/python/onnx/test_onnxifi.py,caffe2/python/pybind_state.cc,caffe2/python/trt/transform.py",8.0,6,1,1.942397206,9.0,2193.0,4.0,1429096.8,3035.0,7293.333333,0.0,,0.0,1
pytorch,ad2d413c0b6a3a6d0f5d4ccb7af15acd43fad07b,45ef25ea2715154bf52c468f60e0bb5b7fa9c11d,Soumith Chintala,soumith@fb.com,Thu Oct 27 04:49:50 2016 -0400,1477543790.0,fix rnn documentation typos and format,23.0,23.0,torch/nn/modules/rnn.py,1.0,3,1,0,2.0,415.0,1.0,262831.0,197.0,912.0954365,0.0,Corrective,1.0,1
pytorch,514f87a16cda5f0a5b0fe8dfbfac4c12fe4162e0,4613eef69e57f8f26e41e1f375458ed9f3de179b,Peter Goldsborough,peter@goldsborough.me,Thu Mar 15 05:12:51 2018 -0700,1521090771.0,"Simplify run_test.py and dont use shell=True (#5767)

* Simplify run_test.py and dont use shell=True

* Fix non-shell output for check_output and always print to stderr

* Use shlex.split instead of str.split

* s/log/print_to_stderr

* with_init -> with_init_file

* Remove bufsize argument",25.0,22.0,test/run_test.py,1.0,1,1,0,5.0,240.0,1.0,62873.0,488.0,2377.5,0.0,Corrective,1.0,1
pytorch,46b83212d111ba0314e11cb7a308d63ae6532ec9,461aafe389b7b5b415c3b4878abaa0ba1769f132,Kshiteej K,kshitijkalambarkar@gmail.com,Wed Dec 23 02:41:13 2020 -0800,1608691273.0,"[numpy] `torch.angle`: promote integer inputs to float (#49163)

Summary:
**BC-Breaking Note:**

This PR updates PyTorch's angle operator to be consistent with NumPy's. Previously angle would return zero for all floating point values (including NaN). Now angle returns `pi` for negative floating point values, zero for non-negative floating point values, and propagates NaNs.

**PR Summary:**

Reference: https://github.com/pytorch/pytorch/issues/42515

TODO:

* [x] Add BC-Breaking Note (Prev all real numbers returned `0` (even `nan`)) -> Fixed to match the correct behavior of NumPy.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49163

Reviewed By: ngimel

Differential Revision: D25681758

Pulled By: mruberry

fbshipit-source-id: 54143fe6bccbae044427ff15d8daaed3596f9685",94.0,32.0,"aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/zmath.h,aten/src/ATen/native/cuda/UnaryComplexKernels.cu,test/test_torch.py,test/test_unary_ufuncs.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",13.0,12,3,3.417512916,45.0,26945.0,9.0,3466228.153846154,7735.0,17438.5,0.0,Corrective,1.0,1
pytorch,084e3a755b0ecb8c725041586235b3abc24596ae,46374ad5c8739f9c186d74aaf2cfeca09cced453,gchanan,gregchanan@gmail.com,Mon Apr 16 14:50:34 2018 -0400,1523890234.0,"Add tensor.to(device) method. (#6588)

* Add tensor.on(device) and tensor.on_device_as(tensor) methods.

* Rename {'on', 'on_device_as'} -> 'to'.

* Fix test ordinal.

* Fix device ordinal again.",64.0,0.0,"test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp",2.0,4,2,0.997180399,39.0,7257.0,2.0,294787.5,886.0,2126.305292,0.0,Corrective,1.0,1
pytorch,56bed0dcfe7ca9047e5c95a6f3d7fcb0ec403b0c,465e0ae2665b5474edf494247c9c809fbaac5210,Mikayla Gawarecki,mikaylagawarecki@gmail.com,Thu May 05 16:06:44 2022 +0000,1651766804.0,"Bugfix scatter_reduce backward formulas

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76523

Approved by: https://github.com/albanD",80.0,16.0,"test/test_autograd.py,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py",3.0,6,2,1.464461202,45.0,32263.0,3.0,260372.0,2911.0,7004.5,0.0,Corrective,1.0,1
pytorch,0025e1c776938cc824974a374ba3156c98941510,46a868dab72a4fce5a12b462389ee7a7865f3210,Leonid Vlasenkov,leo.vlasenkov@gmail.com,Mon Jul 10 14:24:54 2017 +0300,1499696694.0,"[Ready] Limit docs line length (#1900)

* some docs are ready

* docs

* docs

* fix some more

* fix some more",893.0,600.0,"CONTRIBUTING.md,torch/_storage_docs.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/_utils.py,torch/autograd/variable.py,torch/nn/functional.py,torch/nn/init.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/distance.py,torch/nn/modules/dropout.py,torch/nn/modules/linear.py,torch/nn/modules/loss.py,torch/nn/modules/rnn.py,torch/nn/parallel/distributed.py,torch/nn/utils/clip_grad.py,torch/nn/utils/rnn.py,torch/optim/adadelta.py,torch/optim/adagrad.py,torch/optim/asgd.py,torch/optim/lbfgs.py,torch/optim/lr_scheduler.py,torch/optim/rmsprop.py,torch/optim/rprop.py,torch/optim/sgd.py,torch/serialization.py,torch/tensor.py",30.0,7,1,3.257736678,31.0,14703.0,5.0,462814.93333333335,1106.0,16375.4346,0.0,Corrective,1.0,1
pytorch,9f0b2c73f36b0f5276f84cdaaef4d54a60df61f5,46ba0150cbfb8d86c378f0f3ce2d816e530a933b,Huy Do,huydhn@gmail.com,Wed Nov 16 02:39:22 2022 +0000,1668566362.0,"Increase slow grad check timeout (#89079)

Now that periodic jobs are run under `mem_leak_check` mode with parallelization turning off. It's very easy for `linux-bionic-cuda11.6-py3-gcc7-slow-gradcheck / test` to timeout because one of the shards is very close to the 4h mark.

* https://hud.pytorch.org/pytorch/pytorch/commit/2452e3f99a072760fc46d3f9025aaa37ca7ea2ab
* https://hud.pytorch.org/pytorch/pytorch/commit/35e668b5ced25e735b6e523d557ed7fd60267914

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89079
Approved by: https://github.com/clee2000",8.0,1.0,".github/workflows/_linux-test.yml,.github/workflows/periodic.yml",2.0,2,1,0.503258335,1.0,475.0,2.0,524478.0,9538.0,22204.5,0.0,,0.0,1
pytorch,7fa60b2e4452a1a289fe87ce90559196e03aed32,46bc43a80f2b5a4fc20c729fb0c65cd595de5f38,Soumith Chintala,soumith@fb.com,Wed Jan 04 23:40:13 2017 -0500,1483573213.0,fixing loss layer docs,118.0,79.0,"torch/nn/modules/activation.py,torch/nn/modules/loss.py",2.0,3,1,0.081801001,20.0,979.0,2.0,97233.0,310.0,6393.724559,0.0,Corrective,1.0,1
pytorch,d777e490a5843a44b9aadbf3f97cd092be20c729,472a6f2787f5e232ff9c4df9c5467dc55b6879ef,Pearu Peterson,pearu.peterson@gmail.com,Mon Oct 18 18:05:47 2021 -0700,1634580347.0,"Strided masked reductions: sum, amax. Testing of masked reductions. (#65990)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65990

cc nikitaved pearu cpuhrsch IvanYashchuk

Test Plan: Imported from OSS

Reviewed By: zou3519

Differential Revision: D31729532

Pulled By: albanD

fbshipit-source-id: 855a6bb2a7c6e75c780a64ce23c0f29321f0e511",446.0,4.0,"test/test_fx.py,test/test_fx_experimental.py,test/test_reductions.py,torch/__init__.py,torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",6.0,5,2,1.384889157,41.0,20557.0,5.0,179846.4,16325.0,38219.5,0.0,,0.0,1
pytorch,375c30a7177442fb9d6de7516a9ae4031ae324c4,4774c6800b1f088f31967ec34403d1676d2214b4,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Sun Jan 10 11:59:06 2021 -0800,1610279946.0,"Added linalg.inv (#48261)

Summary:
This PR adds `torch.linalg.inv` for NumPy compatibility.

`linalg_inv_out` uses in-place operations on provided `result` tensor.

I modified `apply_inverse` to accept tensor of Int instead of std::vector, that way we can write a function similar to `linalg_inv_out` but removing the error checks and device memory synchronization.

I fixed `lda` (leading dimension parameter which is max(1, n)) in many places to handle 0x0 matrices correctly.
Zero batch dimensions are also working and tested.

Ref https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48261

Reviewed By: gchanan

Differential Revision: D25849590

Pulled By: mruberry

fbshipit-source-id: cfee6f1daf7daccbe4612ec68f94db328f327651",464.0,118.0,"aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cuda/CUDABlas.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/api/include/torch/linalg.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",16.0,19,5,3.040648184,16.0,28948.0,4.0,667375.5625,7981.0,18139.5,0.0,Corrective,1.0,1
pytorch,90c7db8ae3c0638baea262303db13ebf16c6c490,477f1c047c3515a8d237b545d35bbfcea570e786,Nick Gibson,nickg@fb.com,Thu Apr 09 21:32:31 2020 -0700,1586467951.0,"[TensorExpr] add simplication of constant branches to IR Simplifier (#36257)

Summary:
Adds handling of constant branches to the TensorExpr IR Simplifier. This covers both IfThenElse and Cond when the condition expression is a known constant (e.g. `IfThenElse(1, X, Y) => X`), or when both arms of the branch are the same (e.g. `IfThenElse(Y, X, X) => X`).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36257

Differential Revision: D20947777

Pulled By: nickgg

fbshipit-source-id: 974379e42a6d65ce3e7178622afb62d36ad4e380",232.0,0.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",4.0,7,2,1.056494653,2.0,3541.0,3.0,53576.0,928.0,2516.5,0.0,Feature Addition,0.0,1
pytorch,3761adc889a8182b3620d936a1f2ed07d562bdf5,478886be308c5c14f5898bfe4d9833f4debba20a,Ailing Zhang,ailzhang@fb.com,Thu Oct 25 01:10:32 2018 -0700,1540429832.0,"Fix print precision and match numpy behavior (#12746)

Summary:
Fixes #12578 #9395.

* Fix and simplify print logic

* Follow numpy print rule https://github.com/numpy/numpy/blob/eb2bd11870731ea19a0eee72e616c7deb00f6c54/numpy/core/arrayprint.py#L859
> scientific notation is used when absolute value of the smallest number is < 1e-4 or maximum > 1e8 or the ratio of the maximum absolute value to the minimum is > 1e3

I hope I didn't break anything since there seems to be a lot of edge cases here... Here are some easy sanity checks.
```
In [5]: torch.tensor(1)
Out[5]: tensor(1)
Out[2]: array(1) # numpy

In [6]: torch.tensor(10)
Out[6]: tensor(10)
Out[3]: array(10) # numpy

In [8]: torch.tensor(99000000)
Out[8]: tensor(99000000)
Out[5]: array(99000000) # numpy

In [9]: torch.tensor(100000000)
Out[9]: tensor(100000000)
Out[6]: array(100000000) # numpy

In [10]: torch.tensor(100000001)
Out[10]: tensor(100000001)
Out[7]: array(100000001) # numpy

In [11]: torch.tensor(1000000000)
Out[11]: tensor(1000000000)
Out[8]: array(1000000000) # numpy

In [12]: torch.tensor([1, 1000])
Out[12]: tensor([   1, 1000])
Out[9]: array([   1, 1000]) # numpy

In [13]: torch.tensor([1, 1010])
Out[13]: tensor([   1, 1010])
Out[10]: array([   1, 1010]) # numpy
```
For floating points, we use scientific when `max/min > 1000 || max > 1e8 || min < 1e-4`
Lines with ""old"" are old behaviors that either has precision issue, or not aligned with numpy
```
In [14]: torch.tensor(0.01)
Out[14]: tensor(0.0100)
Out[11]: array(0.01) # numpy

In [15]: torch.tensor(0.1)
Out[15]: tensor(0.1000)
Out[12]: array(0.1) # numpy

In [16]: torch.tensor(0.0001)
Out[16]: tensor(0.0001)
Out[14]: array(0.0001) # numpy

In [17]: torch.tensor(0.00002)
Out[17]: tensor(2.0000e-05)
Out[15]: array(2e-05) # numpy
Out[5]: tensor(0.0000) # old

In [18]: torch.tensor(1e8)
Out[18]: tensor(100000000.)
Out[16]: array(100000000.0) # numpy

In [19]: torch.tensor(1.1e8)
Out[19]: tensor(1.1000e+08)
Out[17]: array(1.1e8) # numpy 1.14.5, In <= 1.13 this was not using scientific print
Out[10]: tensor(110000000.) # old

In [20]: torch.tensor([0.01, 10.])
Out[20]: tensor([ 0.0100, 10.0000])
Out[18]: array([  0.01,  10.  ]) # numpy

In [21]: torch.tensor([0.01, 11.])
Out[21]: tensor([1.0000e-02, 1.1000e+01])
Out[19]: array([  1.00000000e-02,   1.10000000e+01]) # numpy
Out[7]: tensor([ 0.0100, 11.0000]) # old
```
When print floating number in int mode, we still need to respect rules to use scientific mode first
```
In [22]: torch.tensor([1., 1000.])
Out[22]: tensor([   1., 1000.])
Out[20]: array([    1.,  1000.]) # numpy

In [23]: torch.tensor([1., 1010.])
Out[23]: tensor([1.0000e+00, 1.0100e+03])
Out[21]: array([  1.00000000e+00,   1.01000000e+03]) # numpy
Out[9]: tensor([   1., 1010.]) # old
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12746

Differential Revision: D10443800

Pulled By: ailzhang

fbshipit-source-id: f5e4e3fe9bf0b44af2c64c93a9ed42b73fa613f5",120.0,74.0,"test/expect/TestTorch.test_print-bigint.expect,test/expect/TestTorch.test_print-default_device.expect,test/expect/TestTorch.test_print-default_dtype.expect,test/expect/TestTorch.test_print-device.expect,test/expect/TestTorch.test_print-dtype.expect,test/expect/TestTorch.test_print-negint.expect,test/expect/TestTorch.test_print-nonfinite.expect,test/expect/TestTorch.test_print-posint.expect,test/expect/TestTorch.test_print-requires_grad.expect,test/expect/TestTorch.test_print-scimode.expect,test/expect/TestTorch.test_print-summary.expect,test/test_torch.py,torch/_tensor_str.py",13.0,3,2,1.513998223,40.0,9482.0,3.0,1853901.923076923,4859.0,14327.83333,0.0,Corrective,1.0,1
pytorch,cc3284cad387ffd483444130a521ed25e63bab93,47bd4be4d35635579242eb99b50a4be14c351def,Richard Zou,zou3519@users.noreply.github.com,Thu Apr 19 17:16:07 2018 -0400,1524158167.0,"[docs] More factory functions (#6709)

* More factory functions

Changes:
- Added the remaining factory and factory-like functions
- Better argument reuse via string templates
- Link under torch.rst's Creation Ops to the randomized creation ops

* Add double tick around False

* fix flake8

* Fix False

* Clarify comment: hopefully it is clearer now",207.0,68.0,"docs/source/torch.rst,torch/_torch_docs.py",2.0,3,2,0.305400571,31.0,6825.0,1.0,106480.0,921.0,2232.305292,0.0,Corrective,1.0,1
pytorch,521910e0e97f6014c976cdab7dff024a038a0a76,47c4dca1ab3fedfde7b1ce383e779454e7903e86,Nikita Shulga,nshulga@fb.com,Sat Apr 25 00:39:53 2020 -0700,1587775193.0,"Remove python-2 or python<3.5 checks from unit tests (#37252)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37252

Test Plan: CI

Differential Revision: D21241083

Pulled By: malfet

fbshipit-source-id: 44164b822f7905288abb2beda0175d2162d86143",20.0,62.0,"test/custom_operator/test_custom_classes.py,test/jit/test_list_dict.py,test/run_test.py,test/test_jit.py,test/test_serialization.py,test/test_type_hints.py",6.0,3,1,2.090843342,14.0,21068.0,5.0,572315.1666666666,1384.0,3707.0,0.0,,0.0,1
pytorch,a6b358d53b495a4971018672b9f0b7ef2994da9a,47c566ebb1a0f413b6300cf9700a9cfddb2e5b40,Winston Smith,76181208+imaginary-person@users.noreply.github.com,Wed May 19 23:03:08 2021 -0700,1621465388.0,"Rename namespace `vec256` to `vec`, struct `Vec256` to `Vectorized` (and other related classes/structs) (#58438)

Summary:
In order to make it more convenient for maintainers to review the ATen AVX512 implementation, the namespace `vec256` is being renamed to `vec` in this PR, as modifying 77 files & creating 2 new files only took a few minutes, as these changes aren't significant, so fewer files would've to be reviewed while reviewing https://github.com/pytorch/pytorch/issues/56992.
The struct `Vec256` is not being renamed to `Vec`, but `Vectorized` instead, because there are some `using Vec=` statements in the codebase, so renaming it to `Vectorized` was more convenient. However, I can still rename it to `Vec`, if required.

### Changes made in this PR -
Created `aten/src/ATen/cpu/vec` with subdirectory `vec256` (vec512 would be added via https://github.com/pytorch/pytorch/issues/56992).
The changes were made in this manner -

1. First, a script was run to rename `vec256` to `vec` & `Vec` to `Vectorized` -
```
# Ref: https://stackoverflow.com/a/20721292
cd aten/src
grep -rli 'vec256\/vec256\.h' * | xargs -i@ sed -i 's/vec256\/vec256\.h/vec\/vec\.h/g' @
grep -rli 'vec256\/functional\.h' * | xargs -i@ sed -i 's/vec256\/functional\.h/vec\/functional\.h/g' @
grep -rli 'vec256\/intrinsics\.h' * | xargs -i@ sed -i 's/vec256\/intrinsics\.h/vec\/vec256\/intrinsics\.h/g' @
grep -rli 'namespace vec256' * | xargs -i@ sed -i 's/namespace vec256/namespace vec/g' @
grep -rli 'Vec256' * | xargs -i@ sed -i 's/Vec256/Vectorized/g' @
grep -rli 'vec256\:\:' * | xargs -i@ sed -i 's/vec256\:\:/vec\:\:/g' @
grep -rli 'at\:\:vec256' * | xargs -i@ sed -i 's/at\:\:vec256/at\:\:vec/g' @
cd ATen/cpu
mkdir vec
mv vec256 vec
cd vec/vec256
grep -rli 'cpu\/vec256\/' * | xargs -i@ sed -i 's/cpu\/vec256\//cpu\/vec\/vec256\//g' @
grep -rli 'vec\/vec\.h' * | xargs -i@ sed -i 's/vec\/vec\.h/vec\/vec256\.h/g' @
```

2. `vec256` & `VEC256` were replaced with `vec` & `VEC` respectively in 4 CMake files.

3. In `pytorch_vec/aten/src/ATen/test/`, `vec256_test_all_types.h` & `vec256_test_all_types.cpp` were renamed.

4. `pytorch_vec/aten/src/ATen/cpu/vec/vec.h` & `pytorch_vec/aten/src/ATen/cpu/vec/functional.h` were created.
Both currently have one line each & would have 5 when AVX512 support would be added for ATen.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58438

Reviewed By: malfet

Differential Revision: D28509615

Pulled By: ezyang

fbshipit-source-id: 63840df5f23b3b59e203d25816e2977c6a901780",15901.0,15898.0,"aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/cpu/FlushDenormal.cpp,aten/src/ATen/cpu/vec/functional.h,aten/src/ATen/cpu/vec/vec.h,aten/src/ATen/cpu/vec/vec256/functional.h,aten/src/ATen/cpu/vec/vec256/intrinsics.h,aten/src/ATen/cpu/vec/vec256/missing_vld1_neon.h,aten/src/ATen/cpu/vec/vec256/missing_vst1_neon.h,aten/src/ATen/cpu/vec/vec256/vec256.h,aten/src/ATen/cpu/vec/vec256/vec256_base.h,aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec/vec256/vec256_double.h,aten/src/ATen/cpu/vec/vec256/vec256_float.h,aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vec/vec256/vec256_int.h,aten/src/ATen/cpu/vec/vec256/vec256_qint.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_common_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_float_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_int16_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_int32_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_int64_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_qint32_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_qint8_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vec256_quint8_vsx.h,aten/src/ATen/cpu/vec/vec256/vsx/vsx_helpers.h,aten/src/ATen/cpu/vec256/functional.h,aten/src/ATen/cpu/vec256/intrinsics.h,aten/src/ATen/cpu/vec256/missing_vld1_neon.h,aten/src/ATen/cpu/vec256/missing_vst1_neon.h,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/cpu/vec256/vsx/vec256_common_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_complex_double_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_complex_float_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_double_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_float_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_int16_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_int32_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_int64_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_qint32_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_qint8_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_quint8_vsx.h,aten/src/ATen/cpu/vec256/vsx/vsx_helpers.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/cpu/Activation.cpp,aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cpu/CatKernel.cpp,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,aten/src/ATen/native/cpu/FillKernel.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.cpp,aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp,aten/src/ATen/native/cpu/Loops.h,aten/src/ATen/native/cpu/MaxPoolKernel.cpp,aten/src/ATen/native/cpu/MaxPooling.cpp,aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp,aten/src/ATen/native/cpu/PowKernel.cpp,aten/src/ATen/native/cpu/README.md,aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/SoftMaxKernel.cpp,aten/src/ATen/native/cpu/StackKernel.cpp,aten/src/ATen/native/cpu/SumKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/Unfold2d.cpp,aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp,aten/src/ATen/native/cpu/UpSampleKernel.cpp,aten/src/ATen/native/cpu/batch_norm_kernel.cpp,aten/src/ATen/native/cpu/group_norm_kernel.cpp,aten/src/ATen/native/cpu/layer_norm_kernel.cpp,aten/src/ATen/native/cpu/utils.h,aten/src/ATen/native/mkldnn/MkldnnTensorMath.cpp,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qadd.cpp,aten/src/ATen/native/quantized/cpu/qmul.cpp,aten/src/ATen/native/quantized/cpu/qrelu.cpp,aten/src/ATen/native/quantized/cpu/qsort.cpp,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/cpu_caching_allocator_test.cpp,aten/src/ATen/test/vec256_test_all_types.cpp,aten/src/ATen/test/vec256_test_all_types.h,aten/src/ATen/test/vec_test_all_types.cpp,aten/src/ATen/test/vec_test_all_types.h,caffe2/CMakeLists.txt,setup.py",105.0,17,2,5.63448449,53.0,35777.0,31.0,4930493.333333333,12249.0,27733.5,0.0,Feature Addition,0.0,1
pytorch,b2cfd961d3ef63f5dc02e1e0b66756a6297d6f3a,47ee86776efd36db9074a151354f36d1c10d19a5,Tongzhou Wang,SsnL@users.noreply.github.com,Wed Feb 07 03:11:43 2018 -0500,1517973103.0,"Fix CPU torch.multinomial with noncontiguous prob tensor (#5093)

* fix CPU torch.multinomial not working on noncontiguous probability distn'

* address comments

* change some tabs to spaces in THStorage.c",177.0,105.0,"aten/src/TH/generic/THStorage.c,aten/src/TH/generic/THTensor.h,aten/src/TH/generic/THTensorRandom.cpp,aten/src/THC/THCTensorRandom.cuh,aten/src/THC/generic/THCTensor.h,aten/src/THC/generic/THCTensorRandom.cu,test/test_cuda.py,test/test_distributions.py,test/test_torch.py,torch/_torch_docs.py,torch/distributions/categorical.py,torch/distributions/exp_family.py,torch/distributions/multinomial.py,torch/distributions/one_hot_categorical.py,torch/distributions/utils.py",15.0,9,3,3.012635426,38.0,17156.0,11.0,1935992.2666666664,2349.0,24553.85823,0.0,Corrective,1.0,1
pytorch,d0fe7db1f65e7e1a70e94375c0efb408b6433ddf,47f11730ecac36a4df60e0f24408ad76d682dd69,soulitzer,soulitzer@gmail.com,Wed Dec 15 07:32:28 2021 -0800,1639553548.0,"Add testing for forward over reverse gradgrad (#69740)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69740

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D33031727

Pulled By: soulitzer

fbshipit-source-id: 2bcba422b4bcea3bbc936d07ba45171a6531e578",461.0,39.0,"test/test_ops.py,tools/autograd/derivatives.yaml,torch/autograd/gradcheck.py,torch/testing/_internal/common_methods_invocations.py",4.0,7,3,0.707178866,29.0,20696.0,4.0,152459.5,17750.0,41937.0,0.0,Feature Addition,0.0,1
pytorch,"b4018c4c3041faeeed7ba368da8a23e6a96f6ac4,43fbdd3b45d4351623a4aa9c8d5e6dba9eac259a",47f56f0230490c1b7b7d47d10b83b8c19842d917,soumith,soumith@fb.com,Sat Dec 31 01:46:04 2016 -0800,1483148764.0,Merge commit '43fbdd3b45d4351623a4aa9c8d5e6dba9eac259a',7.0,7.0,torch/lib/THCUNN/CMakeLists.txt,1.0,3,1,0,19.0,58.0,1.0,89.0,209.0,12044.43299,0.0,,0.0,1
pytorch,dcb73aa291816d0ee2d2e21b6b8a966da97945ad,4809e838c1c8d6c38ab8c8cb987f243cc3bfcc1e,Richard Zou,zou3519@gmail.com,Tue Dec 13 20:36:06 2022 -0800,1670963766.0,"functorch.jvp support for autograd.Function (#90077)

This PR adds functorch.jvp support for autograd.Function. It does so by
adding a jvp rule for custom_function_call.

For a regular PyTorch operation (like at::sin), the VariableType kernel:
- re-dispatches to at::sin
- calls the jvp rule for at::sin

The jvp rule for custom_function_call does just that. It constructs a
new autograd.Function (because the above logic already exists). Inside
the forward, it re-dispatches to custom_function_call. In the jvp rule,
it just calls whatever the jvp rule is supposed to be.

Since this logic is really close to the custom_function_call_grad, I
just put them together.

Test Plan:
- added jvp rules to the autograd.Function in autograd_function_db
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90077
Approved by: https://github.com/albanD, https://github.com/soulitzer",131.0,20.0,"test/functorch/test_ops.py,torch/_C/_functorch.pyi,torch/_functorch/autograd_function.py,torch/_functorch/pyfunctorch.py,torch/csrc/functorch/init.cpp,torch/testing/_internal/autograd_function_db.py",6.0,9,2,2.18425817,1.0,3135.0,2.0,93698.33333333331,10537.0,24059.0,0.0,Feature Addition,1.0,1
pytorch,1bac5fd0d346c6735b97c8db80c31883a613b02c,480d1849b04548148bc7c7cf9c883408e8753104,neginraoof,neginmr@utexas.edu,Mon Mar 16 22:22:02 2020 -0700,1584397322.0,"[ONNX] Fix for expand -1 dim value (#34069)

Summary:
PyTorch expand allows size with -1 dim value. -1 dim value means to infer the dimension from input tensor. This can be exported to ONNX expand with 1 dim value since ONNX expand supports two-way broadcast.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34069

Reviewed By: hl475

Differential Revision: D20195532

Pulled By: houseroad

fbshipit-source-id: c90e7d51b9d7422c09c5ed6e135ca8263105b8c9",106.0,3.0,"test/onnx/expect/TestOperators.test_expand.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",3.0,5,2,1.140561242,3.0,5500.0,3.0,2845606.0,143.0,471.0,0.0,Corrective,1.0,1
pytorch,8c97f0b19e04a9d5e5d4c9837441dc1ae3cfb0fe,481b6d026848b6d94edc0d84f32fb59c50590625,Ilia Cherniavskii,iliacher@fb.com,Tue May 07 02:25:55 2019 -0700,1557195955.0,"Allow a non-OpenMP based build (#19749)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19749
ghimport-source-id: a6636c0acddbdc5fd5b0dcb20b9f80cbdb9159b9

Differential Revision: D15141993

Pulled By: ilia-cher

fbshipit-source-id: 96085608398b2a4c97c68b2948f5184d07f9ad3d",211.0,52.0,".jenkins/pytorch/test.sh,aten/src/ATen/Parallel.cpp,aten/src/ATen/Parallel.h,aten/src/ATen/Version.cpp,aten/src/ATen/Version.h,aten/src/ATen/test/thread_init_test.cpp,binaries/CMakeLists.txt,binaries/parallel_info.cc,cmake/Modules/FindMKLDNN.cmake,docs/source/__config__.rst,setup.py,test/test_torch.py,tools/build_pytorch_libs.py,torch/CMakeLists.txt,torch/__config__.py,torch/csrc/Module.cpp",16.0,15,8,2.63454421,42.0,14944.0,11.0,2592589.533333333,8503.0,25243.83333,0.0,,0.0,1
pytorch,029600813eff9ce1d4de4c6b13fc3cbc9eb8fc73,483ba553bdfa8e92d0d8bb69ca76e133fdb927f6,Jane Wang,janewang@fb.com,Mon Dec 10 22:29:41 2018 -0800,1544480981.0,"add gloo allgather support on GPU (#14576)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14576

as titled

Reviewed By: pietern

Differential Revision: D13266063

fbshipit-source-id: e262f77d63724a7504a7112907bbfba49612fe75",208.0,21.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.740730838,3.0,2769.0,2.0,244775.5,5957.0,18506.33333,0.0,Feature Addition,0.0,1
pytorch,544783fa1d5eb2d787b1dd4de6460496d1c8a688,48a35135fb654bbecb8a1dfe08c7e378829e9b99,Edward Yang,ezyang@fb.com,Tue Apr 09 15:02:30 2019 -0700,1554822150.0,"Convert all tabs to spaces, add CI. (#18959)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18959
ghimport-source-id: a934163fa34cb2019732d5f49dc7290c376bf156

Differential Revision: D14831246

Pulled By: ezyang

fbshipit-source-id: beb92dc4ee8c82f4c8259c081dd72e477fe7a9d0",1558.0,1554.0,".travis.yml,aten/src/ATen/CMakeLists.txt,aten/src/ATen/cpu/vec256/intrinsics.h,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/NNPACK.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/RangeFactories.cpp,aten/src/ATen/native/cpu/avx_mathfun.h,aten/src/ATen/native/cuda/CuFFTPlanCache.h,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/RangeFactories.cu,aten/src/ATen/native/cuda/WeightNorm.cu,aten/src/ATen/native/cudnn/LossCTC.cpp,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/TH/THLapack.h,aten/src/TH/THMemoryFile.cpp,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/TH/generic/THVector.h,aten/src/TH/vector/VSX.cpp,aten/src/TH/vector/simd.h,aten/src/THC/THCBlas.cu,aten/src/THC/THCTensorMath.cuh,aten/src/THC/THCTensorRandom.cuh,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMathReduce.h,aten/src/THC/generic/THCTensorRandom.cu,aten/src/THCUNN/LogSigmoid.cu,aten/src/THCUNN/LookupTable.cu,aten/src/THCUNN/LookupTableBag.cu,aten/src/THCUNN/SpatialUpSamplingNearest.cu,aten/src/THCUNN/TemporalUpSamplingNearest.cu,aten/src/THCUNN/VolumetricUpSamplingNearest.cu,aten/src/THCUNN/common.h,aten/src/THCUNN/generic/LookupTableBag.cu,aten/src/THCUNN/generic/SpatialConvolutionMM.cu,aten/src/THCUNN/generic/SpatialDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialUpSamplingNearest.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricUpSamplingNearest.cu,aten/src/THCUNN/upsampling.h,aten/src/THNN/generic/BCECriterion.c,aten/src/THNN/generic/ClassNLLCriterion.c,aten/src/THNN/generic/MultiMarginCriterion.c,aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c,aten/src/THNN/generic/SpatialAveragePooling.c,aten/src/THNN/generic/SpatialClassNLLCriterion.c,aten/src/THNN/generic/SpatialConvolutionMM.c,aten/src/THNN/generic/SpatialDilatedConvolution.c,aten/src/THNN/generic/SpatialDilatedMaxPooling.c,aten/src/THNN/generic/SpatialFullDilatedConvolution.c,aten/src/THNN/generic/SpatialMaxUnpooling.c,aten/src/THNN/generic/TemporalRowConvolution.c,aten/src/THNN/generic/VolumetricAdaptiveAveragePooling.c,aten/src/THNN/generic/VolumetricAveragePooling.c,aten/src/THNN/generic/VolumetricConvolutionMM.c,aten/src/THNN/generic/VolumetricFullDilatedConvolution.c,aten/src/THNN/init.cpp,c10/test/util/LeftRight_test.cpp,c10/util/Half.h,caffe2/operators/assert_op.cc,caffe2/operators/counter_ops.cc,caffe2/operators/expand_op.cc,cmake/Dependencies.cmake,cmake/Modules/FindCUB.cmake,cmake/Modules/FindMIOpen.cmake,cmake/Modules/Findpybind11.cmake,docs/caffe2/stylesheet.css,docs/cpp/source/notes/tensor_creation.rst,docs/make.bat,docs/source/jit.rst,docs/source/notes/windows.rst,tools/pytorch.version,torch/csrc/api/include/torch/nn/modules/conv.h,torch/csrc/jit/README.md,torch/csrc/utils/pybind.h,torch/csrc/utils/tensor_numpy.cpp",80.0,44,7,3.905498706,52.0,33407.0,45.0,8182067.9,7992.0,24128.33333,0.0,Feature Addition,0.0,1
pytorch,bc1b4c89125a3ef6d46a60274fe44d755165cbd5,48ad4546d21553517d506d857f64d638e8fd61cd,Tongzhou Wang,SsnL@users.noreply.github.com,Fri Mar 30 16:44:11 2018 -0400,1522428251.0,"Move LayerNorm to ATen; remove tracking_running_stats functionality (#5983)

* move LN to aten; remove tracking_stats functionaility

* Address comments about error message and respect cudnn flag for LayerNorm and GroupNorm",106.0,142.0,"aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/normalization.py",5.0,8,3,2.057110787,37.0,10364.0,4.0,269224.4,2514.0,24863.35823,0.0,Feature Addition,0.0,1
pytorch,1768a28a202c3ee5cf748e69de360b44a9dcd051,48e63bf69f9283cf2653837c052a28dc66761f55,Richard Zou,zou3519@gmail.com,Tue Dec 27 02:43:33 2022 -0800,1672109013.0,"[functorch] composition of three transform tests with jvp (#91206)

This PR adds the following tests. They will be useful as test cases for
generate_vmap_rule=True and jvp (to come soon)
- test_jvpvmap
- test_jvpvmapvmap
- test_vmapjvpvmap
- test_jvpjvpvmap
- test_jvpvjpvmap
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91206
Approved by: https://github.com/soulitzer",208.0,2.0,test/functorch/test_ops.py,1.0,2,1,0,1.0,2057.0,1.0,445446.0,10885.0,24784.5,0.0,Feature Addition,0.0,1
pytorch,0acddd6cee32bc7c3715bc8b93d0a33ef19064b1,48e90e3339b8b027e2525da6d1977204ecfbff73,anderspapitto,anderspapitto@gmail.com,Wed Jun 20 21:45:26 2018 -0700,1529531126.0,"Build system changes (#8627)

* All changes needed to get rid of process_github.sh

* allow thnn_h_path",85.0,26.0,"aten/src/ATen/cudnn/Handles.cpp,setup.py,test/common.py,test/test_distributed.py,test/test_torch.py,test/test_utils.py,tools/nnwrap/__init__.py,tools/nnwrap/generate_wrappers.py,tools/setup_helpers/generate_code.py,torch/CMakeLists.txt,torch/__init__.py,torch/_thnn/utils.py,torch/_utils_internal.py",13.0,10,4,3.129167483,42.0,12280.0,9.0,1438102.9166666667,2747.0,25246.85823,0.0,,0.0,1
pytorch,7349dbb7ce09e0810980cbb2aeb0bbd9aa0757ad,48f70ea0a2a3a408c14642419271999663b65ed9,Iurii Zdebskyi,iuriiz@fb.com,Wed Apr 03 14:22:38 2019 -0700,1554301358.0,"Added numpy conversion (#18505)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18505
ghimport-source-id: f3c9b9251e5793f9e192f587194ddfebb45facc1

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18505 [WIP]Added numpy conversion**
* #18166 Bool Tensor for CUDA

Differential Revision: D14646403

fbshipit-source-id: 79d39d692c778ce1981c1d35b1c33e3d93111041",55.0,12.0,"c10/core/ScalarType.h,test/common_utils.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/utils/tensor_numpy.cpp",5.0,9,4,1.875534247,40.0,13092.0,5.0,837683.2,7852.0,23764.83333,0.0,Feature Addition,0.0,1
pytorch,1632ab29791e8063ca76e80aa43ba4ed2dac95ce,492e26fbcdb9a942f4685e1f09363335c5d65ea7,Sherin Thomas,sherinct@live.com,Fri Dec 22 15:14:09 2017 +0500,1513955649.0,Pad sequences and Pack sequences (#3875),186.0,1.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/modules/rnn.py,torch/nn/utils/rnn.py",4.0,7,3,1.397170803,37.0,7185.0,4.0,1582947.5,862.0,6636.672317,0.0,,0.0,1
pytorch,40ac95dd3c3f88c68143adf354e493a26d18e2ae,4959981cff864ba0f685fef8801e5d856c516656,shubhambhokare1,shubhambhokare1@gmail.com,Fri Aug 07 02:29:56 2020 -0700,1596767396.0,"[ONNX] Export tensor (#41872)

Summary:
Adding tensor symbolic for opset 9

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41872

Reviewed By: houseroad

Differential Revision: D22968426

Pulled By: bzinodev

fbshipit-source-id: 70e1afc7397e38039e2030e550fd72f09bac7c7c",59.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.887417927,3.0,6489.0,1.0,992.0,4140.0,9641.5,0.0,Feature Addition,0.0,1
pytorch,9b3277c09508afd178948b1911a5a33ffd9f68de,496c0a207be84e120d129133aa9ea76502f1ecaf,albanD,desmaison.alban@gmail.com,Mon Feb 06 18:32:23 2023 +0000,1675708343.0,"Make segment_reduce properly private. (#93166)

I am attempting not to change the aten function to reduce the amount of BC issues on the torchscript side.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/93166
Approved by: https://github.com/ngimel",47.0,33.0,"test/distributed/_tensor/test_dtensor_ops.py,test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,test/test_meta.py,test/test_ops.py,test/test_proxy_tensor.py,test/test_segment_reductions.py,torch/__init__.py,torch/fx/node.py,torch/jit/_builtins.py,torch/masked/_ops.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py,torchgen/static_runtime/generator.py",16.0,13,3,3.742407858,45.0,43134.0,14.0,1449072.1875,12096.0,28193.5,0.0,,0.0,1
pytorch,1b3d6ab86469e1bca4a19958f6507efae6361f9a,4970e733045b1e802c6dfb42c675f29f846be9f4,gchanan,gregchanan@gmail.com,Thu Jan 25 00:22:05 2018 -0500,1516839725.0,"Add support for distributions and test_distributions when WITH_SCALARâ¦ (#4834)

* Add support for distributions and test_distributions when WITH_SCALARS enabled.

* Fix flake8.",47.0,46.0,"test/test_distributions.py,torch/distributions/distribution.py,torch/distributions/exponential.py,torch/distributions/laplace.py,torch/distributions/studentT.py,torch/distributions/utils.py",6.0,3,2,0.845310959,8.0,2534.0,5.0,497704.5,462.0,1410.905869,0.0,Corrective,1.0,1
pytorch,d810e738b92734b2966560f1c020bfc23779cbd2,49a1d7bfcb9b0fc5f0ee62d91f5d6c76e4cf9dc8,kshitij12345,kshitijkalambarkar@gmail.com,Thu Oct 14 17:10:05 2021 -0700,1634231405.0,"[opinfo] elemwise parcel : isfinite, isinf, isposinf, isneginf, isnan, isreal (#66400)

Summary:
Adds OpInfo for `isfinite, isinf, isposinf, isneginf, isnan, isreal`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/66400

Reviewed By: bdhirsh

Differential Revision: D31602998

Pulled By: mruberry

fbshipit-source-id: 235cc414f373f014f4822a72deb1a04a58ad4a7c",34.0,121.0,"test/test_jit_fuser_te.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.800979517,2.0,14770.0,3.0,1069373.0,16251.0,37577.5,0.0,Feature Addition,0.0,1
pytorch,41181169ae9d76459936e5faebfd98bcff269f82,49a3e49627c71a4abe551c832e4720e8b21704fe,Praveen Palanisamy,4770482+praveen-palanisamy@users.noreply.github.com,Mon Jun 25 22:06:51 2018 -0400,1529964411.0,"Fixes #8508. Upcasted loc to 1-d if a scalar loc is provided to MultivariateNormal (#8543)

* Fixes #8508 Broadcasted loc to 1-d if a scalar loc is provided to MultivariateNormal.

* move to non-inplace",2.0,0.0,torch/distributions/multivariate_normal.py,1.0,2,1,0,4.0,191.0,1.0,2013118.0,2768.0,25272.35823,0.0,Corrective,1.0,1
pytorch,26e8f8f2238b4e25b2fb5bf4232456ce835ff1fa,49a923c8b57b8c3ae0751879945b6e15b6897c2a,Shubham Bhokare,32080845+shubhambhokare1@users.noreply.github.com,Fri Feb 19 18:52:54 2021 -0800,1613760774.0,"[ONNX] Update LayerNorm symbolic to handle autocasting (#52199) (#52350)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52350

When onnx export creates a 0-dim tensor of constant type, this action overrides the type promotion logic as quoted in #9515. In order to prevent this from happening this PR adds the following functionality.
If the data type is a floating point type, it is converted to a 0-dim double tensor, else it is converted to a 0-dim tensor of its original type

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D26490325

Pulled By: SplitInfinity

fbshipit-source-id: 4c47c69c9b6523d2e45b74c2541d6d8ca7e28fc9",33.0,2.0,"test/onnx/test_pytorch_onnx_onnxruntime_cuda.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.397758476,3.0,3807.0,3.0,698333.0,9052.0,20277.5,0.0,Feature Addition,0.0,1
pytorch,96f8755034e1c1c19800525319f2911f1cd1f393,49b198c45444eb44919201e6319db38b0ce443c1,Rong Rong,rongr@fb.com,Mon Sep 28 23:26:42 2020 -0700,1601335602.0,"type check for torch.testing._internal.common_utils (#45375)

Summary:
part of torch.testing._internal.* effort

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45375

Reviewed By: malfet

Differential Revision: D23964315

Pulled By: walterddr

fbshipit-source-id: efdd643297f5c7f75670ffe60ff7e82fc413d18d",37.0,28.0,"mypy.ini,tools/pyi/gen_pyi.py,torch/_C/__init__.pyi.in,torch/testing/_internal/common_utils.py",4.0,6,2,1.130564013,5.0,3685.0,4.0,202665.25,5526.0,12986.5,0.0,,0.0,1
pytorch,dc6fab44525c2613906b9f4094689b05f02a3a86,49b59e347217e200e7d244ec9a2490ff7dea4345,kshitij12345,kshitijkalambarkar@gmail.com,Tue Feb 23 18:09:32 2021 -0800,1614103772.0,"Add OpInfo entries for i0 and logical_not (#51956)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51956

Reviewed By: albanD

Differential Revision: D26404440

Pulled By: mruberry

fbshipit-source-id: dd73e63155dd4a200afb38a5e566eb2132e69fde",33.0,30.0,"test/test_ops.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,1.100982814,2.0,6034.0,3.0,579003.6666666666,9131.0,20392.0,0.0,Feature Addition,0.0,1
pytorch,eaca6f32b056decb3b3e4c7fdd7c1cd9d115f479,49e4e41fdcbe20632e8c7794ae7bcb8e06855a46,Luca Wehrstedt,lcw@fb.com,Thu May 28 17:42:29 2020 -0700,1590687749.0,"[TensorPipe] Always complete futures from thread pool (#38930)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38930

Any time we mark a future as complete or set an error on it we call its callbacks, which could be arbitrary user functions and could thus be slow or blocking. The safest behavior is to always defer to the loop.
ghstack-source-id: 104760682

Test Plan: None... :(

Differential Revision: D21703017

fbshipit-source-id: ad2bdc6be25844628ae6f318ef98b496f3d93ffd",40.0,26.0,"torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h",2.0,4,1,0.32984607,1.0,950.0,1.0,5.0,2398.0,6017.5,0.0,,0.0,1
pytorch,afaad94fed963e93552cc12b4b2dd6de21c11e3e,49ec984c406e67107aae2891d24c8839b7dc7c33,Gregory Chanan,gchanan@fb.com,Wed Jun 07 20:39:22 2017 -0700,1496867962.0,Ensure warnings are repeated in python2 for tests.,3.0,1.0,"test/test_torch.py,tools/cwrap/plugins/Broadcast.py",2.0,4,2,0.811278124,31.0,4017.0,2.0,0.0,917.0,11502.94394,0.0,,0.0,1
pytorch,9f21ec7ca27d82e080f09f6a740fbcb1114eff43,49f8581745ecb5ded063cfff61c53440f17d4d9a,Sebastian MeÃmer,smessmer@users.noreply.github.com,Tue May 29 18:38:02 2018 -0700,1527619082.0,"Update from facebook (#7855)

* [mpscnn] MPSCNNChannelShuffle

att

* [Easy] Adding tags as an argument to the functional layer

Without it ""tags"" would be added as an argument to the operator.

The change here is based on the assumption that there is no operator that takes ""tags"" as an argument.

* Fix locally_connected_op schema check.

Fix locally_connected_op schema check.

* [C2] Add TypeAndShape inference for few more operators

As desc

* [c2] Shape inference should support 0 as dimension

Tensors can have 0 in their dimension.

* Make MockHiveReader loop over and support max_examples

Replace DatasetReader with RandomDatasetReader.

So that Mock Hive Reader can simulate a large data input using a small sample file as source.

* Utility function to wipe cache between benchmark runs

Caffe2 benchmark does not wipe out cache between runs, and this potentially creates an unrealistically optimistic picture of performance. This diff adds utility function to wipe out the cache.

* Allow caffe2 GlobalInit to be invoked multiple times

Allow caffe2 GlobalInit to be invoked multiple times. Will re-parse gflags and update logging levels on successive invocations, but will not re-run init functions or perform other one-time initialization.

* Add Caffe2 GlobalInitIsCalledGuard to base net and operator classes

Warn if caffe2's GlobalInit function has not been invoked before creating an operator or net object. This is based on discussion here: https://fb.quip.com/kqGIAbmK7vNG

* Rethrow current exception on failure

Rethrow current exception instead of copy constructing a new one on op failure.

* Make `clone()` return subclass of List/Struct

`clone()` is not working correctly when we subclass those classes

* Wipe the cache before the net run

the util function is copied from D7409424
will rebase once D7409424 is landed.

* [Caffe2] [Mobile] Support utils/cast.h::GetCastDataType with LITE_PROTO builds

* Correct includes

async_polling include -> async_base include

* Prepare execution flags for executor migration

Making async_scheduling aware of underlying net type to prepare for executor
migration

* Add operator level observers into async executor

Adding operator level observers into RunAsync operators' calls

* Cleanup TEST_Benchmark

Remove duplicate code and provide default implementation in NetBase

* [C2] Fix type and shape inference for binary comparison ops

As desc.

* Add GlobalInit to predictor to ensure initialization is always done before prediction

FACEBOOK:

Redo D7651453 the correct way.

Now use a static variable for the arguments passed to GLog

* Remove spammy log message

This method is currently used in various places inside Caffe itself.

* Disable events for operators inside a chain

We don't need to use events in operators within a chain because the chain is
always scheduled on a single stream, keeping only first and last event for
scheduling purposes

* Ensure correct finish run order

In rare cases we might call finishRun and trigger net's destruction while
another worker is still holding shared_ptr to a thread pool, that can cause
thread pool destruction from within a worker thread in case no other nets are
using the pool. This diff fixes the order of calling finishRun and also changes
pool() to return raw pointer to keep pool's ownership within the net

* Reduce unnecessary polling

Make sure we don't waste CPU by polling operators that we can set an efficient
callbacks on

* Squash commit of syncing 9506eeb from github to fbcode

Patch xplat buck fix

add virtual destructor to OptimizationPass

add virtual destructor to OptimizationPass

build fixes for sync

build fixes for sync

* Fix net tracing

Fix net tracing from async_scheduling

* Fix logging",1287.0,287.0,"aten/src/ATen/native/native_functions.yaml,binaries/benchmark_helper.cc,binaries/speed_benchmark.cc,caffe2/contrib/gloo/allgather_ops.h,caffe2/contrib/gloo/allreduce_ops.h,caffe2/contrib/gloo/barrier_ops.h,caffe2/contrib/gloo/broadcast_ops.h,caffe2/contrib/gloo/common_world_ops.h,caffe2/contrib/gloo/reduce_scatter_ops.h,caffe2/core/db.h,caffe2/core/event.cc,caffe2/core/event.h,caffe2/core/event_cpu.h,caffe2/core/init.cc,caffe2/core/init.h,caffe2/core/init_test.cc,caffe2/core/logging.cc,caffe2/core/logging.h,caffe2/core/net.cc,caffe2/core/net.h,caffe2/core/net_async_base.cc,caffe2/core/net_async_base.h,caffe2/core/net_async_gpu_thread_pool.h,caffe2/core/net_async_gpu_thread_pool_gpu.cc,caffe2/core/net_async_scheduling.cc,caffe2/core/net_async_scheduling.h,caffe2/core/net_dag.cc,caffe2/core/net_dag.h,caffe2/core/net_dag_utils.cc,caffe2/core/net_dag_utils.h,caffe2/core/net_simple_async.cc,caffe2/core/net_simple_async.h,caffe2/core/observer.h,caffe2/core/operator.cc,caffe2/core/operator.h,caffe2/core/predictor.cc,caffe2/mobile/contrib/ios/mpscnn/MPSCNN.metal,caffe2/mobile/contrib/ios/mpscnn/mpscnn.mm,caffe2/mobile/contrib/ios/mpscnn/mpscnn_kernels.h,caffe2/mobile/contrib/ios/mpscnn/mpscnn_test.mm,caffe2/operators/conv_pool_op_base.h,caffe2/operators/elementwise_op_schema.cc,caffe2/operators/locally_connected_op.cc,caffe2/operators/one_hot_ops.cc,caffe2/operators/percentile_op.cc,caffe2/operators/utility_ops.cc,caffe2/opt/backend_cutting.cc,caffe2/opt/passes.h,caffe2/python/dataset.py,caffe2/python/db_file_reader.py,caffe2/python/ideep/convfusion_op_test.py,caffe2/python/layers/functional.py,caffe2/python/operator_test/elementwise_ops_test.py,caffe2/python/operator_test/one_hot_ops_test.py,caffe2/python/operator_test/percentile_op_test.py,caffe2/python/operator_test/shape_inference_test.py,caffe2/python/operator_test/utility_ops_test.py,caffe2/python/schema.py,caffe2/python/schema_test.py,caffe2/utils/bench_utils.cc,caffe2/utils/bench_utils.h,caffe2/utils/cast.h,caffe2/utils/cast_test.cc",63.0,20,3,5.096058522,25.0,23234.0,22.0,3945652.783333333,1206.0,3319.805292,0.0,Corrective,1.0,1
pytorch,601aca2a2dedfe7b46f2815649682924a31d50ce,4a033be4482441d3f61a887502d75356a90e6a6a,Richard Zou,zou3519@gmail.com,Thu Aug 18 13:57:22 2022 -0700,1660831042.0,"[functorch] reclassify svd as an allowed failure; add test (#83612)

svd when done on a batch of inputs vs the input in a for-loop may return
different results because svd isn't unique. So, instead of checking that
the output of vmap and the output of a for-loop are the same, we check
that matrix-multiplying the decomposed tensors results in the same
tensor when doing it under vmap vs under a for-loop.

Test Plan:
- new test
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83612
Approved by: https://github.com/samdow",36.0,7.0,functorch/test/test_vmap.py,1.0,2,1,0,2.0,4459.0,1.0,1.0,6561.0,15190.5,0.0,Feature Addition,0.0,1
pytorch,e58d7dde6251b5a7479d59e0507717c9583af1a2,4a20c215cee48c2e6f48a6eb7906d62272ce5673,Richard Zou,zou3519@gmail.com,Thu Jun 03 14:03:15 2021 -0700,1622728995.0,"[functorch] vmap-of-vjp and vjp-of-vmap OpInfo testing

Plus some refactoring of the vmap testing to reuse functions between all
of the mentioned tests.

Fixes pytorch/functorch#28.",455.0,337.0,"functorch/test/common_utils.py,functorch/test/test_grad.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,2,1,1.552767221,1.0,3377.0,3.0,1.0,131.0,262.0,0.0,Corrective,1.0,1
pytorch,057be231688f712212688530f0105ba143e9a1bf,4a3a37886cdbdca108536a2730c034db4f96f095,Xiang Gao,qasdfgtyuiop@gmail.com,Wed Jan 13 09:29:45 2021 -0800,1610530185.0,"Fix fft slow tests (#50435)

Summary:
The failure is:
```
______________________________________________________________________________________________________ TestCommonCUDA.test_variant_consistency_jit_fft_rfft_cuda_float64 _______________________________________________________________________________________________________
../.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py:889: in wrapper
    method(*args, **kwargs)
../.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py:889: in wrapper
    method(*args, **kwargs)
../.local/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py:267: in instantiated_test
    if op is not None and op.should_skip(generic_cls.__name__, name,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <torch.testing._internal.common_methods_invocations.SpectralFuncInfo object at 0x7f7375f9b550>, cls_name = 'TestCommon', test_name = 'test_variant_consistency_jit', device_type = 'cuda', dtype = torch.float64

    def should_skip(self, cls_name, test_name, device_type, dtype):
>       for si in self.skips:
E       TypeError: 'NoneType' object is not iterable

../.local/lib/python3.9/site-packages/torch/testing/_internal/common_methods_invocations.py:186: TypeError

```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50435

Reviewed By: izdeby

Differential Revision: D25886650

Pulled By: mruberry

fbshipit-source-id: 722a45247dc79be86858306cd1b51b0a63df8b37",2.0,1.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,2800.0,1.0,67171.0,8066.0,18272.5,0.0,Corrective,1.0,1
pytorch,ef6f776e82f179dfd09c97e56ba987ae1197c4cc,4a6a5d163012a81268f0db20e9fcc3a926dd4d54,anjali411,chourdiaanjali123@gmail.com,Fri Dec 17 01:49:42 2021 -0800,1639705782.0,"OpInfos for torch.{flatten, column_stack} (#69237)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69237

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D32988956

Pulled By: anjali411

fbshipit-source-id: b7f5c537ff9731f56232aa5647910f03edf4582a",49.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.14372617,2.0,17416.0,2.0,182014.0,17817.0,42131.0,0.0,,0.0,1
pytorch,9ae520640e358f2e3d6e66844f8f27ad9e9ee6e3,4aa14f1fe3ac60f991e7a8a5c55ac37d88915389,Richard Zou,zou3519@users.noreply.github.com,Wed Mar 02 23:01:17 2022 -0500,1646262077.0,"[functorch] Convolution double backward batch rule (pytorch/functorch#557)

https://github.com/pytorch/functorch/issues/516",31.0,1.0,"functorch/functorch/csrc/BatchRulesDecompositions.cpp,functorch/test/test_vmap.py",2.0,4,1,0.337290067,1.0,3956.0,2.0,1.5,844.0,1179.5,0.0,,0.0,1
pytorch,b5fa5a56033dda7bb5852fe873486134f3395fe9,4aa22833cf6ff885cd1690de9e4639c0e77a0224,Iurii Zdebskyi,iuriiz@fb.com,Mon Mar 11 23:58:09 2019 -0700,1552348689.0,"Bool tensor creation (cpu) (#17376)

Summary:
This PR enables bool tensor creation and some basic operations for the CPU backend. This is a part of Bool Tensor feature implementation work. The whole plan looks like this:
    1. Storage Implementation [Done]
    2. Tensor Creation.
        a) CPU (this PR)
        b) CUDA
    3. Tensor Conversions.
    4. Tensor Indexing.
    5. Tensor Operations.
    6. Back compatibility related changes.

**Change**:
Enable CPU tensors and these operations:
- torch.zeros
- torch.tensor
- torch.ones
- torch.randint
- torch.full
- torch.full_like
- torch.empty
- torch.empty_like

**Tested via**:
1) unit tests

2)
torch.zeros(2,2, dtype=torch.bool)
torch.tensor([True, False], dtype=torch.bool)
torch.tensor([-1, -1.1, 0, 1, 1.1, 2], dtype=torch.bool)
torch.ones([1,2], dtype=torch.bool)
torch.randint(10, (2, 2), dtype=torch.bool)
torch.full((2, 3), True, dtype=torch.bool)
torch.empty(4, dtype=torch.bool)

a = torch.tensor([0,0,1])
b = torch.full_like(a, True)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17376

Reviewed By: ezyang

Differential Revision: D14375995

Pulled By: izdeby

fbshipit-source-id: a65490b5360ee0e6e3accc54ce7e32e49ad2d2a8",190.0,75.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/Dispatch.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/Scalar.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/ATen/preprocess_declarations.py,aten/src/TH/THTensor.h,aten/src/TH/THTensorFill.cpp,aten/src/TH/THTensorRandom.cpp,aten/src/TH/THVector.cpp,aten/src/TH/THVector.h,aten/src/TH/generic/THTensorApply.hpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorRandom.cpp,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.cpp,aten/src/TH/generic/THVectorDispatch.cpp,c10/core/ScalarType.h,test/common_utils.py,test/test_torch.py,torch/csrc/utils/python_scalars.h,torch/csrc/utils/tensor_types.cpp",24.0,12,4,3.807281538,42.0,24662.0,14.0,2716615.6666666665,7429.0,22786.33333,0.0,Feature Addition,0.0,1
pytorch,f0ec3bfa566722be8cade8590fa92c46b274a453,4aa5075caebdf4dfbfff316e30c82614e3581503,Yi Cheng,eason@fb.com,Thu Aug 23 01:54:32 2018 -0700,1534989272.0,"update the constructor to accept the PredictorConfg only to set up the predictor (#9483)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9483

The interface is updated to accept the config to construct the predictor.

Reviewed By: highker

Differential Revision: D8872999

fbshipit-source-id: 3ca54d644970823fc33c0ade9a005e12f52e2b24",142.0,104.0,"caffe2/onnx/backend_rep.cc,caffe2/predictor/CMakeLists.txt,caffe2/predictor/predictor.cc,caffe2/predictor/predictor.h,caffe2/predictor/predictor_config.cc,caffe2/predictor/predictor_config.h,caffe2/predictor/predictor_test.cc,caffe2/predictor/predictor_utils.cc,caffe2/predictor/predictor_utils.h,caffe2/python/pybind_state.cc",10.0,4,1,2.122415254,9.0,2427.0,6.0,2995119.5555555555,3607.0,9852.833333,0.0,,0.0,1
pytorch,b7431d815f7c2c4a2575d89582bac27d496a88f3,4add06eb5c64e8bf811c71151a60a8da8692caad,eqy,eddiey@nvidia.com,Tue Aug 15 05:52:49 2023 +0000,1692078769.0,"[CUDNN][CUDNN V8 API] LRU Cache for cuDNN frontend `ExecutionPlan` (#104369)

Adds LRU functionality to the cuDNN frontend `ExecutionPlan` cache to address high memory usage as observed in #98688, #104122 via the `TORCH_CUDNN_V8_LRU_CACHE_LIMIT` environment variable. By default this limit is set to 10000, which corresponds to about 2GiB of host memory usage as observed empirically. Note that we are still following up with cuDNN to see if the size of an `ExecutionPlan` can be reduced, as it appears to currently be around 200KiB (!!) for a single plan.

This implementation is a bit heavy on the internal asserts for now as it's a bit difficult to directly test the state of the cache without instrumenting it explicitly in tests. Once we are confident that the implementation is stable, we can remove the asserts.

CC @malfet who @ptrblck mentioned may have also been looking into this

CC @colesbury

Pull Request resolved: https://github.com/pytorch/pytorch/pull/104369
Approved by: https://github.com/malfet",77.0,4.0,aten/src/ATen/native/cudnn/Conv_v8.cpp,1.0,5,1,0,2.0,732.0,1.0,4109960.0,18613.0,42176.5,0.0,Feature Addition,0.0,1
pytorch,4d91cead10bbe065e4c5178faad9965acdc582ae,4afc9a53edab14050860c0f2a2b1e1a559f4583c,Richard Zou,zou3519@gmail.com,Wed Aug 18 21:41:23 2021 -0700,1629322883.0,[functorch] Use new xfail mechanism in test_vmap_exhaustive; turn on CUDA for that test,58.0,60.0,"functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,2,1,1.52970654,1.0,3736.0,3.0,0.6666666666666666,280.0,433.0,0.0,,0.0,1
pytorch,34b20941704b468be27cae0a5d72830cb836cdaf,4b2ded615d961e11d89b76458587b24f24ed54ae,Samantha Andow,samdow@fb.com,Mon Apr 25 20:24:08 2022 -0400,1650918248.0,"[functorch] Add expand_copy and masked_select_backward to reenable tests from upstream breakages (pytorch/functorch#736)

* fix expand_copy

* add masked_select_backward",32.0,21.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/functorch/csrc/BatchRulesViews.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,1.702810112,1.0,6808.0,4.0,2.75,998.0,1381.5,0.0,Corrective,1.0,1
pytorch,25d5b63acf2645ba836cd46a8cd805c7e5fbf21f,4b311a9633f721f4d60d9cd096959a8e5774f554,kshitij12345,kshitijkalambarkar@gmail.com,Fri Apr 22 14:21:27 2022 +0000,1650637287.0,"[complex32] conj

Reference: https://github.com/pytorch/pytorch/issues/74537

Required for `complex32` support by `fft` module.

Tested with `test_dtypes` and `test_complex_half_reference_testing` in test_ops.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76132
Approved by: https://github.com/anjali411",56.0,10.0,"aten/src/ATen/cpu/vec/vec_base.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/zmath.h,aten/src/ATen/native/cuda/UnaryComplexKernels.cu,torch/testing/_internal/common_methods_invocations.py",5.0,11,2,1.722745239,10.0,18952.0,5.0,6454380.4,2521.0,5971.0,0.0,,0.0,1
pytorch,7ebac74d0a215a00f54c7660a2ac4a782e98ab8b,4b3ea927873f80c30de05140a49f96a1a9e40295,Kexuan Sun,me@kianasun.com,Tue Aug 20 23:32:50 2019 -0700,1566343970.0,"Test if descriptions of args are in the template (#24161)

Summary:
As in https://github.com/pytorch/pytorch/issues/23439, some descriptions of arguments in `_torch_docs.py` have been replaced by `common_args`, it would be helpful to check if any descriptions can be replaced for new docs in the future.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24161

Differential Revision: D16889293

Pulled By: ezyang

fbshipit-source-id: bf6f581494482d6eb32e634f73e84a4586766230",15.0,3.0,"test/test_torch.py,torch/_torch_docs.py",2.0,2,2,0.852405179,41.0,20095.0,2.0,236564.0,10776.0,30576.33333,0.0,Non Functional,0.0,1
pytorch,469c6c88a3267c5f760a054940a9cecb35615d5f,4b6c884b99282fa4ea9eda67d8460b90c2e524a7,Bram Wasti,bwasti@fb.com,Tue May 15 22:57:06 2018 -0700,1526425026.0,[caffe2][nomnigraph] Add optimize function to opt:: namespace that takes in a level and optimizes the graph/workspace accordingly.  Adding it to predictor and speed_benchmark arguments (#7558),85.0,9.0,"binaries/speed_benchmark.cc,caffe2/core/predictor.cc,caffe2/core/predictor.h,caffe2/opt/converter.h,caffe2/opt/fusion.h,caffe2/opt/mobile.h,caffe2/opt/optimizer.cc,caffe2/opt/optimizer.h",8.0,4,2,2.186942184,6.0,641.0,6.0,1273880.5,1140.0,3055.805292,0.0,Feature Addition,0.0,1
pytorch,872bab22c6f6828cbf83f77d87ebbb2e633dfbee,4ba28deb6ecb3f85b52eb161b58fb86f05c4d881,Karl Ostmo,kostmo@gmail.com,Fri May 10 16:44:49 2019 -0700,1557506689.0,"Unify libtorch and libcaffe2 (#17783)

Summary:
This PR is an intermediate step toward the ultimate goal of eliminating ""caffe2"" in favor of ""torch"".  This PR moves all of the files that had constituted ""libtorch.so"" into the ""libcaffe2.so"" library, and wraps ""libcaffe2.so"" with a shell library named ""libtorch.so"".  This means that, for now, `caffe2/CMakeLists.txt` becomes a lot bigger, and `torch/CMakeLists.txt` becomes smaller.

The torch Python bindings (`torch_python.so`) still remain in `torch/CMakeLists.txt`.

The follow-up to this PR will rename references to `caffe2` to `torch`, and flatten the shell into one library.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17783

Differential Revision: D15284178

Pulled By: kostmo

fbshipit-source-id: a08387d735ae20652527ced4e69fd75b8ff88b05",748.0,584.0,".jenkins/pytorch/win-test-helpers/test_custom_script_ops.bat,CMakeLists.txt,caffe2/CMakeLists.txt,docs/libtorch.rst,scripts/build_windows.bat,test/cpp/api/CMakeLists.txt,test/cpp/jit/CMakeLists.txt,tools/build_pytorch_libs.py,tools/run-clang-tidy-in-ci.sh,torch/CMakeLists.txt,torch/csrc/WindowsTorchApiMacro.h,torch/csrc/autograd/functions/comm.h,torch/csrc/jit/passes/alias_analysis.h,torch/csrc/jit/passes/utils/memory_dag.h,torch/utils/cpp_extension.py",15.0,19,7,1.47836403,71.0,4199.0,14.0,2803283.6666666665,8603.0,25485.33333,0.0,,0.0,1
pytorch,850b53bbee82fb194af85b566aedee94b96def32,4baa78bb1f2b2b1b6fde1428efb2697d17c9fb72,Philip Meier,github.pmeier@posteo.de,Thu Dec 01 08:28:02 2022 +0100,1669883282.0,"enable ufmt for torch/testing/*.py (#89525)

I've tried to soft-enforce this manually already, albeit with a line length of 120. This just adds it to the CI. Note that this only applies to `torch/testing/*.py` and thus everything under `torch/testing/_internal/**/*` is *not* affected.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89525
Approved by: https://github.com/kit1980",256.0,79.0,".lintrunner.toml,torch/testing/__init__.py,torch/testing/_comparison.py,torch/testing/_creation.py,torch/testing/_deprecated.py",5.0,2,1,0.927695848,7.0,2479.0,4.0,1540020.2,10064.0,23081.5,0.0,Feature Addition,0.0,1
pytorch,7920b9229bee366fe89cb8ce908cffe7408a2259,4bad029fd4d2a48c0d21e7c5452f5873968581dd,Adam Paszke,adam.paszke@gmail.com,Thu Sep 15 14:35:36 2016 -0700,1473950136.0,Add more functions to autograd,1065.0,103.0,"test/test_autograd.py,test/test_torch.py,torch/Tensor.py,torch/autograd/engine.py,torch/autograd/functions/TorchNode.py,torch/autograd/functions/__init__.py,torch/autograd/functions/all_reduce.py,torch/autograd/functions/basic_ops.py,torch/autograd/functions/blas.py,torch/autograd/functions/pointwise.py,torch/autograd/functions/reduce.py,torch/autograd/functions/tensor.py,torch/autograd/variable.py",13.0,4,2,2.892387402,7.0,3382.0,1.0,811.0,168.0,3731.032937,0.0,Feature Addition,0.0,1
pytorch,aabdef51f971f2786f89e1b43532250d311bb0e7,4bafca1a690ae0626c2d525f525cf45ce6903986,Mike Ruberry,mruberry@devfair044.maas,Tue Aug 11 18:30:59 2020 -0700,1597170659.0,"Adds list of operator-related information for testing (#41662)

Summary:
This PR adds:

- an ""OpInfo"" class in common_method_invocations that can contain useful information about an operator, like what dtypes it supports
- a more specialized ""UnaryUfuncInfo"" class designed to help test the unary ufuncs
- the `ops` decorator, which can generate test variants from lists of OpInfos
- test_unary_ufuncs.py, a new test suite stub that shows how the `ops` decorator and operator information can be used to improve the thoroughness of our testing

The single test in test_unary_ufuncs.py simply ensures that the dtypes associated with a unary ufunc operator in its OpInfo entry are correct. Writing a test like this previously, however, would have required manually constructing test-specific operator information and writing a custom test generator. The `ops` decorator and a common place to put operator information make writing tests like this easier and allows what would have been test-specific information to be reused.

The `ops` decorator extends and composes with the existing device generic test framework, allowing its decorators to be reused. For example, the `onlyOnCPUAndCUDA` decorator works with the new `ops` decorator. This should keep the tests readable and consistent.

Future PRs will likely:

- continue refactoring the too large test_torch.py into more verticals (unary ufuncs, binary ufuncs, reductions...)
- add more operator information to common_method_invocations.py
- refactor tests for unary ufuncs into test_unary_ufunc

Examples of possible future extensions are [here](https://github.com/pytorch/pytorch/pull/41662/commits/616747e50dbb5a3338deedf41ff44957b162ab51), where an example unary ufunc test is added, and [here](https://github.com/pytorch/pytorch/pull/41662/commits/d0b624f110d470b9a37ad02b389d2f4258c3d632), where example autograd tests are added. Both tests leverage the operator info in common_method_invocations to simplify testing.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41662

Reviewed By: ngimel

Differential Revision: D23048416

Pulled By: mruberry

fbshipit-source-id: ecce279ac8767f742150d45854404921a6855f2c",307.0,46.0,"test/run_test.py,test/test_unary_ufuncs.py,torch/testing/__init__.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",5.0,4,2,1.886162465,8.0,3011.0,4.0,849252.0,4223.0,9888.5,0.0,Corrective,0.0,1
pytorch,e2f49c8437421919bdd3a55a49c6b35af1e36a7d,4bbb6adff590a0d813589a2e856db786308f2a61,Nick Gibson,nickg@fb.com,Mon Sep 21 16:25:53 2020 -0700,1600705553.0,"[NNC] fix SyncThreads insertion and reenable CudaSharedMem test (#44909)

Summary:
A previous fix for masking Cuda dimensions (https://github.com/pytorch/pytorch/issues/44733) changed the behaviour of inserting thread synchronization barriers in the Cuda CodeGen, causing the CudaSharedMemReduce_1 to be flaky and ultimately disabled.

The issue is working out where these barriers must be inserted - solving this optimally is very hard, and I think not possible without dependency analysis we don't have, so I've changed our logic to be quite pessimistic. We'll insert barriers before and after any blocks that have thread dimensions masked (even between blocks that have no data dependencies). This should be correct, but it's an area we could improve performance. To address this somewhat I've added a simplifier pass that removes obviously unnecessary syncThreads.

To avoid this test being flaky again, I've added a check against the generated code to ensure there is a syncThread in the right place.

Also fixed a couple of non-functional but clarity issues in the generated code: fixed the missing newline after Stores in the CudaPrinter, and prevented the PrioritizeLoad mutator from pulling out loads contained within simple Let statements (such as those produced by the Registerizer).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44909

Reviewed By: agolynski

Differential Revision: D23800565

Pulled By: nickgg

fbshipit-source-id: bddef1f40d8d461da965685f01d00b468d8a2c2f",187.0,31.0,"test/cpp/tensorexpr/test_cuda.cpp,test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/cuda_codegen.cpp,torch/csrc/jit/tensorexpr/cuda_codegen.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/stmt.h",8.0,7,2,2.048726761,2.0,11308.0,4.0,301590.875,5280.0,12084.0,0.0,Corrective,1.0,1
pytorch,c2c859bdf2268b4f9cad74cb06c3629e2ab12126,4bcff4733d3e567e714dcee9ff893abef9b5fdc0,Saketh Are,saketh@fb.com,Fri Nov 19 18:35:08 2021 -0800,1637346908.0,"Add OpInfos for parcel Elementwise Binary II (#68085)

Summary:
Adds OpInfos for `torch.lcm`, `torch.gcd`, `torch.heaviside`, `torch.bitwise_or`, `torch.bitwise_xor`, `torch.isclose`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68085

Reviewed By: ngimel

Differential Revision: D32533310

Pulled By: saketh-are

fbshipit-source-id: 1616ebec61164cd1b44672f36220787a878b96a4",94.0,4.0,"torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",2.0,3,1,0.14372617,2.0,14687.0,2.0,69797.0,17213.0,40533.5,0.0,Feature Addition,0.0,1
pytorch,f9633b954268c493dabfe7c2047f9331cb4a621e,4bf0202cac4cdf95bf437df5d2e8a8c5bb892569,Orion Reblitz-Richardson,orionr@gmail.com,Thu May 24 14:47:27 2018 -0700,1527173247.0,"[build] Have PyTorch depend on minimal libcaffe2.so instead of libATen.so (#7399)

* Have PyTorch depend on minimal libcaffe2.so instead of libATen.so

* Build ATen tests as a part of Caffe2 build

* Hopefully cufft and nvcc fPIC fixes

* Make ATen install components optional

* Add tests back for ATen and fix TH build

* Fixes for test_install.sh script

* Fixes for cpp_build/build_all.sh

* Fixes for aten/tools/run_tests.sh

* Switch ATen cmake calls to USE_CUDA instead of NO_CUDA

* Attempt at fix for aten/tools/run_tests.sh

* Fix typo in last commit

* Fix valgrind call after pushd

* Be forgiving about USE_CUDA disable like PyTorch

* More fixes on the install side

* Link all libcaffe2 during test run

* Make cuDNN optional for ATen right now

* Potential fix for non-CUDA builds

* Use NCCL_ROOT_DIR environment variable

* Pass -fPIC through nvcc to base compiler/linker

* Remove THCUNN.h requirement for libtorch gen

* Add Mac test for -Wmaybe-uninitialized

* Potential Windows and Mac fixes

* Move MSVC target props to shared function

* Disable cpp_build/libtorch tests on Mac

* Disable sleef for Windows builds

* Move protos under BUILD_CAFFE2

* Remove space from linker flags passed with -Wl

* Remove ATen from Caffe2 dep libs since directly included

* Potential Windows fixes

* Preserve options while sleef builds

* Force BUILD_SHARED_LIBS flag for Caffe2 builds

* Set DYLD_LIBRARY_PATH and LD_LIBRARY_PATH for Mac testing

* Pass TORCH_CUDA_ARCH_LIST directly in cuda.cmake

* Fixes for the last two changes

* Potential fix for Mac build failure

* Switch Caffe2 to build_caffe2 dir to not conflict

* Cleanup FindMKL.cmake

* Another attempt at Mac cpp_build fix

* Clear cpp-build directory for Mac builds

* Disable test in Mac build/test to match cmake",1861.0,1618.0,".gitignore,.jenkins/caffe2/build.sh,.jenkins/pytorch/build.sh,.jenkins/pytorch/dirty.sh,.jenkins/pytorch/macos-build-test.sh,.jenkins/pytorch/test.sh,.travis.aten.yml,CMakeLists.txt,aten/CMakeLists.txt,aten/README.md,aten/contrib/data/CMakeLists.txt,aten/contrib/meter/CMakeLists.txt,aten/src/ATen/ATenGeneral.h,aten/src/ATen/CMakeLists.txt,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cuda/ATenCUDAGeneral.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/test/CMakeLists.txt,aten/src/TH/CMakeLists.txt,aten/src/TH/THAllocator.cpp,aten/src/TH/THGeneral.h.in,aten/src/THC/THCGeneral.h.in,aten/src/THCUNN/CMakeLists.txt,aten/tools/run_tests.sh,aten/tools/test_install.sh,caffe2/CMakeLists.txt,caffe2/contrib/CMakeLists.txt,caffe2/contrib/aten/CMakeLists.txt,cmake/BuildVariables.cmake,cmake/Caffe2Config.cmake.in,cmake/Codegen.cmake,cmake/Dependencies.cmake,cmake/External/nccl.cmake,cmake/MiscCheck.cmake,cmake/Modules/FindMKL.cmake,cmake/Modules/FindNCCL.cmake,cmake/ProtoBuf.cmake,cmake/Summary.cmake,cmake/public/cuda.cmake,cmake/public/utils.cmake,setup.py,tools/build_pytorch_libs.bat,tools/build_pytorch_libs.sh,tools/cpp_build/build_all.sh,tools/cpp_build/build_aten.sh,tools/cpp_build/build_caffe2.sh,tools/cpp_build/build_common.sh,tools/cpp_build/build_libtorch.sh,tools/cpp_build/libtorch/CMakeLists.txt,tools/test_aten_install.sh,torch/lib/THD/CMakeLists.txt,torch/lib/libshm/CMakeLists.txt,torch/utils/cpp_extension.py",54.0,34,6,3.780043525,78.0,10326.0,37.0,1757120.0961538462,1189.0,3252.805292,0.0,Corrective,1.0,1
pytorch,21017ad1a1bf762b0991d27d15068618bad20657,4bf7959de2372203541d7725641f3d547f5a9441,soulitzer,soulitzer@gmail.com,Wed Sep 15 19:43:54 2021 -0700,1631735034.0,"Remove `run_functional_checks` from `test_autograd` and create necessary OpInfos (#64993)

Summary:
OpInfo tracker: https://github.com/pytorch/pytorch/issues/54261

 - Eliminate duplicated testing logic in test_autograd
 - Moved tests that rely on this testing logic to use OpInfos
   - `cat` already has OpInfo (no action needed)
   - Created OpInfo for `block_diag` and `broadcast_tensors`

Running into some FX errors. Added op to skip-list and created an issue here: https://github.com/pytorch/pytorch/issues/64997
Both `block_diag` and `broadcast_tensors` are variadic, so skipping `test_variant_consistency_jit` (from comments on other OpInfos, it looks like JIT does not support variadic tensors)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64993

Reviewed By: jbschlosser

Differential Revision: D30961736

Pulled By: soulitzer

fbshipit-source-id: e169305384a683acae1178c4e12e9e214a67226a",45.0,91.0,"test/test_autograd.py,test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,1.172863979,42.0,24830.0,4.0,202024.25,15464.0,35348.0,0.0,Feature Addition,0.0,1
pytorch,b2ab6891c5486f75cf6b40a29d833cac0f47b3cc,4bf7be7bd59f2e2f4910dd005594f0b6c4f108a6,Soumith Chintala,soumith@fb.com,Wed Jan 04 18:22:02 2017 -0500,1483554122.0,fix RNN module docs for rst,167.0,121.0,torch/nn/modules/rnn.py,1.0,3,1,0,18.0,419.0,1.0,175337.0,305.0,6388.224559,0.0,Corrective,1.0,1
pytorch,08bdd694f96b5ed7923befb6875aa788c5ed995a,4c23c34e7927fd55d9007929e0a65a810294817f,Igor Fedan,igor.fedan@gmail.com,Wed May 15 23:39:26 2019 -0700,1557963566.0,"Computing var/stddev and mean at the same time (#18731)

Summary:
The current variance kernels compute mean at the same time. Many times we want both statistics together, so it seems reasonable to have a kwarg/function that allows us to get both values without launching an extra kernel.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18731

Differential Revision: D14726082

Pulled By: ifedan

fbshipit-source-id: 473cba0227b69eb2240dca5e61a8f4366df0e029",539.0,56.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/SharedReduceOps.h,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/TensorIteratorReduce.cpp,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cuda/Reduce.cuh,aten/src/ATen/native/cuda/ReduceOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/common_methods_invocations.py,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_torch_docs.py,torch/tensor.py",19.0,14,5,3.4965832,43.0,35497.0,15.0,1657949.3684210526,8685.0,25764.83333,0.0,,0.0,1
pytorch,1c0efffc2e8e86b32dcfce62c705ba6793491aaa,4c417e1be8089452636141d2b0764031c48178c1,Kshiteej K,kshitijkalambarkar@gmail.com,Tue Oct 05 05:26:00 2021 +0500,1633411560.0,"[functorch] add cosine_similairity batching rule (pytorch/functorch#171)

* add cosine_similairity batching rule

* update test file

* update comment

* add rule for clamp_min_ and clamp_max_

* update test

* update xfail in test_ops

* undo line change in BatchRulesLoss",38.0,4.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/functorch/csrc/BatchRulesStopDecomposition.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,0.904614765,1.0,4777.0,2.0,2.0,419.0,596.5,0.0,Feature Addition,0.0,1
pytorch,7a408576dd509b8ea021685f791292105d7ed177,4c4816ad07656e9af0a52846e11ddf26bd407758,anjali411,chourdiaanjali123@gmail.com,Wed May 06 21:10:32 2020 -0700,1588799432.0,"[CPU] addmv for complex tensors (#37924)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37924

Test Plan: Imported from OSS

Differential Revision: D21429384

Pulled By: anjali411

fbshipit-source-id: 8b1b76ed13d2e5785a4d552aedb2e6f58d304c46",31.0,6.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/native/Blas.cpp,aten/src/ATen/native/BlasKernel.cpp,test/test_torch.py",4.0,5,2,1.498560125,41.0,19005.0,3.0,527091.5,1762.0,4587.5,0.0,Feature Addition,0.0,1
pytorch,396637cdd6691559334465f884c1e67e1ad961db,4c4a42b3f901d0dea21521b1a82ed6a1a029b1d3,li-roy,8813817+li-roy@users.noreply.github.com,Thu Mar 08 22:54:24 2018 -0800,1520549664.0,"implement CosineEmbeddingLoss as a native function and add reduce arg (#5646)

* implement CosineEmbeddingLoss as a native function and add reduce=True arg to it

* fix flake8

* address comments

* add reference function to tests

* fix flake8",87.0,108.0,"aten/src/ATen/native/Loss.cpp,aten/src/ATen/native/native_functions.yaml,test/common_nn.py,test/test_nn.py,torch/nn/_functions/loss.py,torch/nn/backends/thnn.py,torch/nn/functional.py,torch/nn/modules/loss.py",8.0,10,3,2.318197241,37.0,11439.0,1.0,13627.0,473.0,2347.5,0.0,Corrective,1.0,1
pytorch,6dfaa1071aaea2e5eab269909345457b6e70d9d5,4c5b95a43358b73ce925b09b527afeef994ce6ae,gchanan,gregchanan@gmail.com,Thu Apr 19 18:28:48 2018 -0400,1524162528.0,"Revert ""Terminate dataloader workers properly when parent process is SIGKILL'ed (#6606)"" (#6772)

This reverts commit 8d6a50aaeba2166ce870016da7488f879395ebb1.",5.0,125.0,"test/test_dataloader.py,torch/utils/data/dataloader.py",2.0,4,2,0.961236605,37.0,1229.0,2.0,45582.0,925.0,2236.305292,0.0,,0.0,1
pytorch,1661370ac5f88ef11fedbeac8d0398e8369fc1f3,4c7219b3b086d73d3a0b48206118b84c08610a18,gchanan,gregchanan@gmail.com,Thu Nov 30 04:13:04 2017 -0500,1512015184.0,"Implement matmul as a native function; use it for Variable impl (#3943)

* Implement matmul as a native function; use it for Variable impl.

This also includes an (inefficient) version of allclose, which was necessary for testing.
A more efficient version would use some apply logic to fuse the ops and exit early (coming in future PR).

On small tensors [(2, 5, 5) @ (5,5)], this yields ~2.5x speedup over the python implementation.

* Make maybeSqueeze static.",184.0,15.0,"aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/test_assert.h,tools/autograd/templates/python_variable_methods.cpp,torch/autograd/variable.py",6.0,10,3,1.506976424,37.0,1386.0,2.0,695036.0,353.0,1025.905869,0.0,,0.0,1
pytorch,63519df07a597e528b7be635781c50aef002ad65,4ca1a54526c3bb4583ffee1a0e75075750fabfbd,bhushan,bhushan.s.94@gmail.com,Tue Feb 26 22:11:18 2019 -0800,1551219078.0,"Make transpose consistent with numpy's behavior (#17462)

Summary:
Pytorch's tensor.t() is now equivalent with Numpy's ndarray.T for 1D tensor
i.e. tensor.t() == tensor

Test case added:
- test_t

fixes #9687
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17462

Differential Revision: D14214838

Pulled By: soumith

fbshipit-source-id: c5df1ecc8837be22478e3a82ce4854ccabb35765",18.0,8.0,"aten/src/ATen/native/TensorShape.cpp,test/test_jit.py,test/test_torch.py",3.0,5,2,1.44524582,41.0,24871.0,3.0,149612.66666666666,7235.0,22126.33333,0.0,Corrective,1.0,1
pytorch,eb36f67dcc7000caa58fa4dfa9c089ce17c6d523,4caca7a15b6edd9f99bba1fd0a5b8fb64ed991c3,Heitor Schueroff,heitorschueroff@fb.com,Thu Jun 17 11:47:33 2021 -0700,1623930453.0,"Improved torch.einsum testing and fixed bug (#59731)

Summary:
Improved torch.einsum testing and fixed a bug where lower case letters appeared before upper case letters in the sorted order which is inconsistent with NumPy.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59731

Reviewed By: SplitInfinity, ansley

Differential Revision: D29183078

Pulled By: heitorschueroff

fbshipit-source-id: a33980d273707da2d60a387a2af2fa41527ddb68",182.0,124.0,"aten/src/ATen/native/Linear.cpp,test/test_linalg.py,torch/functional.py",3.0,6,3,0.395547312,30.0,10179.0,3.0,1217765.3333333333,13095.0,29640.0,0.0,Corrective,1.0,1
pytorch,5a10ee71d6ca9d557496ee472adab0f184d8a3ee,4cb534f92ef6f5b2ec99109b0329f93a859ae831,Nikita Shulga,nshulga@fb.com,Wed Apr 28 21:09:06 2021 -0700,1619644146.0,"Make PyTorch code-base clang-tidy compliant (#56892)

Summary:
This is an automatic change generated by the following script:
```
#!/usr/bin/env python3
from subprocess import check_output, check_call
import os

def get_compiled_files_list():
    import json
    with open(""build/compile_commands.json"") as f:
        data = json.load(f)
    files = [os.path.relpath(node['file']) for node in data]
    for idx, fname in enumerate(files):
        if fname.startswith('build/') and fname.endswith('.DEFAULT.cpp'):
            files[idx] = fname[len('build/'):-len('.DEFAULT.cpp')]
    return files

def run_clang_tidy(fname):
    check_call([""python3"", ""tools/clang_tidy.py"", ""-c"", ""build"", ""-x"", fname,""-s""])
    changes = check_output([""git"", ""ls-files"", ""-m""])
    if len(changes) == 0:
        return
    check_call([""git"", ""commit"",""--all"", ""-m"", f""NOLINT stubs for {fname}""])

def main():
    git_files = check_output([""git"", ""ls-files""]).decode(""ascii"").split(""\n"")
    compiled_files = get_compiled_files_list()
    for idx, fname in enumerate(git_files):
        if fname not in compiled_files:
            continue
        if fname.startswith(""caffe2/contrib/aten/""):
            continue
        print(f""[{idx}/{len(git_files)}] Processing {fname}"")
        run_clang_tidy(fname)

if __name__ == ""__main__"":
    main()
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56892

Reviewed By: H-Huang

Differential Revision: D27991944

Pulled By: malfet

fbshipit-source-id: 5415e1eb2c1b34319a4f03024bfaa087007d7179",20540.0,148.0,"aten/src/ATen/BatchedFallback.cpp,aten/src/ATen/BatchedTensorImpl.cpp,aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/CPUGeneratorImpl.cpp,aten/src/ATen/Context.cpp,aten/src/ATen/DLConvertor.cpp,aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/NamedTensorUtils.cpp,aten/src/ATen/ParallelNative.cpp,aten/src/ATen/ParallelThreadPoolNative.cpp,aten/src/ATen/SequenceNumber.cpp,aten/src/ATen/SparseTensorUtils.cpp,aten/src/ATen/TensorIndexing.h,aten/src/ATen/TensorNames.cpp,aten/src/ATen/Utils.cpp,aten/src/ATen/VmapMode.cpp,aten/src/ATen/VmapTransforms.cpp,aten/src/ATen/autocast_mode.cpp,aten/src/ATen/benchmarks/quantize_per_channel.cpp,aten/src/ATen/benchmarks/stateful_conv1d.cpp,aten/src/ATen/benchmarks/tensor_add.cpp,aten/src/ATen/core/Dimname.cpp,aten/src/ATen/core/Formatting.cpp,aten/src/ATen/core/List_test.cpp,aten/src/ATen/core/NamedTensor.cpp,aten/src/ATen/core/TensorAccessor.h,aten/src/ATen/core/TensorImpl_test.cpp,aten/src/ATen/core/VariableHooksInterface.cpp,aten/src/ATen/core/Vitals.cpp,aten/src/ATen/core/blob.h,aten/src/ATen/core/boxing/KernelFunction_test.cpp,aten/src/ATen/core/boxing/impl/kernel_function_legacy_test.cpp,aten/src/ATen/core/boxing/impl/kernel_function_test.cpp,aten/src/ATen/core/boxing/impl/kernel_lambda_legacy_test.cpp,aten/src/ATen/core/boxing/impl/kernel_lambda_test.cpp,aten/src/ATen/core/boxing/impl/kernel_stackbased_test.cpp,aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor_test.cpp,aten/src/ATen/core/dispatch/CppSignature_test.cpp,aten/src/ATen/core/dispatch/Dispatcher.cpp,aten/src/ATen/core/dispatch/OperatorEntry.cpp,aten/src/ATen/core/dispatch/backend_fallback_test.cpp,aten/src/ATen/core/interned_strings.cpp,aten/src/ATen/core/ivalue.cpp,aten/src/ATen/core/library.cpp,aten/src/ATen/core/op_registration/infer_schema.cpp,aten/src/ATen/core/op_registration/op_registration.cpp,aten/src/ATen/core/op_registration/op_registration_test.cpp,aten/src/ATen/core/register_symbols.cpp,aten/src/ATen/core/type.cpp,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/detail/CPUGuardImpl.cpp,aten/src/ATen/detail/CUDAHooksInterface.cpp,aten/src/ATen/detail/HIPHooksInterface.cpp,aten/src/ATen/detail/MetaGuardImpl.cpp,aten/src/ATen/metal/Context.cpp,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/AdaptiveAveragePooling.cpp,aten/src/ATen/native/AdaptiveAveragePooling3d.cpp,aten/src/ATen/native/AdaptiveMaxPooling2d.cpp,aten/src/ATen/native/AveragePool2d.cpp,aten/src/ATen/native/AveragePool3d.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/Batching.cpp,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/Blas.cpp,aten/src/ATen/native/BlasKernel.cpp,aten/src/ATen/native/CPUBlas.cpp,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/ConvolutionMM2d.cpp,aten/src/ATen/native/ConvolutionMM3d.cpp,aten/src/ATen/native/ConvolutionTBC.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Cross.cpp,aten/src/ATen/native/DilatedMaxPool2d.cpp,aten/src/ATen/native/DilatedMaxPool3d.cpp,aten/src/ATen/native/Distance.cpp,aten/src/ATen/native/DistributionTemplates.h,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/Fill.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/FunctionOfAMatrixUtils.cpp,aten/src/ATen/native/GatedLinearUnit.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/IndexingUtils.cpp,aten/src/ATen/native/Integration.cpp,aten/src/ATen/native/Lerp.cpp,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/Loss.cpp,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/LossMultiLabelMargin.cpp,aten/src/ATen/native/LossMultiMargin.cpp,aten/src/ATen/native/LossNLL.cpp,aten/src/ATen/native/LossNLL2d.cpp,aten/src/ATen/native/MaxPooling.cpp,aten/src/ATen/native/MaxUnpooling.cpp,aten/src/ATen/native/MetaTensor.cpp,aten/src/ATen/native/NNPACK.cpp,aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp,aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp,aten/src/ATen/native/NaiveDilatedConvolution.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/PixelShuffle.cpp,aten/src/ATen/native/PointwiseOps.cpp,aten/src/ATen/native/Pow.cpp,aten/src/ATen/native/QuantizedLinear.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/RangeFactories.cpp,aten/src/ATen/native/ReduceAllOps.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReflectionPad.cpp,aten/src/ATen/native/ReplicationPadding.cpp,aten/src/ATen/native/Resize.cpp,aten/src/ATen/native/SegmentReduce.cpp,aten/src/ATen/native/SobolEngineOps.cpp,aten/src/ATen/native/SobolEngineOpsUtils.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/TensorConversions.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorIteratorReduce.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/TensorTransformations.cpp,aten/src/ATen/native/TriangularOps.cpp,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/Unfold2d.cpp,aten/src/ATen/native/UnfoldBackward.cpp,aten/src/ATen/native/UpSample.cpp,aten/src/ATen/native/UpSampleBicubic2d.cpp,aten/src/ATen/native/UpSampleBilinear2d.cpp,aten/src/ATen/native/UpSampleLinear1d.cpp,aten/src/ATen/native/UpSampleNearest1d.cpp,aten/src/ATen/native/UpSampleNearest2d.cpp,aten/src/ATen/native/UpSampleNearest3d.cpp,aten/src/ATen/native/UpSampleTrilinear3d.cpp,aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp,aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp,aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp,aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp,aten/src/ATen/native/cpu/Activation.cpp,aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cpu/BlasKernel.cpp,aten/src/ATen/native/cpu/CatKernel.cpp,aten/src/ATen/native/cpu/ComplexKernel.cpp,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cpu/CrossKernel.cpp,aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp,aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,aten/src/ATen/native/cpu/FillKernel.cpp,aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.cpp,aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cpu/LerpKernel.cpp,aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp,aten/src/ATen/native/cpu/MaxPoolKernel.cpp,aten/src/ATen/native/cpu/MaxPooling.cpp,aten/src/ATen/native/cpu/MultinomialKernel.cpp,aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp,aten/src/ATen/native/cpu/PowKernel.cpp,aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp,aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/ScatterGatherKernel.cpp,aten/src/ATen/native/cpu/SoftMaxKernel.cpp,aten/src/ATen/native/cpu/SortingKernel.cpp,aten/src/ATen/native/cpu/StackKernel.cpp,aten/src/ATen/native/cpu/SumKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/Unfold2d.cpp,aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp,aten/src/ATen/native/cpu/UpSampleKernel.cpp,aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp,aten/src/ATen/native/cpu/batch_norm_kernel.cpp,aten/src/ATen/native/cpu/group_norm_kernel.cpp,aten/src/ATen/native/cpu/layer_norm_kernel.cpp,aten/src/ATen/native/group_norm.cpp,aten/src/ATen/native/layer_norm.cpp,aten/src/ATen/native/metal/MetalGuardImpl.cpp,aten/src/ATen/native/metal/MetalPrepackOpRegister.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/mkldnn/IDeepRegistration.cpp,aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp,aten/src/ATen/native/mkldnn/MkldnnTensorMath.cpp,aten/src/ATen/native/mkldnn/Normalization.cpp,aten/src/ATen/native/mkldnn/Pooling.cpp,aten/src/ATen/native/quantized/QTensor.cpp,aten/src/ATen/native/quantized/affine_quantizer.cpp,aten/src/ATen/native/quantized/affine_quantizer_base.cpp,aten/src/ATen/native/quantized/cpu/conv_serialization.h,aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp,aten/src/ATen/native/quantized/cpu/fbgemm_utils.h,aten/src/ATen/native/quantized/cpu/int_repr_quant.cpp,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool3d.cpp,aten/src/ATen/native/quantized/cpu/qadd.cpp,aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp,aten/src/ATen/native/quantized/cpu/qclamp.cpp,aten/src/ATen/native/quantized/cpu/qconcat.cpp,aten/src/ATen/native/quantized/cpu/qconv.cpp,aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp,aten/src/ATen/native/quantized/cpu/qelu.cpp,aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp,aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp,aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp,aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp,aten/src/ATen/native/quantized/cpu/qhardswish.cpp,aten/src/ATen/native/quantized/cpu/qlinear.cpp,aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp,aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp,aten/src/ATen/native/quantized/cpu/qmul.cpp,aten/src/ATen/native/quantized/cpu/qnnpack/include/conv_utils.h,aten/src/ATen/native/quantized/cpu/qnnpack_utils.h,aten/src/ATen/native/quantized/cpu/qnormalization.cpp,aten/src/ATen/native/quantized/cpu/qpool.cpp,aten/src/ATen/native/quantized/cpu/qrelu.cpp,aten/src/ATen/native/quantized/cpu/qsigmoid.cpp,aten/src/ATen/native/quantized/cpu/qsort.cpp,aten/src/ATen/native/quantized/cpu/qtanh.cpp,aten/src/ATen/native/quantized/cpu/qthreshold.cpp,aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp,aten/src/ATen/native/quantized/cpu/tensor_operators.cpp,aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp,aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp,aten/src/ATen/native/sparse/SoftMax.cpp,aten/src/ATen/native/sparse/SparseCsrTensor.cpp,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp,aten/src/ATen/native/sparse/SparseMatMul.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/xnnpack/Convolution.cpp,aten/src/ATen/native/xnnpack/Init.cpp,aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp,aten/src/ATen/nnapi/nnapi_bind.cpp,aten/src/ATen/nnapi/nnapi_model_loader.cpp,aten/src/ATen/nnapi/nnapi_wrapper.cpp,aten/src/ATen/quantized/QTensorImpl.cpp,aten/src/ATen/quantized/Quantizer.cpp,aten/src/ATen/record_function.cpp,aten/src/ATen/test/Dict_test.cpp,aten/src/ATen/test/Dimname_test.cpp,aten/src/ATen/test/MaybeOwned_test.cpp,aten/src/ATen/test/NamedTensor_test.cpp,aten/src/ATen/test/apply_utils_test.cpp,aten/src/ATen/test/atest.cpp,aten/src/ATen/test/basic.cpp,aten/src/ATen/test/broadcast_test.cpp,aten/src/ATen/test/cpu_caching_allocator_test.cpp,aten/src/ATen/test/cpu_generator_test.cpp,aten/src/ATen/test/cpu_profiling_allocator_test.cpp,aten/src/ATen/test/cpu_rng_test.cpp,aten/src/ATen/test/dlconvertor_test.cpp,aten/src/ATen/test/extension_backend_test.cpp,aten/src/ATen/test/half_test.cpp,aten/src/ATen/test/ivalue_test.cpp,aten/src/ATen/test/math_kernel_test.cpp,aten/src/ATen/test/memory_format_test.cpp,aten/src/ATen/test/memory_overlapping_test.cpp,aten/src/ATen/test/mobile_memory_cleanup.cpp,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/pow_test.cpp,aten/src/ATen/test/quantized_test.cpp,aten/src/ATen/test/reduce_ops_test.cpp,aten/src/ATen/test/scalar_tensor_test.cpp,aten/src/ATen/test/scalar_test.cpp,aten/src/ATen/test/tensor_interop_test.cpp,aten/src/ATen/test/tensor_iterator_test.cpp,aten/src/ATen/test/test_parallel.cpp,aten/src/ATen/test/test_thread_pool_guard.cpp,aten/src/ATen/test/thread_init_test.cpp,aten/src/ATen/test/type_test.cpp,aten/src/ATen/test/undefined_tensor_test.cpp,aten/src/ATen/test/variant_test.cpp,aten/src/ATen/test/vec256_test_all_types.cpp,aten/src/ATen/test/vitals.cpp,aten/src/ATen/test/vmap_test.cpp,aten/src/ATen/test/vulkan_test.cpp,aten/src/ATen/test/weakref_test.cpp,aten/src/ATen/test/wrapdim_test.cpp,aten/src/ATen/test/xla_tensor_test.cpp,aten/src/ATen/vulkan/Context.cpp,aten/src/TH/THAllocator.cpp,aten/src/TH/THBlas.cpp,aten/src/TH/THGeneral.cpp,aten/src/TH/THLapack.cpp,aten/src/TH/THStorageFunctions.cpp,aten/src/TH/THTensor.cpp,aten/src/TH/THTensorEvenMoreMath.cpp,aten/src/TH/THTensorLapack.cpp,aten/src/TH/THTensorMath.cpp,aten/src/TH/THTensorMoreMath.cpp,benchmarks/cpp/convolution.cpp,c10/core/GradMode.cpp,c10/core/TensorImpl.cpp,c10/test/util/C++17_test.cpp,c10/util/intrusive_ptr.h,caffe2/core/blob_serialization.cc,caffe2/core/blob_test.cc,caffe2/core/common.cc,caffe2/core/common_test.cc,caffe2/core/context.cc,caffe2/core/context_base.cc,caffe2/core/context_test.cc,caffe2/core/db.cc,caffe2/core/event.cc,caffe2/core/event_test.cc,caffe2/core/export_c10_op_to_caffe2.cc,caffe2/core/export_c10_op_to_caffe2.h,caffe2/core/graph.cc,caffe2/core/graph_test.cc,caffe2/core/init.cc,caffe2/core/init_denormals.cc,caffe2/core/init_intrinsics_check.cc,caffe2/core/init_omp.cc,caffe2/core/init_test.cc,caffe2/core/int8_serialization.cc,caffe2/core/memonger.cc,caffe2/core/module_test.cc,caffe2/core/net.cc,caffe2/core/net_async_base.cc,caffe2/core/net_async_scheduling.cc,caffe2/core/net_async_task.cc,caffe2/core/net_async_task_future.cc,caffe2/core/net_async_tracing.cc,caffe2/core/net_async_tracing.h,caffe2/core/net_async_tracing_test.cc,caffe2/core/net_dag_utils_test.cc,caffe2/core/net_parallel.cc,caffe2/core/net_simple.cc,caffe2/core/net_simple_refcount.cc,caffe2/core/net_simple_refcount_test.cc,caffe2/core/net_test.cc,caffe2/core/nomnigraph/Representations/NeuralNet.cc,caffe2/core/nomnigraph/tests/AlgorithmsTest.cc,caffe2/core/nomnigraph/tests/BinaryMatchImplTest.cc,caffe2/core/nomnigraph/tests/GraphTest.cc,caffe2/core/nomnigraph/tests/MatchTest.cc,caffe2/core/nomnigraph/tests/NeuralNetTest.cc,caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc,caffe2/core/nomnigraph/tests/TarjansImplTest.cc,caffe2/core/nomnigraph/tests/TopoSortTest.cc,caffe2/core/observer_test.cc,caffe2/core/operator.cc,caffe2/core/operator_schema.cc,caffe2/core/operator_schema_test.cc,caffe2/core/operator_test.cc,caffe2/core/parallel_net_test.cc,caffe2/core/plan_executor.cc,caffe2/core/plan_executor_test.cc,caffe2/core/prof_dag_counters.cc,caffe2/core/qtensor.h,caffe2/core/qtensor_serialization.cc,caffe2/core/serialization_test.cc,caffe2/core/stats.cc,caffe2/core/stats_test.cc,caffe2/core/tensor.cc,caffe2/core/test_utils.h,caffe2/core/timer_test.cc,caffe2/core/transform.cc,caffe2/core/transform_test.cc,caffe2/core/workspace.cc,caffe2/core/workspace_test.cc,caffe2/db/create_db_op.cc,caffe2/db/protodb.cc,caffe2/distributed/file_store_handler.cc,caffe2/distributed/file_store_handler_op.cc,caffe2/distributed/store_handler.cc,caffe2/distributed/store_ops.cc,caffe2/ideep/operators/adam_op.cc,caffe2/ideep/operators/channel_shuffle_op.cc,caffe2/ideep/operators/concat_split_op.cc,caffe2/ideep/operators/conv_op.cc,caffe2/ideep/operators/conv_transpose_op.cc,caffe2/ideep/operators/dropout_op.cc,caffe2/ideep/operators/elementwise_sum_op.cc,caffe2/ideep/operators/expand_squeeze_dims_op.cc,caffe2/ideep/operators/fully_connected_op.cc,caffe2/ideep/operators/local_response_normalization_op.cc,caffe2/ideep/operators/momentum_sgd_op.cc,caffe2/ideep/operators/operator_fallback_ideep.cc,caffe2/ideep/operators/order_switch_ops.cc,caffe2/ideep/operators/pool_op.cc,caffe2/ideep/operators/quantization/int8_add_op.cc,caffe2/ideep/operators/quantization/int8_conv_op.cc,caffe2/ideep/operators/quantization/int8_dequantize_op.cc,caffe2/ideep/operators/quantization/int8_fully_connected_op.cc,caffe2/ideep/operators/quantization/int8_given_tensor_fill_op.cc,caffe2/ideep/operators/quantization/int8_pool_op.cc,caffe2/ideep/operators/quantization/int8_quantize_op.cc,caffe2/ideep/operators/quantization/int8_relu_op.cc,caffe2/ideep/operators/queue_ops.cc,caffe2/ideep/operators/relu_op.cc,caffe2/ideep/operators/reshape_op.cc,caffe2/ideep/operators/shape_op.cc,caffe2/ideep/operators/sigmoid_op.cc,caffe2/ideep/operators/spatial_batch_norm_op.cc,caffe2/ideep/operators/transpose_op.cc,caffe2/ideep/operators/utility_ops.cc,caffe2/ideep/utils/ideep_register.cc,caffe2/observers/time_observer.cc,caffe2/observers/time_observer_test.cc,caffe2/onnx/backend.cc,caffe2/onnx/helper.cc,caffe2/onnx/offline_tensor.cc,caffe2/onnx/onnx_exporter.cc,caffe2/onnx/ssa_test.cc,caffe2/onnx/torch_ops/defs.cc,caffe2/onnx/torch_ops/schema.cc,caffe2/operators/abs_op.cc,caffe2/operators/accumulate_op.cc,caffe2/operators/accuracy_op.cc,caffe2/operators/acos_op.cc,caffe2/operators/affine_channel_op.cc,caffe2/operators/alias_with_name.cc,caffe2/operators/apmeter_op.cc,caffe2/operators/arg_ops.cc,caffe2/operators/asin_op.cc,caffe2/operators/assert_op.cc,caffe2/operators/async_net_barrier_op.cc,caffe2/operators/atan_op.cc,caffe2/operators/atomic_ops.cc,caffe2/operators/batch_box_cox_op.cc,caffe2/operators/batch_bucketize_op.cc,caffe2/operators/batch_gather_ops.cc,caffe2/operators/batch_matmul_op.cc,caffe2/operators/batch_matmul_op_test.cc,caffe2/operators/batch_moments_op.cc,caffe2/operators/batch_permutation_op.cc,caffe2/operators/batch_sparse_to_dense_op.cc,caffe2/operators/bbox_transform_op.cc,caffe2/operators/bisect_percentile_op.cc,caffe2/operators/boolean_mask_ops.cc,caffe2/operators/boolean_unmask_ops.cc,caffe2/operators/boolean_unmask_ops_test.cc,caffe2/operators/box_with_nms_limit_op.cc,caffe2/operators/bucketize_op.cc,caffe2/operators/byte_weight_dequant_op.cc,caffe2/operators/cast_op.cc,caffe2/operators/cbrt_op.cc,caffe2/operators/cc_bmm_bg_op.cc,caffe2/operators/ceil_op.cc,caffe2/operators/channel_backprop_stats_op.cc,caffe2/operators/channel_shuffle_op.cc,caffe2/operators/channel_stats_op.cc,caffe2/operators/clip_op.cc,caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,caffe2/operators/communicator_op.cc,caffe2/operators/concat_split_op.cc,caffe2/operators/conditional_op.cc,caffe2/operators/conv_gradient_op.cc,caffe2/operators/conv_op.cc,caffe2/operators/conv_op_eigen.cc,caffe2/operators/conv_op_impl.h,caffe2/operators/conv_op_shared.cc,caffe2/operators/conv_pool_op_base.h,caffe2/operators/conv_transpose_gradient_op.cc,caffe2/operators/conv_transpose_op.cc,caffe2/operators/conv_transpose_op_mobile_test.cc,caffe2/operators/conv_transpose_unpool_op_base.h,caffe2/operators/copy_op.cc,caffe2/operators/copy_rows_to_tensor_op.cc,caffe2/operators/copy_rows_to_tensor_op.h,caffe2/operators/cos_op.cc,caffe2/operators/cosh_op.cc,caffe2/operators/cosine_embedding_criterion_op.cc,caffe2/operators/counter_ops.cc,caffe2/operators/create_scope_op.cc,caffe2/operators/crf_viterbi_op.cc,caffe2/operators/cross_entropy_op.cc,caffe2/operators/ctc_beam_search_decoder_op.cc,caffe2/operators/ctc_greedy_decoder_op.cc,caffe2/operators/cube_op.cc,caffe2/operators/data_couple.cc,caffe2/operators/dataset_ops.cc,caffe2/operators/deform_conv_gradient_op.cc,caffe2/operators/deform_conv_op.cc,caffe2/operators/dense_vector_to_id_list_op.cc,caffe2/operators/distance_op.cc,caffe2/operators/do_op.cc,caffe2/operators/do_op.h,caffe2/operators/dropout_op.cc,caffe2/operators/elementwise_add_gradient_op.cc,caffe2/operators/elementwise_add_op.cc,caffe2/operators/elementwise_div_gradient_op.cc,caffe2/operators/elementwise_div_op.cc,caffe2/operators/elementwise_linear_op.cc,caffe2/operators/elementwise_logical_ops.cc,caffe2/operators/elementwise_mul_gradient_op.cc,caffe2/operators/elementwise_mul_op.cc,caffe2/operators/elementwise_op_test.cc,caffe2/operators/elementwise_ops.cc,caffe2/operators/elementwise_ops_schema.cc,caffe2/operators/elementwise_ops_utils.cc,caffe2/operators/elementwise_sub_gradient_op.cc,caffe2/operators/elementwise_sub_op.cc,caffe2/operators/elementwise_sum_op.cc,caffe2/operators/elu_op.cc,caffe2/operators/enforce_finite_op.cc,caffe2/operators/ensure_clipped_op.cc,caffe2/operators/ensure_cpu_output_op.cc,caffe2/operators/erf_op.cc,caffe2/operators/exp_op.cc,caffe2/operators/expand_op.cc,caffe2/operators/expand_squeeze_dims_op.cc,caffe2/operators/expand_squeeze_dims_op.h,caffe2/operators/feature_maps_ops.cc,caffe2/operators/feed_blob_op.cc,caffe2/operators/filler_op.cc,caffe2/operators/find_duplicate_elements_op.cc,caffe2/operators/find_op.cc,caffe2/operators/flatten_op.cc,caffe2/operators/flexible_top_k.cc,caffe2/operators/floor_op.cc,caffe2/operators/free_op.cc,caffe2/operators/fully_connected_op.cc,caffe2/operators/fused_rowwise_8bit_conversion_ops.cc,caffe2/operators/fused_rowwise_8bit_conversion_ops.h,caffe2/operators/fused_rowwise_nbit_conversion_ops.cc,caffe2/operators/fused_rowwise_nbit_conversion_ops.h,caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc,caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h,caffe2/operators/fused_rowwise_random_quantization_ops.cc,caffe2/operators/gather_fused_8bit_rowwise_op.cc,caffe2/operators/gather_op.cc,caffe2/operators/gather_ranges_to_dense_op.cc,caffe2/operators/gelu_op.cc,caffe2/operators/generate_proposals_op.cc,caffe2/operators/generate_proposals_op_test.cc,caffe2/operators/generate_proposals_op_util_boxes_test.cc,caffe2/operators/generate_proposals_op_util_nms.h,caffe2/operators/generate_proposals_op_util_nms_test.cc,caffe2/operators/given_tensor_byte_string_to_uint8_fill_op.cc,caffe2/operators/given_tensor_byte_string_to_uint8_fill_op.h,caffe2/operators/given_tensor_fill_op.cc,caffe2/operators/given_tensor_fill_op.h,caffe2/operators/glu_op.cc,caffe2/operators/group_norm_op.cc,caffe2/operators/gru_unit_op.cc,caffe2/operators/h_softmax_op.cc,caffe2/operators/half_float_ops.cc,caffe2/operators/half_float_ops_test.cc,caffe2/operators/hard_sigmoid_op.cc,caffe2/operators/heatmap_max_keypoint_op.cc,caffe2/operators/histogram_op.cc,caffe2/operators/histogram_op.h,caffe2/operators/if_op.cc,caffe2/operators/im2col_op.cc,caffe2/operators/index_hash_ops.cc,caffe2/operators/index_hash_ops.h,caffe2/operators/index_ops.cc,caffe2/operators/index_ops.h,caffe2/operators/inference_lstm_op.cc,caffe2/operators/instance_norm_gradient_op.cc,caffe2/operators/instance_norm_op.cc,caffe2/operators/integral_image_op.cc,caffe2/operators/is_empty_op.cc,caffe2/operators/jsd_op.cc,caffe2/operators/key_split_ops.cc,caffe2/operators/last_n_window_collector.cc,caffe2/operators/layer_norm_op.cc,caffe2/operators/layer_norm_op.h,caffe2/operators/leaky_relu_op.cc,caffe2/operators/length_split_op.cc,caffe2/operators/lengths_pad_op.cc,caffe2/operators/lengths_reducer_fused_8bit_rowwise_ops.cc,caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.cc,caffe2/operators/lengths_reducer_ops.cc,caffe2/operators/lengths_reducer_rowwise_8bit_ops.cc,caffe2/operators/lengths_tile_op.cc,caffe2/operators/lengths_top_k_op.cc,caffe2/operators/listwise_l2r_op.cc,caffe2/operators/load_save_op.cc,caffe2/operators/load_save_op.h,caffe2/operators/local_response_normalization_op.cc,caffe2/operators/locally_connected_op.cc,caffe2/operators/log1p_op.cc,caffe2/operators/log_op.cc,caffe2/operators/logit_op.cc,caffe2/operators/loss_op.cc,caffe2/operators/lp_pool_op.cc,caffe2/operators/lpnorm_op.cc,caffe2/operators/lstm_unit_op.cc,caffe2/operators/map_ops.cc,caffe2/operators/margin_ranking_criterion_op.cc,caffe2/operators/matmul_op.cc,caffe2/operators/mean_op.cc,caffe2/operators/merge_id_lists_op.cc,caffe2/operators/merge_id_lists_op.h,caffe2/operators/minmax_gradient_ops.cc,caffe2/operators/minmax_ops.cc,caffe2/operators/mish_op.cc,caffe2/operators/mod_op.cc,caffe2/operators/moments_op.cc,caffe2/operators/multi_class_accuracy_op.cc,caffe2/operators/negate_gradient_op.cc,caffe2/operators/negative_op.cc,caffe2/operators/ngram_ops.cc,caffe2/operators/norm_planar_yuv_op.cc,caffe2/operators/normalize_l1_op.cc,caffe2/operators/normalize_op.cc,caffe2/operators/numpy_tile_op.cc,caffe2/operators/numpy_tile_op.h,caffe2/operators/one_hot_ops.cc,caffe2/operators/onnx_while_op.cc,caffe2/operators/onnx_while_op.h,caffe2/operators/order_switch_ops.cc,caffe2/operators/pack_rnn_sequence_op.cc,caffe2/operators/pack_segments.cc,caffe2/operators/pad_op.cc,caffe2/operators/partition_ops.cc,caffe2/operators/percentile_op.cc,caffe2/operators/perplexity_op.cc,caffe2/operators/piecewise_linear_transform_op.cc,caffe2/operators/pool_gradient_op.cc,caffe2/operators/pool_op.cc,caffe2/operators/pow_op.cc,caffe2/operators/prelu_op.cc,caffe2/operators/prepend_dim_op.cc,caffe2/operators/prepend_dim_op.h,caffe2/operators/quant_decode_op.cc,caffe2/operators/quantile_op.cc,caffe2/operators/quantized/int8_add_op.cc,caffe2/operators/quantized/int8_average_pool_op.cc,caffe2/operators/quantized/int8_channel_shuffle_op.cc,caffe2/operators/quantized/int8_concat_op.cc,caffe2/operators/quantized/int8_conv_op.cc,caffe2/operators/quantized/int8_conv_op_relu.cc,caffe2/operators/quantized/int8_conv_transpose_op.cc,caffe2/operators/quantized/int8_dequantize_op.cc,caffe2/operators/quantized/int8_fc_op.cc,caffe2/operators/quantized/int8_flatten_op.cc,caffe2/operators/quantized/int8_given_tensor_fill_op.cc,caffe2/operators/quantized/int8_leaky_relu_op.cc,caffe2/operators/quantized/int8_max_pool_op.cc,caffe2/operators/quantized/int8_quantize_op.cc,caffe2/operators/quantized/int8_relu_op.cc,caffe2/operators/quantized/int8_reshape_op.cc,caffe2/operators/quantized/int8_resize_nearest_op.cc,caffe2/operators/quantized/int8_roi_align_op.cc,caffe2/operators/quantized/int8_roi_align_op_test.cc,caffe2/operators/quantized/int8_sigmoid_op.cc,caffe2/operators/quantized/int8_slice_op.cc,caffe2/operators/quantized/int8_softmax_op.cc,caffe2/operators/quantized/int8_test.cc,caffe2/operators/quantized/int8_transpose_op.cc,caffe2/operators/rank_loss_op.cc,caffe2/operators/reciprocal_gradient_op.cc,caffe2/operators/reciprocal_op.cc,caffe2/operators/reduce_front_back_max_ops.cc,caffe2/operators/reduce_front_back_mean_ops.cc,caffe2/operators/reduce_front_back_sum_ops.cc,caffe2/operators/reduce_ops.cc,caffe2/operators/reduction_ops.cc,caffe2/operators/relu_n_op.cc,caffe2/operators/relu_op.cc,caffe2/operators/remove_data_blocks_op.cc,caffe2/operators/replace_nan_op.cc,caffe2/operators/reservoir_sampling.cc,caffe2/operators/reshape_op.cc,caffe2/operators/reshape_op.h,caffe2/operators/resize_3d_op.cc,caffe2/operators/resize_op.cc,caffe2/operators/reverse_packed_segs_op.cc,caffe2/operators/rmac_regions_op.cc,caffe2/operators/rms_norm_op.cc,caffe2/operators/rnn/recurrent_network_blob_fetcher_op.cc,caffe2/operators/rnn/recurrent_network_blob_fetcher_op.h,caffe2/operators/rnn/recurrent_network_executor.cc,caffe2/operators/rnn/recurrent_network_executor_incl.h,caffe2/operators/rnn/recurrent_network_op.cc,caffe2/operators/rnn/recurrent_network_op.h,caffe2/operators/roi_align_gradient_op.cc,caffe2/operators/roi_align_op.cc,caffe2/operators/roi_align_rotated_gradient_op.cc,caffe2/operators/roi_align_rotated_op.cc,caffe2/operators/roi_pool_op.cc,caffe2/operators/rowmul_op.cc,caffe2/operators/rsqrt_op.cc,caffe2/operators/scale_blobs_op.cc,caffe2/operators/scale_op.cc,caffe2/operators/segment_reduction_op.cc,caffe2/operators/segment_reduction_op.h,caffe2/operators/self_binning_histogram_op.cc,caffe2/operators/selu_op.cc,caffe2/operators/sequence_ops.cc,caffe2/operators/shape_op.cc,caffe2/operators/sigmoid_gradient_op.cc,caffe2/operators/sigmoid_op.cc,caffe2/operators/sin_op.cc,caffe2/operators/sinh_op.cc,caffe2/operators/sinusoid_position_encoding_op.cc,caffe2/operators/slice_op.cc,caffe2/operators/softmax_op.cc,caffe2/operators/softmax_with_loss_op.cc,caffe2/operators/softplus_op.cc,caffe2/operators/softsign_op.cc,caffe2/operators/space_batch_op.cc,caffe2/operators/sparse_dropout_with_replacement_op.cc,caffe2/operators/sparse_lp_regularizer_op.cc,caffe2/operators/sparse_normalize_op.cc,caffe2/operators/sparse_to_dense_mask_op.cc,caffe2/operators/sparse_to_dense_mask_op.h,caffe2/operators/sparse_to_dense_op.cc,caffe2/operators/spatial_batch_norm_gradient_op.cc,caffe2/operators/spatial_batch_norm_op.cc,caffe2/operators/spatial_softmax_with_loss_op.cc,caffe2/operators/sqr_op.cc,caffe2/operators/sqrt_op.cc,caffe2/operators/square_root_divide_op.cc,caffe2/operators/square_root_divide_op.h,caffe2/operators/stats_ops.cc,caffe2/operators/stats_put_ops.cc,caffe2/operators/stats_put_ops.h,caffe2/operators/stop_gradient.cc,caffe2/operators/string_ops.cc,caffe2/operators/string_ops_test.cc,caffe2/operators/stump_func_op.cc,caffe2/operators/stylizer_ops.cc,caffe2/operators/summarize_op.cc,caffe2/operators/swish_op.cc,caffe2/operators/tan_op.cc,caffe2/operators/tanh_gradient_op.cc,caffe2/operators/tanh_op.cc,caffe2/operators/tensor_protos_db_input.cc,caffe2/operators/text_file_reader.cc,caffe2/operators/text_file_reader_utils.cc,caffe2/operators/text_file_reader_utils_test.cc,caffe2/operators/thresholded_relu_op.cc,caffe2/operators/tile_op.cc,caffe2/operators/top_k.cc,caffe2/operators/transpose_op.cc,caffe2/operators/transpose_op.h,caffe2/operators/tt_linear_op.cc,caffe2/operators/tt_linear_op.h,caffe2/operators/unique_ops.cc,caffe2/operators/unsafe_coalesce.cc,caffe2/operators/upsample_op.cc,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_test.cc,caffe2/operators/variable_length_sequence_padding.cc,caffe2/operators/weighted_multi_sampling_op.cc,caffe2/operators/weighted_sample_op.cc,caffe2/operators/while_op.cc,caffe2/operators/workspace_ops.cc,caffe2/operators/zero_gradient_op.cc,caffe2/opt/backend_cutting.cc,caffe2/opt/backend_cutting_test.cc,caffe2/opt/backend_transformer_base.cc,caffe2/opt/bound_shape_inference_test.cc,caffe2/opt/bound_shape_inferencer.cc,caffe2/opt/converter.cc,caffe2/opt/converter_nomigraph_test.cc,caffe2/opt/dead_code_elim.cc,caffe2/opt/dead_code_elim_test.cc,caffe2/opt/device.cc,caffe2/opt/device_test.cc,caffe2/opt/distributed_converter.cc,caffe2/opt/distributed_test.cc,caffe2/opt/fakefp16_transform.cc,caffe2/opt/fusion.cc,caffe2/opt/glow_net_transform.cc,caffe2/opt/mobile.cc,caffe2/opt/mobile_test.cc,caffe2/opt/onnxifi_op.cc,caffe2/opt/onnxifi_op.h,caffe2/opt/onnxifi_transformer.cc,caffe2/opt/optimize_ideep.cc,caffe2/opt/passes.cc,caffe2/opt/shape_info.cc,caffe2/opt/split_slss_test.cc,caffe2/opt/tvm_transformer.cc,caffe2/perfkernels/embedding_lookup.cc,caffe2/perfkernels/embedding_lookup_avx2.cc,caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc,caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc,caffe2/perfkernels/embedding_lookup_idx_avx2.cc,caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup.cc,caffe2/perfkernels/fused_nbit_rowwise_conversion.cc,caffe2/perfkernels/math_cpu_avx2.cc,caffe2/perfkernels/math_cpu_base.cc,caffe2/perfkernels/typed_axpy.cc,caffe2/perfkernels/typed_axpy_avx.cc,caffe2/perfkernels/typed_axpy_avx2.cc,caffe2/predictor/emulator/data_filler.cc,caffe2/predictor/emulator/data_filler_test.cc,caffe2/predictor/predictor.cc,caffe2/predictor/predictor_test.cc,caffe2/predictor/predictor_utils.cc,caffe2/python/pybind_state.cc,caffe2/python/pybind_state_dlpack.cc,caffe2/python/pybind_state_ideep.cc,caffe2/python/pybind_state_int8.cc,caffe2/python/pybind_state_nomni.cc,caffe2/python/pybind_state_registry.cc,caffe2/quantization/server/activation_distribution_observer.cc,caffe2/quantization/server/batch_matmul_dnnlowp_op.cc,caffe2/quantization/server/caffe2_dnnlowp_utils.cc,caffe2/quantization/server/channel_shuffle_dnnlowp_op.cc,caffe2/quantization/server/concat_dnnlowp_op.cc,caffe2/quantization/server/conv_dnnlowp_acc16_op.cc,caffe2/quantization/server/conv_dnnlowp_op.cc,caffe2/quantization/server/conv_relu_op.cc,caffe2/quantization/server/dequantize_dnnlowp_op.cc,caffe2/quantization/server/dnnlowp.cc,caffe2/quantization/server/dynamic_histogram.cc,caffe2/quantization/server/elementwise_add_dnnlowp_op.cc,caffe2/quantization/server/elementwise_linear_dnnlowp_op.cc,caffe2/quantization/server/elementwise_mul_dnnlowp_op.cc,caffe2/quantization/server/elementwise_sum_dnnlowp_op.cc,caffe2/quantization/server/elementwise_sum_dnnlowp_op_avx2.cc,caffe2/quantization/server/elementwise_sum_relu_op.cc,caffe2/quantization/server/fb_fc_packed_op.cc,caffe2/quantization/server/fbgemm_fp16_pack_op.cc,caffe2/quantization/server/fbgemm_pack_op.cc,caffe2/quantization/server/fully_connected_dnnlowp_acc16_op.cc,caffe2/quantization/server/fully_connected_dnnlowp_op.cc,caffe2/quantization/server/fully_connected_fake_lowp_op.cc,caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc,caffe2/quantization/server/group_norm_dnnlowp_op.cc,caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc,caffe2/quantization/server/int8_gen_quant_params.cc,caffe2/quantization/server/kl_minimiz",1327.0,145,8,8.160157196,66.0,490833.0,719.0,31441717.92313489,11406.0,25782.5,0.0,,0.0,1
pytorch,9ad19af9357f44122786b5d29785a8578f4a473d,4cf2c646c2d00a6f1ec6859ce28ce972366d343e,Heitor Schueroff,heitorschueroff@fb.com,Sun May 09 11:49:35 2021 -0700,1620560975.0,"Added torch.linalg.matrix_norm (#57127)

Summary:
This PR is focused on  the API for `linalg.matrix_norm` and delegates computations to `linalg.norm` for the moment.

The main difference between the norms is when `dim=None`. In this case
- `linalg.norm` will compute a vector norm on the flattened input if `ord=None`, otherwise it requires the input to be either 1D or 2D in order to disambiguate between vector and matrix norm
- `linalg.vector_norm` will flatten the input
- `linalg.matrix_norm` will compute the norm over the last two dimensions, treating the input as batch of matrices

In future PRs, the computations will be moved to `torch.linalg.matrix_norm` and `torch.norm` and `torch.linalg.norm` will delegate computations to either `linalg.vector_norm` or `linalg.matrix_norm` based on the arguments provided.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57127

Reviewed By: mrshenli

Differential Revision: D28186736

Pulled By: mruberry

fbshipit-source-id: 99ce2da9d1c4df3d9dd82c0a312c9570da5caf25",379.0,137.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,torch/csrc/api/include/torch/linalg.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",9.0,16,4,1.792770313,12.0,31195.0,6.0,212321.5555555556,11824.0,26922.5,0.0,Feature Addition,0.0,1
pytorch,89be1a22d4de608a58c005dee34fb72ca650347b,4d30415f12746fb1d03b033e09076e636af555d4,Lara,lahaidar@microsoft.com,Tue Dec 03 18:22:09 2019 -0800,1575397329.0,"Add ONNX Scripting Conv Support (#30618)

Summary:
Convolution nodes are traced as aten:_convolution and are currently supported in ONNX.
Scripting convolution uses aten:conv<1,2,3>d which are currently not supported in ONNX.
This PR adds the symbolics for aten:conv<1,2,3>d and aten:conv_transpose<1,2,3>d
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30618

Reviewed By: hl475

Differential Revision: D18778145

Pulled By: houseroad

fbshipit-source-id: 4af0379f29974a1ce8443024d1d87b3eb8d2dd36",83.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.943876757,2.0,4344.0,1.0,525240.0,13573.0,37165.33333,0.0,Feature Addition,0.0,1
pytorch,0a95613cef99bf7f1b51b8391a6e4d3fc9f2d2f6,4d5075add2f6051c121e7639978200cc2949c30a,Sam Gross,colesbury@gmail.com,Thu Jun 29 04:10:13 2017 -0400,1498709413.0,Add ignore_index to nnl_loss and cross_entropy (#1937),35.0,20.0,"torch/nn/functional.py,torch/nn/modules/loss.py",2.0,3,1,0.994030211,29.0,1537.0,1.0,137119.0,1057.0,12580.04911,0.0,Feature Addition,0.0,1
pytorch,5d7e48c9fc01c0d4fe56dbbaf8fb5310ce0caf87,4d72538f8045a2e330e3d0f34f2336a22534fb83,Edward Yang,ezyang@fb.com,Wed Apr 28 16:23:07 2021 -0700,1619626987.0,"Give Tensor a trivial (for now) metaclass _TensorMeta (#56147)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56147

This is support of #55686, you can see the broader context of the metaclass in
a more complete PR #56017.  The short story is that in the future I want to
give Tensor a non-trivial metaclass, so to derisk the change first I give it a
trivial metaclass to shake out any bugs that might be caused by it.  The
metaclass shouldn't have any performance impact on Tensor as it only gets
invoked upon subclass creation.

By the way, it was totally not documented how to create metaclasses in the Python
C API, and it took a good bit of trial error to figure it out (and the answer is
now immortalized in https://stackoverflow.com/q/67077317/23845 -- the things
that I got wrong in earlier versions of the PR included setting tp_basicsize
incorrectly, incorrectly setting Py_TPFLAGS_HAVE_GC on the metaclass--you want
to leave it unset so that it inherits, and determining that tp_init is what
actually gets called when you construct a class, not tp_call as another
not-to-be-named StackOverflow question suggests).

Aside: Ordinarily, adding a metaclass to a class is a user visible change, as
it means that it is no longer valid to mixin another class with a different
metaclass. However, because _C._TensorBase is a C extension object, it will
typically conflict with most other metaclasses, so this is not BC breaking.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D28028747

Pulled By: ezyang

fbshipit-source-id: c1e35a986aeb3db540c73d188f53dce951eeed33",75.0,3.0,"test/test_jit.py,torch/_C/__init__.pyi.in,torch/csrc/autograd/python_variable.cpp",3.0,5,2,0.560279319,30.0,17735.0,3.0,428042.6666666667,11392.0,25100.0,0.0,Corrective,1.0,1
pytorch,82a7a6a8cdee0b2974a02743ccbde8874e5cbb37,4d91cead10bbe065e4c5178faad9965acdc582ae,Richard Zou,zou3519@gmail.com,Wed Aug 18 21:27:18 2021 -0700,1629322038.0,"[functorch] New expected failure mechanism for test_ops.py

I'm a little worried that this might get too noisy (e.g. it depends on
what pytorch binary we're using; the CI is currently always on the
latest nightly while we develop against pytorch master). But we can
always flip a switch to make the xfails just skips.",97.0,125.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,475.0,1.0,1.0,279.0,431.5,0.0,,0.0,1
pytorch,4bc0491752b6d891c5bb33a3d18c5da7334fc6df,4d9920fa9c89583fa7f9b70d6753fddbc07a872a,Kurt Mohler,kmohler@quansight.com,Tue Jan 24 19:20:28 2023 -0600,1674588028.0,"Move PyInterpreter code in `python_variable.cpp` to its own files (#92647)

Part of #91395

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92647
Approved by: https://github.com/ezyang, https://github.com/albanD",828.0,802.0,"aten/src/ATen/core/PythonOpRegistrationTrampoline.h,build_variables.bzl,torch/csrc/PyInterpreter.cpp,torch/csrc/PyInterpreter.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/utils/python_dispatch.cpp,torch/csrc/utils/python_symnode.h",8.0,8,2,1.086008326,30.0,5414.0,6.0,1476986.1666666667,11644.0,26762.5,0.0,,0.0,1
pytorch,d9b4788e5de915399c082bb77b9619fce1458a1d,4d9c017dee8386a734ba058a96bf1258cebba89d,Lingyi Liu,lingyiliu@fb.com,Mon Oct 21 18:01:24 2019 -0700,1571680884.0,"Fix the padding issue of quantized average pool operator (#28260)

Summary:
This is actually a bug in both testing and the average pool implementation.
In testing, we used the quantized value as float input and failed to padding the value with zero_point.
In op implementation, the size for averaging is not correct for padding case when count_include_pad is true.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28260

Differential Revision: D18039960

Pulled By: lly-zero-one

fbshipit-source-id: 7b5d34498b60f5d574a276a22798c9f576944734",22.0,18.0,"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool.cpp,test/test_quantized.py",3.0,8,2,0.992184942,1.0,3053.0,3.0,828464.6666666666,12420.0,34658.83333,0.0,Corrective,1.0,1
pytorch,2df249f0ab9c3f79d084980f8e8df2f258e1fde3,4d9d03fe47faf2edd1c6cd7e0f3e1c2b4078ec60,anjali411,chourdiaanjali123@gmail.com,Tue Dec 22 15:56:55 2020 -0800,1608652615.0,"Complex backward for torch.sqrt (#49461)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/49461

resolves https://github.com/pytorch/pytorch/issues/48398

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D25589454

Pulled By: anjali411

fbshipit-source-id: 46e9f913c8ab3e18c98d6f623b2394044b6fe079",4.0,9.0,"tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.198183948,14.0,5323.0,3.0,63217.66666666666,7713.0,17372.5,0.0,,0.0,1
pytorch,9547e5764385ee3f305c2b20ef2bece95eaa4b9a,4d9fd8958bd1f6dbbf1df7bd17b6683af282f73b,Akifumi Imanishi,imanishi@preferred.jp,Wed Jul 07 20:08:39 2021 -0700,1625688519.0,"Support `__rand__`, `__ror__` and `__rxor__` (#59240)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/58120.

This PR implements `torch.Tensor.{__rand__/__ror__/__rxor__}` for the compatibility with NumPyâs interface.
(cc: mruberry, rgommers, emcastillo, kmaehashi)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59240

Reviewed By: ngimel

Differential Revision: D29482304

Pulled By: mruberry

fbshipit-source-id: 13789202c1d8dddf8658a45381aeedcc31e2f603",87.0,43.0,"test/test_binary_ufuncs.py,test/test_fx.py,test/test_fx_experimental.py,tools/autograd/templates/python_variable_methods.cpp,tools/pyi/gen_pyi.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",7.0,8,3,1.397549736,13.0,18965.0,6.0,1122382.857142857,13631.0,30850.5,0.0,Corrective,1.0,1
pytorch,a076731066563581c6444dad50aa64d45668e94b,4dba6743246eebded79aa1398f898b99370d6905,Sam Gross,colesbury@gmail.com,Thu Dec 21 22:07:46 2017 -0500,1513894066.0,Move factional max pooling to ATen (#4290),91.0,136.0,"aten/src/ATen/nn.yaml,aten/src/ATen/nn_parse.py,aten/src/THNN/generic/THNN.h,test/test_nn.py,tools/autograd/derivatives.yaml,torch/nn/_functions/thnn/__init__.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py,torch/nn/modules/pooling.py",9.0,13,4,2.095685441,37.0,11501.0,6.0,309447.7777777777,393.0,1248.905869,0.0,,0.0,1
pytorch,e5143863154a38f873c0cb89f1b60c4f4323a696,4dc063821dbd9374c7a4dffdfb1508add1d2c367,Peter Bell,peterbell10@live.co.uk,Mon Jul 31 20:43:36 2023 +0100,1690836216.0,"[inductor] Fix lowerings that create unexpected aliases (#105173)

This may give the wrong result in some cases, e.g.
```python
@torch.compile()
def fn(x):
    tmp = x.ceil()
    x.add_(10)
    return tmp

a = torch.zeros((), dtype=torch.int64)
fn(a)  # tensor(10)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105173
Approved by: https://github.com/lezcano",19.0,18.0,"test/inductor/test_torchinductor_codegen_dynamic_shapes.py,torch/_inductor/lowering.py",2.0,4,2,0.303374836,2.0,4959.0,2.0,735567.0,18226.0,41396.5,0.0,Corrective,1.0,1
pytorch,b4b6e356ef57060ca8f618faedd0bd7b63984b96,4dc13ecdd8fd61f6c053ac3c6236c3346a51aaf2,Adam Paszke,adam.paszke@gmail.com,Mon Oct 24 08:34:50 2016 +0200,1477298090.0,Make tests deterministic,5.0,0.0,test/common.py,1.0,1,1,0,10.0,166.0,1.0,189233.0,260.0,2324.14599,0.0,,0.0,1
pytorch,69532c42272160c80da57c6abc7ac7789b9670d2,4de40dad5d9f5f6166da54c444ca2cfca434cf62,BowenBao,bowbao@microsoft.com,Wed Nov 11 02:28:42 2020 -0800,1605061722.0,"[ONNX] Improve stability of gemm export (#46570)

Summary:
Export as `onnx::MatMul` if possible since it has less constraint. Resolves issue with exporting `weight_norm` in scripting that fails onnx shape inference with `onnx::Gemm` in unreachable `if` subgraph.

Updates skipped tests list.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46570

Reviewed By: ngimel

Differential Revision: D24657480

Pulled By: bzinodev

fbshipit-source-id: 08d47cc9fc01c4a73a9d78c964fef102d12cc21c",52.0,5.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.918295834,3.0,7895.0,1.0,14267.0,6637.0,15070.5,0.0,Perfective,0.0,1
pytorch,c9d0c855f76620b91654c3bc77ea52c11273c7cd,4e110528bdaf102acf33df5da2e4caaf95bb1cf9,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Tue Mar 30 17:12:34 2021 -0700,1617124354.0,"Added cuSOLVER path for torch.linalg.eigh/eigvalsh (#53040)

Summary:
This PR adds the cuSOLVER based path for `torch.linalg.eigh/eigvalsh`.
The device dispatching helper function was removed from native_functions.yml, it is replaced with `DECLARE/DEFINE_DISPATCH`.

cuSOLVER is used if CUDA version >= 10.1.243. In addition if CUDA version >= 11.1 (cuSOLVER version >= 11.0) then the new 64-bit API is used.

I compared cuSOLVER's `syevd` vs MAGMA's `syevd`. cuSOLVER is faster than MAGMA for all matrix sizes.
I also compared cuSOLVER's `syevj` (Jacobi algorithm) vs `syevd` (QR based divide-and-conquer algorithm). Despite it is said that `syevj` is better than `syevd` for smaller matrices, in my tests it is the case only for float32 dtype and matrix sizes 32x32 - 512x512.

For batched inputs comparing a for loop of `syevd/syevj` calls to `syevjBatched` shows that for batches of matrices up to 32x32 the batched routine is much better. However, there are bugs in `syevjBatched`, sometimes it doesn't compute the result leaving eigenvectors as a unit diagonal matrix and eigenvalues as the real diagonal of the input matrix.  The output is the same with `cupy.cusolver.syevj` so the problem is definitely on the cuSOLVER side. This bug is not present in the non-batched `syevj`.

The performance of 64-bit `syevd` is the same as 32-bit version.

Ref. https://github.com/pytorch/pytorch/issues/47953

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53040

Reviewed By: H-Huang

Differential Revision: D27401218

Pulled By: mruberry

fbshipit-source-id: aef91eefb57ed73fef87774ff9a36d50779903f7",1493.0,169.0,"aten/src/ATen/cuda/CUDASolver.cpp,aten/src/ATen/cuda/CUDASolver.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py",10.0,8,2,2.322475966,12.0,16345.0,7.0,859841.8,10209.0,22553.5,0.0,Corrective,1.0,1
pytorch,dd96c26066fd8e31dc768002e207477c38f86b7a,4e15a6f495ac7f42927a175261238b91632e8494,Mikhail Zolotukhin,mvz@fb.com,Tue Aug 24 07:29:22 2021 -0700,1629790162.0,"[TensorExpr] Switch Exprs and Stmt from kernel-arena to shared_ptr. (#63216)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63216

Currently there are three classes managed by KernelArena: Expr, Stmt,
and Tensor (and derived classes). KernelArena has been a long standing
painpoint for NNC devs and we're moving away from that memory management
model to ref-count based memory model (using shared_ptr). This commit
switches Expr and Stmt to shared_ptr and is the biggest change in this
transition. Later commits will detach Tensor from KernelArena and kill
the arena + scope altogether.

Differential Revision:
D30353195
D30353195

Test Plan: Imported from OSS

Reviewed By: navahgar

Pulled By: ZolotukhinM

fbshipit-source-id: 9575225ada3d0fb65087ae40435f3dfea4792cae",213.0,147.0,"test/test_tensorexpr_pybind.py,torch/csrc/jit/tensorexpr/eval.cpp,torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/fwd_decls.h,torch/csrc/jit/tensorexpr/hash_provider.cpp,torch/csrc/jit/tensorexpr/hash_provider.h,torch/csrc/jit/tensorexpr/ir.h,torch/csrc/jit/tensorexpr/ir_cloner.cpp,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_printer.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/ir_verifier.cpp,torch/csrc/jit/tensorexpr/ir_visitor.cpp,torch/csrc/jit/tensorexpr/llvm_codegen.cpp,torch/csrc/jit/tensorexpr/loopnest.cpp,torch/csrc/jit/tensorexpr/stmt.h,torch/csrc/jit/tensorexpr/tensorexpr_init.cpp",18.0,5,2,3.361351965,2.0,15839.0,4.0,503381.77777777775,14865.0,34046.0,0.0,,0.0,1
pytorch,7368c09280b08a00f3c640dbbc2a48b913c55647,4e190c2fedd1cb78affbb37d004ed2bb44a29d87,Richard Zou,zou3519@users.noreply.github.com,Sat Mar 10 04:53:14 2018 -0500,1520657594.0,"Fix floor latex rendering (#5682)

* Make floors larger

* Improve Latex rendering of floor

* Improve latex rendering of ceil

* Fix flake8",67.0,59.0,"torch/_torch_docs.py,torch/nn/modules/conv.py,torch/nn/modules/pooling.py,torch/nn/modules/upsampling.py",4.0,3,1,1.718845412,37.0,7859.0,2.0,93339.5,2444.0,24768.85823,0.0,Corrective,1.0,1
pytorch,81a8fdc40d7c504f99d5796a5b187551493685e4,4e1d19c5a577b947a3dc84d9eec4a186ad3cd52f,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Thu Nov 17 04:58:53 2022 +0000,1668661133.0,"Revert ""Redefine the simdlen semantic: (#88482)""

This reverts commit fce6d6b3dcc879720bc45143426b86232106818a.

Reverted https://github.com/pytorch/pytorch/pull/88482 on behalf of https://github.com/kit1980 due to Broke multiple tests in several trunk workflows, for example https://github.com/pytorch/pytorch/actions/runs/3485086792/jobs/5830429554",80.0,327.0,"test/inductor/test_torchinductor.py,torch/_inductor/codecache.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/cpp.py",4.0,5,2,1.549301963,1.0,7935.0,1.0,5459.0,9617.0,22378.5,0.0,,0.0,1
pytorch,2f52748515c91f6157361b5365fce17f1b8ee9f6,4e4626a23d1cf8b2a3c87d2b5f2f63ccd14e44c3,Rohan Varma,rvarm1@fb.com,Mon Aug 31 20:26:10 2020 -0700,1598905570.0,"Join-based API to support DDP uneven inputs (#42577)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42577

Closes https://github.com/pytorch/pytorch/issues/38174. Implements a join-based API to support training with the DDP module in the scenario where different processes have different no. of inputs. The implementation follows the description in https://github.com/pytorch/pytorch/issues/38174. Details are available in the RFC, but as a summary, we make the following changes:

#### Approach
1) Add a context manager `torch.nn.parallel.distributed.join`
2) In the forward pass, we schedule a ""present"" allreduce where non-joined process contribute 1 and joined processes contribute 0. This lets us keep track of joined processes and know when all procs are joined.
3) When a process depletes its input and exits the context manager, it enters ""joining"" mode and attempts to ""shadow"" the collective comm. calls made in the model's forward and backward pass. For example we schedule the same allreduces in the same order as the backward pass, but with zeros
4) We adjust the allreduce division logic to divide by the effective world size (no. of non-joined procs) rather than the absolute world size to maintain correctness.
5) At the end of training, the last joined process is selected to be the ""authoritative"" model copy

We also make some misc. changes such as adding a `rank` argument to `_distributed_broadcast_coalesced` and exposing some getters/setters on `Reducer` to support the above changes.

#### How is it tested?
We have tests covering the following models/scenarios:
- [x] Simple linear model
- [x] Large convolutional model
- [x] Large model with module buffers that are broadcast in the forward pass (resnet). We verify this with a helper function `will_sync_module_buffers` and ensure this is true for ResNet (due to batchnorm)
- [x] Scenario where a rank calls join() without iterating at all, so without rebuilding buckets (which requires collective comm)
- [x] Model with unused params (with find unused parameters=True)
- [x] Scenarios where different processes iterate for a varying number of different iterations.
- [x] Test consistency in tie-breaking when multiple ranks are the last ones to join
- [x] Test that we divide by the effective world_size (no. of unjoined processes)

#### Performance implications

###### Trunk vs PR patched, 32 GPUs, batch size = 32
P50, forward + backward + optimizer batch latency & total QPS: 0.121 264/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.087 369/s vs 0.087 368/s

###### join(enable=True) vs without join, 32 GPUs, batch size = 32, even inputs
P50, forward + backward + optimizer batch latency & total QPS: 0.120 265/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.088 364/s vs 0.087 368/s

###### join(enable=False) vs without join, 32 GPUs, batch size = 32, even inputs
P50 forward + backward + optimizer batch latency & total QPS: 0.121 264/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.087 368/s vs 0.087 368/s

###### join(enable=True) with uneven inputs (offset = 2000), 32 GPUs, batch size = 32
P50 forward + backward + optimizer batch latency & total QPS: 0.183 174/s vs 0.121 264/s
P50 backwards only batch latency & total QPS: 0.150 213/s vs 0.087 368/s

###### join(enable=True) with uneven inputs ((offset = 2000)), 8 GPUs, batch size = 32
P50 forward + backward + optimizer batch latency & total QPS: 0.104 308/s vs 0.104 308/s
P50 backwards only batch latency & total QPS: 0.070 454/s vs 0.070 459/s

The 2 above uneven inputs benchmark was conducted 32 GPUs and 4 GPUs immediately depleting their inputs and entering ""join"" mode (i.e. not iterating at all), while the other 28 iterating as normal. It looks like there is a pretty significant perf hit for this case when there are uneven inputs and multi-node training. Strangely, when there is a single node (8 GPUs), this does not reproduce.

#### Limitations
1) This is only implemented for MPSD, not SPMD. Per a discussion with mrshenli we want to encourage the use of MPSD over SPMD for DDP.
2) This does not currently work with SyncBN or custom collective calls made in the model's forward pass. This is because the `join` class only shadows the `broadcast` for buffers in the forward pass, the gradient allreduces in the bwd pass, unused parameters reduction, and (optionally) the rebuild buckets broadcasting in the backwards pass. Supporting this will require additional design thought.
3) Has not been tested with the [DDP comm. hook](https://github.com/pytorch/pytorch/issues/39272) as this feature is still being finalized/in progress. We will add support for this in follow up PRs.
ghstack-source-id: 111033819

Reviewed By: mrshenli

Differential Revision: D22893859

fbshipit-source-id: dd02a7aac6c6cd968db882c62892ee1c48817fbe",894.0,59.0,"test/distributed/test_c10d.py,test/distributed/test_distributed.py,torch/csrc/distributed/c10d/comm.cpp,torch/csrc/distributed/c10d/comm.h,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h,torch/nn/parallel/distributed.py,torch/testing/_internal/common_distributed.py",9.0,10,2,2.104013931,19.0,10494.0,6.0,576130.1111111111,4693.0,10945.5,0.0,Corrective,1.0,1
pytorch,97917fd26d07c3b4db4dfe41065a887b08f86887,4e6e11c139e4d4bcf948853dbdbf165284a0b3c8,neginraoof,neginmr@utexas.edu,Sat Aug 03 00:28:22 2019 -0700,1564792102.0,"added opset10 ORT tests (#22993)

Summary:
Added a number of opset10 tests from Caffe2 to ORT
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22993

Differential Revision: D16467954

Pulled By: bddppq

fbshipit-source-id: 0b92694c7c0213bdf8e77e6f8e07e6bc8a85170a",421.0,29.0,"test/onnx/test_pytorch_common.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",4.0,4,2,0.359001549,2.0,4646.0,2.0,694245.5,10379.0,29655.33333,0.0,Feature Addition,0.0,1
pytorch,4ceac494259e6b7d57e721793fb0a9ad598a0e12,4ebc4890ddfd432677d46a3096c09a6afe3532f7,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Mon May 09 19:12:29 2022 +0000,1652123549.0,"Revert ""Add linalg.lu_solve""

This reverts commit fc5b4a5a33f1906ca335c26ec4da9357ed196419.

Reverted https://github.com/pytorch/pytorch/pull/72935 on behalf of https://github.com/malfet",443.0,721.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/allowlist_for_publicAPI.json,test/test_linalg.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",19.0,17,5,3.041553969,20.0,69327.0,7.0,224273.63157894736,2988.0,7193.0,0.0,Feature Addition,0.0,1
pytorch,e2bc95e1bd39f4cd1813a21290f2f8f6fb51c0eb,4ee0a78ee6452b7c1584b6020aef791b9d3e8ab6,Amitesh Arora,amitesharora1@gmail.com,Tue Sep 18 14:36:15 2018 -0700,1537281375.0,"varargs for meshgrid (#11600)

Summary:
Adds vararg support for meshgrid and adds checks for all the tensor arguments to have the same dtype and device.

Fixes: [#10823](https://github.com/pytorch/pytorch/issues/10823), #11446

The earlier pull request closed without any changes because I had some rebasing issues, so I made another pull request to close out #10823. Sorry for the inconvenience.

Differential Revision: D9892876

Pulled By: ezyang

fbshipit-source-id: 93d96cafc876102ccbad3ca2cc3d81cb4c9bf556",49.0,33.0,"aten/src/ATen/native/TensorShape.cpp,test/test_torch.py,torch/_torch_docs.py,torch/functional.py",4.0,6,3,1.558311323,41.0,16304.0,3.0,311344.0,4186.0,11667.33333,0.0,Corrective,1.0,1
pytorch,7abdc303c69de74b5362f69e30037b186e3c3db0,4f20a0e43969833c8dd54ad8886f8a62ef63360d,gchanan,gregchanan@gmail.com,Fri May 18 17:51:41 2018 +0200,1526665901.0,"Fix various sparse transpose issues; remove dead code from Declaratioâ¦ (#7200)

* Fix various sparse transpose issues; remove dead code from Declarations.yaml.

1) Fixes some checks in t_, transpose_ that don't allow transposing empty sparse tensors.
2) Remove out= variants from docs since they don't exist (and haven't since at least v0.3.1).
3) Unify implementations of t_, transpose_, t, transpose.
4) Move dead checking code from Declarations.cwrap to actual implementations.
5) Fix test which never tested transpose_.

* Add test for error with t, t_.

* Address review comments.

* Fix jit tests.

* Fix test_jit.",90.0,64.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/test_jit.py,test/test_sparse.py,test/test_torch.py,torch/_torch_docs.py,torch/csrc/jit/test_jit.cpp",8.0,8,3,2.034342909,39.0,23090.0,6.0,202201.625,1169.0,3139.805292,0.0,Corrective,1.0,1
pytorch,c38dcd45d70b2850047d9956e45ff3312966a078,4f3946a89b639f3b87c37b4190e2bc3dc22ee608,anjali411,chourdiaanjali123@gmail.com,Fri Apr 24 22:03:38 2020 -0700,1587765818.0,"Added complex dtypes to get_all_math_dtypes, complex acc type for cpu, fixed rdiv and pow for complex (#37193)

Summary:
Resolves https://github.com/pytorch/pytorch/issues/36730 https://github.com/pytorch/pytorch/issues/36057
Partially resolves: https://github.com/pytorch/pytorch/issues/36671
```
>>> 2j / torch.tensor([4], dtype = torch.complex64)
tensor([(0.0000+0.5000j)], dtype=torch.complex64)
>>> 1 / torch.tensor(3+4j)
tensor((0.1200-0.1600j), dtype=torch.complex64)
```
rdiv is more generally broken for all dtypes because it doesn't promote the types properly
eg.
```
>>> 1 / torch.tensor(2)
tensor(0)
>>> 2j / torch.tensor(4)
tensor(0)
```
so that issue should be fixed in a separate PR

Adding CPU acc types for complex
Added cumsum, cumprod for complex dtypes

Added complex dtypes to get_all_math_dtypes to expand testing for complex dtypes

Old PR - https://github.com/pytorch/pytorch/pull/36747
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37193

Differential Revision: D21229373

Pulled By: anjali411

fbshipit-source-id: 8a086136d8c10dabe62358d276331e3f22bb2342",195.0,98.0,"aten/src/ATen/AccumulateType.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,c10/core/ScalarType.h,test/test_nn.py,test/test_torch.py,test/test_type_promotion.py,torch/tensor.py,torch/testing/__init__.py",9.0,12,4,1.302371268,44.0,31910.0,6.0,112092.0,1378.0,3700.0,0.0,Corrective,1.0,1
pytorch,35ba948dde18ec51759aded423386ca1dab8dafe,4f479a98d45074386291e93fdea9d973640e6a50,Soumith Chintala,soumith@fb.com,Mon Jan 02 21:31:48 2017 -0500,1483392708.0,"fix indentation issue for all examples, add doc for add",168.0,4.0,"docs/source/tensors.rst,torch/docs.py",2.0,3,2,0.21832103,8.0,1655.0,2.0,53702.5,293.0,6377.224559,0.0,Corrective,1.0,1
pytorch,1eae0ac8b18dd2aa9a92280ac93f61eddb4328c5,4f4e0df68f92f5916ebf05a21cfd9df887a23d78,Neeraj Pradhan,prad.neeraj@gmail.com,Thu Dec 14 08:37:03 2017 -0800,1513240623.0,Allow for broadcasting of distribution parameters (#4140),120.0,20.0,"test/test_distributions.py,torch/distributions/bernoulli.py,torch/distributions/gamma.py,torch/distributions/normal.py,torch/distributions/utils.py",5.0,3,2,1.768240058,8.0,409.0,2.0,128746.6,840.0,6585.172317,0.0,,0.0,1
pytorch,a06f2edab63adc951afe1a8e3bf9ba606b729af1,4f6027b78a8f2e1fc07a50f9e0096de28ede429d,kshitij12345,kshitijkalambarkar@gmail.com,Mon Sep 12 16:59:05 2022 +0000,1663001945.0,"[opinfo] narrow: add new sample for Tensor overload (#84785)

`narrow` accepts `start` argument to be a Tensor. We add a sample to test this overload.

NOTE: This leads to a bunch of failed tests and hence the skips and xfails
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84785
Approved by: https://github.com/zou3519",24.0,2.0,"functorch/test/test_ops.py,functorch/test/test_vmap.py,test/test_meta.py,test/test_ops.py,test/test_proxy_tensor.py,torch/testing/_internal/common_methods_invocations.py",6.0,6,3,1.796217603,7.0,27552.0,4.0,232978.8333333333,7252.0,16949.5,0.0,Feature Addition,0.0,1
pytorch,64faa043f7dbf148a1e7935726c0fcc98735e017,4f8b986e28736b59bc46cd0873a0f36fdaa6f5b8,Ryan Spring,rdspring1@gmail.com,Mon Feb 14 03:32:11 2022 -0800,1644809531.0,"Implement Tanh Gelu Approximation (#61439)

Summary:
1. Implements https://github.com/pytorch/pytorch/issues/39853
2. Adds approximate boolean flag to Gelu
3. Enables Tanh Gelu approximation
4. Adds double backward support for Gelu
5. Enable Tanh Gelu in NvFuser

```
def gelu(x, approximate : str = 'none'):
    if approximate == 'tanh':
        # sqrt(2/pi) = 0.7978845608028654
        return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * (x + 0.044715 * torch.pow(x, 3.0))))
    else:
        return x * normcdf(x)
```

Linking XLA PR - https://github.com/pytorch/xla/pull/3039

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61439

Reviewed By: VitalyFedyunin

Differential Revision: D33894937

Pulled By: jbschlosser

fbshipit-source-id: b65e8fb6ea66168af8f34f45ed50e92737a33851
(cherry picked from commit 6e986f91a958dd73514b4e64984c0b149157dc6f)",828.0,273.0,"aten/src/ATen/autocast_mode.cpp,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/Activation.h,aten/src/ATen/native/cpu/Activation.cpp,aten/src/ATen/native/cuda/Activation.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/cuda/Activation.h,aten/src/ATen/native/mkldnn/Gelu.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qgelu.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,caffe2/serialize/versions.h,test/cpp/api/functional.cpp,test/cpp/api/modules.cpp,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/jit/test_autodiff_subgraph_slicing.py,test/onnx/test_custom_ops.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_utility_funs.py,test/quantization/core/test_quantized_op.py,test/test_jit_cuda_fuser.py,test/test_jit_fuser_te.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/nn/functional/activation.h,torch/csrc/api/include/torch/nn/modules/activation.h,torch/csrc/api/include/torch/nn/options/activation.h,torch/csrc/api/src/nn/modules/activation.cpp,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/csrc/jit/codegen/cuda/graph_fuser.cpp,torch/csrc/jit/codegen/cuda/parser.cpp,torch/csrc/jit/mobile/upgrader_mobile.cpp,torch/csrc/jit/operator_upgraders/upgraders_entry.cpp,torch/csrc/jit/operator_upgraders/version_map.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/runtime/symbolic_script.cpp,torch/csrc/jit/runtime/symbolic_shape_registry_util.cpp,torch/csrc/jit/tensorexpr/kernel.cpp,torch/csrc/jit/tensorexpr/lowerings.cpp,torch/csrc/jit/tensorexpr/lowerings.h,torch/nn/functional.py,torch/nn/functional.pyi.in,torch/nn/modules/activation.py,torch/onnx/symbolic_opset9.py,torch/overrides.py,torch/testing/_internal/autocast_test_lists.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_nn.py",51.0,48,5,4.662594839,48.0,133216.0,18.0,1131401.68627451,687.0,1610.5,0.0,Feature Addition,0.0,1
pytorch,a82dcc7bba5924ff41a555eebb779a56f22b38d9,4f985e9244d049671150096a7d96e655fa54769e,Kaiyu Yang,yangky11@2379590902.vpn.umich.net,Thu Apr 14 08:49:53 2016 +0800,1460623793.0,"BatchNormalization: add evaluation mode, add doc for nn.Jacobian",38.0,15.0,"generic/BatchNormalization.c,generic/THNN.h",2.0,1,1,0.509515718,12.0,1162.0,2.0,216043.5,21.0,164.3333333,0.0,Feature Addition,0.0,1
pytorch,26b0011fb86f1686b98d6fad41e7cbc020db7aa5,4fe6a5dc349f97a57708a666f4d8c7525f7471e0,Catherine Lee,csl@fb.com,Wed Aug 07 18:42:53 2024 +0000,1723056173.0,"Move slow tests to be in repo (#132379)

Move the slow test json to be in the pytorch/pytorch repo and make a job that will update it weekly.  The job uses the same environment as the commit hash.  It uses similar code to the hash updates, but the hash update contains a lot of code that is specific to the hash update, so I chose to pick out the parts that are relevant

Remove references to the old file and set up testing to read from the new file instead

The old update cadence was every day, the new one is every week

The auto slow test infra + the lack of pinning between pytorch and test-infra makes it really hard to tell if a test started failing because of a change or because of the slow test json changing.  While this can have benefits, like disable test issues being effective everywhere immediately, it can also be very confusing, especially since we don't have the same insight into slow tests like we do for disable issues.

Example PR made: https://github.com/pytorch/pytorch/pull/132383 (with all the changes from this PR because it was working on top of this)

We should just get rid of this at some point in favor of the slowTest decorator, but there are some tests that take 5+ minutes to run and I don't want to track them down right now
Pull Request resolved: https://github.com/pytorch/pytorch/pull/132379
Approved by: https://github.com/huydhn",540.0,18.0,".github/workflows/weekly.yml,.gitignore,scripts/release/apply-release-changes.sh,test/slow_tests.json,tools/stats/import_test_stats.py,tools/testing/test_selections.py,tools/testing/update_slow_tests.py,torch/testing/_internal/common_utils.py",8.0,11,5,1.460913305,46.0,6216.0,5.0,4142103.5,32438.0,82275.5,0.0,,0.0,1
pytorch,d920bf148833147769b8fc75262a92f6fe65b010,4fe7059a315d156ecd080ff7bd5b4fe3d3a9efad,Adam Paszke,adam.paszke@gmail.com,Thu Aug 04 15:43:13 2016 -0700,1470325393.0,Mark optional arguments in THNN.h,17.0,17.0,generic/THNN.h,1.0,1,1,0,16.0,1168.0,1.0,45972.0,73.0,297.25,0.0,,0.0,1
pytorch,e7f28d424125d426084e8a26654e5317c8209b3b,5003d417d41346aed628aa802eafa8e8bd9b79cc,Dhruv Matani,dhruvbird@fb.com,Wed Feb 17 21:38:05 2021 -0800,1613597885.0,"[PyTorch Mobile] Outline DispatchStub::get_call_ptr() (#51908)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51908

As suggested by swolchok. The idea is to outline `DispatchStub::get_call_ptr()` and also not have it specialized per instantiation of the `DispatchStub` class since it results in size bloat for mobile.

ghstack-source-id: 121873712

Test Plan:
Build + Circle CI.

### lightspeed

```
D26324255-V8 (https://www.internalfb.com/intern/diff/D26324255/?dest_number=121462800)

messenger-experimental-optimized-device: Succeeded
Change in Download Size for arm64 + 3x assets variation: -13.0 KiB
Change in Uncompressed Size for arm64 + 3x assets variation: -51.4 KiB

Mbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:325359338899206@base/bsb:325359338899206@diff/
```

### igios

```
D26324255-V8 (https://www.internalfb.com/intern/diff/D26324255/?dest_number=121462800)

igios: Succeeded
Change in Download Size for arm64 + 3x assets variation: -9.2 KiB
Change in Uncompressed Size for arm64 + 3x assets variation: -23.4 KiB

Mbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:823799391811488@base/bsb:823799391811488@diff/
```

### fbios-pika

```
D26324255-V8 (https://www.internalfb.com/intern/diff/D26324255/?dest_number=121462800)

fbios-pika: Succeeded
Change in Download Size for arm64 + 3x assets variation: -8.0 KiB
Change in Uncompressed Size for arm64 + 3x assets variation: -22.7 KiB

Mbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:1345469719167068@base/bsb:1345469719167068@diff/
```

Reviewed By: swolchok

Differential Revision: D26324255

fbshipit-source-id: 61aba8687f4c1b742fa9d9d917a026abc8d9c328",161.0,59.0,"aten/src/ATen/native/DispatchStub.cpp,aten/src/ATen/native/DispatchStub.h",2.0,4,1,0.962412735,2.0,294.0,1.0,118.0,8972.0,20145.0,0.0,Perfective,1.0,1
pytorch,2f895f790a84bffd2aedb1dc184da9837ceb814a,50057e560bc0b863dcb0fa67b7dec891eaa4023f,kshitij12345,kshitijkalambarkar@gmail.com,Thu Apr 15 13:04:44 2021 -0700,1618491884.0,"[special] Add `i0e` (#54409)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50345

Changes:
* Add `i0e`
* Move some kernels from `UnaryOpsKernel.cu` to `UnarySpecialOpsKernel.cu` to decrease compilation time per file.

Time taken by i0e_vs_scipy tests: around 6.33.s

<details>

<summary>Test Run Log</summary>

```
(pytorch-cuda-dev) kshiteej@qgpu1:~/Pytorch/pytorch_module_special$ pytest test/test_unary_ufuncs.py -k _i0e_vs
======================================================================= test session starts ========================================================================
platform linux -- Python 3.8.6, pytest-6.1.2, py-1.9.0, pluggy-0.13.1
rootdir: /home/kshiteej/Pytorch/pytorch_module_special, configfile: pytest.ini
plugins: hypothesis-5.38.1
collected 8843 items / 8833 deselected / 10 selected

test/test_unary_ufuncs.py ...sss....                                                                                                                         [100%]

========================================================================= warnings summary =========================================================================
../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:73
test/test_unary_ufuncs.py::TestUnaryUfuncsCUDA::test_special_i0e_vs_scipy_cuda_bfloat16
  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:73: UserWarning: PyTorch was compiled without cuDNN/MIOpen support. To use cuDNN/MIOpen, rebuild PyTorch making sure the library is visible to the build system.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/warnings.html
===================================================================== short test summary info ======================================================================
SKIPPED [3] test/test_unary_ufuncs.py:1182: not implemented: Could not run 'aten::_copy_from' with arguments from the 'Meta' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_copy_from' is only available for these backends: [BackendSelect, Named, InplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].

BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
InplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:56 [backend fallback]
AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
UNKNOWN_TENSOR_TYPE_ID: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]
Tracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9348 [kernel]
Autocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:250 [backend fallback]
Batched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]
VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
==================================================== 7 passed, 3 skipped, 8833 deselected, 2 warnings in 6.33s =====================================================
```

</details>

TODO:
* [x] Check rendered docs (https://11743402-65600975-gh.circle-artifacts.com/0/docs/special.html)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54409

Reviewed By: jbschlosser

Differential Revision: D27760472

Pulled By: mruberry

fbshipit-source-id: bdfbcaa798b00c51dc9513c34626246c8fc10548",464.0,269.0,"BUILD.bazel,aten/src/ATen/core/interned_strings.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vec256/vsx/vec256_double_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_float_vsx.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_unary_ufuncs.py,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",24.0,21,4,3.056425432,13.0,28541.0,17.0,2488124.652173913,10826.0,23912.0,0.0,Corrective,1.0,1
pytorch,9ccae891026eb69396ef9e2521d53e01731ef57b,505f6f325f5e67d1249bfb37ee63217b8cf75493,albanD,desmaison.alban@gmail.com,Tue Apr 13 13:17:20 2021 -0700,1618319840.0,"port addcdiv to opinfo (#55518)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55518

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D27649411

Pulled By: albanD

fbshipit-source-id: cfb0a235d94ef62589acbeb9bf11d2ea17248484",12.0,19.0,"test/test_autograd.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.965527262,45.0,20841.0,1.0,97.0,10699.0,23647.5,0.0,Feature Addition,0.0,1
pytorch,3e4076aa9c3e94d7cf3dde2683b1be38ebc1c6be,506996c77e94a17b12bf2f1173d452d98756653e,Daya Khudia,dskhudia@fb.com,Mon Mar 23 17:03:19 2020 -0700,1584982999.0,"[pt][quant] Optimized qadd_scalar (#34925)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34925

Optimized path for qadd scalar. qadd_scalar time goes down from 55.840ms for a model to 4.637ms.

### Before
```
  -------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
-------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
quantize_per_tensor        0.12%            155.807us        0.12%            155.807us        155.807us        1
quantized::conv2d          25.50%           31.981ms         25.50%           31.981ms         273.343us        117
quantized::add_scalar      44.53%           55.840ms         44.53%           55.840ms         809.281us        69
quantized::relu6           1.25%            1.570ms          1.25%            1.570ms          22.749us         69
quantized::mul_scalar      10.73%           13.449ms         10.73%           13.449ms         194.914us        69
quantized::mul             16.67%           20.904ms         16.67%           20.904ms         227.220us        92
adaptive_avg_pool2d        0.03%            41.713us         0.69%            862.922us        35.955us         24
_adaptive_avg_pool2d       0.65%            821.209us        0.65%            821.209us        34.217us         24
sigmoid                    0.15%            182.344us        0.15%            182.344us        7.928us          23
quantized::add             0.34%            431.939us        0.34%            431.939us        26.996us         16
dropout                    0.00%            1.936us          0.00%            1.936us          1.936us          1
view                       0.01%            10.281us         0.01%            10.281us         10.281us         1
dequantize                 0.00%            4.562us          0.00%            4.562us          4.562us          1
-------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
Self CPU time total: 125.394ms
```
### After
```
 -------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
Name                       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
-------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
quantize_per_tensor        0.18%            130.534us        0.18%            130.534us        130.534us        1
quantized::conv2d          42.29%           31.267ms         42.29%           31.267ms         267.243us        117
quantized::add_scalar      6.27%            4.637ms          6.27%            4.637ms          67.205us         69
quantized::relu6           1.77%            1.312ms          1.77%            1.312ms          19.008us         69
quantized::mul_scalar      18.92%           13.991ms         18.92%           13.991ms         202.768us        69
quantized::mul             28.49%           21.059ms         28.49%           21.059ms         228.904us        92
adaptive_avg_pool2d        0.06%            45.242us         1.27%            942.522us        39.272us         24
_adaptive_avg_pool2d       1.21%            897.280us        1.21%            897.280us        37.387us         24
sigmoid                    0.22%            160.282us        0.22%            160.282us        6.969us          23
quantized::add             0.56%            416.276us        0.56%            416.276us        26.017us         16
dropout                    0.00%            1.245us          0.00%            1.245us          1.245us          1
view                       0.01%            7.122us          0.01%            7.122us          7.122us          1
dequantize                 0.01%            5.952us          0.01%            5.952us          5.952us          1
-------------------------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
Self CPU time total: 73.930ms
```
ghstack-source-id: 100595212

Test Plan: buck test //caffe2/test:quantized -- 'test_qadd'  --print-passing-details

Differential Revision: D20500848

fbshipit-source-id: c292d15da121e6d13cc4eb92f10549874ff6ab0f",103.0,11.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qadd.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h",4.0,9,1,1.710893647,2.0,3312.0,3.0,511498.75,348.0,1051.5,0.0,Feature Addition,0.0,1
pytorch,65b66264d49d153060b820e1f33919038b7978b6,50731328372721fb681024b246a131951541b624,Sam Gross,sgross@fb.com,Thu Mar 02 22:52:20 2017 -0800,1488495140.0,Implement 'pre' and 'post' hooks at the C++ autograd level,419.0,410.0,"test/test_autograd.py,test/test_nn.py,torch/autograd/function.py,torch/autograd/variable.py,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/function_hook.h,torch/csrc/autograd/grad_hook.h,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_cpp_function.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_hook.cpp,torch/csrc/autograd/python_hook.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/lib/THPP/Type.hpp,torch/nn/modules/module.py",20.0,9,2,3.071788127,23.0,7713.0,2.0,247788.57894736843,121.0,7280.914316,0.0,,0.0,1
pytorch,be843eb26b261fff14920fa761d441cf7f89adaa,507ddc4cde01a6f7e437c855ef9d4bdb7c11fa30,Christian Sarofeen,csarofeen@nvidia.com,Thu May 11 18:18:56 2017 -0700,1494526736.0,Temporary fix for multiple backwards with fused pointwise RNN (#1540),34.0,28.0,"test/test_nn.py,torch/lib/THCUNN/generic/FusedRNNKernel.cu,torch/nn/_functions/thnn/rnnFusedPointwise.py",3.0,8,2,1.544822618,28.0,3965.0,2.0,134414.66666666666,758.0,11397.06049,0.0,Corrective,1.0,1
pytorch,6cf450744fd9eee73a55d0aa961d3cce56f83c65,508f676c5028fd484c3d418806182641fe9c6022,Jerry Zhang,jerryzh@fb.com,Wed Nov 07 00:34:00 2018 -0800,1541550840.0,"Rename ndim() -> dim() - 5/6

Summary:
Codemod generated with clangr shard mode, 50 files per diff,
clangr code(ndim()->dim()): diffusion/FBS/browse/master/fbcode/caffe2/caffe2/fb/codemods/TensorMethodRename.cpp

Reviewed By: salexspb

Differential Revision: D12935787

fbshipit-source-id: 303d71d3eb050789af2ab9575e5dcc48f6037086",158.0,158.0,"caffe2/operators/quantized/server/op_wrapper.h,caffe2/operators/rank_loss_op.cc,caffe2/operators/reduce_front_back_max_ops.h,caffe2/operators/reduce_front_back_sum_mean_ops.h,caffe2/operators/reduce_ops.h,caffe2/operators/reducer_functors.h,caffe2/operators/reduction_ops.cc,caffe2/operators/reduction_ops.h,caffe2/operators/remove_data_blocks_op.h,caffe2/operators/reservoir_sampling.cc,caffe2/operators/reshape_op.h,caffe2/operators/reshape_op_gpu_test.cc,caffe2/operators/resize_op.cc,caffe2/operators/reverse_packed_segs_op.h,caffe2/operators/rnn/recurrent_network_op.h,caffe2/operators/rnn/recurrent_op_cudnn.cc,caffe2/operators/roi_align_gradient_op.cc,caffe2/operators/roi_align_op.cc,caffe2/operators/roi_align_rotated_op.cc,caffe2/operators/segment_reduction_op.h,caffe2/operators/sequence_ops.cc,caffe2/operators/sequence_ops.h,caffe2/operators/shape_op.h,caffe2/operators/sinusoid_position_encoding_op.h,caffe2/operators/slice_op.h,caffe2/operators/softmax_with_loss_op.cc,caffe2/operators/space_batch_op.h,caffe2/operators/sparse_normalize_op.cc,caffe2/operators/sparse_to_dense_mask_op.h,caffe2/operators/sparse_to_dense_op.h,caffe2/operators/spatial_batch_norm_op.h,caffe2/operators/spatial_softmax_with_loss_op.cc,caffe2/operators/string_ops.cc,caffe2/operators/string_ops_test.cc,caffe2/operators/stylizer_ops.cc,caffe2/operators/tile_op.h,caffe2/operators/transpose_op.h,caffe2/operators/transpose_op_cudnn.cc,caffe2/operators/tt_linear_op.h,caffe2/operators/unique_ops.cc,caffe2/operators/upsample_op.cc,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_gpu_test.cc,caffe2/operators/utility_ops_test.cc,caffe2/operators/weighted_multi_sampling_op.cc,caffe2/python/pybind_state.h,caffe2/python/pybind_state_dlpack.h,caffe2/queue/queue_ops.h,caffe2/queue/rebatching_queue.cc,caffe2/sgd/adadelta_op.h",50.0,8,1,5.076754358,18.0,15741.0,13.0,1219603.24,5199.0,15609.33333,0.0,,0.0,1
pytorch,0f86f64398b20bf6e9bdd5187737949fee044e45,509aed6ca377859b19e2730758126c23f3b5f3b2,Sam Gross,colesbury@gmail.com,Wed Feb 28 21:46:47 2018 -0500,1519854407.0,More Variable/Tensor clean-ups (#5464),71.0,242.0,"test/common_nn.py,test/test_autograd.py,test/test_cuda.py,test/test_nn.py,test/test_sparse.py,test/test_torch.py,torch/_tensor_docs.py,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Module.cpp,torch/csrc/jit/pybind.h,torch/nn/parallel/_functions.py",12.0,6,2,3.051057085,40.0,21548.0,7.0,233771.41666666663,569.0,1735.905869,0.0,,0.0,1
pytorch,89e79f844dae9ceee7c58f26e5c4c8d08c7fe2b1,50b6959c0f8a4184ee9b24418bca0289ec4a7396,BowenBao,bowbao@microsoft.com,Thu Apr 07 00:16:26 2022 +0000,1649290586.0,"[ONNX] Support torch.amax and torch.amin

Fixes #75167

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75268
Approved by: https://github.com/garymm",19.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.998000884,4.0,14525.0,2.0,7698.0,2042.0,4913.0,0.0,Corrective,1.0,1
pytorch,17fcdfb9257c1fd2e594c2f30abd8632b3e6b7b2,50df3e5e2e2c51db6389b5c47c048f9db112c5c3,Edward Yang,ezyang@fb.com,Mon Mar 25 17:22:54 2019 -0700,1553534574.0,"Add ability to query if built with CUDA and MKL-DNN. (#18362)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18362
ghimport-source-id: 374b7ab97e2d6a894368007133201f510539296f

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18242 Test running a CUDA build on CPU machine.
* **#18362 Add ability to query if built with CUDA and MKL-DNN.**

Fixes #18108.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Differential Revision: D14584430

fbshipit-source-id: 7605a1ac4e8f2a7c70d52e5a43ad7f03f0457473",40.0,1.0,"aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,test/test_torch.py,torch/backends/cuda/__init__.py,torch/backends/mkldnn/__init__.py,torch/csrc/Module.cpp",6.0,9,3,2.546371981,41.0,11881.0,5.0,6407111.6,7665.0,23252.83333,0.0,Corrective,1.0,1
pytorch,623e720e02e3f0d433830db34ac738bad8cc4cb8,50df65bda79e81d0930070f9d870913442e4737e,fsuzanomassa,fvsmassa@gmail.com,Mon Jul 13 12:42:40 2015 +0200,1436791360.0,Fix for gemm with zero strides plus add unit test for torch.mm,12.0,6.0,generic/THTensorMath.c,1.0,1,1,0,17.0,2084.0,1.0,-586619.0,1.0,1.0,0.0,Corrective,1.0,1
pytorch,b60936b9ae70a4f8bcb20fb9cfb1596b4df1eb23,50f5a4dd18cbbfb202bd0df9dd3ef7dbc3510e13,Edgar Riba,edgar.riba@gmail.com,Thu Mar 23 15:27:21 2017 +0100,1490282841.0,fix BCE loss formula visualization (#1072),2.0,2.0,torch/nn/modules/loss.py,1.0,3,1,0,24.0,433.0,1.0,4841.0,560.0,4179.555248,0.0,Corrective,1.0,1
pytorch,08035b1eb9ac6e6d783b3280f00bbb1b036f5ac9,5112f44dc48900f9d15580772e98222c028ea515,Li-Huai (Allan) Lin,qqaatw@gmail.com,Mon Jan 30 08:08:33 2023 +0000,1675066113.0,"Add vmap support for torch.index_fill (#91364)

Fixes #91177

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91364
Approved by: https://github.com/zou3519",235.0,5.0,"aten/src/ATen/functorch/BatchRulesScatterOps.cpp,test/functorch/test_ops.py,test/functorch/test_vmap.py",3.0,6,2,0.986173972,1.0,8347.0,3.0,258027.0,11800.0,27514.5,0.0,Corrective,1.0,1
pytorch,f74c42bf00cf1526a2d20636982f7962157beab0,5114d94ad9bc41940d5b5d6e7faaa6bc4b848a06,Soumith Chintala,soumith@fb.com,Sun Sep 18 16:32:45 2016 -0400,1474216365.0,"docstrings for conv, dropout, linear, pooling and sparse functions",427.0,10.0,"torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/linear.py,torch/nn/modules/pooling.py,torch/nn/modules/sparse.py",5.0,3,1,1.803904723,5.0,434.0,1.0,290609.0,162.0,816.469697,0.0,Non Functional,0.0,1
pytorch,469dce4a2d68fe40d8629dcd92bdaad16de9891f,513d902df108420dedf3d777738387145998826f,Soumith Chintala,soumith@fb.com,Mon Nov 07 01:07:26 2016 -0500,1478480846.0,adding __repr__ for nn,220.0,3.0,"torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/linear.py,torch/nn/modules/normalization.py,torch/nn/modules/padding.py,torch/nn/modules/pooling.py",9.0,3,1,2.58005719,11.0,1725.0,1.0,345918.0,209.0,1700.748261,0.0,Feature Addition,0.0,1
pytorch,5bdc4db26e91db9639e85b1c8564b0a99c4f1075,516c7e44568f021f42debbbf8d54dc92783b907c,Vitaly Fedyunin,vitalyf@fb.com,Wed Jun 26 18:40:31 2019 -0700,1561574431.0,"Adding memory_format to empty and empty_like operators (#20558)

Summary:
Original RFC https://github.com/pytorch/pytorch/issues/19092

To ensure that we are not introducing BC breaking change, empty_like returns contiguous tensor by default.

```python
nCwh = torch.randn(N, C, H, W)
nhwC = nCwh.contiguous(memory_format=torch.channels_last)

new_nCwh = torch.empty_like(nhwC)
new_nCwh.is_contiguous(memory_format=torch.channels_last) == False
```

Now we need a way to preserve memory format in `empty_like`

```python
nCwh = torch.randn(N, C, H, W)
nhwC = nCwh.contiguous(memory_format=torch.channels_last)

new_nhwC = torch.empty_like(nhwC, memory_format=torch.preserve_format)
new_nhwC.is_contiguous(memory_format=torch.channels_last) == True

like_nCwh = torch.empty_like(nCwh, memory_format=torch.preserve_format)
like_nCwh.is_contiguous(memory_format=torch.channels_last) == False
```

Usage of `torch.preserve_format` allows us to avoid `if` constructs.

We can also generate different memory format outputs

```python
nCwh = torch.randn(N, C, H, W)
nhwC = nCwh.contiguous(memory_format=torch.channels_last)

new_nhwC = torch.empty_like(nCwh, memory_format=torch.channels_last)
new_nhwC.is_contiguous(memory_format=torch.channels_last) == True

new_nCwh = torch.empty_like(nhwC, memory_format=torch.contiguous_format)
new_nCwh.is_contiguous(memory_format=torch.channels_last) == False
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20558

Differential Revision: D15502474

Pulled By: VitalyFedyunin

fbshipit-source-id: 2e120d57eefad6fb8e04b8322c79871392f64331",228.0,99.0,"aten/src/ATen/OpaqueTensorImpl.h,aten/src/ATen/SparseTensorImpl.h,aten/src/ATen/core/Tensor.h,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorProperties.cpp,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/mkldnn/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/TensorFactories.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/quantized/Quantizer.cpp,aten/src/ATen/quantized/Quantizer.h,aten/src/ATen/templates/Tensor.h,aten/src/ATen/test/extension_backend_test.cpp,c10/core/MemoryFormat.h,c10/core/TensorImpl.h,caffe2/core/tensor.h,test/cpp_extensions/complex_registration_extension.cpp,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_variable_methods.cpp,tools/autograd/templates/variable_factories.h,tools/jit/gen_jit_dispatch.py,tools/jit/templates/register_aten_ops.cpp,tools/pyi/gen_pyi.py,torch/__init__.pyi.in,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/tracer.cpp,torch/csrc/jit/tracer.h,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_memoryformats.cpp",31.0,29,6,3.954371083,46.0,30985.0,23.0,1391060.677419355,9647.0,28076.83333,0.0,Feature Addition,0.0,1
pytorch,a7b3197b2d4b63685629e903e95350eab143f976,517c7c98610402e2746586c78987c64c28e024aa,Edward Yang,ezyang@fb.com,Sun Dec 09 03:32:01 2018 -0800,1544326321.0,"Canonicalize all includes in PyTorch. (#14849)

Summary:
Anywhere we used #include ""foo.h"", we now say #include <foo.h>
Paths are adjusted to be rooted out of aten/src, torch/lib, or
the root level directory.

I modified CMakeLists.txt by hand to remove TH and THC from
the include paths.

I used the following script to do the canonicalization:

```
  import subprocess
  import re
  import os.path

  files = subprocess.check_output(['git', 'ls-files']).decode('utf-8').rstrip().split('\n')
  for fn in files:
      if not any(fn.endswith(suff) for suff in ['.cu', '.cpp', '.in', '.h', '.hpp', '.cu', '.cuh', '.cc']):
          continue
      if not any(fn.startswith(pref) for pref in [""aten/"", ""torch/""]):
          continue
      with open(fn, 'r') as f:
          c = f.read()
      def fmt(p):
          return ""#include <{}>"".format(p)
      def repl(m):
          p = m.group(1)
          if p in [""dlfcn.h"", ""unistd.h"", ""nvrtc.h"", ""cuda.h"", ""cuda_runtime.h"", ""cstdint"", ""cudnn.h"", ""Python.h"", ""cusparse.h"", ""cuda_runtime_api.h"", ""cuda_fp16.h"", ""cublas_v2.h"", ""stdint.h"", ""curand_kernel.h""]:
              return fmt(p)
          if any(p.startswith(pref) for pref in [""torch/csrc"", ""c10/"", ""ATen/"", ""caffe2/"", ""TH/"", ""THC/"", ""Eigen/"", ""gtest/"", ""zdl/"", ""gloo/"", ""onnx/"", ""miopen/""]):
              return fmt(p)
          for root in [""aten/src"", ""torch/lib"", """"]:
              for bad_root in [os.path.dirname(fn), ""aten/src/TH"", ""aten/src/THC"", ""torch/csrc""]:
                  new_p = os.path.relpath(os.path.join(bad_root, p), root)
                  if not new_p.startswith(""../"") and (os.path.exists(os.path.join(root, new_p)) or os.path.exists(os.path.join(root, new_p + "".in""))):
                      return fmt(new_p)
          print(""ERROR: "", fn, p)
          return m.group(0)
      new_c = re.sub(r'#include ""([^""]+)""', repl, c)
      if new_c != c:
          print(fn)
          with open(fn, 'w') as f:
              f.write(new_c)
```

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14849

Reviewed By: dzhulgakov

Differential Revision: D13363445

Pulled By: ezyang

fbshipit-source-id: 52361f878a672785f9306c9e9ab2513128092b68",3770.0,3806.0,"aten/CMakeLists.txt,aten/src/ATen/ATen.h,aten/src/ATen/AccumulateType.h,aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/CPUFixedAllocator.h,aten/src/ATen/CPUGeneral.h,aten/src/ATen/CPUGenerator.cpp,aten/src/ATen/CheckGenerator.h,aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/DLConvertor.cpp,aten/src/ATen/DLConvertor.h,aten/src/ATen/ExpandUtils.cpp,aten/src/ATen/ExpandUtils.h,aten/src/ATen/ScalarOps.h,aten/src/ATen/SparseTensorImpl.h,aten/src/ATen/TensorOperators.h,aten/src/ATen/TensorUtils.cpp,aten/src/ATen/TensorUtils.h,aten/src/ATen/UndefinedType.cpp,aten/src/ATen/UndefinedType.h,aten/src/ATen/Utils.cpp,aten/src/ATen/Utils.h,aten/src/ATen/WrapDimUtils.h,aten/src/ATen/WrapDimUtilsMulti.h,aten/src/ATen/core/ATenGeneral.h,aten/src/ATen/core/Backtrace.h,aten/src/ATen/core/Formatting.cpp,aten/src/ATen/core/Half.h,aten/src/ATen/core/Macros.h,aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorImpl_test.cpp,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/core/UndefinedTensorImpl.cpp,aten/src/ATen/core/UndefinedTensorImpl.h,aten/src/ATen/core/WrapDimMinimal.h,aten/src/ATen/core/alias_info.h,aten/src/ATen/core/interned_strings.cpp,aten/src/ATen/core/interned_strings_class.h,aten/src/ATen/core/register_symbols.cpp,aten/src/ATen/cpu/vec256/functional.h,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/cuda/CUDAApplyUtils.cuh,aten/src/ATen/cuda/CUDAContext.cpp,aten/src/ATen/cuda/CUDAContext.h,aten/src/ATen/cuda/CUDADevice.h,aten/src/ATen/cuda/CUDAEvent.h,aten/src/ATen/cuda/CUDAGenerator.cpp,aten/src/ATen/cuda/CUDAStream.h,aten/src/ATen/cuda/CUDATensorMethods.cuh,aten/src/ATen/cuda/Exceptions.h,aten/src/ATen/cuda/detail/CUDAHooks.cpp,aten/src/ATen/cuda/detail/IndexUtils.cu,aten/src/ATen/cuda/detail/IndexUtils.cuh,aten/src/ATen/cuda/detail/KernelUtils.h,aten/src/ATen/cudnn/Descriptors.cpp,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/cudnn/Handle.cpp,aten/src/ATen/cudnn/Handle.h,aten/src/ATen/cudnn/Handles.h,aten/src/ATen/cudnn/Types.cpp,aten/src/ATen/cudnn/Types.h,aten/src/ATen/cudnn/Utils.h,aten/src/ATen/detail/CUDAHooksInterface.h,aten/src/ATen/detail/ComplexHooksInterface.h,aten/src/ATen/detail/HIPHooksInterface.h,aten/src/ATen/miopen/Descriptors.cpp,aten/src/ATen/miopen/Descriptors.h,aten/src/ATen/miopen/Exceptions.h,aten/src/ATen/miopen/Handle.cpp,aten/src/ATen/miopen/Handle.h,aten/src/ATen/miopen/Types.cpp,aten/src/ATen/miopen/Types.h,aten/src/ATen/miopen/Utils.h,aten/src/ATen/mkl/Descriptors.h,aten/src/ATen/mkldnn/Runtime.cpp,aten/src/ATen/native/AffineGridGenerator.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/ConstantPadNd.cpp,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/ConvolutionTBC.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Copy.h,aten/src/ATen/native/DispatchStub.cpp,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/Dropout.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/GridSampler.h,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/Loss.cpp,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/Memory.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/PackedSequence.cpp,aten/src/ATen/native/PixelShuffle.cpp,aten/src/ATen/native/Pooling.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOps.h,aten/src/ATen/native/Resize.cpp,aten/src/ATen/native/Resize.h,aten/src/ATen/native/RoiPooling.cpp,aten/src/ATen/native/Scalar.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorConversions.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/TensorProperties.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/TensorTransformations.cpp,aten/src/ATen/native/TensorTransformations.h,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/Unique.cpp,aten/src/ATen/native/WeightNorm.cpp,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/SoftMaxKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/avx_mathfun.h,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/CUDAScalar.cu,aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/CuFFTPlanCache.h,aten/src/ATen/native/cuda/CuFFTUtils.h,aten/src/ATen/native/cuda/DistanceKernel.cu,aten/src/ATen/native/cuda/Distributions.cu,aten/src/ATen/native/cuda/Dropout.cu,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/GridSampler.cu,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/cuda/Loss.cu,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/MiscUtils.h,aten/src/ATen/native/cuda/Normalization.cu,aten/src/ATen/native/cuda/Normalization.cuh,aten/src/ATen/native/cuda/RNN.cu,aten/src/ATen/native/cuda/Resize.cu,aten/src/ATen/native/cuda/Resize.cuh,aten/src/ATen/native/cuda/RoiPooling.cu,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/SparseMM.cu,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/TensorCompare.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cuda/TensorTransformations.cu,aten/src/ATen/native/cuda/Unique.cu,aten/src/ATen/native/cuda/WeightNorm.cu,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/ATen/native/mkl/LinearAlgebra.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,aten/src/ATen/templates/Functions.h,aten/src/ATen/templates/GeneratorDerived.h,aten/src/ATen/templates/SparseTypeDerived.cpp,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/TensorMethods.h,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeDefault.cpp,aten/src/ATen/templates/TypeDefault.h,aten/src/ATen/templates/TypeDerived.cpp,aten/src/ATen/templates/TypeDerived.h,aten/src/ATen/test/apply_utils_test.cpp,aten/src/ATen/test/atest.cpp,aten/src/ATen/test/basic.cpp,aten/src/ATen/test/broadcast_test.cpp,aten/src/ATen/test/cuda_apply_test.cpp,aten/src/ATen/test/cuda_cudnn_test.cpp,aten/src/ATen/test/cuda_half_test.cu,aten/src/ATen/test/cuda_integer_divider_test.cu,aten/src/ATen/test/cuda_optional_test.cu,aten/src/ATen/test/cuda_packedtensoraccessor_test.cu,aten/src/ATen/test/cuda_rng_test.cpp,aten/src/ATen/test/cuda_stream_test.cpp,aten/src/ATen/test/dlconvertor_test.cpp,aten/src/ATen/test/half_test.cpp,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/scalar_tensor_test.cpp,aten/src/ATen/test/scalar_test.cpp,aten/src/ATen/test/tbb_init_test.cpp,aten/src/ATen/test/test_install/main.cpp,aten/src/ATen/test/test_parallel.cpp,aten/src/ATen/test/undefined_tensor_test.cpp,aten/src/ATen/test/weakref_test.cpp,aten/src/ATen/test/wrapdim_test.cpp,aten/src/TH/TH.h,aten/src/TH/THAllocator.cpp,aten/src/TH/THBlas.cpp,aten/src/TH/THBlas.h,aten/src/TH/THDiskFile.cpp,aten/src/TH/THDiskFile.h,aten/src/TH/THFile.cpp,aten/src/TH/THFile.h,aten/src/TH/THFilePrivate.h,aten/src/TH/THGenerateAllTypes.h,aten/src/TH/THGenerateFloatTypes.h,aten/src/TH/THGenerateHalfType.h,aten/src/TH/THGenerateIntTypes.h,aten/src/TH/THLapack.cpp,aten/src/TH/THLapack.h,aten/src/TH/THLogAdd.cpp,aten/src/TH/THMemoryFile.cpp,aten/src/TH/THMemoryFile.h,aten/src/TH/THRandom.cpp,aten/src/TH/THSize.cpp,aten/src/TH/THStorage.h,aten/src/TH/THStorageFunctions.cpp,aten/src/TH/THStorageFunctions.h,aten/src/TH/THStorageFunctions.hpp,aten/src/TH/THTensor.cpp,aten/src/TH/THTensor.h,aten/src/TH/THTensor.hpp,aten/src/TH/THTensorConv.cpp,aten/src/TH/THTensorEvenMoreMath.cpp,aten/src/TH/THTensorLapack.cpp,aten/src/TH/THTensorMath.cpp,aten/src/TH/THTensorMoreMath.cpp,aten/src/TH/THTensorRandom.cpp,aten/src/TH/THVector.cpp,aten/src/TH/THVector.h,aten/src/TH/generic/THBlas.cpp,aten/src/TH/generic/THBlas.h,aten/src/TH/generic/THLapack.cpp,aten/src/TH/generic/THLapack.h,aten/src/TH/generic/THStorage.cpp,aten/src/TH/generic/THStorage.h,aten/src/TH/generic/THStorageCopy.cpp,aten/src/TH/generic/THStorageCopy.h,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensor.h,aten/src/TH/generic/THTensor.hpp,aten/src/TH/generic/THTensorConv.cpp,aten/src/TH/generic/THTensorConv.h,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorFastGetSet.hpp,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorLapack.h,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/TH/generic/THTensorRandom.cpp,aten/src/TH/generic/THTensorRandom.h,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.cpp,aten/src/TH/generic/THVectorDispatch.cpp,aten/src/TH/vector/AVX.cpp,aten/src/TH/vector/AVX2.cpp,aten/src/THC/THC.h,aten/src/THC/THCAllocator.cpp,aten/src/THC/THCAllocator.h,aten/src/THC/THCApply.cuh,aten/src/THC/THCAtomics.cuh,aten/src/THC/THCBlas.cu,aten/src/THC/THCBlas.h,aten/src/THC/THCCachingAllocator.cpp,aten/src/THC/THCCachingAllocator.h,aten/src/THC/THCCachingHostAllocator.cpp,aten/src/THC/THCCachingHostAllocator.h,aten/src/THC/THCDeviceTensor.cuh,aten/src/THC/THCDeviceTensorUtils.cuh,aten/src/THC/THCGeneral.cpp,aten/src/THC/THCGeneral.h.in,aten/src/THC/THCGeneral.hpp,aten/src/THC/THCGenerateAllTypes.h,aten/src/THC/THCGenerateFloatTypes.h,aten/src/THC/THCGenerateHalfType.h,aten/src/THC/THCNumerics.cuh,aten/src/THC/THCReduce.cuh,aten/src/THC/THCReduceAll.cuh,aten/src/THC/THCReduceApplyUtils.cu,aten/src/THC/THCReduceApplyUtils.cuh,aten/src/THC/THCScanUtils.cuh,aten/src/THC/THCSleep.cu,aten/src/THC/THCSleep.h,aten/src/THC/THCSortUtils.cu,aten/src/THC/THCSortUtils.cuh,aten/src/THC/THCStorage.cpp,aten/src/THC/THCStorage.cu,aten/src/THC/THCStorage.h,aten/src/THC/THCStorage.hpp,aten/src/THC/THCStorageCopy.cpp,aten/src/THC/THCStorageCopy.cu,aten/src/THC/THCStorageCopy.h,aten/src/THC/THCStream.cpp,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensor.cu,aten/src/THC/THCTensor.h,aten/src/THC/THCTensor.hpp,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorCopy.h,aten/src/THC/THCTensorCopy.hpp,aten/src/THC/THCTensorIndex.cu,aten/src/THC/THCTensorInfo.cuh,aten/src/THC/THCTensorMasked.cuh,aten/src/THC/THCTensorMath.cu,aten/src/THC/THCTensorMath.h,aten/src/THC/THCTensorMathBlas.cu,aten/src/THC/THCTensorMathCompare.cuh,aten/src/THC/THCTensorMathCompareT.cuh,aten/src/THC/THCTensorMathMagma.cu,aten/src/THC/THCTensorMathMagma.cuh,aten/src/THC/THCTensorMathPairwise.cu,aten/src/THC/THCTensorMathPointwise.cuh,aten/src/THC/THCTensorMathReduce.cu,aten/src/THC/THCTensorMathReduce.cuh,aten/src/THC/THCTensorMathScan.cu,aten/src/THC/THCTensorMode.cu,aten/src/THC/THCTensorMode.cuh,aten/src/THC/THCTensorRandom.cpp,aten/src/THC/THCTensorRandom.cu,aten/src/THC/THCTensorRandom.cuh,aten/src/THC/THCTensorRandom.h,aten/src/THC/THCTensorScatterGather.cu,aten/src/THC/THCTensorSort.cu,aten/src/THC/THCTensorSort.cuh,aten/src/THC/THCTensorTopK.cu,aten/src/THC/THCTensorTypeUtils.cuh,aten/src/THC/generated/THCTensorMaskedByte.cu,aten/src/THC/generated/THCTensorMaskedChar.cu,aten/src/THC/generated/THCTensorMaskedDouble.cu,aten/src/THC/generated/THCTensorMaskedFloat.cu,aten/src/THC/generated/THCTensorMaskedHalf.cu,aten/src/THC/generated/THCTensorMaskedInt.cu,aten/src/THC/generated/THCTensorMaskedLong.cu,aten/src/THC/generated/THCTensorMaskedShort.cu,aten/src/THC/generated/THCTensorMathCompareByte.cu,aten/src/THC/generated/THCTensorMathCompareChar.cu,aten/src/THC/generated/THCTensorMathCompareDouble.cu,aten/src/THC/generated/THCTensorMathCompareFloat.cu,aten/src/THC/generated/THCTensorMathCompareHalf.cu,aten/src/THC/generated/THCTensorMathCompareInt.cu,aten/src/THC/generated/THCTensorMathCompareLong.cu,aten/src/THC/generated/THCTensorMathCompareShort.cu,aten/src/THC/generated/THCTensorMathCompareTByte.cu,aten/src/THC/generated/THCTensorMathCompareTChar.cu,aten/src/THC/generated/THCTensorMathCompareTDouble.cu,aten/src/THC/generated/THCTensorMathCompareTFloat.cu,aten/src/THC/generated/THCTensorMathCompareTHalf.cu,aten/src/THC/generated/THCTensorMathCompareTInt.cu,aten/src/THC/generated/THCTensorMathCompareTLong.cu,aten/src/THC/generated/THCTensorMathCompareTShort.cu,aten/src/THC/generated/THCTensorMathPointwiseByte.cu,aten/src/THC/generated/THCTensorMathPointwiseChar.cu,aten/src/THC/generated/THCTensorMathPointwiseDouble.cu,aten/src/THC/generated/THCTensorMathPointwiseFloat.cu,aten/src/THC/generated/THCTensorMathPointwiseHalf.cu,aten/src/THC/generated/THCTensorMathPointwiseInt.cu,aten/src/THC/generated/THCTensorMathPointwiseLong.cu,aten/src/THC/generated/THCTensorMathPointwiseShort.cu,aten/src/THC/generated/THCTensorMathReduceByte.cu,aten/src/THC/generated/THCTensorMathReduceChar.cu,aten/src/THC/generated/THCTensorMathReduceDouble.cu,aten/src/THC/generated/THCTensorMathReduceFloat.cu,aten/src/THC/generated/THCTensorMathReduceHalf.cu,aten/src/THC/generated/THCTensorMathReduceInt.cu,aten/src/THC/generated/THCTensorMathReduceLong.cu,aten/src/THC/generated/THCTensorMathReduceShort.cu,aten/src/THC/generated/THCTensorSortByte.cu,aten/src/THC/generated/THCTensorSortChar.cu,aten/src/THC/generated/THCTensorSortDouble.cu,aten/src/THC/generated/THCTensorSortFloat.cu,aten/src/THC/generated/THCTensorSortHalf.cu,aten/src/THC/generated/THCTensorSortInt.cu,aten/src/THC/generated/THCTensorSortLong.cu,aten/src/THC/generated/THCTensorSortShort.cu,aten/src/THC/generic/THCStorage.cpp,aten/src/THC/generic/THCStorage.cu,aten/src/THC/generic/THCStorage.h,aten/src/THC/generic/THCStorageCopy.cpp,aten/src/THC/generic/THCStorageCopy.cu,aten/src/THC/generic/THCStorageCopy.h,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensor.cu,aten/src/THC/generic/THCTensor.h,aten/src/THC/generic/THCTensor.hpp,aten/src/THC/generic/THCTensorCopy.cu,aten/src/THC/generic/THCTensorCopy.h,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorIndex.h,aten/src/THC/generic/THCTensorMasked.cu,aten/src/THC/generic/THCTensorMasked.h,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMath.h,aten/src/THC/generic/THCTensorMathBlas.cu,aten/src/THC/generic/THCTensorMathBlas.h,aten/src/THC/generic/THCTensorMathCompare.cu,aten/src/THC/generic/THCTensorMathCompare.h,aten/src/THC/generic/THCTensorMathCompareT.cu,aten/src/THC/generic/THCTensorMathCompareT.h,aten/src/THC/generic/THCTensorMathMagma.cu,aten/src/THC/generic/THCTensorMathMagma.h,aten/src/THC/generic/THCTensorMathPairwise.cu,aten/src/THC/generic/THCTensorMathPairwise.h,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathPointwise.h,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorMathReduce.h,aten/src/THC/generic/THCTensorMathScan.cu,aten/src/THC/generic/THCTensorMathScan.h,aten/src/THC/generic/THCTensorMode.cu,aten/src/THC/generic/THCTensorMode.h,aten/src/THC/generic/THCTensorRandom.cu,aten/src/THC/generic/THCTensorRandom.h,aten/src/THC/generic/THCTensorScatterGather.cu,aten/src/THC/generic/THCTensorScatterGather.h,aten/src/THC/generic/THCTensorSort.cu,aten/src/THC/generic/THCTensorSort.h,aten/src/THC/generic/THCTensorTopK.cu,aten/src/THC/generic/THCTensorTopK.h,aten/src/THCUNN/Abs.cu,aten/src/THCUNN/AbsCriterion.cu,aten/src/THCUNN/BCECriterion.cu,aten/src/THCUNN/ClassNLLCriterion.cu,aten/src/THCUNN/Col2Im.cu,aten/src/THCUNN/DistKLDivCriterion.cu,aten/src/THCUNN/ELU.cu,aten/src/THCUNN/FeatureLPPooling.cu,aten/src/THCUNN/GatedLinearUnit.cu,aten/src/THCUNN/HardTanh.cu,aten/src/THCUNN/Im2Col.cu,aten/src/THCUNN/IndexLinear.cu,aten/src/THCUNN/L1Cost.cu,aten/src/THCUNN/LeakyReLU.cu,aten/src/THCUNN/LogSigmoid.cu,aten/src/THCUNN/LookupTable.cu,aten/src/THCUNN/LookupTableBag.cu,aten/src/THCUNN/MSECriterion.cu,aten/src/THCUNN/MarginCriterion.cu,aten/src/THCUNN/MultiLabelMarginCriterion.cu,aten/src/THCUNN/MultiMarginCriterion.cu,aten/src/THCUNN/RReLU.cu,aten/src/THCUNN/Sigmoid.cu,aten/src/THCUNN/SmoothL1Criterion.cu,aten/src/THCUNN/SoftMarginCriterion.cu,aten/src/THCUNN/SoftPlus.cu,aten/src/THCUNN/SoftShrink.cu,aten/src/THCUNN/SparseLinear.cu,aten/src/THCUNN/SpatialAdaptiveAveragePooling.cu,aten/src/THCUNN/SpatialAdaptiveMaxPooling.cu,aten/src/THCUNN/SpatialAveragePooling.cu,aten/src/THCUNN/SpatialClassNLLCriterion.cu,aten/src/THCUNN/SpatialConvolutionLocal.cu,aten/src/THCUNN/SpatialConvolutionMM.cu,aten/src/THCUNN/SpatialCrossMapLRN.cu,aten/src/THCUNN/SpatialDepthwiseConvolution.cu,aten/src/THCUNN/SpatialDilatedConvolution.cu,aten/src/THCUNN/SpatialDilatedMaxPooling.cu,aten/src/THCUNN/SpatialFractionalMaxPooling.cu,aten/src/THCUNN/SpatialFullConvolution.cu,aten/src/THCUNN/SpatialFullDilatedConvolution.cu,aten/src/THCUNN/SpatialMaxPooling.cu,aten/src/THCUNN/SpatialMaxUnpooling.cu,aten/src/THCUNN/SpatialReflectionPadding.cu,aten/src/THCUNN/SpatialReplicationPadding.cu,aten/src/THCUNN/SpatialSubSampling.cu,aten/src/THCUNN/SpatialUpSamplingBilinear.cu,aten/src/THCUNN/SpatialUpSamplingNearest.cu,aten/src/THCUNN/Sqrt.cu,aten/src/THCUNN/Square.cu,aten/src/THCUNN/THCHalfAutoNumerics.cuh,aten/src/THCUNN/THCUNN.h,aten/src/THCUNN/Tanh.cu,aten/src/THCUNN/TemporalConvolution.cu,aten/src/THCUNN/TemporalMaxPooling.cu,aten/src/THCUNN/TemporalReflectionPadding.cu,aten/src/THCUNN/TemporalReplicationPadding.cu,aten/src/THCUNN/TemporalRowConvolution.cu,aten/src/THCUNN/TemporalUpSamplingLinear.cu,aten/src/THCUNN/TemporalUpSamplingNearest.cu,aten/src/THCUNN/VolumetricAdaptiveAveragePooling.cu,aten/src/THCUNN/VolumetricAdaptiveMaxPooling.cu,aten/src/THCUNN/VolumetricAveragePooling.cu,aten/src/THCUNN/VolumetricConvolution.cu,aten/src/THCUNN/VolumetricDilatedConvolution.cu,aten/src/THCUNN/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/VolumetricFractionalMaxPooling.cu,aten/src/THCUNN/VolumetricFullConvolution.cu,aten/src/THCUNN/VolumetricFullDilatedConvolution.cu,aten/src/THCUNN/VolumetricMaxPooling.cu,aten/src/THCUNN/VolumetricMaxUnpooling.cu,aten/src/THCUNN/VolumetricReplicationPadding.cu,aten/src/THCUNN/VolumetricUpSamplingNearest.cu,aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu,aten/src/THCUNN/generic/Abs.cu,aten/src/THCUNN/generic/AbsCriterion.cu,aten/src/THCUNN/generic/BCECriterion.cu,aten/src/THCUNN/generic/ClassNLLCriterion.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/DistKLDivCriterion.cu,aten/src/THCUNN/generic/ELU.cu,aten/src/THCUNN/generic/FeatureLPPooling.cu,aten/src/THCUNN/generic/GatedLinearUnit.cu,aten/src/THCUNN/generic/HardTanh.cu,aten/src/THCUNN/generic/Im2Col.cu,aten/src/THCUNN/generic/IndexLinear.cu,aten/src/THCUNN/generic/L1Cost.cu,aten/src/THCUNN/generic/LeakyReLU.cu,aten/src/THCUNN/generic/LogSigmoid.cu,aten/src/THCUNN/generic/LookupTable.cu,aten/src/THCUNN/generic/LookupTableBag.cu,aten/src/THCUNN/generic/MSECriterion.cu,aten/src/THCUNN/generic/MarginCriterion.cu,aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/MultiMarginCriterion.cu,aten/src/THCUNN/generic/RReLU.cu,aten/src/THCUNN/generic/Sigmoid.cu,aten/src/THCUNN/generic/SmoothL1Criterion.cu,aten/src/THCUNN/generic/SoftMarginCriterion.cu,aten/src/THCUNN/generic/SoftPlus.cu,aten/src/THCUNN/generic/SoftShrink.cu,aten/src/THCUNN/generic/SparseLinear.cu,aten/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/SpatialAveragePooling.cu,aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu,aten/src/THCUNN/generic/SpatialConvolutionLocal.cu,aten/src/THCUNN/generic/SpatialConvolutionMM.cu,aten/src/THCUNN/generic/SpatialCrossMapLRN.cu,aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu,aten/src/THCUNN/generic/SpatialFractionalMaxPooling.cu,aten/src/THCUNN/generic/SpatialFullConvolution.cu,aten/src/THCUNN/generic/SpatialFullDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialMaxPooling.cu,aten/src/THCUNN/generic/SpatialMaxUnpooling.cu,aten/src/THCUNN/generic/SpatialReflectionPadding.cu,aten/src/THCUNN/generic/SpatialReplicationPadding.cu,aten/src/THCUNN/generic/SpatialSubSampling.cu,aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu,aten/src/THCUNN/generic/SpatialUpSamplingNearest.cu,aten/src/THCUNN/generic/Sqrt.cu,aten/src/THCUNN/generic/Square.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THCUNN/generic/Tanh.cu,aten/src/THCUNN/generic/TemporalConvolution.cu,aten/src/THCUNN/generic/TemporalMaxPooling.cu,aten/src/THCUNN/generic/TemporalReflectionPadding.cu,aten/src/THCUNN/generic/TemporalReplicationPadding.cu,aten/src/THCUNN/generic/TemporalRowConvolution.cu,aten/src/THCUNN/generic/TemporalUpSamplingLinear.cu,aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFullConvolution.cu,aten/src/THCUNN/generic/VolumetricFullDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricMaxPooling.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricReplicationPadding.cu,aten/src/THCUNN/generic/VolumetricUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricUpSamplingTrilinear.cu,aten/src/THCUNN/im2col.h,aten/src/THCUNN/row2col.h,aten/src/THCUNN/vol2col.h,aten/src/THNN/THNN.h,aten/src/THNN/generic/AbsCriterion.c,aten/src/THNN/generic/BCECriterion.c,aten/src/THNN/generic/ClassNLLCriterion.c,aten/src/THNN/generic/Col2Im.c,aten/src/THNN/generic/ELU.c,aten/src/THNN/generic/FeatureLPPooling.c,aten/src/THNN/generic/GatedLinearUnit.c,aten/src/THNN/generic/HardTanh.c,aten/src/THNN/generic/Im2Col.c,aten/src/THNN/generic/IndexLinear.c,aten/src/THNN/generic/LeakyReLU.c,aten/src/THNN/generic/LogSigmoid.c,aten/src/THNN/generic/MSECriterion.c,aten/src/THNN/generic/MultiLabelMarginCriterion.c,aten/src/THNN/generic/MultiMarginCriterion.c,aten/src/THNN/generic/RReLU.c,aten/src/THNN/generic/Sigmoid.c,aten/src/THNN/generic/SmoothL1Criterion.c,aten/src/THNN/generic/SoftMarginCriterion.c,aten/src/THNN/generic/SoftPlus.c,aten/src/THNN/generic/SoftShrink.c,aten/src/THNN/generic/SparseLinear.c,aten/src/THNN/generic/SpatialAdaptiveAveragePooling.c,aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c,aten/src/THNN/generic/SpatialAveragePooling.c,aten/src/THNN/generic/SpatialClassNLLCriterion.c,aten/src/THNN/generic/SpatialConvolutionMM.c,aten/src/THNN/generic/SpatialDilatedConvolution.c,aten/src/THNN/generic/SpatialDilatedMaxPooling.c,aten/src/THNN/generic/SpatialFractionalMaxPooling.c,aten/src/THNN/generic/SpatialFullDilatedConvolution.c,aten/src/THNN/generic/SpatialMaxUnpooling.c,aten/src/THNN/generic/SpatialReflectionPadding.c,aten/src/THNN/generic/SpatialReplicationPadding.c,aten/src/THNN/generic/SpatialUpSamplingBilinear.c,aten/src/THNN/generic/SpatialUpSamplingNearest.c,aten/src/THNN/generic/THNN.h,aten/src/THNN/generic/Tanh.c,aten/src/THNN/generic/TemporalReflectionPadding.c,aten/src/THNN/generic/TemporalReplicationPadding.c,aten/src/THNN/generic/TemporalRowConvolution.c,aten/src/THNN/generic/TemporalUpSamplingLinear.c,aten/src/THNN/generic/TemporalUpSamplingNearest.c,aten/src/THNN/generic/VolumetricAdaptiveAveragePooling.c,aten/src/THNN/generic/VolumetricAdaptiveMaxPooling.c,aten/src/THNN/generic/VolumetricAveragePooling.c,aten/src/THNN/generic/VolumetricConvolutionMM.c,aten/src/THNN/generic/VolumetricDilatedConvolution.c,aten/src/THNN/generic/VolumetricDilatedMaxPooling.c,aten/src/THNN/generic/VolumetricFractionalMaxPooling.c,aten/src/THNN/generic/VolumetricFullDilatedConvolution.c,aten/src/THNN/generic/VolumetricMaxUnpooling.c,aten/src/THNN/generic/VolumetricReplicationPadding.c,aten/src/THNN/generic/VolumetricUpSamplingNearest.c,aten/src/THNN/generic/VolumetricUpSamplingTrilinear.c,aten/src/THNN/generic/unfold.c,aten/src/THNN/init.cpp,torch/CMakeLists.txt,torch/csrc/DataLoader.cpp,torch/csrc/DataLoader.h,torch/csrc/Device.cpp,torch/csrc/Device.h,torch/csrc/Dtype.cpp,torch/csrc/Dtype.h,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Exceptions.cpp,torch/csrc/Exceptions.h,torch/csrc/Generator.cpp,torch/csrc/Generator.h,torch/csrc/Layout.cpp,torch/csrc/Layout.h,torch/csrc/Module.cpp,torch/csrc/PtrWrapper.cpp,torch/csrc/PtrWrapper.h,torch/csrc/PythonTypes.h,torch/csrc/Size.cpp,torch/csrc/Size.h,torch/csrc/Storage.cpp,torch/csrc/Storage.h,torch/csrc/THP.h,torch/csrc/THP_API.h,torch/csrc/TypeInfo.cpp,torch/csrc/TypeInfo.h,torch/csrc/Types.h,torch/csrc/api/include/torch/nn/pimpl.h,torch/csrc/autograd/VariableTypeManual.cpp,torch/csrc/autograd/VariableTypeUtils.h,torch/csrc/autograd/anomaly_mode.cpp,torch/csrc/autograd/anomaly_mode.h,torch/csrc/autograd/autograd.h,torch/csrc/autograd/edge.h,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/engine.h,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/functions/accumulate_grad.cpp,torch/csrc/autograd/functions/accumulate_grad.h,torch/csrc/autograd/functions/basic_ops.cpp,torch/csrc/autograd/functions/basic_ops.h,torch/csrc/autograd/functions/comm.cpp,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/pybind.h,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/functions/tensor.h,torch/csrc/autograd/functions/utils.cpp,torch/csrc/autograd/functions/utils.h,torch/csrc/autograd/grad_mode.cpp,torch/csrc/autograd/grad_mode.h,torch/csrc/autograd/init.cpp,torch/csrc/autograd/input_buffer.cpp,torch/csrc/autograd/input_buffer.h,torch/csrc/autograd/profiler.cpp,torch/csrc/autograd/profiler.h,torch/csrc/autograd/python_anomaly_mode.cpp,torch/csrc/autograd/python_anomaly_mode.h,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_cpp_function.h,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_engine.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_hook.cpp,torch/csrc/autograd/python_hook.h,torch/csrc/autograd/python_legacy_variable.cpp,torch/csrc/autograd/python_legacy_variable.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/autograd/python_variable_indexing.h,torch/csrc/autograd/saved_variable.cpp,torch/csrc/autograd/saved_variable.h,torch/csrc/autograd/symbolic.h,torch/csrc/autograd/utils/python_arg_parsing.h,torch/csrc/autograd/utils/wrap_outputs.h,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/byte_order.cpp,torch/csrc/copy_utils.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Storage.cpp,torch/csrc/cuda/Storage.h,torch/csrc/cuda/Stream.cpp,torch/csrc/cuda/Stream.h,torch/csrc/cuda/THCP.h,torch/csrc/cuda/Tensor.cpp,torch/csrc/cuda/comm.cpp,torch/csrc/cuda/comm.h,torch/csrc/cuda/nccl.cpp,torch/csrc/cuda/override_macros.h,torch/csrc/cuda/python_comm.cpp,torch/csrc/cuda/python_nccl.cpp,torch/csrc/cuda/python_nccl.h,torch/csrc/cuda/serialization.cpp,torch/csrc/cuda/serialization.h,torch/csrc/cuda/utils.cpp,torch/csrc/cuda/utils.h,torch/csrc/distributed/Module.cpp,torch/csrc/distributed/THDP.h,torch/csrc/distributed/c10d/c10d.h,torch/csrc/distributed/c10d/ddp.h,torch/csrc/generic/Storage.cpp,torch/csrc/generic/Storage.h,torch/csrc/generic/serialization.cpp,torch/csrc/generic/serialization.h,torch/csrc/generic/utils.cpp,torch/csrc/generic/utils.h,torch/csrc/jit/argument_spec.h,torch/csrc/jit/attributes.h,torch/csrc/jit/autodiff.cpp,torch/csrc/jit/autodiff.h,torch/csrc/jit/batched/BatchTensor.cpp,torch/csrc/jit/batched/BatchTensor.h,torch/csrc/jit/constants.cpp,torch/csrc/jit/constants.h,torch/csrc/jit/dynamic_dag.h,torch/csrc/jit/export.cpp,torch/csrc/jit/export.h,torch/csrc/jit/fuser/arg_spec.h,torch/csrc/jit/fuser/codegen.cpp,torch/csrc/jit/fuser/codegen.h,torch/csrc/jit/fuser/compiler.cpp,torch/csrc/jit/fuser/compiler.h,torch/csrc/jit/fuser/cpu/dynamic_library.h,torch/csrc/jit/fuser/cpu/fused_kernel.cpp,torch/csrc/jit/fuser/cpu/fused_kernel.h,torch/csrc/jit/fuser/cpu/resource_strings.h,torch/csrc/jit/fuser/cpu/temp_file.h,torch/csrc/jit/fuser/cuda/fused_kernel.cpp,torch/csrc/jit/fuser/cuda/fused_kernel.h,torch/csrc/jit/fuser/cuda/resource_strings.h,torch/csrc/jit/fuser/executor.cpp,torch/csrc/jit/fuser/executor.h,torch/csrc/jit/fuser/fallback.cpp,torch/csrc/jit/fuser/fallback.h,torch/csrc/jit/fuser/fused_kernel.h,torch/csrc/jit/fuser/interface.cpp,torch/csrc/jit/fuser/interface.h,torch/csrc/jit/fuser/kernel_cache.cpp,torch/csrc/jit/fuser/kernel_cache.h,torch/csrc/jit/fuser/kernel_spec.h,torch/csrc/jit/fuser/partition_desc.h,torch/csrc/jit/fuser/tensor_desc.h,torch/csrc/jit/fuser/tensor_info.h,torch/csrc/jit/graph_executor.cpp,torch/csrc/jit/graph_executor.h,torch/csrc/jit/graph_node_list.h,torch/csrc/jit/hooks_for_testing.cpp,torch/csrc/jit/hooks_for_testing.h,torch/csrc/jit/import.cpp,torch/csrc/jit/import.h,torch/csrc/jit/import_method.cpp,torch/csrc/jit/import_method.h,torch/csrc/jit/init.cpp,torch/csrc/jit/interpreter.cpp,torch/csrc/jit/interpreter.h,torch/csrc/jit/ir.cpp,torch/csrc/jit/ir.h,torch/csrc/jit/ir_views.h,torch/csrc/jit/named_value.h,torch/csrc/jit/node_hashing.cpp,torch/csrc/jit/node_hashing.h,torch/csrc/jit/operator.cpp,torch/csrc/jit/operator.h,torch/csrc/jit/passes/alias_analysis.cpp,torch/csrc/jit/passes/batch_mm.cpp,torch/csrc/jit/passes/batch_mm.h,torch/csrc/jit/passes/canonicalize.cpp,torch/csrc/jit/passes/canonicalize.h,torch/csrc/jit/passes/canonicalize_ops.cpp,torch/csrc/jit/passes/canonicalize_ops.h,torch/csrc/jit/passes/common_subexpression_elimination.cpp,torch/csrc/jit/passes/common_subexpression_elimination.h,torch/csrc/jit/passes/constant_pooling.cpp,torch/csrc/jit/passes/constant_pooling.h,torch/csrc/jit/passes/constant_propagation.cpp,torch/csrc/jit/passes/constant_propagation.h,torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,torch/csrc/jit/passes/create_autodiff_subgraphs.h,torch/csrc/jit/passes/dead_code_elimination.cpp,torch/csrc/jit/passes/dead_code_elimination.h,torch/csrc/jit/passes/erase_number_types.cpp,torch/csrc/jit/passes/erase_number_types.h,torch/csrc/jit/passes/graph_fuser.cpp,torch/csrc/jit/passes/graph_fuser.h,torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp,torch/csrc/jit/passes/inline_autodiff_subgraphs.h,torch/csrc/jit/passes/inplace_check.cpp,torch/csrc/jit/passes/inplace_check.h,torch/csrc/jit/passes/loop_unrolling.cpp,torch/csrc/jit/passes/loop_unrolling.h,torch/csrc/jit/passes/lower_grad_of.cpp,torch/csrc/jit/passes/lower_grad_of.h,torch/csrc/jit/passes/lower_tuples.cpp,torch/csrc/jit/passes/lower_tuples.h,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx.h,torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,torch/csrc/jit/passes/onnx/fixup_onnx_loop.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/peephole.h,torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h,torch/csrc/jit/passes/peephole.cpp,torch/csrc/jit/passes/peephole.h,torch/csrc/jit/passes/python_print.cpp,torch/csrc/jit/passes/python_print.h,torch/csrc/jit/passes/remove_expands.cpp,torch/csrc/jit/passes/remove_expands.h,torch/csrc/jit/passes/remove_inplace_ops.cpp,torch/csrc/jit/passes/remove_inplace_ops.h,torch/csrc/jit/passes/requires_grad_analysis.cpp,torch/csrc/jit/passes/requires_grad_analysis.h,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/passes/shape_analysis.h,torch/csrc/jit/passes/specialize_undef.cpp,torch/csrc/jit/passes/specialize_undef.h,torch/csrc/jit/passes/to_batch.cpp,torch/csrc/jit/passes/to_batch.h,torch/csrc/jit/passes/utils/check_alias_annotation.cpp,torch/csrc/jit/passes/utils/check_alias_annotation.h,torch/csrc/jit/passes/utils/subgraph_utils.cpp,torch/csrc/jit/passes/utils/subgraph_utils.h,torch/csrc/jit/pybind.h,torch/csrc/jit/pybind_utils.h,torch/csrc/jit/python_arg_flatten.cpp,torch/csrc/jit/python_arg_flatten.h,torch/csrc/jit/python_interpreter.cpp,torch/csrc/jit/python_ir.cpp,torch/csrc/jit/python_ir.h,torch/csrc/jit/python_trace",998.0,71,2,9.276335617,49.0,178461.0,285.0,5568645.867735471,5934.0,18459.33333,0.0,,0.0,1
pytorch,34bcd4c2376aca476174c5259816feea7c47a083,518cb6ec7c880690e27b4763ffa13cda8f0d84fe,Adam Paszke,adam.paszke@gmail.com,Sun Oct 09 19:06:29 2016 -0700,1476039989.0,Allow specifying output size in MaxUnpooling,59.0,1.0,"test/test_nn.py,torch/nn/modules/pooling.py",2.0,4,2,0.987137774,8.0,1162.0,2.0,345383.0,241.0,2289.64599,0.0,,0.0,1
pytorch,7b48a7c3f659f4749ebb612110879d4ffccd24e2,519570def878eb48cc9a50183f90f7009c3afdb8,Jerry Zhang,jerryzh@fb.com,Fri Nov 02 21:25:52 2018 -0700,1541193952.0,"Rename dim(i) -> size(i) - 2/2

Summary:
Codemod generated with clangr shard mode, 50 files per diff,
clangr code(dim->size): diffusion/FBS/browse/master/fbcode/caffe2/caffe2/fb/codemods/TensorMethodRename.cpp

Reviewed By: salexspb

Differential Revision: D12896721

fbshipit-source-id: deb0290354a1ffd69d080f0f126479844bf04e3c",141.0,141.0,"caffe2/operators/quantized/int8_concat_op.h,caffe2/operators/quantized/int8_conv_op.h,caffe2/operators/quantized/int8_conv_transpose_op.h,caffe2/operators/quantized/int8_fc_op.h,caffe2/operators/quantized/int8_sigmoid_op.h,caffe2/operators/quantized/int8_softmax_op.h,caffe2/operators/reservoir_sampling.cc,caffe2/operators/reshape_op.h,caffe2/operators/rnn/recurrent_network_op.h,caffe2/operators/rnn/recurrent_op_cudnn.cc,caffe2/operators/segment_reduction_op.h,caffe2/operators/sequence_ops.cc,caffe2/operators/sparse_to_dense_mask_op.h,caffe2/operators/sparse_to_dense_op.h,caffe2/operators/square_root_divide_op.h,caffe2/operators/string_ops.cc,caffe2/operators/string_ops_test.cc,caffe2/operators/utility_ops.h,caffe2/operators/variable_length_sequence_padding.h,caffe2/operators/weighted_sample_op.cc,caffe2/predictor/predictor_test.cc,caffe2/sgd/adam_op.h,caffe2/sgd/ftrl_op.cc,caffe2/sgd/gftrl_op.cc,caffe2/sgd/momentum_sgd_op.h",25.0,6,1,3.768142369,13.0,9020.0,9.0,1016070.72,5118.0,15284.83333,0.0,,0.0,1
pytorch,f9ae296a85c9e3835cd8664d18fea9282c205e58,5195d727b57c19f1d5e201338a062f4d1d0636c1,Brian Hirsh,hirsheybar@fb.com,Thu Sep 24 16:14:00 2020 -0700,1600964040.0,"adding a test for ddp save()/load() (#44906)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44906

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D23825386

Pulled By: bdhirsh

fbshipit-source-id: 2276e6e030ef9cffd78fc78c2ffe34d60a1e160e",86.0,0.0,test/distributed/test_c10d.py,1.0,2,1,0,2.0,3859.0,1.0,1768.0,5396.0,12600.5,0.0,Feature Addition,0.0,1
pytorch,ec86d0b2bacdc90b1e92a859e1345f33b58b4ec3,51b60354a53b9ac83172b7615aa9b12663714b5a,Soumith Chintala,soumith@gmail.com,Fri Aug 25 11:33:03 2017 -0400,1503660783.0,cudnn 7 grouped convolutions,82.0,41.0,"test/test_nn.py,torch/csrc/cudnn/Conv.cpp,torch/csrc/cudnn/Descriptors.h",3.0,4,2,1.175787985,34.0,4809.0,2.0,17478.333333333332,1359.0,15537.56175,0.0,,0.0,1
pytorch,ba694520e5004b74b575614f9d7f86a26436d61b,51d14b6859babbc7646cff1d58852bc4e8efc18f,BowenBao,bowbao@microsoft.com,Thu May 27 19:03:59 2021 -0700,1622142239.0,"[ONNX] Update instance_norm2 symbolic to handle track_running_stats=True (#55051) (#58690)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58690

Fixes [#53887](https://github.com/pytorch/pytorch/issues/53887)
Not input calling running_mean and running_var when track_running_stats=True

Test Plan: Imported from OSS

Reviewed By: driazati

Differential Revision: D28714812

Pulled By: SplitInfinity

fbshipit-source-id: 3f2f2ec9a7eaf8a1432a552d751cbd5974b20195

Co-authored-by: hwangdeyu <deyhuang@qq.com>",69.0,16.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.987885353,3.0,12161.0,1.0,81379.0,12527.0,28410.0,0.0,Corrective,1.0,1
pytorch,569d4fe3f95af913d5cc20ded09c2d000e974cc9,51d8543ac7f640063e7ddc706e9d5cdde695cdbe,James Reed,jamesreed@fb.com,Fri Feb 26 01:05:05 2021 -0800,1614301505.0,"[FX] Use precompiled regex in graph name processing (#52853)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52853

ghstack-source-id: 122531132

Test Plan: waitforsadcastle

Reviewed By: anjali411

Differential Revision: D26668527

fbshipit-source-id: bd34d860cd3a71d3b29f2430df97a0501d542f5b",5.0,2.0,torch/fx/graph.py,1.0,2,1,0,1.0,990.0,1.0,153518.0,9213.0,20527.5,0.0,,0.0,1
pytorch,55844dfdbc26383cc03ac212a515257c6a99594c,51fe53e619ea265d02e3168ffade376fcc08bccc,Khushi,khushiagrawal411@gmail.com,Wed May 10 11:32:45 2023 +0000,1683718365.0,"[opinfo] item (#100313)

Follows #100223

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100313
Approved by: https://github.com/ezyang",79.0,3.0,"test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,test/test_decomp.py,test/test_meta.py,test/test_mps.py,test/test_ops.py,test/test_proxy_tensor.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",9.0,7,2,1.485299575,7.0,48400.0,7.0,288623.8888888889,15654.0,35128.5,0.0,,0.0,1
pytorch,8f5ad00e134190deeac9d9487d3cabef9a460f92,5205cc1c625ae69eafb15a533234a676e168684c,James Reed,jamesreed@fb.com,Wed Jan 20 07:10:59 2021 -0800,1611126659.0,"[FX] Fix NoneType annotation in generated code (#50777)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50777

Test Plan: Imported from OSS

Reviewed By: Chillee

Differential Revision: D25966026

Pulled By: jamesr66a

fbshipit-source-id: 8e36521eee03eade7e1b602e801229c085b03488",6.0,2.0,"test/test_fx.py,torch/fx/graph.py,torch/fx/graph_module.py",3.0,3,2,1.561278124,1.0,2545.0,3.0,1490366.6666666667,8198.0,18524.0,0.0,Corrective,1.0,1
pytorch,f87a4cc23fae6b1561d35103149c4ce5ccdabe76,520982d1df0cf62b464fa4bf5a3ae58327d96d4d,Xiang Gao,qasdfgtyuiop@gmail.com,Mon Aug 05 14:27:43 2019 -0700,1565015263.0,"Zero sized tensor support for repeat_interleave (#23717)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/22753
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23717

Differential Revision: D16623598

Pulled By: mrshenli

fbshipit-source-id: 297a3274fb5a5b2fcc0c3ad601337d7eb29fdca2",12.0,0.0,"aten/src/ATen/native/Repeat.h,test/test_torch.py",2.0,5,2,0.811278124,40.0,12673.0,2.0,3743052.5,10391.0,29688.33333,0.0,Corrective,1.0,1
pytorch,cf3ca13321fb556edb9eb1a27931f662ddc53ff5,5215640a41d616d508a6e3b67a043bcb71d09988,Richard Zou,zou3519@users.noreply.github.com,Tue Nov 21 23:33:41 2017 -0500,1511307221.0,Fix cosine_similarity's output shape (#3811),8.0,1.0,"test/test_nn.py,torch/nn/functional.py",2.0,3,2,0.764204507,37.0,6563.0,2.0,448968.0,2146.0,24124.85823,0.0,Corrective,1.0,1
pytorch,b7bc49ad703faf3ea92aaef804351126e63158da,521894c490289dfdd4cd2e812113e028a61c8169,SsnL,tongzhou.wang.1994@gmail.com,Mon Dec 24 17:08:50 2018 -0800,1545671330.0,"Allow converting char tensor to numpy; add [fi]info.min (#15046)

Summary:
https://github.com/pytorch/pytorch/pull/14710 with test fixed.

Also added `finfo.min` and `iinfo.min` to get castable tensors.

cc soumith
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15046

Reviewed By: soumith

Differential Revision: D13429388

Pulled By: SsnL

fbshipit-source-id: 9a08004419c83bc5ef51d03b6df3961a9f5dbf47",61.0,21.0,"docs/source/type_info.rst,test/test_torch.py,test/test_type_info.py,torch/csrc/TypeInfo.cpp,torch/csrc/utils/tensor_numpy.cpp",5.0,6,3,1.486790737,40.0,10189.0,5.0,2602697.0,6215.0,19268.83333,0.0,Corrective,1.0,1
pytorch,40bb880009e86831d5186f619b33ad390d702b16,522041a0fde5fbbf2a484ea6c7c3fad3c64fe0c2,Andrew Gu,andgu@fb.com,Wed Mar 30 14:08:05 2022 -0700,1648649285.0,"[FSDP] Add full optim state dict (#74215)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74215

###  Overview of API
This PR introduces full optimizer state dict checkpointing.
- This allows users to save the optimizer state for a `torch.nn.Module` (not necessarily a `FullyShardedDataParallel` instance) that contains `FullyShardedDataParallel` instances and later load that optimizer state.
- This supports loading to a module with a different world size, but the `FSDP` wrapping scheme must be the same.

To **save** the optimizer state, run the following (on all ranks):
```
model: torch.nn.Module = ...
optim = torch.optim.Adam(model.parameters(), ...)
# Train for some steps...
full_osd = FSDP.full_optim_state_dict(model, optim)  # returns non-empty dict only on rank 0
if rank == 0:
    torch.save(full_osd, ...)
```
To **load** the optimizer state, run the following (on all ranks):
```
new_model: torch.nn.Module = ...  # may use different world size
full_osd = torch.load(...)
sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)
optim = torch.optim.Adam(new_model.parameters(), ...)
optim.load_state_dict(sharded_osd)
```

To support **multiple parameter groups**, we require using an additional argument `optim_input`, which is the first argument that the user passes into the optimizer constructor.
```
optim_input = ...
optim = torch.optim.Adam(optim_input, ...)
FSDP.full_optim_state_dict(model, optim, optim_input)  # one more argument
...
new_optim_input = ...
new_optim = torch.optim.Adam(new_optim_input, ...)
FSDP.shard_full_optim_state_dict(full_osd, new_model, new_optim_input)  # one more argument
```
One caveat is that the user should be careful of generators, which are exhausted after their first use. The `optim_input` passed into the `FSDP` APIs should be refreshed version of the generator if using generators.

### Test Plan
**`full_optim_state_dict()`**
- [x] `full_optim_state_dict()` for a non-`FSDP` root model matches that of an equivalent local model, up to parameter IDs being rearranged, when optimizer input is `model.parameters()`.
- [x] `full_optim_state_dict()` for a non-`FSDP` root model matches that of an equivalent local model, up to parameter IDs being rearranged, when optimizer input is multiple parameter groups (changing parameter order).

**`shard_full_optim_state_dict()`**
- [x] `shard_full_optim_state_dict()` for a non-`FSDP` root model matches the local `optim.state_dict()` of the same model with halved world size, when optimizer input is `model.parameters()`.
- [x] `shard_full_optim_state_dict()` for a non-`FSDP` root model matches the local `optim.state_dict()` of the same model with halved world size, when optimizer input is multiple parameter groups (changing parameter order).
- [x] `shard_full_optim_state_dict()` raises a `ValueError` when changing the `FSDP` wrapping scheme.

On the AWS cluster, the TTS contribution for these tests is ~45 seconds.

###  Developer Notes
**Relaxing the Problem**
For optimizer state checkpointing, we have relaxed the problem to **not support changing the `FSDP` wrapping scheme** between save and load time. It is unclear how to solve without this relaxation. This was the least restrictive way to relax the problem since it does not affect most expected use cases. Rather, the expected change between save and load time is the **world size**, which this implementation **does support**.

Even with the relaxation, the `optim_input` argument is necessary to determine the `flat_param_id_to_param` mapping, which is important to know which parameter IDs in the flattened space correspond to `FlatParameter`s that hence need to be unflattened.

**Differences with Local Equivalent**
Suppose `full_osd = full_optim_state_dict()` and `local_osd = state_dict()` for a purely local equivalent. The difference between `full_osd` and `local_osd` is that the parameter IDs of unflattened parameters comprising a single flattened parameter are always consecutive in `full_osd`, while they may be non-consecutive in `local_osd`. Suppose in the following that each layer has 1 parameter `param`:
```
FSDP(model)
    layer1
    FSDP(layer2)
    layer3
```
`layer1.param` and `layer3.param` are flattened and attributed to `model`. `layer2.param` is flattened and attributed to itself.
- In `local_osd`, the parameter IDs would be `0: layer1.param`, `1: layer2.param`, and `2: layer3.param`.
- In `full_osd`, the parameter IDs would be `0: layer1.param`, `1: layer3.param`, and `2: layer2.param`. (Parameter IDs of unflattened parameters sharing a flattened parameter are consecutive.)

The idea is that as long as `full_optim_state_dict()` and `shard_full_optim_state_dict()` are internally consistent, then there is no need to match the local equivalent (assuming no change in `FSDP` wrapping).

### Follow-Ups
**API**
- If needed, we can follow-up this PR by adding an argument `key_by_name: bool = False` to both methods that may be set to `True` to key parameters by `str` names instead of `int` parameter IDs. We still need to investigate if keying by name enables changing the `FSDP` wrapping scheme.

**Refactoring**
- In this optimizer state checkpointing, all optimizer state is saved to CPU on rank 0 (set as `OPTIM_TARGET_RANK`). We should unify and refactor these assumptions with model state checkpointing.

**Testing**
- The code path for unused parameters is not tested. The testing and any needed implementation fixes can be done in a follow-up.
- The code path for non-tensor states (e.g. `Adam` `""step""` as `float` instead of as zero-dimension `FloatTensor`) is not tested. However, it is identical to that of zero-dimension tensor states, so I have some confidence. If needed, I can add tests for it in a follow-up.
    - Would I have to write my own optimizer? I do not want to introduce dependencies on third party libraries like Nvidia `apex`.
- We may want to add end-to-end checkpointing tests that include both model state dict and optimizer state dict.

Test Plan: Imported from OSS

Reviewed By: zhaojuanmao

Differential Revision: D35045121

Pulled By: awgu

fbshipit-source-id: 33c650dc960acbd7613d4f444a852b9f76ca4a9b
(cherry picked from commit 2bbc2e344296dc455cf686f3a9b097989504be81)",1462.0,9.0,"test/distributed/fsdp/test_fsdp_optim_state.py,torch/distributed/fsdp/flatten_params_wrapper.py,torch/distributed/fsdp/fully_sharded_data_parallel.py,torch/distributed/fsdp/optim_utils.py,torch/distributed/optim/zero_redundancy_optimizer.py,torch/testing/_internal/common_fsdp.py",6.0,9,2,1.671570573,2.0,4727.0,3.0,803189.75,1806.0,4279.0,0.0,Corrective,1.0,1
pytorch,ecaa208fd63fe2ba4d4398d6c2e7e26a21bd514f,522dca4ab0e7f30beb2f65f47a63d30b31efb274,Eddie Yan,eddiey@nvidia.com,Tue Apr 27 21:47:05 2021 -0700,1619560025.0,"Port `topk` from THC to ATen, migrate most of sort as well (#55392)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/24648

The large tensor codepath is ported, but there is a legacy codepath that depends on an inplace sort in THC that is not callable from `at::`. At first glance, THC `topk` seems to be the only function that uses this `sortKeyValueInplace`.
Is the correct change to wrap `sortKeyValueInplace` in legacy functions for visibility in the `at::` namespace?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55392

Reviewed By: ezyang

Differential Revision: D28014257

Pulled By: ngimel

fbshipit-source-id: e297423c763f0691151cb62a4f5eff4cb31fb2b3",655.0,1210.0,"BUILD.bazel,aten/src/ATen/LegacyTHFunctionsCUDA.h,aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp,aten/src/ATen/native/cuda/Sort.cu,aten/src/ATen/native/cuda/SortUtils.cuh,aten/src/ATen/native/cuda/TensorTopK.cu,aten/src/ATen/native/native_functions.yaml,aten/src/THC/CMakeLists.txt,aten/src/THC/THCSortUtils.cuh,aten/src/THC/THCTensorMath.h,aten/src/THC/THCTensorSort.cu,aten/src/THC/THCTensorSort.cuh,aten/src/THC/THCTensorTopK.cuh,aten/src/THC/generated/THCTensorSortBFloat16.cu,aten/src/THC/generated/THCTensorSortByte.cu,aten/src/THC/generated/THCTensorSortChar.cu,aten/src/THC/generated/THCTensorSortDouble.cu,aten/src/THC/generated/THCTensorSortFloat.cu,aten/src/THC/generated/THCTensorSortHalf.cu,aten/src/THC/generated/THCTensorSortInt.cu,aten/src/THC/generated/THCTensorSortLong.cu,aten/src/THC/generated/THCTensorSortShort.cu,aten/src/THC/generated/THCTensorTopKBFloat16.cu,aten/src/THC/generated/THCTensorTopKByte.cu,aten/src/THC/generated/THCTensorTopKChar.cu,aten/src/THC/generated/THCTensorTopKDouble.cu,aten/src/THC/generated/THCTensorTopKFloat.cu,aten/src/THC/generated/THCTensorTopKHalf.cu,aten/src/THC/generated/THCTensorTopKInt.cu,aten/src/THC/generated/THCTensorTopKLong.cu,aten/src/THC/generated/THCTensorTopKShort.cu,aten/src/THC/generic/THCTensorSort.cu,aten/src/THC/generic/THCTensorSort.h,aten/src/THC/generic/THCTensorTopK.cu,aten/src/THC/generic/THCTensorTopK.h,test/test_sort_and_select.py,torch/testing/_internal/common_methods_invocations.py",37.0,13,3,3.751360451,13.0,21902.0,15.0,30423322.485714287,11355.0,25031.0,0.0,Corrective,1.0,1
pytorch,71b1858de770e613ae72a9213ba46c30b197b5ad,5241cdf546ae5ae55271d163237acc024cc6904f,Sam Gross,colesbury@gmail.com,Tue Dec 05 19:24:11 2017 -0500,1512501851.0,"Implement Variable.numpy() (#4006)

Implement Variable.numpy() and dispatch Tensor.numpy() through Variable.numpy()

Variable.numpy() is disallowed on variables that require grad.",156.0,81.0,"aten/src/ATen/Storage.h,aten/src/ATen/templates/StorageDerived.cpp,aten/src/ATen/templates/StorageDerived.h,setup.py,test/test_autograd.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/autograd/python_variable_numpy.cpp,torch/csrc/autograd/python_variable_numpy.h,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/methods/TensorSerialization.cwrap,torch/tensor.py",12.0,13,4,2.117624989,40.0,9645.0,8.0,820640.4,361.0,1078.405869,0.0,,1.0,1
pytorch,fa32317780e7b6a75089bca80a39a4d331ed49d2,52472508e92f0820bd3b383dbcc9007909c8937a,Pieter Noordhuis,pietern@fb.com,Wed Sep 19 16:49:03 2018 -0700,1537375743.0,"Add env:// rendezvous test (#11782)

Summary:
A missing environment variable raised a missing key error. Now it
raises a more descriptive error of the actual problem, for example:

ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable WORLD_SIZE expected, but not set

Pull Request resolved: https://github.com/pytorch/pytorch/pull/11782

Differential Revision: D9888962

Pulled By: pietern

fbshipit-source-id: 5947e7a7bf7aa45f13bbd7b5e997529f26cc92d6",91.0,12.0,"test/test_c10d.py,torch/distributed/rendezvous.py",2.0,3,2,0.882474452,2.0,897.0,2.0,578676.5,4217.0,11812.83333,0.0,Feature Addition,0.0,1
pytorch,d3cf482c815a35c1cd01bfecf1de7794d659e716,5257d1d64ba8d5faffe8082342d2949c4e1c0e1e,Jing Xu,jing.xu@intel.com,Fri Jul 29 12:57:19 2022 +0000,1659099439.0,"A Launch script with Best Recipe of Deep Learning on Intel Xeon CPU (#63932)

Fixes https://github.com/pytorch/pytorch/issues/63556

Usage: `python -m torch.backends.xeon.launch [--knobs] <script> [script parameters]`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/63932
Approved by: https://github.com/albanD",730.0,0.0,"docs/source/backends.rst,test/backends/xeon/test_launch.py,torch/backends/xeon/__init__.py,torch/backends/xeon/run_cpu.py",4.0,8,3,0.500891804,2.0,127.0,1.0,134867.0,5936.0,13797.5,0.0,Corrective,1.0,1
pytorch,d253cdd8ff194239e86b70293793e814e44da2c0,528ee0fa75a53686c555a03c46975e73aa94798f,Richard Zou,zou3519@gmail.com,Mon Jul 11 14:27:45 2022 -0700,1657549665.0,"Fix composite compliance testing to check for .item() calls (#81060)

Composite compliance is supposed to check if a composite function
calls .item()
([ref](https://github.com/pytorch/pytorch/blob/39db8b3823b8db82396cb979105a83e5e137a02f/torch/testing/_internal/composite_compliance.py#L135-L138)).
This PR fixes that and adds some more documentation.

Why do we need this check? The original motivations are that Tensor subclasses
may not support .item calls (e.g. vmap and ProxyTensor).
There is no way for these subclasses to meaningfully override the .item() calls
in composite functions that exist inside the PyTorch framework without raising
an error* so we should aim to rewrite composite operations to not call .item().

*We're open to other solutions, this is just the one we decided on when we
wrote composite compliance testing and these tests help us keep track of the
failing functionality.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81060
Approved by: https://github.com/ezyang",116.0,9.0,"aten/src/ATen/native/README.md,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/composite_compliance.py",3.0,7,2,0.91786424,9.0,22612.0,3.0,398700.6666666667,5209.0,12292.5,0.0,Corrective,1.0,1
pytorch,827e00c9146ada67eb9702db8ddf37544400a63a,52b2ed65c04b0010522c7fc3f70981ce57b8f1d8,Natalia Gimelshein,ngimel@fb.com,Thu Jun 10 04:01:28 2021 -0700,1623297688.0,"Revert D29007258: Revert D28926135: [pytorch][PR] Refactor Foreach Tests: Unary Functions

Test Plan: revert-hammer

Differential Revision:
D29007258

Original commit changeset: c15f51661641

fbshipit-source-id: 98236153136a5c6b6c2911079b7bd214da6cb424",250.0,139.0,"aten/src/ATen/native/cuda/ForeachUnaryOp.cu,test/test_foreach.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.139727604,2.0,8642.0,1.0,25020.0,12899.0,29262.0,0.0,Perfective,0.0,1
pytorch,bc92ce9e0733c2aaae41febf83d196712ca39327,52b4221bfa61b3d5e0f7157feb4ddb3a77d36015,Iurii Zdebskyi,iuriiz@fb.com,Thu Aug 15 15:37:13 2019 -0700,1565883433.0,"Enabled masked methods for bfloat16 (#24183)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24183

-----------
Fix: Enabled masked select/scatter/fill  for BFloat16 on CPU
Test: via unit tests

Test Plan: Imported from OSS

Differential Revision: D16763461

Pulled By: izdeby

fbshipit-source-id: fe733635a2064e5a088a108ff77c2a1a1487a27c",265.0,216.0,"aten/src/ATen/Declarations.cwrap,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,test/test_torch.py",5.0,6,2,1.32821945,41.0,18318.0,3.0,189501.6,10669.0,30285.83333,0.0,Corrective,1.0,1
pytorch,64a3fbae5e6e4fe5a5b71d065b30549ca7a03847,52b8a581970830ad1b9a0c7ec66d16f2e9eae5b8,Richard Zou,zou3519@gmail.com,Tue Aug 16 16:02:50 2022 -0700,1660665770.0,"[functorch] audit skips and xfails for vjp tests (#83518)

Went through test_vjp, test_grad, test_vjpvjp
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83518
Approved by: https://github.com/samdow",12.0,16.0,functorch/test/test_ops.py,1.0,2,1,0,2.0,1418.0,1.0,9265.0,6466.0,14982.0,0.0,,0.0,1
pytorch,11fc16dc98791dbf550e700a834d956287e9ba7c,52cc073212845419af788a97a3ae17a4f401e6fd,vishwakftw,cs15btech11043@iith.ac.in,Tue Jul 17 15:40:07 2018 -0700,1531842007.0,"Implement reshape_as (#9452)

Summary:
1. Added tests
2. Added doc string
3. Remove view_as redundant definition from tensor.py

Closes #9416
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9452

Differential Revision: D8851794

Pulled By: ezyang

fbshipit-source-id: 0aa0430dd0a174e1a5caddbc50a7e2c9eb7802bc",59.0,13.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/tensor.py",8.0,10,5,1.994698651,43.0,18790.0,6.0,252848.125,2964.0,6969.833333,0.0,Feature Addition,0.0,1
pytorch,6c997538b7c98970792836a25793dabb16aaa990,52de34062912b074b73e11b2dcd64c5dafe878cc,BowenBao,bowbao@microsoft.com,Tue Jul 16 17:50:59 2019 -0700,1563299459.0,"Export torch.masked_fill with onnx::where

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22521

Reviewed By: zrphercule

Differential Revision: D16155168

Pulled By: houseroad

fbshipit-source-id: 5d419f08213324d474b839ba1ae13c799aeee92a",43.0,0.0,"test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.574472312,1.0,4368.0,2.0,521102.75,9953.0,28858.33333,0.0,,0.0,1
pytorch,a0c4aae3d59a3637e7b39acdaf3c231414dc6e87,52ec8b9340bd7fce956817dca75498e5723d1b80,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu Nov 12 00:05:56 2020 -0800,1605139556.0,"Added CUDA support for complex input for torch.triangular_solve (#46916)

Summary:
`torch.triangular_solve` now works for complex inputs on GPU.
I moved the existing tests to `test_linalg.py` and modified them to test complex and float32 dtypes.

Ref. https://github.com/pytorch/pytorch/issues/33152

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46916

Reviewed By: navahgar, agolynski

Differential Revision: D24706647

Pulled By: anjali411

fbshipit-source-id: fe780eac93d2ae1b2549539bb385e5fac25213b3",235.0,144.0,"aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,test/test_autograd.py,test/test_linalg.py,test/test_torch.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",6.0,9,3,1.709684864,45.0,44059.0,1.0,18123.0,6680.0,15166.5,0.0,Feature Addition,0.0,1
pytorch,adbb74ded9a2a15bff7c9cbca93cbbf930341354,533c837833dd5cec712e1c32dff5c389ed9465cf,Peter Bell,peterbell10@live.co.uk,Tue Dec 08 01:16:41 2020 -0800,1607390201.0,"Register OpInfos for torch.fft transforms (#48427)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48427

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D25266218

Pulled By: mruberry

fbshipit-source-id: 406e7ed5956bc7445daf8c027c9b4d2c8ff88fa1",216.0,50.0,"test/test_jit.py,test/test_ops.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_metaprogramming_utils.py",5.0,4,2,1.156394688,14.0,19812.0,5.0,336380.0,7271.0,16402.5,0.0,,0.0,1
pytorch,841ce42dafcb5a43686d8da4d48605d31f97aac2,5343b71a629772093d785079b16040bdf06f83d5,"HE, Tao",sighingnow@gmail.com,Fri Jan 12 20:32:45 2018 -0600,1515789165.0,"More strict shape check on Conv operators. (#4637)

* More strict shape check on Conv operators.

Signed-off-by: HE, Tao <sighingnow@gmail.com>

* Test case for conv's shape check.

Signed-off-by: HE, Tao <sighingnow@gmail.com>

* Fix lint.

Signed-off-by: HE, Tao <sighingnow@gmail.com>",84.0,11.0,"aten/src/THCUNN/generic/SpatialConvolutionMM.cu,aten/src/THCUNN/generic/VolumetricConvolution.cu,aten/src/THNN/generic/SpatialConvolutionMM.c,aten/src/THNN/generic/VolumetricConvolutionMM.c,test/test_nn.py",5.0,7,2,2.288520829,37.0,7450.0,3.0,4104839.8,2278.0,24369.35823,0.0,Corrective,1.0,1
pytorch,3ca272cf5aea72c8120028ac96be9af3cff1ba4d,537d671829211679483da210ea1d42f537826295,Jerry Zhang,jerryzh@fb.com,Fri Oct 26 23:45:35 2018 -0700,1540597535.0,"Renaming size() to numel() - 4/6

Summary: Codemod generated with clangr shard mode, 50 files per diff

Reviewed By: li-roy

Differential Revision: D10866391

fbshipit-source-id: 3badc4e86edaac376918fca8d09dbfa396ac3a2c",244.0,243.0,"caffe2/operators/reduce_front_back_sum_mean_ops.h,caffe2/operators/reduction_ops.cc,caffe2/operators/reduction_ops.h,caffe2/operators/replace_nan_op.h,caffe2/operators/reservoir_sampling.cc,caffe2/operators/reshape_op.h,caffe2/operators/reshape_op_gpu_test.cc,caffe2/operators/resize_op.cc,caffe2/operators/rmac_regions_op.cc,caffe2/operators/rnn/recurrent_network_op.h,caffe2/operators/rnn/recurrent_op_cudnn.cc,caffe2/operators/roi_align_gradient_op.cc,caffe2/operators/roi_align_op.cc,caffe2/operators/roi_align_op_gpu_test.cc,caffe2/operators/roi_align_rotated_op.cc,caffe2/operators/rowmul_op.h,caffe2/operators/scale_op.h,caffe2/operators/segment_reduction_op.h,caffe2/operators/selu_op.cc,caffe2/operators/sequence_ops.cc,caffe2/operators/sequence_ops.h,caffe2/operators/slice_op.h,caffe2/operators/softmax_op.cc,caffe2/operators/softmax_with_loss_op.cc,caffe2/operators/softplus_op.cc,caffe2/operators/sparse_normalize_op.cc,caffe2/operators/sparse_to_dense_mask_op.h,caffe2/operators/sparse_to_dense_op.h,caffe2/operators/spatial_batch_norm_op.h,caffe2/operators/spatial_softmax_with_loss_op.cc,caffe2/operators/stats_ops.cc,caffe2/operators/string_ops.cc,caffe2/operators/string_ops_test.cc,caffe2/operators/stump_func_op.cc,caffe2/operators/stylizer_ops.cc,caffe2/operators/summarize_op.cc,caffe2/operators/swish_op.cc,caffe2/operators/tensor_protos_db_input.h,caffe2/operators/thresholded_relu_op.cc,caffe2/operators/tile_op.h,caffe2/operators/top_k.cc,caffe2/operators/tt_linear_op.h,caffe2/operators/upsample_op.cc,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_cudnn.cc,caffe2/operators/utility_ops_gpu_test.cc,caffe2/operators/utility_ops_test.cc,caffe2/operators/weighted_multi_sampling_op.cc,caffe2/operators/while_op.h",50.0,3,1,4.991603748,17.0,15313.0,22.0,3199671.06,4938.0,14678.33333,0.0,,0.0,1
pytorch,d4c0538be27ac6f537bf1d82559d805baef5101d,53876c46065691348455cbd6b80d6709748e5a36,Peter Goldsborough,peter@goldsborough.me,Fri Mar 09 21:02:02 2018 -0800,1520629322.0,Rewrite run_test.sh in Python (#5615),231.0,212.0,".jenkins/macos-build-test.sh,.jenkins/test.sh,.jenkins/win-test.sh,test/run_test.bat,test/run_test.py,test/run_test.sh",6.0,2,2,1.451359001,37.0,384.0,5.0,2453111.4,985.0,6853.172317,0.0,,0.0,1
pytorch,2f359e7d55f8de14fcd74231fc0f256d9fd8c607,53aa9b8c829edfc4194259f2c14b194171074cf9,James Reed,jamesreed@fb.com,Fri Dec 11 23:43:04 2020 -0800,1607730184.0,"[FX] Move none assignments to same line (#49209)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49209

Test Plan: Imported from OSS

Reviewed By: Chillee

Differential Revision: D25484975

Pulled By: jamesr66a

fbshipit-source-id: 44207be878f95ec9420e87af79833191d5cc0c7e",16.0,9.0,torch/fx/graph.py,1.0,2,1,0,1.0,847.0,1.0,275457.0,7445.0,16699.0,0.0,,0.0,1
pytorch,318ea29a8661d316c13dfc2d3a0230abee91a433,53ac2d46c672bdfd23a36654e0b257ed52f9b118,Kongsea,kongsea@gmail.com,Mon Jul 10 14:35:46 2017 -0500,1499697346.0,Fix typos in docstrings. (#2034),3.0,3.0,"torch/nn/modules/conv.py,torch/nn/modules/pooling.py",2.0,3,1,0.918295834,31.0,1521.0,1.0,322.0,1109.0,17762.4346,0.0,Corrective,1.0,1
pytorch,2fa062002e75c2b900cb4e4b56b59fb8787106b4,53aea60bcee8bea63d2580431dfffb2e96e16e05,James Reed,jamesreed@fb.com,Sat Oct 03 00:05:42 2020 -0700,1601683542.0,"[FX] Make output a non-special Node (#45599)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45599

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D24027586

Pulled By: jamesr66a

fbshipit-source-id: 747c25e3c7668ca45f03bed0be71fd3c9af67286",113.0,79.0,"test/fx/quantization.py,test/test_fx.py,torch/fx/__init__.py,torch/fx/experimental/GraphManipulation.py,torch/fx/experimental/Partitioner.py,torch/fx/experimental/shape_prop.py,torch/fx/experimental/subgraph_creation_example.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/fx/node.py,torch/fx/symbolic_trace.py,torch/quantization/fx/fuse.py,torch/quantization/fx/quantize.py",13.0,7,2,2.889872351,1.0,3511.0,9.0,488066.8461538461,5698.0,13345.0,0.0,,0.0,1
pytorch,30876229a791ae49eaa2fed8eebe72f8dc1feacf,53bfba0d72b02d7a7318761247ba77f77b3942d0,Nikita Karetnikov,nikita@karetnikov.org,Sun Jan 22 23:57:22 2023 +0100,1674431842.0,"[inductor] run CPU and CUDA tests with dynamic shapes (#92667)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92667
Approved by: https://github.com/ezyang",203.0,13.0,"test/dynamo/test_dynamic_shapes.py,test/inductor/test_torchinductor.py,torch/_dynamo/testing.py",3.0,5,2,0.263887551,1.0,6869.0,1.0,105034.0,11564.0,26509.0,0.0,,0.0,1
pytorch,b4a881afac62989d130c8a92b4c83d16ccc7384a,53c71e214233b0e97c1cb2bf02676a9f800c1e91,kshitij12345,kshitijkalambarkar@gmail.com,Tue Sep 13 21:02:53 2022 +0000,1663102973.0,"[functorch] test - vmapjvpvjp (#83375)

Adds `vmapjvpvjp` test to `functorch`

Runtime of the test:
```
= 856 passed, 250 skipped, 16175 deselected, 137 xfailed, 197 warnings in 2231.84s (0:37:11) =
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83375
Approved by: https://github.com/zou3519",160.0,0.0,functorch/test/test_ops.py,1.0,2,1,0,2.0,1484.0,1.0,88181.0,7305.0,17050.0,0.0,Feature Addition,0.0,1
pytorch,38bc10ae25c6fd2f445926fdee148ac19a4a1c08,53e0d7a3bae621ef0d07cfd44957736e9149dd8e,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Fri May 20 14:30:40 2022 +0000,1653057040.0,"Revert ""MPS: Fix some memory leak issues in release pools (#77934)""

This reverts commit 2bc2adf2ba99eff404443e5b230f5a4ac426a21c.

Reverted https://github.com/pytorch/pytorch/pull/77934 on behalf of https://github.com/janeyx99 due to Caused mac arm build breakages on trunk https://hud.pytorch.org/minihud#2bc2adf2ba99eff404443e5b230f5a4ac426a21c",12.0,12.0,"aten/src/ATen/mps/MPSDevice.mm,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Convolution.mm",3.0,7,1,1.28067213,1.0,1005.0,1.0,6261.0,3485.0,8287.0,0.0,Corrective,1.0,1
pytorch,f3367f917e916a9f57461089889f4f4ddbb16d20,53f9fc1802e9a65e825ab487507829923b512a8d,Xiong Wei,xiongw.fnst@fujitsu.com,Fri Apr 09 18:39:02 2021 -0700,1617993542.0,"Port hypot method_tests() to OpInfo (#55140)

Summary:
Related https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55140

Reviewed By: ngimel

Differential Revision: D27562164

Pulled By: mruberry

fbshipit-source-id: fc698ddc624d2abf5d540aac76baa5d398993f1f",14.0,1.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,5011.0,1.0,86498.0,10610.0,23474.5,0.0,,0.0,1
pytorch,e64f40ae5b2a013b6cc4ea57dc8991d1b5342973,53fe804322640653d2dddaed394838b868ce9a26,Edward Z. Yang,ezyang@fb.com,Thu Oct 19 20:34:55 2017 -0700,1508445295.0,"Make ONNX work with new C++ autograd world.

The general strategy is there is a new module, torch.onnx.symbolic, which
contains a function for every ATen method name with the ONNX translation.
While implementing this, I took the opportunity to expunge all references
of 'g' from the public API; instead, it is managed by a global variable in
torch.onnx which tracks the ""current graph"".

Other changes:

- If you pass a Tensor to op as an argument, it will now automatically be
  converted into a Constant ONNX node.  This lets us remove needing to
  implement ONNX

- Rename value to other, wherever there is both a Scalar and Tensor overload.
  This way, keyword dispatch can work uniformly in both cases.

- Deleted any autograd Function classes that both had a symbolic and were ported
  to the new C++ autograd implementation.  There may still be some straggling
  classes that didn't have symbolic.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",566.0,887.0,"test/common.py,test/test_autograd.py,test/test_jit.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,torch/autograd/_functions/basic_ops.py,torch/autograd/_functions/pointwise.py,torch/autograd/_functions/reduce.py,torch/autograd/_functions/tensor.py,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/python_ir.cpp,torch/csrc/jit/tracer.cpp,torch/nn/_functions/thnn/auto_symbolic.py,torch/nn/_functions/thnn/pooling.py,torch/onnx.py,torch/onnx/__init__.py,torch/onnx/symbolic.py",20.0,14,3,3.11499394,39.0,13440.0,4.0,4019.3333333333335,37.0,66.5,0.0,,1.0,1
pytorch,a7ae73a2380c3e45394998d2d1d9bceb14f2ee55,5401159b8f8988c7dc7489d44e71192bb679cf85,Richard Zou,zou3519@gmail.com,Mon Aug 30 22:58:50 2021 -0700,1630364330.0,"OpInfo for nn.functional.interpolate (#61956)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61956

Each mode goes through a different implementation so they are listed as
different variants.

Test Plan: - run tests

Reviewed By: malfet

Differential Revision: D30013751

Pulled By: zou3519

fbshipit-source-id: 4253b40b55667d7486ef2d98b441c13d807ab292",114.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,9345.0,1.0,2854.0,15074.0,34526.5,0.0,,0.0,1
pytorch,f2f6e6c0e8b271a96197a27b315be4ea3994b2a5,542aadd9a7609892e207c1e15de08a975b697752,Adam Paszke,adam.paszke@gmail.com,Sat Aug 25 03:20:04 2018 -0700,1535167204.0,"Stop using symbolic override for tracing RNNs (#10638)

Summary:
This disables the symbolic override hacks and makes tracing emit the recently added ATen ops for RNNs (`aten::lstm`, `aten::gru`, ...). I managed to reuse pretty much all of the translation code for their symbolics.

zdevito
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10638

Differential Revision: D9385830

Pulled By: apaszke

fbshipit-source-id: ff06ef7b1ae7c3b7774825e0991bc3887e1ff59b",199.0,312.0,"aten/src/ATen/native/cudnn/RNN.cpp,torch/nn/_functions/rnn.py,torch/nn/modules/rnn.py,torch/onnx/symbolic.py",4.0,10,2,1.100801727,39.0,3350.0,4.0,341347.75,3651.0,9970.333333,0.0,Feature Addition,0.0,1
pytorch,55af142b449633edbde151643b98d664568c1979,542fbcc127e2b6a230901a77bdfd68365b780bf1,Richard Zou,zou3519@users.noreply.github.com,Mon Mar 12 23:54:49 2018 -0400,1520898889.0,Add optimization to norm for common norms (#5722),54.0,14.0,"aten/src/TH/generic/THTensorMath.c,test/test_torch.py",2.0,5,2,0.977417818,38.0,10321.0,2.0,127351.5,2454.0,24784.35823,0.0,Feature Addition,0.0,1
pytorch,6305a85c0c026056850f5d137ebc4b235f7bfa63,5436134f327fd7b39cc833b3b7509f0ef3473751,Kulin Seth,kulinseth@gmail.com,Wed Jul 06 03:39:20 2022 +0000,1657078760.0,"[MPS] Move the View ops to a separate file and reduce the number of graphs created (#80491)

This is dependent on the PR to go in first: https://github.com/pytorch/pytorch/pull/79939

Remove the data_ptr from the View Graph key which reduces the number of
graphs created significantly.

Don't wait when copying from MPS to MPS tensors

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80491
Approved by: https://github.com/malfet",439.0,459.0,"aten/src/ATen/mps/MPSAllocator.h,aten/src/ATen/mps/MPSAllocator.mm,aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/mps/operations/View.mm,test/test_mps.py",8.0,8,2,2.43009146,1.0,8850.0,3.0,947019.5714285716,5018.0,11914.5,0.0,,0.0,1
pytorch,bef70aa377fe6996567d97f49e71c0f95f17d379,5497b1babba1e17f33b3729b08a51e683233fb9d,Adam Paszke,adam.paszke@gmail.com,Tue Dec 27 16:22:51 2016 +0100,1482855771.0,Use TypeError in invalidArguments,5.0,5.0,"test/test_torch.py,tools/cwrap/plugins/StandaloneExtension.py,torch/csrc/utils.cpp",3.0,6,3,1.521928095,20.0,3233.0,2.0,4066.6666666666665,346.0,2992.696975,0.0,,0.0,1
pytorch,3eff333bff7d85ee4215a3dcde3bb98b305c5200,549c4da9177644f1a68538d33105b09a9ed6c6b0,Edward Yang,ezyang@fb.com,Thu Mar 21 18:08:11 2019 -0700,1553191691.0,"Add a decorator for marking slow tests. (#18231)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18231
ghimport-source-id: 78c230f60c41877fe91b89c8c979b160f36f856b

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18231 Add a decorator for marking slow tests.**

The general strategy:
- It's a normal skip decorator, which triggers a skip if
  PYTORCH_TEST_WITH_SLOW is not set.
- It also annotates the method in question that says it's
  slow.  We use this to implement a catch-all skipper in
  setUp that skips all non-slow tests when
  PYTORCH_TEST_SKIP_FAST is set.

I added a little smoketest to test_torch and showed that I get:

```
Ran 432 tests in 0.017s
OK (skipped=431)
```

when running with PYTORCH_TEST_WITH_SLOW=1 and PYTORCH_TEST_SKIP_FAST=1

CI integration coming in later patch, as well as nontrivial uses of
this decorator.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Differential Revision: D14544441

fbshipit-source-id: 54435ce4ec827193e019887178c09ebeae3ae2c9",30.0,1.0,"test/common_utils.py,test/test_torch.py",2.0,1,1,0.770629069,40.0,11645.0,2.0,425430.0,7606.0,23161.33333,0.0,Feature Addition,1.0,1
pytorch,806239d6bdf61f65897dba3f445c9877b7030b5a,54b4cdeffa9703dcf53e34f7eeb43a08116ae804,Sam Gross,colesbury@gmail.com,Fri Mar 02 19:26:11 2018 -0500,1520018771.0,"Replace all uses of 'Tensor or Variable' with 'Tensor' (#5508)

Replace all uses of 'Tensor or Variable'  and 'Variable or Tensor' with 'Tensor'",73.0,73.0,"torch/autograd/__init__.py,torch/distributions/bernoulli.py,torch/distributions/beta.py,torch/distributions/binomial.py,torch/distributions/categorical.py,torch/distributions/cauchy.py,torch/distributions/chi2.py,torch/distributions/dirichlet.py,torch/distributions/distribution.py,torch/distributions/exponential.py,torch/distributions/fishersnedecor.py,torch/distributions/gamma.py,torch/distributions/geometric.py,torch/distributions/gumbel.py,torch/distributions/kl.py,torch/distributions/laplace.py,torch/distributions/log_normal.py,torch/distributions/multinomial.py,torch/distributions/normal.py,torch/distributions/one_hot_categorical.py,torch/distributions/pareto.py,torch/distributions/poisson.py,torch/distributions/relaxed_bernoulli.py,torch/distributions/relaxed_categorical.py,torch/distributions/studentT.py,torch/distributions/transforms.py,torch/distributions/uniform.py,torch/distributions/utils.py,torch/nn/init.py",29.0,4,1,4.500263459,37.0,3824.0,14.0,1503042.2758620689,585.0,1785.905869,0.0,,0.0,1
pytorch,3aec9f79247d65ecc7338f1875158f357d053c94,54ed6fd3ee1a033b9395ebf07dfabb8e8e66907a,Brian Vaughan,bvaughan@fb.com,Sun Apr 19 13:14:27 2020 -0700,1587302067.0,"Use both absolute and relative tolerance in testing (#34258)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34258

This PR allows both atol and rtol to be specified, uses defaults based on the prior analysis (spreadsheet attached to https://github.com/pytorch/pytorch/pull/32538), but retains the absolute tolerance behavior in cases where precision was previously specified explicitly.

Test Plan: Imported from OSS

Differential Revision: D21110255

Pulled By: nairbv

fbshipit-source-id: 57b3a004c7d5ac1be80ee765f03668b1b13f4a7e",324.0,262.0,"test/quantization/test_qat.py,test/quantization/test_quantized.py,test/quantization/test_quantized_nn_mods.py,test/test_autograd.py,test/test_distributions.py,test/test_jit.py,test/test_jit_fuser.py,test/test_nn.py,test/test_torch.py,test/test_type_promotion.py,test/test_utils.py,torch/testing/__init__.py,torch/testing/_internal/common_utils.py",13.0,5,2,2.485814923,47.0,65450.0,13.0,706285.3076923077,1194.0,3093.5,0.0,,0.0,1
pytorch,00b7d84eb709ba4a6c60b54b2bcd5d269849451b,54f265249ce5601c3794c9feac3719a418c47eb4,Di Wu,allwu@fb.com,Tue Apr 21 18:12:58 2020 -0700,1587492778.0,"Optimize grouped Conv3d performance (#36355)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36355

Resolving issue in https://github.com/pytorch/pytorch/issues/36155, by:
- supporting grouped conv3d in ```slow_conv3d```
- adding a fast path in ```__convolution``` to call ```slow_conv3d``` when
  running grouped conv3d on CPU
- bypassing unfolding when kernel_size = 1

Test Plan:
Added the following test cases in test_nn.py, testing both forward and
backward:
- test_Conv3d_groups_nobias
- test_Conv3d_groups_wbias
- test_Conv_1x1

Imported from OSS

Differential Revision: D20957073

fbshipit-source-id: 29afd1e6be8c484859eaedd51463954e2fdccc38",207.0,30.0,"aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/ConvolutionMM3d.cpp,test/test_nn.py",3.0,5,2,1.260344351,43.0,12738.0,3.0,1096906.3333333333,1250.0,3249.0,0.0,Feature Addition,0.0,1
pytorch,27bbaf633b6056d6257695da9f3c5c3464c03f34,55041eaab6681a9e79e25bd4427ac5fefeed2168,Soumith Chintala,soumith@fb.com,Wed Jul 27 04:20:15 2016 -0400,1469593215.0,python 2.7 compatibility,21.0,14.0,"test/smoke.py,test/test.py,torch/Tensor.py,torch/TensorPrinting.py",4.0,2,2,1.481916078,2.0,2699.0,3.0,1312494.25,3.0,9.5,0.0,,0.0,1
pytorch,d56017a14f34b5130fa70c0cba010e3d2506deb0,553eaaba7c173cbd4507f2929d39d4b61c246bf6,Richard Zou,zou3519@gmail.com,Wed Oct 12 19:27:17 2022 +0000,1665602837.0,"Disable tf32 in functorch transform tests (#86799)

This PR applies a large hammer and disables TF32 in specific functorch transform tests. TF32 isn't precise enough to test correctness.

We could have applied a smaller hammer by disabling TF32 per-OpInfo, but that doesn't seem to have too much additional benefit (e.g. if a convolution batching rule is correct on fp32 then I would expect it to be correct under TF32 modulo precision issues because the actual sequence of PyTorch operators we invoke has not changed, only the backend did).

Test Plan:
- I tested this locally on a machine with A100 GPUs.

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86799
Approved by: https://github.com/malfet",9.0,0.0,"test/functorch/test_ops.py,test/functorch/test_vmap.py",2.0,2,1,0.764204507,1.0,6307.0,2.0,82860.5,8276.0,19644.5,0.0,Corrective,1.0,1
pytorch,669a8acc543dfc8586f5d33d1cf6a1b237e1086f,55432982d2021983bce4696b59880ee9a136f67f,Rong Rong (AI Infra),rongr@fb.com,Wed Apr 14 23:00:11 2021 -0700,1618441211.0,"[OpInfo][take2] move matmul to OpInfo (#55947)

Summary:
This is a reland of https://github.com/pytorch/pytorch/issues/55543 after fixing bfloat16 issues.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55947

Reviewed By: mruberry

Differential Revision: D27765035

Pulled By: walterddr

fbshipit-source-id: b27a769de7686777012194ebbc1f38fc5d4acb67",36.0,11.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,5289.0,1.0,44967.0,10806.0,23867.5,0.0,Corrective,1.0,1
pytorch,8f24306aff7fae0e0a6a95dca5361e95d875e362,559053d3a8e87989ac8b14c0cb2c492428df7a6c,Yangqing Jia,jiayq84@gmail.com,Fri May 13 21:43:48 2016 -0700,1463175828.0,chunky sync,14989.0,4919.0,"caffe2/binaries/convert_caffe_image_db.cc,caffe2/binaries/convert_db.cc,caffe2/binaries/convert_encoded_to_raw_leveldb.cc,caffe2/binaries/db_throughput.cc,caffe2/binaries/fb_run_plan_mpi.cc,caffe2/binaries/inspect_gpus.cc,caffe2/binaries/make_cifar_db.cc,caffe2/binaries/make_image_db.cc,caffe2/binaries/make_mnist_db.cc,caffe2/binaries/print_core_object_sizes.cc,caffe2/binaries/print_registered_core_operators.cc,caffe2/binaries/run_plan.cc,caffe2/binaries/run_plan_mpi.cc,caffe2/binaries/speed_benchmark.cc,caffe2/binaries/split_db.cc,caffe2/binaries/zmq_db_throughput.sh,caffe2/binaries/zmq_feeder.cc,caffe2/contrib/docker-ubuntu-12.04/Dockerfile,caffe2/contrib/docker-ubuntu-14.04/Dockerfile,caffe2/contrib/nccl/cuda_nccl_gpu.cc,caffe2/contrib/nccl/cuda_nccl_gpu.h,caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,caffe2/contrib/nccl/nccl_ops_test.py,caffe2/contrib/nervana/nervana.h,caffe2/contrib/nervana/nervana_c_api.cu,caffe2/contrib/nervana/nervana_c_api.h,caffe2/contrib/nervana/nervana_fc_op_gpu.cc,caffe2/contrib/nervana/nervana_fc_op_gpu_test.cc,caffe2/contrib/nervana/nervana_init_gpu.cc,caffe2/contrib/nervana/nervana_math_gpu.cc,caffe2/contrib/warpctc/ctc_op.cpp,caffe2/contrib/warpctc/ctc_op.h,caffe2/contrib/warpctc/ctc_op_gpu.cpp,caffe2/contrib/warpctc/ctc_ops_test.py,caffe2/core/blob.h,caffe2/core/blob_gpu_test.cc,caffe2/core/blob_serialization.cc,caffe2/core/blob_serialization.h,caffe2/core/blob_serialization_gpu.cc,caffe2/core/blob_test.cc,caffe2/core/common.h,caffe2/core/common_cudnn.cc,caffe2/core/common_cudnn.h,caffe2/core/common_gpu.cc,caffe2/core/common_gpu.h,caffe2/core/context.cc,caffe2/core/context.h,caffe2/core/context_gpu.cc,caffe2/core/context_gpu.h,caffe2/core/context_gpu_test.cc,caffe2/core/context_test.cc,caffe2/core/cuda_memorypool_gpu.cc,caffe2/core/db.cc,caffe2/core/db.h,caffe2/core/flags.cc,caffe2/core/flags.h,caffe2/core/init.cc,caffe2/core/init.h,caffe2/core/init_test.cc,caffe2/core/logging.cc,caffe2/core/net.cc,caffe2/core/net.h,caffe2/core/net_test.cc,caffe2/core/operator.cc,caffe2/core/operator.h,caffe2/core/operator_gradient.h,caffe2/core/operator_schema.cc,caffe2/core/operator_schema.h,caffe2/core/operator_schema_test.cc,caffe2/core/operator_test.cc,caffe2/core/registry.h,caffe2/core/registry_test.cc,caffe2/core/tensor.cc,caffe2/core/tensor.h,caffe2/core/timer.h,caffe2/core/typeid.cc,caffe2/core/typeid.h,caffe2/core/typeid_test.cc,caffe2/core/types.h,caffe2/core/workspace.cc,caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,caffe2/cuda_rtc/pool_op_rtc_gpu.cc,caffe2/db/BREW,caffe2/db/create_db_op.cc,caffe2/db/db_test.cc,caffe2/db/leveldb.cc,caffe2/db/lmdb.cc,caffe2/db/protodb.cc,caffe2/db/rocksdb.cc,caffe2/db/zmqdb.cc,caffe2/image/image_input_op.cc,caffe2/image/image_input_op.h,caffe2/mpi/mpi_common.cc,caffe2/mpi/mpi_common.h,caffe2/mpi/mpi_gpu_test.cc,caffe2/mpi/mpi_ops.cc,caffe2/mpi/mpi_ops.h,caffe2/mpi/mpi_ops_fallback.h,caffe2/mpi/mpi_ops_gpu.cc,caffe2/mpi/mpi_test.cc,caffe2/operators/accumulate_op.cc,caffe2/operators/accumulate_op.h,caffe2/operators/accuracy_op.cc,caffe2/operators/accuracy_op.cu,caffe2/operators/accuracy_op.h,caffe2/operators/clip_op.cc,caffe2/operators/clip_op.cu,caffe2/operators/clip_op.h,caffe2/operators/conv_op.cc,caffe2/operators/conv_op.h,caffe2/operators/conv_op_cudnn.cc,caffe2/operators/conv_op_impl.h,caffe2/operators/conv_pool_op_base.h,caffe2/operators/cross_entropy_op.cc,caffe2/operators/cross_entropy_op.cu,caffe2/operators/cross_entropy_op.h,caffe2/operators/depth_split_op.cc,caffe2/operators/depth_split_op.h,caffe2/operators/dropout_op.cc,caffe2/operators/dropout_op.cu,caffe2/operators/dropout_op.h,caffe2/operators/elementwise_op.cc,caffe2/operators/elementwise_op.h,caffe2/operators/filler_op.cc,caffe2/operators/filler_op.cu,caffe2/operators/filler_op.h,caffe2/operators/fully_connected_op.cc,caffe2/operators/fully_connected_op.h,caffe2/operators/fully_connected_op_gpu_test.cc,caffe2/operators/fully_connected_op_test.cc,caffe2/operators/half_float_ops.cu,caffe2/operators/l2_distance_op.cc,caffe2/operators/l2_distance_op.cu,caffe2/operators/l2_distance_op.h,caffe2/operators/load_save_op.cc,caffe2/operators/load_save_op.h,caffe2/operators/local_response_normalization_op.cc,caffe2/operators/local_response_normalization_op.cu,caffe2/operators/local_response_normalization_op.h,caffe2/operators/loss_op.cc,caffe2/operators/loss_op.h,caffe2/operators/operator_fallback_gpu.h,caffe2/operators/operator_fallback_gpu_test.cc,caffe2/operators/order_switch_ops.cc,caffe2/operators/order_switch_ops.cu,caffe2/operators/order_switch_ops.h,caffe2/operators/pool_op.cc,caffe2/operators/pool_op.cu,caffe2/operators/pool_op.h,caffe2/operators/pool_op_cudnn.cc,caffe2/operators/prefetch_op.h,caffe2/operators/recurrent_op.h,caffe2/operators/recurrent_op_gpu.cc,caffe2/operators/reducer_functors.h,caffe2/operators/relu_op.cc,caffe2/operators/relu_op.cu,caffe2/operators/relu_op.h,caffe2/operators/relu_op_cudnn.cc,caffe2/operators/relu_op_fp16.cu,caffe2/operators/segment_reduction_op.cc,caffe2/operators/sigmoid_op.cc,caffe2/operators/sigmoid_op.cu,caffe2/operators/softmax_op.cc,caffe2/operators/softmax_op.cu,caffe2/operators/softmax_op.h,caffe2/operators/softmax_op_cudnn.cc,caffe2/operators/spatial_batch_norm_op.cc,caffe2/operators/spatial_batch_norm_op.h,caffe2/operators/spatial_batch_norm_op_cudnn.cc,caffe2/operators/summarize_op.cc,caffe2/operators/summarize_op.cu,caffe2/operators/summarize_op.h,caffe2/operators/tanh_op.cc,caffe2/operators/tensor_protos_db_input.cc,caffe2/operators/tensor_protos_db_input.h,caffe2/operators/tensor_protos_db_input_test.cc,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,caffe2/proto/caffe2.proto,caffe2/proto/caffe2_legacy.proto,caffe2/python/__init__.py,caffe2/python/_import_c_extension.py,caffe2/python/caffe2_python.cc,caffe2/python/caffe2_python.h,caffe2/python/caffe2_python_gpu.cc,caffe2/python/caffe_translator.py,caffe2/python/caffe_translator_test.py,caffe2/python/cnn.py,caffe2/python/convnet_benchmarks.py,caffe2/python/convnet_benchmarks_test.py,caffe2/python/core.py,caffe2/python/core_gradients_test.py,caffe2/python/core_test.py,caffe2/python/device_checker.py,caffe2/python/gradient_check_test.py,caffe2/python/gradient_checker.py,caffe2/python/hypothesis_test.py,caffe2/python/hypothesis_test_util.py,caffe2/python/mint/app.py,caffe2/python/model_device_test.py,caffe2/python/muji.py,caffe2/python/muji_test.py,caffe2/python/net_drawer.py,caffe2/python/operator_test/sparse_ops_test.py,caffe2/python/test_util.py,caffe2/python/toy_regression_test.py,caffe2/python/utils.py,caffe2/python/visualize.py,caffe2/python/workspace.py,caffe2/python/workspace_gpu_test.py,caffe2/python/workspace_test.py,caffe2/python/zmq_db_test.py,caffe2/sgd/adagrad_op.cc,caffe2/sgd/adagrad_op.h,caffe2/sgd/adagrad_op_gpu.cu,caffe2/sgd/adam_op.cc,caffe2/sgd/adam_op.h,caffe2/sgd/adam_op_gpu.cu,caffe2/sgd/iter_op.cc,caffe2/sgd/learning_rate_functors.h,caffe2/sgd/learning_rate_op.cc,caffe2/sgd/learning_rate_op.h,caffe2/sgd/rmsprop_op.cc,caffe2/sgd/rmsprop_op.h,caffe2/sgd/rmsprop_op_gpu.cu,caffe2/test/caffe2_gtest_main.cc,caffe2/utils/math.h,caffe2/utils/math_gpu.cu,caffe2/utils/proto_utils.cc,caffe2/utils/proto_utils.h,caffe2/utils/proto_utils_test.cc,docs/installation.md",232.0,22,2,6.656921081,4.0,20770.0,41.0,12006855.98275862,229.0,1566.833333,0.0,,0.0,1
pytorch,0919b5247d85481c3a0258f1e13a6de03d167466,55af142b449633edbde151643b98d664568c1979,James Reed,jamesreed@fb.com,Mon Mar 12 23:01:14 2018 -0700,1520895674.0,"Traceable dispatch for cast methods (#5629)

Previously, methods like int() and long() would fail tracing because they eventually dispatch down to toType, which takes a Type as a parameter. We don't (currently) support tracing ops with Type inputs[0], so this PR adds specializations for the ATen scalar types and dispatches to those directly. These specialized ops can be traced into the IR without needing a Type argument.

A more long-term solution would be to add support for Types in the IR.

* Traceable dispatch for Variable cast methods

* Add ONNX symbolics

* Fix test

* Fix cross-backend copy issue

* Prepend underscores to cast identifiers

* Metaprogram symbolics

* clang-format

* stupid lint

* Add comments for all code fragments",97.0,2.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,tools/autograd/templates/python_variable_methods.cpp,torch/onnx/symbolic.py",4.0,9,3,1.978446262,10.0,2212.0,3.0,139584.75,484.0,2367.0,0.0,Corrective,1.0,1
pytorch,ce096e1adfbe6f87308a8785482718f592c7758b,55fd0ac6366078ff09cfca50e90035679ff0cc42,Richard Zou,zou3519@users.noreply.github.com,Fri Apr 22 13:56:52 2022 -0400,1650635812.0,[functorch] Add functorch-only conversion opinfo variants (pytorch/functorch#734),145.0,1.0,functorch/test/functorch_additional_op_db.py,1.0,2,1,0,1.0,355.0,1.0,0.0,986.0,1367.5,0.0,Feature Addition,0.0,1
pytorch,ecc37184a54a5a2c9cf5814af6c4a3b5dc546fe4,5609c2e59cdea2c6eefc7461d4ad3e4fe0792832,Mike Ruberry,mruberry@devfair044.maas,Fri Jun 18 10:39:22 2021 -0700,1624012762.0,"Adds an OpInfo note (#57428)

Summary:
Like the title says. The OpInfo pattern can be confusing when first encountered, so this note links the Developer Wiki and tracking issue, plus elaborates on the goals and structure of the OpInfo pattern.

cc imaginary-person, who I can't add as a reviewer, unfortunately

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57428

Reviewed By: SplitInfinity

Differential Revision: D29221874

Pulled By: mruberry

fbshipit-source-id: aa73228748c9c96eadf2b2397a8b2ec31383971e",408.0,82.0,"torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",2.0,3,1,0.999987983,2.0,8565.0,1.0,75948.0,13134.0,29707.5,0.0,Feature Addition,0.0,1
pytorch,1666d90161cd342b52bcd8814fb72576c53b37a9,56378e986ca0b217d81f2ee8f5ac3395ecddd5bf,Horace He,horacehe2007@yahoo.com,Fri Jun 25 01:12:07 2021 -0700,1624583527.0,"[functorch] Added batching rules for convolution, conv1d, conv2d, conv3d, etc.",57.0,28.0,"functorch/functorch/csrc/BatchRulesConv.cpp,functorch/test/common_utils.py,functorch/test/test_vmap.py",3.0,4,1,1.257132916,1.0,3332.0,3.0,0.6666666666666666,157.0,308.5,0.0,Feature Addition,0.0,1
pytorch,dca416b578eee9ba0cd272e6626f85a77354a855,563c2719bff5a13d7cd1e3c492a4d01edfc307eb,Justin Chu,justinchuby@users.noreply.github.com,Mon May 16 14:44:24 2022 +0000,1652712264.0,"[ONNX] Refactor to remove inline imports - attempt 2 (#77448)

Re-land
- #77142

(diff: https://github.com/pytorch/pytorch/compare/c08b8f0..justinchuby:justinchu/remove-patch2)

Fixed:
- Delay import symbolic_opsets in the registry.

Tested locally with torchvision
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77448
Approved by: https://github.com/garymm",546.0,510.0,"test/onnx/test_onnx_export.py,test/onnx/test_onnx_opset.py,test/onnx/test_pytorch_onnx_onnxruntime_cuda.py,test/onnx/test_pytorch_onnx_shape_inference.py,test/onnx/test_utility_funs.py,tools/onnx/update_default_opset_version.py,torch/csrc/jit/passes/onnx.cpp,torch/onnx/__init__.py,torch/onnx/_constants.py,torch/onnx/_globals.py,torch/onnx/_patch_torch.py,torch/onnx/onnx_supported_ops.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset14.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",22.0,9,3,2.902700201,15.0,15653.0,2.0,240887.27272727276,3246.0,7810.5,0.0,Corrective,1.0,1
pytorch,2f3b194dc2771b38e852a5ad00c9c64b2188cb45,564456ac4472d44bc848681aa30c79132689ee82,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu Mar 18 12:34:36 2021 -0700,1616070876.0,"Added autograd support for torch.orgqr (#52637)

Summary:
This PR adds autograd support for `torch.orgqr`.

Since `torch.orgqr` is one of few functions that expose LAPACK's naming and all other linear algebra routines were renamed a long time ago, I also added a new function with a new name and `torch.orgqr` now is an alias for it.

The new proposed name is `householder_product`. For a matrix `input` and a vector `tau` LAPACK's orgqr operation takes columns of `input` (called Householder vectors or elementary reflectors) scalars of `tau` that together represent Householder matrices and then the product of these matrices is computed. See https://www.netlib.org/lapack/lug/node128.html.
Other linear algebra libraries that I'm aware of do not expose this LAPACK function, so there is some freedom in naming it. It is usually used internally only for QR decomposition, but can be useful for deep learning tasks now when it supports differentiation.

Resolves https://github.com/pytorch/pytorch/issues/50104

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52637

Reviewed By: agolynski

Differential Revision: D27114246

Pulled By: mruberry

fbshipit-source-id: 9ab51efe52aec7c137aa018c7bd486297e4111ce",342.0,59.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,test/test_ops.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/api/include/torch/linalg.h,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/csrc/jit/passes/normalize_ops.cpp,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",18.0,22,5,3.095002668,33.0,46017.0,13.0,257680.61111111112,9883.0,21879.5,0.0,Feature Addition,0.0,1
pytorch,fbf110293d927ca606a81f09d425338b64fefaac,564de515f5be59384030e1ce307bd9557433ecda,Owen Anderson,resistor@mac.com,Fri May 01 22:10:13 2020 -0700,1588371013.0,"Add an iterator to Block. (#37542)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37542

Differential Revision: D21314421

Pulled By: resistor

fbshipit-source-id: e54d7a8a5c9c1186be59f69b5b8af030fc054b32",94.0,56.0,"test/cpp/tensorexpr/test_llvm.cpp,test/cpp/tensorexpr/test_loopnest.cpp,test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/bounds_inference.cpp,torch/csrc/jit/tensorexpr/cuda_codegen.cpp,torch/csrc/jit/tensorexpr/hash_provider.cpp,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_printer.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_visitor.cpp,torch/csrc/jit/tensorexpr/kernel.cpp,torch/csrc/jit/tensorexpr/llvm_codegen.cpp,torch/csrc/jit/tensorexpr/loopnest.cpp,torch/csrc/jit/tensorexpr/stmt.h",14.0,7,2,2.744717342,2.0,14623.0,12.0,868111.0,1610.0,4221.5,0.0,Feature Addition,0.0,1
pytorch,fa7c81c6403632153412320754ad51ad3b1f58b0,56539f5fe1618ea93f733e196710e8c424f549db,Teng Li,tengli@fb.com,Wed Aug 29 19:54:55 2018 -0700,1535572495.0,"PT1 Distributed Release MileStone No.1 - Completed Distributed Package and CI tests (#10871)

Summary:
The PR includes:
(1) torch.distributed.c10d, which now includes the complete backward compatible frontend API for `torch.distributed`
(2) `env://` init method functionality
(3) Minor change to `test_distributed.py`, which is now a test for `torch.distributed.c10d`.
(4) The old `test_distributed.py' is now moved to `test_distributed_thd`
(5) Miscellaneous bug fixes.
(6) DDP CPU test is removed since c10d doesn't have this support yet, but this is a very easy test after moving DDP CPU's dependency to torch.distributed.c10d.
(7) CI config to test MPI, NCCL, and Gloo backend of c10d

**Now all the distributed test including c10d DDP can pass with the c10d frontend API**

TODO: (in a separate PR)
MPI subgroup support, once this is added, CI group test will be enabled.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10871

Differential Revision: D9554514

Pulled By: teng-li

fbshipit-source-id: fb686ad42258526c8b4372148e82969fac4f42dd",2404.0,118.0,".jenkins/pytorch/build.sh,test/common.py,test/run_test.py,test/test_c10d.py,test/test_distributed.py,test/test_thd_distributed.py,torch/csrc/distributed/c10d/init.cpp,torch/distributed/c10d/__init__.py,torch/distributed/c10d/distributed_c10d.py,torch/distributed/c10d/rendezvous.py,torch/lib/c10d/ProcessGroupMPI.cpp,torch/nn/parallel/distributed_c10d.py",12.0,13,3,1.783355391,40.0,4247.0,8.0,1605601.4,3720.0,10232.33333,0.0,Corrective,1.0,1
pytorch,3711f7c59f772190059ebee7fbd58978e1082267,567362cedbc12caf2b2f52631319020d51ac37ea,Nikita Karetnikov,nikita@karetnikov.org,Sun Feb 19 14:15:11 2023 +0000,1676816111.0,"[inductor] move dynamic shapes tests into a new file (#94971)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94971
Approved by: https://github.com/ezyang",100.0,42.0,"test/inductor/test_torchinductor.py,test/inductor/test_torchinductor_dynamic_shapes.py",2.0,2,1,0.900979971,1.0,7308.0,1.0,178761.0,12630.0,29669.5,0.0,,0.0,1
pytorch,1910c5847edbf2f92debc8f73fc7d9056b9fd9a0,56a41b5998a28566984fee70e1dc9604896bd180,kshitij12345,kshitijkalambarkar@gmail.com,Thu Sep 22 00:21:11 2022 +0000,1663806071.0,"[composite compliance] ctc_loss (#84752)

#Ref #69991

I have mixed feelings about adding new (private) operators. Backends writers will have to override them as well.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84752
Approved by: https://github.com/zou3519",155.0,10.0,"aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/cudnn/LossCTC.cpp,aten/src/ATen/native/native_functions.yaml,functorch/test/test_ops.py,test/test_meta.py,test/test_proxy_tensor.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py,torchgen/native_function_generation.py",9.0,14,6,2.253693324,20.0,39791.0,7.0,5505446.333333333,7561.0,17733.5,0.0,Feature Addition,0.0,1
pytorch,3a02ed822b13bf914f8f84fda241edb950c9312f,56de8853dad19a050bc9ce5f89c3f7572ca8a15c,Elias Ellison,eellison@fb.com,Thu Dec 12 15:52:00 2019 -0800,1576165920.0,"Resubmit overload v2 (#31123)

Summary:
Resubmit of https://github.com/pytorch/pytorch/pull/30356 and https://github.com/pytorch/pytorch/pull/31014 :'(

The last commit contains the fix. There was an internal FBcode error not able to compile the previous `impl_default->second.equal(default_val.second))` line. I tried various fixes in C++ internally but couldn't figure anything out. This is a good example of the programming costs of going from python -> c++ for different types of objects, because the conceptual overhead has expanded in scope from (python) -> (python, c++, pybind).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31123

Differential Revision: D18936128

Pulled By: eellison

fbshipit-source-id: 7d8fd66a6dd4a3e9838f3a0b68c219b6565a9462",309.0,67.0,"aten/src/ATen/core/function_schema.h,test/test_jit.py,torch/csrc/jit/script/init.cpp,torch/jit/__init__.py,torch/jit/_recursive.py",5.0,10,3,1.728580287,15.0,21926.0,3.0,120861.6,13818.0,37653.83333,0.0,Corrective,1.0,1
pytorch,902e4e3926a9333178510f032580e4acd56c40da,57353c9608263df98156a73aaa6ed35a2a2306ad,Brian Hirsh,hirsheybar@fb.com,Wed Nov 23 16:29:08 2022 -0800,1669220948.0,"first draft of input mutation handling for aot autograd (#88817)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88817
Approved by: https://github.com/ezyang, https://github.com/wconstab",1711.0,152.0,"functorch/_src/aot_autograd.py,functorch/_src/partitioners.py,test/dynamo/test_aot_autograd.py,test/dynamo/test_aot_cudagraphs.py,test/functorch/test_aotdispatch.py,test/inductor/test_torchinductor.py,test/inductor/test_torchinductor_opinfo.py,torch/_dynamo/optimizations/training.py,torch/_dynamo/variables/torch.py,torch/_inductor/graph.py,torch/_inductor/ir.py,torch/csrc/DynamicTypes.cpp,torch/csrc/autograd/python_torch_functions_manual.cpp",13.0,13,3,1.537128225,28.0,15993.0,12.0,879866.5384615385,9816.0,22691.5,0.0,,0.0,1
pytorch,9440a8cbec52ce5c2eb9b95b4a8d9f16055d611d,574442ba01bebe1aa0ace49a2874fdb7ac3a9eb1,Nikita Shulga,nshulga@meta.com,Tue Aug 15 03:07:01 2023 +0000,1692068821.0,"CI upgradeapalooza `bionic`->`focal`, `gcc7`->`gcc9`, `clang7`->`clang10` (#105260)

Bionic support was finished back in April 2023, see https://ubuntu.com/blog/ubuntu-18-04-eol-for-devices

And neither gcc-7 nor clang7 are fully compatible with c++17, update minimal tested gcc to gcc9 and clang to clang-10

Note: OpenMP support is  broken in Focal's `clang9`, so move up to a `clang10`

- Suppress `-Wuninitialized` in complex_test as gcc-11 fires a seemingly false-positive warning:
```
In file included from /home/malfet/git/pytorch/pytorch/c10/test/util/complex_test.cpp:1:
/home/malfet/git/pytorch/pytorch/c10/test/util/complex_test_common.h: In member function âvirtual void memory::TestMemory_ReinterpretCast_Test::TestBody()â:
/home/malfet/git/pytorch/pytorch/c10/test/util/complex_test_common.h:38:25: warning: âzâ is used uninitialized [-Wuninitialized]
   38 |     c10::complex<float> zz = *reinterpret_cast<c10::complex<float>*>(&z);
      |                         ^~
/home/malfet/git/pytorch/pytorch/c10/test/util/complex_test_common.h:37:25: note: âzâ declared here
   37 |     std::complex<float> z(1, 2);
      |                         ^
```
- Downgrade `ucc` to 2.15, as 2.16 brings an incompatible libnccl, that causes crash during the initialization
- Install `pango` from condo environment for `doctr` torch bench tests to pass, as one available in the system is too new for conda
- Suppress some functorch tests when used with python-3.8+dynamo, see https://github.com/pytorch/pytorch/issues/107173
Pull Request resolved: https://github.com/pytorch/pytorch/pull/105260
Approved by: https://github.com/huydhn, https://github.com/Skylion007, https://github.com/ZainRizvi, https://github.com/seemethere",292.0,267.0,".ci/docker/build.sh,.ci/docker/common/install_base.sh,.ci/docker/common/install_inductor_benchmark_deps.sh,.github/workflows/docker-builds.yml,.github/workflows/inductor-perf-compare.yml,.github/workflows/inductor-perf-test-nightly.yml,.github/workflows/inductor-periodic.yml,.github/workflows/inductor.yml,.github/workflows/nightly.yml,.github/workflows/periodic.yml,.github/workflows/pull.yml,.github/workflows/slow.yml,.github/workflows/trunk.yml,c10/test/util/complex_test.cpp,c10/test/util/intrusive_ptr_test.cpp,test/functorch/test_vmap.py",16.0,10,4,3.172860183,4.0,10795.0,13.0,8894595.0,18605.0,42156.0,0.0,Non Functional,0.0,1
pytorch,699755e04f8bbb4378a70f03dfe0849094fd0255,574cfe3cf3a88cc688242acfd2bd5eedf42c007b,gchanan,gregchanan@gmail.com,Tue May 02 21:22:02 2017 -0400,1493760122.0,"Improve kthvalue documentation. (#1448)

1) Fix ""kth"" attr specification -- I can't get sphinx to generate `k`th,
but `k` th works with a space, unlike now where the highlighting continues
until the next attr.
2) Specify the size of the return tensors.
3) Add an example of the return tensor sizes with more than 1 dimension.",19.0,2.0,torch/_torch_docs.py,1.0,1,1,0,17.0,4448.0,1.0,9794.0,704.0,8745.817468,0.0,Corrective,1.0,1
pytorch,a8bdce38fefeff29915df77aaab7553bf6ebbe4d,57549b7e442f18f4d02de28d7386c461edfc0a7e,Sam Gross,colesbury@gmail.com,Wed Jan 17 23:27:42 2018 -0500,1516231662.0,"Bind functions with out= arguments in VariableType (#4565)

This adds overrides in VariableType for the xxx_out ATen functions and
implements Python bindings. There is no support for automatic
differentiation. If any of the inputs (or outputs) requires grad, then the
function will throw an exception unless it's running in ""no-grad"" mode.

The bindings for calling torch.xxx functions on Variables are moved to a
different object. Previously, they were static method on VariableBase.
This change prevents users from accidentally calling static methods as if
they were instance methods.",692.0,372.0,"setup.py,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_autograd.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/python_nn_functions.cpp,tools/autograd/templates/python_nn_functions_dispatch.h,tools/autograd/templates/python_torch_functions.cpp,tools/autograd/templates/python_torch_functions_dispatch.h,tools/autograd/templates/python_variable_methods.cpp,tools/autograd/templates/python_variable_methods_dispatch.h,torch/__init__.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/utils/python_arg_parser.h,torch/nn/_functions/vision.py,torch/nn/functional.py",22.0,11,3,3.162214999,40.0,16990.0,15.0,1798588.25,443.0,1366.905869,0.0,Feature Addition,0.0,1
pytorch,7524699d58b3bf3d1ddb560b5c97a1e9309c9825,575e7497f6740ed98337346b12926d4dab4c945b,James Reed,jamesreed@fb.com,Tue Aug 11 16:57:01 2020 -0700,1597165021.0,"Introduce experimental FX library (#42741)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42741

Test Plan: Imported from OSS

Reviewed By: dzhulgakov

Differential Revision: D23006383

Pulled By: jamesr66a

fbshipit-source-id: 6cb6d921981fcae47a07df581ffcf900fb8a7fe8",629.0,0.0,"test/run_test.py,test/test_fx.py,torch/fx/__init__.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/fx/node.py,torch/fx/symbolic_trace.py",7.0,3,2,2.508559844,8.0,718.0,1.0,499900.0,4218.0,9875.0,0.0,Feature Addition,0.0,1
pytorch,1cdb9d2ab50eec301168803e70f5e2bef48776bb,576880febf2fa3aee714620c5bb105cb0150fec4,mfkasim91,firman.kasim@gmail.com,Mon Aug 31 15:19:38 2020 -0700,1598887178.0,"Print all traceback for nested backwards in detect_anomaly (#43626)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/43405.

This pull request adds a feature of printing all tracebacks if a `detect_anomaly` mode detects `nan` in nested backward operations.
The way I did it is by assigning a node as a parent to all nodes it produces during its backward calculation. Then if one of the children produces `nan`, it will print the traceback from the parent and grand parents (if any).

The parent is assigned in `parent_node_` member in `Node` class which is accessible in C++ by function `node->parent()` and in Python by `node.parent_function`.
A node has a parent iff:

1. it is created from a backward operation, and
2. created when anomaly mode and grad mode are both enabled.

An example of this feature:

    import torch

    def example():
        x = torch.tensor(1.0, requires_grad=True)
        y = torch.tensor(1e-8, requires_grad=True)  # small to induce nan in n-th backward
        a = x * y
        b = x * y
        z1 = a / b  # can produce nan in n-th backward as long as https://github.com/pytorch/pytorch/issues/43414 is unsolved
        z = z1 * z1
        gy , = torch.autograd.grad( z , (y,), create_graph=True)
        gy2, = torch.autograd.grad(gy , (y,), create_graph=True)
        gy3, = torch.autograd.grad(gy2, (y,), create_graph=True)
        gy4, = torch.autograd.grad(gy3, (y,), create_graph=True)
        return gy4

    with torch.autograd.detect_anomaly():
        gy4 = example()

with output:

    example.py:16: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
      with torch.autograd.detect_anomaly():
    /home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py:190: UserWarning: Error detected in DivBackward0. Traceback of forward call that caused the error:
      File ""example.py"", line 17, in <module>
        gy4 = example()
      File ""example.py"", line 12, in example
        gy3, = torch.autograd.grad(gy2, (y,), create_graph=True)
      File ""/home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 190, in grad
        return Variable._execution_engine.run_backward(
     (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:61.)
      return Variable._execution_engine.run_backward(
    /home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py:190: UserWarning:

    Traceback of forward call that induces the previous calculation:
      File ""example.py"", line 17, in <module>
        gy4 = example()
      File ""example.py"", line 11, in example
        gy2, = torch.autograd.grad(gy , (y,), create_graph=True)
      File ""/home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 190, in grad
        return Variable._execution_engine.run_backward(
     (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:65.)
      return Variable._execution_engine.run_backward(
    /home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py:190: UserWarning:

    Traceback of forward call that induces the previous calculation:
      File ""example.py"", line 17, in <module>
        gy4 = example()
      File ""example.py"", line 8, in example
        z1 = a / b  # can produce nan in n-th backward as long as https://github.com/pytorch/pytorch/issues/43414 is unsolved
     (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:65.)
      return Variable._execution_engine.run_backward(
    Traceback (most recent call last):
      File ""example.py"", line 17, in <module>
        gy4 = example()
      File ""example.py"", line 13, in example
        gy4, = torch.autograd.grad(gy3, (y,), create_graph=True)
      File ""/home/mfkasim/anaconda2/envs/base3/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 190, in grad
        return Variable._execution_engine.run_backward(
    RuntimeError: Function 'DivBackward0' returned nan values in its 1th output.

cc & thanks to albanD

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43626

Reviewed By: malfet

Differential Revision: D23397499

Pulled By: albanD

fbshipit-source-id: aa7435ec2a7f0d23a7a02ab7db751c198faf3b7d",175.0,5.0,"test/test_autograd.py,torch/csrc/autograd/anomaly_mode.h,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/python_anomaly_mode.cpp,torch/csrc/autograd/python_anomaly_mode.h,torch/csrc/autograd/python_function.cpp",8.0,4,2,2.233350489,43.0,9799.0,7.0,6609444.75,4671.0,10872.5,0.0,Corrective,1.0,1
pytorch,9163e8171e33d5562cb25a251d5f49602f0a8a03,57c18127dc5156e6d8e259e9a816e91b573377a8,BowenBao,bowbao@microsoft.com,Mon Sep 28 20:43:50 2020 -0700,1601325830.0,"[ONNX] Update div export to perform true divide (#44831)

Summary:
related https://github.com/pytorch/pytorch/issues/43787

Now that PyTorch div is actually performing true divide, update onnx export code to stay consistent.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44831

Reviewed By: eellison

Differential Revision: D23880316

Pulled By: bzinodev

fbshipit-source-id: 3bb8db34142ac4fed4039295ad3c4cb79487987f",27.0,23.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.851314689,3.0,8282.0,2.0,544166.5,5512.0,12965.5,0.0,,0.0,1
pytorch,ab1457ad14d3ba02ed0ed144fc1481c92eccb7ab,57e37080cd4b6dc03661d53b008f6a73d6cc38c8,Heitor Schueroff,heitorschueroff@fb.com,Tue Apr 27 14:34:33 2021 -0700,1619534073.0,"Added OpInfo for torch.einsum (#56276)

Summary:
Adds OpInfo testing for torch.einsum.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56276

Reviewed By: mruberry

Differential Revision: D27967095

Pulled By: heitorschueroff

fbshipit-source-id: 60524273d2ca885e7eeb932db3e7fd697ae5ca8e",77.0,33.0,"aten/src/ATen/native/Linear.cpp,test/test_fx.py,test/test_fx_experimental.py,test/test_jit.py,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",6.0,8,3,1.820730534,15.0,34294.0,6.0,261724.16666666663,11321.0,24964.5,0.0,Feature Addition,0.0,1
pytorch,9ec9acc0cd09806e9353f91da8601b07186d8c87,57ffe64cbe67c6ab4e6bf0a73cf801b93c41fe7d,Marcin Elantkowski,marcin.elantkowski@gmail.com,Wed Oct 18 21:38:07 2017 +0200,1508362687.0,"Embedding related fixes (#3128)

* Fix docs for nn.Embedding and F.embedding.
  - add description of 'sparse' argument (#3104)
  - fix F.embedding example (resulted in RuntimeError)
* Make EmbeddingBag a New Style Function.
* Add a functional interface for EmbeddingBag
* Fix failing tests: add max_norm and norm_type to context,
and fix typo in backend call.
* Docfix: remove torch.manual_seed from example code.
* Add a note about using sparse keyword in Embedding function.",171.0,87.0,"torch/nn/_functions/thnn/sparse.py,torch/nn/functional.py,torch/nn/modules/sparse.py",3.0,5,1,1.406673879,35.0,1846.0,2.0,326304.6666666667,763.0,5661.172317,0.0,Corrective,1.0,1
pytorch,93b20b0232c61adf89c0d9b484320079d8632480,580a053832cea61affce5fdb61c737036c8954af,Mike Ruberry,mruberry@fb.com,Wed May 18 13:57:26 2022 +0000,1652882246.0,"[primTorch] Enforces stride metadata (#77542)

This PR...

**Filed the Following Issues**
- https://github.com/pytorch/pytorch/issues/77553
- https://github.com/pytorch/pytorch/issues/77526
- https://github.com/pytorch/pytorch/issues/77600

**Testing**
- Updates test_dtypes to longer attempt to test the backward of sample inputs where no inputs require grad
- Adds a new test_python_reference_errors; it ensures the meta operations for references throw errors as expected
- Updates compare_tensor_meta to better handle CUDA devices, and (temporarily) restricts stride checking to the CUDA device type
- Elementwise unary and elementwise binary operators now have arbitrarily strided reference inputs
- Reference inputs for _like functions are added
- An OpInfo for torch.empty is added
- Reference inputs for torch.clone are added
- A NumPy reference for clone is added
- Adds OpInfos for refs.empty and refs.empty_like

**Prims**
- Renames the ""max"" and ""min"" prims have been renamed to ""maximum"" and ""minimum,"" respectively, to better conform to their ATen names
- Adds the empty, empty_like, full, and full_like prims
- Fixes the elementwise meta function's stride propagation
- Fixes clone's meta function's stride propagation
- Fixes convert_element_type's meta's stride propagation
- Adds a (temporary) _to_dtype pprivate prim that casts a tensor while preserving its stride permutation
- Removes the _set prim comment
- Adds utils.compute_elementwise_output_strides, which computes the correct output strides for elementwise operations
- Corrects an issue where utils.make_contiguous_strides_for was creating the incorrect strides for tensors with no elements

**References**
- Adds the empty, empty_like, full, full_like, and ones_like refs
- Extends make_elementwise_unary_reference to accept an additional callable to perform extra input validation
- Adds an extra validation function to handle refs.neg(BoolTensor)
- Updates the isfinite ref to call ones_like when appropriate
- Models Python scalar handling for elementwise binary operations
- Added a 64 dim check for the amin and amax references
- opmath is now a flag that can be set separately for cpu and CUDA
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77542
Approved by: https://github.com/ezyang",978.0,304.0,"test/test_meta.py,test/test_ops.py,torch/_decomp/decompositions.py,torch/_prims/__init__.py,torch/_prims/utils.py,torch/_prims/wrappers.py,torch/_refs/__init__.py,torch/_refs/nn/functional/__init__.py,torch/testing/_internal/common_methods_invocations.py",9.0,9,2,2.266941058,5.0,26880.0,8.0,261825.22222222225,3364.0,8054.0,0.0,Corrective,1.0,1
pytorch,1cab27d4851b13868fb68d2b6b33111a3c58a518,581a364437baf5cddc279f0845649af61ab998ec,Xiang Gao,qasdfgtyuiop@gmail.com,Mon Sep 21 21:20:51 2020 -0700,1600723251.0,"CUDA BFloat16 unary ops part 1 (#44813)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44813

Reviewed By: mruberry

Differential Revision: D23805816

Pulled By: ngimel

fbshipit-source-id: 28c645dc31f094c8b6c3d3803f0b4152f0475a64",66.0,80.0,"aten/src/ATen/native/cuda/UnaryGeometricKernels.cu,aten/src/ATen/native/cuda/UnaryLogKernels.cu,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,2.23851197,42.0,22671.0,5.0,3417430.0,5292.0,12112.5,0.0,,0.0,1
pytorch,6eaea39867abf6d11bf4d7183923168962be051f,5835ad07cbe9cfb961e6e6c7c00001447502c21a,Igor Fedan,ifedan@fb.com,Mon Oct 28 15:18:11 2019 -0700,1572275891.0,"provide memory format as Contiguous explicitly when calling to clone() (#28029)

Summary:
provide memory format explicitly when calling to clone():
```
clone(MemoryFormat::Contiguous); \\instead of clone()
```

This change is based on https://github.com/pytorch/pytorch/pull/27106
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28029

Differential Revision: D17937468

Pulled By: ifedan

fbshipit-source-id: 0a6a600af76fc616f88893e5db16aabd7981ce14",27.0,18.0,"aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,test/test_torch.py,torch/csrc/autograd/functions/accumulate_grad.cpp",6.0,12,3,2.04359123,40.0,16936.0,6.0,2965924.833333333,12581.0,35001.33333,0.0,,0.0,1
pytorch,32f6a1e2a2e0afb0f94c7f2cfc84fdaae5c90f85,5843fea94dde059db913e26b8f345e41976edf87,BowenBao,bowbao@microsoft.com,Thu Feb 17 18:45:23 2022 -0800,1645123523.0,"[ONNX] Add export support for linalg norm (#66575)

* Add matrix_norm

* Add vector norm

* Fixe flake

* Fixe flake

* nit fixes

* Nit fixes

* Restructure and add comments

Pull Request resolved: https://github.com/pytorch/pytorch/pull/72987",221.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.230863694,4.0,14960.0,2.0,197471.3333333333,848.0,2115.0,0.0,Corrective,1.0,1
pytorch,eccf42fd1532b2a34822118d6fb4fcbfa8c18832,584be86c3f2855c2c0cb8b32edb548cb011b6e30,BowenBao,bowbao@microsoft.com,Tue Nov 26 04:59:25 2019 -0800,1574744365.0,"Try exporting ONNX with force_outplace=False (#29466)

Summary:
This should resolve https://github.com/pytorch/pytorch/issues/29008. This flag has two effects on the tracer.
- Remove the underscroll for inplace operators. E.g.: index_put_ ==> index_put. This is handled in utils.py separately as well.
- Add out as input for backward computation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29466

Reviewed By: hl475

Differential Revision: D18422815

Pulled By: houseroad

fbshipit-source-id: 317b6a3c8a5751fe6fe49d7543e429d281ed0d6d",114.0,7.0,"aten/src/ATen/core/interned_strings.h,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/ir.cpp,torch/csrc/jit/ir.h,torch/csrc/jit/passes/remove_inplace_ops.cpp,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",8.0,11,3,2.631503352,13.0,11179.0,6.0,2599154.125,13493.0,36861.33333,0.0,Feature Addition,0.0,1
pytorch,3cc46002a3b8490a89a0cf46b9150b676ba9a962,586c2e8d624ca9bc1756470d6581d9993de2e085,BowenBao,bowbao@microsoft.com,Thu Feb 04 20:35:27 2021 -0800,1612470927.0,"[ONNX] Fix graph sequence output from loop node (#51305) (#51521)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51521

* Add loop & if node to the list of nodes that could produce sequence type output.
* Switch from `[]` to `at()` to avoid segfault of out of range access.

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D26203112

Pulled By: SplitInfinity

fbshipit-source-id: e990eeed933124b195be0be159271e33fb485063",29.0,27.0,"scripts/onnx/test.sh,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/shape_type_inference.cpp",3.0,9,3,0.904086206,4.0,7515.0,3.0,73851.0,8681.0,19526.0,0.0,Corrective,1.0,1
pytorch,52abd3bd7b65c8e5cb714e7f60fac6b0e6c23f4c,587034617304a3234a7a753a452f85a05068f729,lezcano,lezcano-93@hotmail.com,Tue Mar 23 05:32:36 2021 -0700,1616477556.0,"Port index_copy from TH to ATen (#52203)

Summary:
The design of the `TensorIterator` was similar to that in https://github.com/pytorch/pytorch/pull/50578

Resolves https://github.com/pytorch/pytorch/issues/24670
Resolves https://github.com/pytorch/pytorch/issues/24523

Timings:
<details>
<summary>Script</summary>

```python
from IPython import get_ipython
import torch

torch.manual_seed(13)
torch.set_num_threads(1)

ipython = get_ipython()

cpu = torch.device('cpu')
cuda = torch.device('cuda')

def run_test(ndims, size, index_len, device):
    print(f""ndims: {ndims}, tensor_size: {size}, index_len: {index_len}, device: {device}"")

    x = torch.rand(*([size] * ndims), device=device)
    index = torch.randint(size, (index_len,), dtype=torch.long, device=device)
    for d in range(ndims):
        shape_t = [size] * d + [index_len] + [size] * (ndims - d - 1)
        t = torch.rand(*shape_t, device=device)
        command = ""x.index_copy(d, index, t)""
        if device == cuda:
            command = command + ""; torch.cuda.synchronize()""
        ipython.magic(f""timeit {command}"")
    print()

run_test(3, 700, 10, cpu)
run_test(3, 700, 100, cpu)
run_test(3, 700, 700, cpu)
run_test(2, 10000, 10000, cpu)

run_test(3, 700, 10, cuda)
run_test(3, 700, 100, cuda)
run_test(3, 700, 700, cuda)
run_test(2, 10000, 10000, cuda)
```

</details>

<details>
<summary>CPU ATen</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cpu
327 ms Â± 309 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
329 ms Â± 456 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
378 ms Â± 1.44 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 3, tensor_size: 700, index_len: 100, device: cpu
348 ms Â± 1.52 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
359 ms Â± 330 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
526 ms Â± 686 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 3, tensor_size: 700, index_len: 700, device: cpu
560 ms Â± 19 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
552 ms Â± 2.61 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
932 ms Â± 2.52 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 2, tensor_size: 10000, index_len: 10000, device: cpu
163 ms Â± 5.05 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
302 ms Â± 5.75 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```
</details>

<details>
<summary>CUDA ATen</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cuda
9.63 ms Â± 441 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
9.65 ms Â± 230 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
12.4 ms Â± 881 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 700, index_len: 100, device: cuda
10.8 ms Â± 1.51 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
11 ms Â± 417 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
21.2 ms Â± 18.2 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 3, tensor_size: 700, index_len: 700, device: cuda
19 ms Â± 4.42 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
17.8 ms Â± 493 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
25.8 ms Â± 1.22 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 2, tensor_size: 10000, index_len: 10000, device: cuda
5.59 ms Â± 109 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
10 ms Â± 25.5 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```

</details>

<details>
<summary>CPU TH</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cpu
333 ms Â± 2.42 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
327 ms Â± 1.04 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
366 ms Â± 753 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 3, tensor_size: 700, index_len: 100, device: cpu
336 ms Â± 1.24 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
345 ms Â± 914 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
884 ms Â± 4.32 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 3, tensor_size: 700, index_len: 700, device: cpu
441 ms Â± 3.58 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
514 ms Â± 1.17 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
7.46 s Â± 6.46 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)

ndims: 2, tensor_size: 10000, index_len: 10000, device: cpu
141 ms Â± 233 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
1.13 s Â± 855 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

</details>

<details>
<summary>CUDA TH</summary>

```
ndims: 3, tensor_size: 700, index_len: 10, device: cuda
9.64 ms Â± 390 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)
9.68 ms Â± 3.26 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
13.9 ms Â± 928 ns per loop (mean Â± std. dev. of 7 runs, 100 loops each)

ndims: 3, tensor_size: 700, index_len: 100, device: cuda
11.6 ms Â± 1.38 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
12.1 ms Â± 3.72 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
30.3 ms Â± 27.2 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 3, tensor_size: 700, index_len: 700, device: cuda
27.2 ms Â± 19.8 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
30.6 ms Â± 43.6 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
146 ms Â± 204 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)

ndims: 2, tensor_size: 10000, index_len: 10000, device: cuda
6.5 ms Â± 3.99 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
64.7 ms Â± 55.5 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
```

</details>

According to these we see a slight performance improvement across both CPU and GPU.

cc: nikitaved

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52203

Reviewed By: jbschlosser

Differential Revision: D27066572

Pulled By: mruberry

fbshipit-source-id: 6101e461cf731afa3db042a383b723d3d6bfdc26",289.0,453.0,"aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/LegacyTHFunctionsCPU.h,aten/src/ATen/LegacyTHFunctionsCUDA.h,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/TensorAdvancedIndexing.h,aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cuda/IndexKernel.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/THC/THCTensorIndex.cu,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorIndex.h,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,torch/testing/_internal/common_methods_invocations.py",20.0,18,4,3.434378869,44.0,31860.0,11.0,944957.4,9983.0,22138.5,0.0,Feature Addition,0.0,1
pytorch,588c1787ba0b1a7d7f698a326afc22f78aac2d25,5883523c1d4bb66e8672c84a5717a67f58b65075,Kurt Mohler,kmohler@quansight.com,Tue Oct 05 20:48:45 2021 -0700,1633466925.0,"Remove dtype from torch.Storage and use only torch.ByteStorage (#62030)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62030

Remove dtype tracking from Python Storage interface, remove all the different `<type>Storage` classes except for `ByteStorage`, and update serialization accordingly, while maintaining as much FC/BC as possible

Fixes https://github.com/pytorch/pytorch/issues/47442

* **THE SERIALIZATION FORMAT IS FULLY FC/BC.** We worked very hard to make sure this is the case. We will probably want to break FC at some point to make the serialization structure of tensors make more sense, but not today.
* There is now only a single torch.ByteStorage class. Methods like `Tensor.set_` no longer check that the dtype of storage is appropriate.
* As we no longer know what dtype of a storage is, we've **removed** the size method from Storage, replacing it with nbytes. This is to help catch otherwise silent errors where you confuse number of elements with number of bytes.
* `Storage._new_shared` takes a `nbytes` kwarg and will reject previous positional only calls.  `Storage._new_with_file` and `_set_from_file` require explicit element size arguments.
* It's no longer possible to convert storages to different types using the float/double/etc methods. Instead, do the conversion using a tensor.
* It's no longer possible to allocate a typed storage directly using FloatStorage/DoubleStorage/etc constructors. Instead, construct a tensor and extract its storage. The classes still exist but they are used purely for unpickling.
* The preexisting serialization format stores dtype with storage, and in fact this dtype is used to determine the dtype of the tensor overall.
 To accommodate this case, we introduce a new TypedStorage concept that exists only during unpickling time which is used to temporarily store the dtype so we can construct a tensor. **If you overrode the handling of pickling/unpickling, you MUST add handling for TypedStorage** or your serialization code will degrade to standard file-based serialization.

Original pull request: https://github.com/pytorch/pytorch/pull/59671

Reviewed By: soulitzer, ngimel

Differential Revision: D29466819

Pulled By: ezyang

fbshipit-source-id: 4a14e5d3c2b08e06e558683d97f7378a3180b00e",1509.0,548.0,"test/package/test_package_script.py,test/test_cuda.py,test/test_multiprocessing.py,test/test_serialization.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,tools/pyi/gen_pyi.py,torch/_C/__init__.pyi.in,torch/__init__.py,torch/_storage_docs.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_utils.py,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Module.cpp,torch/csrc/cuda/Module.cpp,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/serialization.cpp,torch/csrc/generic/serialization.h,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_new.cpp,torch/cuda/__init__.py,torch/distributed/pipeline/sync/_balance/profile.py,torch/multiprocessing/reductions.py,torch/overrides.py,torch/package/_directory_reader.py,torch/package/package_exporter.py,torch/package/package_importer.py,torch/serialization.py,torch/storage.py,torch/testing/_internal/common_utils.py,torch/types.py",34.0,21,3,3.911622632,46.0,38892.0,27.0,5869816.617647059,15957.0,36940.5,0.0,Corrective,1.0,1
pytorch,61dcde88a6c2ce60000b59e8e472a9a6c2456f2c,58bb1f747d950c02469f52ebb50f14dfd95a7e78,Thomas Viehmann,tv.code@beamnet.de,Tue May 10 21:03:04 2022 +0000,1652216584.0,"Fix bincount to use acc scalar for the bounds (#76979)

The bounds could overflow when the number of bins is larger than the type can use, e.g. when uint8 inputs want 256 bins.

Thank you, Yang Xiaobo, for reporting a reproducing example in the forums.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76979
Approved by: https://github.com/ngimel",64.0,33.0,"aten/src/ATen/native/cuda/SummaryOps.cu,test/test_reductions.py",2.0,6,2,0.247935838,5.0,3767.0,2.0,2371098.5,3046.0,7310.5,0.0,Corrective,1.0,1
pytorch,c8888574614b06a686c5775d540e9ff6baa648b8,58e4caf80f9fcd86457413d6f84b17b4eda0d7d8,Soumith Chintala,soumith@gmail.com,Thu Jul 13 05:01:04 2017 -0400,1499922064.0,add missing docs,13.0,6.0,"docs/source/nn.rst,torch/_torch_docs.py,torch/nn/functional.py,torch/nn/modules/loss.py",4.0,5,2,1.719553828,31.0,7533.0,1.0,1289.0,1147.0,15141.93107,0.0,Feature Addition,0.0,1
pytorch,97e35858ece696a0627b58c5945eed98bd47a41a,58eb23378f2a376565a66ac32c93a316c45b6131,Chester Liu,skyline75489@outlook.com,Mon Feb 08 21:56:12 2021 -0800,1612821372.0,"Clean up usage of torch._six partially (#49785)

Summary:
See https://github.com/pytorch/pytorch/issues/42919

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49785

Reviewed By: mruberry

Differential Revision: D25963833

Pulled By: bugra

fbshipit-source-id: 11c90d6b8d3f206c9d0a4d8621b773beb10c6ba2",72.0,100.0,"test/run_test.py,test/test_autograd.py,test/test_cuda.py,test/test_dataloader.py,test/test_jit.py,test/test_reductions.py,torch/_jit_internal.py,torch/_six.py,torch/_utils.py,torch/autograd/gradcheck.py,torch/cuda/amp/autocast_mode.py,torch/cuda/amp/grad_scaler.py,torch/cuda/nccl.py,torch/distributed/optim/zero_redundancy_optimizer.py,torch/jit/_recursive.py,torch/jit/_script.py,torch/jit/annotations.py,torch/nn/modules/container.py,torch/nn/modules/utils.py,torch/nn/quantized/modules/utils.py,torch/onnx/utils.py,torch/optim/optimizer.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_utils.py,torch/utils/data/_utils/collate.py,torch/utils/data/_utils/pin_memory.py,torch/utils/data/_utils/worker.py,torch/utils/data/dataloader.py,torch/utils/data/sampler.py",29.0,19,2,4.392853578,47.0,47494.0,24.0,4199982.448275862,8771.0,19731.5,0.0,Corrective,1.0,1
pytorch,24fe4b8af3cad219438b9ab7ba95f42bf0363a17,58f507f9e3d50122592e17c6e2ee7e1807a4dc61,Adam Paszke,adam.paszke@gmail.com,Tue Sep 06 17:09:57 2016 -0700,1473181797.0,Add file descriptor sharing mode to multiprocessing,759.0,133.0,"test/run_test.sh,test/test_multiprocessing.py,torch/Storage.py,torch/Tensor.py,torch/csrc/Module.cpp,torch/csrc/generic/StorageMethods.cpp,torch/lib/libshm/core.cpp,torch/lib/libshm/manager.cpp,torch/multiprocessing/__init__.py,torch/multiprocessing/_storage.py,torch/multiprocessing/_tensor.py,torch/multiprocessing/common.py,torch/multiprocessing/pool.py,torch/multiprocessing/queue.py",14.0,7,2,3.224683847,7.0,1916.0,1.0,5352.0,154.0,2113.679257,0.0,Feature Addition,0.0,1
pytorch,afb7a162fbc95901f569d2108d9807887e2299aa,590619ab8c2d237d4e0b55c8cc3552932afe7da5,jfc4050,jfc4050@gmail.com,Wed Aug 28 17:55:49 2019 -0700,1567014949.0,"Support all_reduce a list of same-device tensors #21640 (#24949)

Summary:
addresses https://github.com/pytorch/pytorch/issues/21640 for CPU tensors and the Gloo backend.

Questions:
- ~~currently takes `AllreduceOptions`, since all of the options are the same. Would it be better to make a new `AllreduceCoalescedOptions` class?~~
- ~~I decided to inherit from `ProcessGroupGloo::AsyncWork` instead of `AsyncAllreduceWork` to shorten the inheritance chain a bit and for consistency with existing classes. However, this means that the two `getFunction` methods are copy-pasted. Would inheriting from `AsyncAllreduceWork` be preferable?~~
- ~~should the work class be named `AsyncCoalescedAllreduceWork` or `AsyncAllreduceCoalescedWork`?~~

thank you!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24949

Differential Revision: D17055580

Pulled By: mrshenli

fbshipit-source-id: e63b5fcaec6021053ea960776a09ee8cf11d1ec2",497.0,0.0,"test/test_c10d.py,test/test_distributed.py,torch/csrc/distributed/c10d/init.cpp,torch/distributed/distributed_c10d.py,torch/lib/c10d/ProcessGroup.hpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/ProcessGroupGloo.hpp,torch/lib/c10d/ProcessGroupMPI.cpp,torch/lib/c10d/ProcessGroupMPI.hpp,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/lib/c10d/Types.hpp",12.0,8,2,2.329066807,29.0,11143.0,7.0,4206824.333333333,10961.0,30998.83333,0.0,Feature Addition,0.0,1
pytorch,7794254851d9228b8c0daa2a8bf3d6a8a3672746,590d3e5774110e4657dcaa6acdb387ef69e41b47,Michael Suo,suo@fb.com,Mon Jun 27 04:26:29 2022 -0700,1656303989.0,"[ci] skip slow gradcheck jobs (#80311)

Skip to restore the CI while we investigate
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80311
Approved by: https://github.com/janeyx99",2.0,0.0,.github/workflows/periodic.yml,1.0,2,1,0,1.0,178.0,1.0,431032.0,4760.0,11238.0,0.0,,0.0,1
pytorch,ee73c752c67116d36cf268dde011b77df704d507,593295daacc28ed016f1d7456bd2ecd21c786544,Wenlei Xie,wxie@fb.com,Fri Mar 26 20:54:01 2021 -0700,1616792041.0,"Migrate kernels with TensorOptions to C10 full dispatcher (#54539)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54539

Codemod commands generated by https://github.com/pytorch/pytorch/pull/54468

ghstack-source-id: 125018630

# Facebook:
The following 2 files are changed on fb side:
```
// Should be hidden
```

Test Plan: buck build //caffe2/aten/...

Reviewed By: smessmer

Differential Revision: D27273744

fbshipit-source-id: 35c1bff63189477645008caaf0dc794096e3fcc4",917.0,243.0,"aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/Blas.cpp,aten/src/ATen/native/QuantizedLinear.cpp,aten/src/ATen/native/SobolEngineOps.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorConversions.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/cuda/MultinomialKernel.cu,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/layer_norm_kernel.cu,aten/src/ATen/native/cudnn/ConvShared.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/group_norm.cpp,aten/src/ATen/native/layer_norm.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/TensorFactories.cpp,aten/src/ATen/native/quantized/cpu/qadd.cpp,aten/src/ATen/native/quantized/cpu/qchannel_shuffle.cpp,aten/src/ATen/native/quantized/cpu/qconv.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,torch/csrc/jit/runtime/register_special_ops.cpp,torch/csrc/jit/runtime/static/ops.cpp,torch/lib/c10d/reducer.cpp",31.0,18,2,3.567339025,13.0,28965.0,16.0,1922000.8709677416,10107.0,22375.5,0.0,,1.0,1
pytorch,9c0caf0384690cb67dcccb7066ece5184f72ca78,594a66d778a660faed0b0fbbe1dd8c2c318707ff,Mike Ruberry,mruberry@fb.com,Wed Feb 10 11:11:14 2021 -0800,1612955474.0,"Warn about floor_divide performing incorrect rounding (#50281) (#50281)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50281

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51745

Test Plan: Imported from OSS

Reviewed By: ngimel

Pulled By: mruberry

Differential Revision: D26257855

fbshipit-source-id: e5d497cf07b0c746838ed081c5d0e82fb4cb701b",304.0,94.0,"aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/cuda/Indexing.cu,aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu,test/jit/test_save_load.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_binary_ufuncs.py,test/test_jit.py,test/test_ops.py,test/test_sparse.py,test/test_torch.py,torch/_torch_docs.py,torch/csrc/jit/frontend/builtin_functions.cpp,torch/nn/functional.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset7.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/testing/__init__.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",20.0,18,3,3.596843982,48.0,65284.0,18.0,1779222.55,8827.0,19822.5,0.0,Corrective,0.0,1
pytorch,1aaa24d99bbb3d8476f7c992aa557ce4f9327d5b,5964394a4ce2131d60cd2f01dd0e098ac3dcb923,Alykhan Tejani,alykhan.tejani@gmail.com,Tue Jul 04 19:51:00 2017 +0100,1499197860.0,return empty iter when tensor is empty,7.0,1.0,"test/test_torch.py,torch/tensor.py",2.0,2,2,0.954434003,32.0,4532.0,1.0,2499.0,1097.0,16353.9346,0.0,,0.0,1
pytorch,92c6690b9ca36311d96fd1d0ec59bbfab7cead9e,596bb41163231a0a2fc66fa54cfd3aa282483477,Kulin Seth,kulin_seth@apple.com,Wed Jul 20 14:27:54 2022 +0000,1658327274.0,"[MPS] Get the correct size of the view tensor when copying from cpu to mps  (#81730)

Fixes: https://github.com/pytorch/pytorch/issues/81567, https://github.com/pytorch/pytorch/issues/80844
* Get the correct size of the view tensor when copying from cpu to mps

* Use 'computeStorageNbytesContiguous' to get the size just when src is a view

* Add asserts and tests to check for storage_offset

* Add testcase for https://github.com/pytorch/pytorch/issues/80844

* Replace assert_allclose with assertEqual

* Replace TORCH_CHECK with TORCH_INTERNAL_ASSERT
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81730
Approved by: https://github.com/razarmehr, https://github.com/albanD",77.0,11.0,"aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Copy.mm,test/test_mps.py",3.0,7,2,1.181899058,1.0,7282.0,2.0,535383.3333333334,5519.0,12951.5,0.0,Corrective,1.0,1
pytorch,a38597967775f94c38a94a399cf62de63ac80b06,5989b05ecce4c41d69b4d41280a5ee47d2bb9fa4,Sam Gross,sgross@fb.com,Mon Oct 16 18:34:58 2017 -0700,1508178898.0,Enable ATen implementation of some NN functions and Variable methods,190.0,632.0,"test/test_autograd.py,tools/autograd/derivatives.yaml,torch/autograd/gradcheck.py,torch/autograd/variable.py,torch/csrc/utils/python_arg_parser.cpp,torch/nn/functional.py",6.0,8,3,1.500306636,38.0,6004.0,2.0,5024.166666666667,34.0,53.5,0.0,,0.0,1
pytorch,4caca7a15b6edd9f99bba1fd0a5b8fb64ed991c3,59b10036d557cf99df90a25d3a3a04d6312ac4f6,Mike Ruberry,mruberry@devfair044.maas,Thu Jun 17 13:33:48 2021 -0700,1623936828.0,"Unifies OpInfo dtype tests (#60157)

Summary:
Simplifies the OpInfo dtype tests and produces nicer error messages, like:

```
AssertionError: Items in the first set but not the second:
torch.bfloat16
Items in the second set but not the first:
torch.int64 : Attempted to compare [set] types: Expected: {torch.float64, torch.float32, torch.float16, torch.bfloat16}; Actual: {torch.float64, torch.float32, torch.float16, torch.int64}.
The supported dtypes for logcumsumexp on cuda according to its OpInfo are
        {torch.float64, torch.float32, torch.float16, torch.int64}, but the detected supported dtypes are {torch.float64, torch.float32, torch.float16, torch.bfloat16}.
        The following dtypes should be added to the OpInfo: {torch.bfloat16}. The following dtypes should be removed from the OpInfo: {torch.int64}.
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60157

Reviewed By: ngimel

Differential Revision: D29188665

Pulled By: mruberry

fbshipit-source-id: e84c9892c6040ea47adb027cfef3a6c0fd2f9f3c",833.0,773.0,"test/test_ops.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.996433812,2.0,9433.0,3.0,828840.0,13096.0,29641.5,0.0,Feature Addition,0.0,1
pytorch,dedcc30c84f126bf25923bd2029297a949c43272,59b10f792985db8875ed972af98329b36a463b55,Zafar,cc.rafaz@zafar.cc,Tue Aug 11 04:03:47 2020 -0700,1597118627.0,"[quant] Sorting the list of dispathes (#42758)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42758

Test Plan: Imported from OSS

Reviewed By: vkuzo

Differential Revision: D23011764

Pulled By: z-a-f

fbshipit-source-id: df87acdcf77ae8961a109eaba20521bc4f27ad0e",63.0,66.0,"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h",2.0,7,1,0.918295834,2.0,2837.0,2.0,1217405.0,4207.0,9853.5,0.0,,0.0,1
pytorch,8c14630e35675d1c55d893bf5e49234061235adb,59b23d79c6e6f8d51aeb1bcdbbc536f65252c5c4,ngimel,ngimelshein@nvidia.com,Fri Jan 13 18:40:27 2017 -0800,1484332827.0,"fix cudnn rnn batch_first with tests (#445)

* fix cudnn rnn batch_first with tests",31.0,25.0,"test/test_nn.py,torch/backends/cudnn/rnn.py",2.0,4,2,0.714727473,21.0,1920.0,2.0,501502.0,319.0,6405.724559,0.0,Corrective,1.0,1
pytorch,e46d942ca68b70075499008c30d14c77769ea22f,59b9eeff495fca1e52451760b97da61d8025f0ce,Adam Paszke,adam.paszke@gmail.com,Mon Dec 19 22:18:05 2016 +0100,1482185885.0,Expose gather and equals for CUDA tensors,11.0,7.0,"test/test_cuda.py,tools/cwrap/plugins/THPPlugin.py,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/methods/Tensor.cwrap",4.0,8,3,1.891061112,17.0,1805.0,2.0,237074.25,241.0,2380.906855,0.0,,0.0,1
pytorch,65a8ac0b8e3bb8674aa97370c8fdf73e88477821,59bda9a8c42e77c7512ac8700d3968d68ff848e2,Tongzhou Wang,SsnL@users.noreply.github.com,Tue Apr 10 14:37:01 2018 -0400,1523371021.0,"Fix reflection padding boundary checks (#6438)

* Fix Reflection padding boundary checks

* Improve padding docs

* fix lint",282.0,68.0,"aten/src/THCUNN/generic/SpatialReflectionPadding.cu,aten/src/THCUNN/generic/TemporalReflectionPadding.cu,aten/src/THNN/generic/SpatialReflectionPadding.c,aten/src/THNN/generic/TemporalReflectionPadding.c,docs/source/nn.rst,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/padding.py",8.0,12,4,1.515791268,38.0,11495.0,4.0,705863.625,850.0,1995.805292,0.0,Corrective,1.0,1
pytorch,f13fadd510fd47b1463117e7ebb19faa8977cccd,59c42595e0c3397c25e56442de40a964349a76ad,iurii zdebskyi,47012416+izdeby@users.noreply.github.com,Thu Jun 27 15:54:44 2019 -0700,1561650884.0,"Enabled gather and scatter for bool tensor (#21924)

Summary:
- moving stuff around in order to enable bool.
- Added implementation of atomicAdd(bool, bool)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21924

Differential Revision: D15883711

Pulled By: izdeby

fbshipit-source-id: 733f35c2bc3d87cec9f9687d72b62d2d2cd7c03e",167.0,124.0,"aten/src/ATen/Declarations.cwrap,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/THC/THCAtomics.cuh,aten/src/THC/THCTensorMath.h,aten/src/THC/THCTensorScatterGather.cu,test/test_cuda.py,test/test_torch.py",8.0,7,2,1.12328567,41.0,19536.0,8.0,3014114.625,9661.0,28114.83333,0.0,Feature Addition,0.0,1
pytorch,46bc43a80f2b5a4fc20c729fb0c65cd595de5f38,59d66e6963437f48eba7990d902ede622ea70e1f,Zeming Lin,ebetica0@gmail.com,Wed Jan 04 23:43:41 2017 -0500,1483573421.0,Sparse Library (#333),3362.0,100.0,"setup.py,test/run_test.sh,test/test_sparse.py,test/test_torch.py,tools/cwrap/plugins/THPPlugin.py,torch/__init__.py,torch/csrc/Module.cpp,torch/csrc/ModuleSparse.cpp,torch/csrc/THP.h,torch/csrc/Tensor.h,torch/csrc/autograd/variable.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/ModuleSparse.cpp,torch/csrc/cuda/THCP.h,torch/csrc/cuda/Tensor.h,torch/csrc/cuda/override_macros.h,torch/csrc/cuda/undef_macros.h,torch/csrc/cuda/utils.h,torch/csrc/generic/SparseTensor.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/Tensor.h,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/methods/SparseTensor.cwrap,torch/csrc/generic/methods/TensorMath.cwrap,torch/csrc/generic/utils.cpp,torch/csrc/generic/utils.h,torch/csrc/utils.h,torch/lib/THCS/CMakeLists.txt,torch/lib/THCS/THCS.h,torch/lib/THCS/THCSGenerateAllTypes.h,torch/lib/THCS/THCSGenerateByteType.h,torch/lib/THCS/THCSGenerateCharType.h,torch/lib/THCS/THCSGenerateDoubleType.h,torch/lib/THCS/THCSGenerateFloatType.h,torch/lib/THCS/THCSGenerateFloatTypes.h,torch/lib/THCS/THCSGenerateHalfType.h,torch/lib/THCS/THCSGenerateIntType.h,torch/lib/THCS/THCSGenerateLongType.h,torch/lib/THCS/THCSGenerateShortType.h,torch/lib/THCS/THCSTensor.c,torch/lib/THCS/THCSTensor.cu,torch/lib/THCS/THCSTensor.h,torch/lib/THCS/cmake/FindMAGMA.cmake,torch/lib/THCS/cmake/select_compute_arch.cmake,torch/lib/THCS/generic/THCSTensor.c,torch/lib/THCS/generic/THCSTensor.cu,torch/lib/THCS/generic/THCSTensor.h,torch/lib/THCS/generic/THCSTensorMath.cu,torch/lib/THCS/generic/THCSTensorMath.h,torch/lib/THS/CMakeLists.txt,torch/lib/THS/THS.h,torch/lib/THS/THSGenerateAllTypes.h,torch/lib/THS/THSGenerateFloatTypes.h,torch/lib/THS/THSGenerateIntTypes.h,torch/lib/THS/THSTensor.c,torch/lib/THS/THSTensor.h,torch/lib/THS/generic/THSTensor.c,torch/lib/THS/generic/THSTensor.h,torch/lib/THS/generic/THSTensorMath.c,torch/lib/THS/generic/THSTensorMath.h,torch/lib/build_all.sh,torch/nn/_functions/thnn/sparse.py,torch/nn/modules/sparse.py,torch/sparse/__init__.py",64.0,21,3,5.01787168,22.0,8534.0,3.0,182561.16,359.0,3084.696975,0.0,,0.0,1
pytorch,b4a098f1fbde6db17c5be61b6d78473c2d65b688,59d794b2c35d14bbe10c33e166d598e56df1c083,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu May 06 11:43:10 2021 -0700,1620301390.0,"Port CPU torch.ormqr to ATen (#57315)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57315

This PR ports `torch.ormqr` from TH to ATen.
CUDA path will be implemented in a follow-up PR.
With ATen port, support for complex and batched inputs is added.
The tests are rewritten and OpInfo entry is added.

We can implement the least squares solver with geqrf + ormqr +
triangular_solve. So it's useful to have this function renewed at least for the
internal code.

Resolves https://github.com/pytorch/pytorch/issues/24748

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D28242070

Pulled By: mruberry

fbshipit-source-id: f070bb6ac2f5a3269b163b22f7354e9089ed3061",373.0,189.0,"aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/LegacyTHFunctionsCPU.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THLapack.cpp,aten/src/TH/generic/THLapack.h,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorLapack.h,test/test_linalg.py,torch/_torch_docs.py,torch/linalg/__init__.py,torch/testing/_internal/common_methods_invocations.py",14.0,11,3,2.91053962,33.0,41963.0,8.0,440142.0714285714,11727.0,26478.0,0.0,Feature Addition,0.0,1
pytorch,474d7ec43b292edb45576775d07a9cfa5e06a727,59dd12042e4e48d17bc6b1e04a7b51f2f1336af5,Raghavan Raman,raghavanr@fb.com,Tue Aug 03 18:43:07 2021 -0700,1628016187.0,"[nnc] Removed const from all fields in IR. (#62336)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62336

This PR was generated by removing `const` for all types of nodes in NNC IR, and fixing compilation errors that were the result of this change.

This is the first step in making all NNC mutations in-place.

Test Plan: Imported from OSS

Reviewed By: iramazanli

Differential Revision: D30049829

Pulled By: navahgar

fbshipit-source-id: ed14e2d2ca0559ffc0b92ac371f405579c85dd63",2368.0,2488.0,"test/cpp/tensorexpr/test_cpp_codegen.cpp,test/cpp/tensorexpr/test_cuda.cpp,test/cpp/tensorexpr/test_expr.cpp,test/cpp/tensorexpr/test_ir_verifier.cpp,test/cpp/tensorexpr/test_loopnest.cpp,test/cpp/tensorexpr/test_memdependency.cpp,test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tutorial.cpp,torch/csrc/jit/tensorexpr/analysis.h,torch/csrc/jit/tensorexpr/block_codegen.cpp,torch/csrc/jit/tensorexpr/block_codegen.h,torch/csrc/jit/tensorexpr/bounds_inference.cpp,torch/csrc/jit/tensorexpr/bounds_inference.h,torch/csrc/jit/tensorexpr/bounds_overlap.cpp,torch/csrc/jit/tensorexpr/bounds_overlap.h,torch/csrc/jit/tensorexpr/codegen.cpp,torch/csrc/jit/tensorexpr/codegen.h,torch/csrc/jit/tensorexpr/cpp_codegen.cpp,torch/csrc/jit/tensorexpr/cpp_codegen.h,torch/csrc/jit/tensorexpr/cuda_codegen.cpp,torch/csrc/jit/tensorexpr/cuda_codegen.h,torch/csrc/jit/tensorexpr/eval.cpp,torch/csrc/jit/tensorexpr/eval.h,torch/csrc/jit/tensorexpr/exceptions.h,torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/external_functions.cpp,torch/csrc/jit/tensorexpr/half_support.h,torch/csrc/jit/tensorexpr/hash_provider.cpp,torch/csrc/jit/tensorexpr/hash_provider.h,torch/csrc/jit/tensorexpr/ir.cpp,torch/csrc/jit/tensorexpr/ir.h,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_mutator.h,torch/csrc/jit/tensorexpr/ir_printer.cpp,torch/csrc/jit/tensorexpr/ir_printer.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/ir_verifier.cpp,torch/csrc/jit/tensorexpr/ir_verifier.h,torch/csrc/jit/tensorexpr/ir_visitor.cpp,torch/csrc/jit/tensorexpr/ir_visitor.h,torch/csrc/jit/tensorexpr/kernel.cpp,torch/csrc/jit/tensorexpr/kernel.h,torch/csrc/jit/tensorexpr/llvm_codegen.cpp,torch/csrc/jit/tensorexpr/llvm_jit.cpp,torch/csrc/jit/tensorexpr/loopnest.cpp,torch/csrc/jit/tensorexpr/loopnest.h,torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp,torch/csrc/jit/tensorexpr/mem_dependency_checker.h,torch/csrc/jit/tensorexpr/reduction.cpp,torch/csrc/jit/tensorexpr/reduction.h,torch/csrc/jit/tensorexpr/registerizer.cpp,torch/csrc/jit/tensorexpr/registerizer.h,torch/csrc/jit/tensorexpr/stmt.h,torch/csrc/jit/tensorexpr/tensor.cpp,torch/csrc/jit/tensorexpr/tensor.h,torch/csrc/jit/tensorexpr/tensorexpr_init.cpp,torch/csrc/jit/tensorexpr/unique_name_manager.cpp,torch/csrc/jit/tensorexpr/unique_name_manager.h,torch/csrc/jit/tensorexpr/var_substitutor.h",60.0,7,2,4.881934725,2.0,46894.0,35.0,5715747.683333334,14375.0,32894.5,0.0,Corrective,1.0,1
pytorch,d07a36e0c1dabff808086bb85a45be5fd76efd4f,5a0d65b06b197c06797a41de04ce99edbe48f669,"Gao, Xiang",qasdfgtyuiop@gmail.com,Sat Sep 05 06:02:04 2020 -0700,1599285724.0,"Further expand coverage of addmm/addmv, fix 0 stride (#43980)

Summary:
- test beta=0, self=nan
- test transposes
- fixes broadcasting of addmv
- not supporting tf32 yet, will do it in future PR together with other testing fixes

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43980

Reviewed By: mruberry

Differential Revision: D23507559

Pulled By: ngimel

fbshipit-source-id: 14ee39d1a0e13b9482932bede3fccb61fe6d086d",72.0,60.0,"aten/src/ATen/native/cuda/Blas.cu,test/test_torch.py",2.0,6,2,0.195909271,42.0,20508.0,2.0,806649.0,4888.0,11301.0,0.0,Corrective,1.0,1
pytorch,f804b65d4e822aedfe7814b42dc9b2e1bc6d18d7,5a455dc717f8cd935117c25e6678f0677b83ed93,BowenBao,bowbao@microsoft.com,Wed Apr 21 05:58:06 2021 -0700,1618984686.0,"[ONNX] Enable tensordot symbolic function. (#55654) (#56166)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56166

Support tensordot in symbolic function of opset 12, and add tests accordingly.

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D27866140

Pulled By: SplitInfinity

fbshipit-source-id: 68e218cfbd630900fb92871fc7c0de3e7e8c8c3d",95.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.122253567,3.0,11374.0,2.0,578125.3333333334,11062.0,24401.0,0.0,Feature Addition,0.0,1
pytorch,5531a23b204b4daa2c0bb3c52610e9a0ba79dacf,5a7c008b300f49b61644576f2d8e87fdd59bf210,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Tue Sep 12 12:29:55 2023 +0000,1694521795.0,"Revert ""[ROCm] Add ROCm AMDGPU support for inductor cpp codegen (#105141)""

This reverts commit 8ff00360a4daab7848307a9a0b1c81b1da873d0c.

Reverted https://github.com/pytorch/pytorch/pull/105141 on behalf of https://github.com/DanilBaibak due to Break internal build ([comment](https://github.com/pytorch/pytorch/pull/105141#issuecomment-1715629007))",12.0,74.0,"test/inductor/test_cpp_wrapper.py,tools/amd_build/build_amd.py,torch/_inductor/codecache.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/triton_heuristics.py,torch/utils/hipify/cuda_to_hip_mappings.py",6.0,9,3,1.987209073,4.0,13556.0,2.0,210867.0,19579.0,44649.0,0.0,Feature Addition,0.0,1
pytorch,4bf813e068f5f8292a4906a6f0efd9d47b4834a9,5aa1f769d3614bbe456ce44a1f2ae07275cc6c18,gchanan,gregchanan@gmail.com,Tue May 02 18:38:48 2017 -0400,1493750328.0,Fix torch.dist documentation: function returns a float. (#1440),2.0,3.0,"torch/_tensor_docs.py,torch/_torch_docs.py",2.0,1,1,0.970950594,17.0,6166.0,2.0,343328.0,699.0,8741.317468,0.0,Corrective,1.0,1
pytorch,485aee7a2215b5f10d27ae8ed3631960ac7bd913,5ab95930987f8be75d56801d2a0cf9e2593c0fdb,Jeffrey Wan,jw3468@fb.com,Sat Dec 19 00:13:38 2020 -0800,1608336818.0,"`torch.reciprocal`: promote integer inputs to float (#49102)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/49091

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49102

Reviewed By: VitalyFedyunin

Differential Revision: D25639541

Pulled By: soulitzer

fbshipit-source-id: 1dd360bd7b77f106d606143d8d3961610bac8cb7",38.0,80.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryFractionKernels.cu,c10/util/BFloat16-inl.h,c10/util/Half-inl.h,test/jit/test_save_load.py,test/test_binary_ufuncs.py,test/test_unary_ufuncs.py,torch/_torch_docs.py,torch/tensor.py,torch/testing/_internal/common_methods_invocations.py",11.0,13,4,2.706700313,41.0,21147.0,8.0,3565246.090909091,7650.0,17226.0,0.0,Corrective,1.0,1
pytorch,b004819f914f8ed342c472031a7cae1a0591a67a,5acf403088641c00bc895a2663f9a1dbbf512b23,Richard Zou,zou3519@gmail.com,Mon Mar 20 20:19:41 2023 -0700,1679343581.0,"Run functorch tests in default shards; delete functorch-specific shards (#96464)

Fixes #96347

This PR:

- Makes the functorch tests run as a part of the ""default"" shards
- Delete the functorch CI shard from all CI job configurations (if it exists)
- Increase the ""default"" shard count by 1 for each job, unless it was
previously set to 1, to accommodate the new functorch tests and not
regress time-to-signal.
- Adds a bunch of skips for ROCM and torchdynamo configurations. We can
investigate them later.

NB: I might go through some more iterations to figure out what other
skips need to be added, but this iteration of the PR seems to pass most CI.
suite.

Test Plan:
- wait for CI
Pull Request resolved: https://github.com/pytorch/pytorch/pull/96464
Approved by: https://github.com/huydhn",149.0,115.0,".ci/pytorch/common_utils.sh,.ci/pytorch/macos-test.sh,.ci/pytorch/test.sh,.ci/pytorch/win-test-helpers/install_test_functorch.bat,.ci/pytorch/win-test.sh,.github/workflows/periodic.yml,.github/workflows/pull.yml,.github/workflows/trunk.yml,test/functorch/test_aotdispatch.py,test/functorch/test_eager_transforms.py,test/functorch/test_functionalize.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/run_test.py",14.0,7,3,3.078568491,9.0,18764.0,13.0,2220914.5,13542.0,31341.5,0.0,Corrective,1.0,1
pytorch,71af7c46bb831330807b87776edfb770999d2aae,5b453554312fa6bb82a00a2fd35829885795068c,Edward Yang,ezyang@fb.com,Tue May 14 22:44:45 2019 -0700,1557873885.0,"Replace AT_CHECK with TORCH_CHECK [shard 2/10]

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/20427

Reviewed By: jerryzh168

Differential Revision: D15318190

fbshipit-source-id: 15518a683d7b662ef00f255134aaf9dbd183f099",103.0,103.0,"aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/ConvolutionTBC.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Cross.cpp,aten/src/ATen/native/Distance.cpp,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Dropout.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/Itertools.cpp,aten/src/ATen/native/Lerp.cpp,aten/src/ATen/native/Linear.cpp",14.0,4,1,3.306934151,8.0,4423.0,11.0,3800651.9285714286,8655.0,25606.83333,0.0,,0.0,1
pytorch,09f22d10a695bfba8ffb3327b9920fd3358c00ee,5b78a5eadb345a84f7f36d1c8c602bf608030e35,Vitaly Fedyunin,vitalyf@fb.com,Thu May 16 14:15:34 2019 -0700,1558016134.0,"Memory format support for contiguous and is_contiguous (#20455)

Summary:
#19975 was separated by 2 PRs.

This one:

Introduce MemoryFormat argument to the `x.is_contiguous(memory_format=torch.channels_last)` and to the `y = x.contiguous(memory_format=torch.channels_last)` functions.

At this moment both functions just operate with strides and doesn't store any tensor state.

(Original RFC #19092)

-----

Expands functionality of two tensor functions `.is_contiguous` and `.contiguous` (both python and c++ api).

Note: We had several complaints about `.to(memory_format)` function, and decided not to support it.

1.  `.contiguous` now support optional keyword-only argument - `memory_format`, which can be either `torch.contiguous_format` or `torch.channels_last`.

    - Using `torch.contiguous_format` will preserve existing `.contiguous()` behavior.

    - Calling `x.contiguous(memory_format=torch.channels_last)` returns new tensor which maintain same semantical layout (NCHW), but have different memory allocation pattern.

        `x.contiguous(memory_format=torch.channels_last)` expects input tensor to be 3d, 4d or 5d; and fails otherwise.

2. `.is_contiguous` now support optional keyword-only argument - `memory_format`, which can be either `torch.contiguous_format` or `torch.channels_last`.

    - `x.is_contiguous(memory_format=torch.contiguous_format)` preserves same functionality as `x.is_contiguous()` and remains unchanged.

    - `x.is_contiguous(memory_format=torch.channels_last)` returns true if A) input tensor is contiguous in memory AND B) allocated in the memory in NWHC (or similar for 3d,5d) format.

Note: By the end of the phase one `x.is_contiguous(memory_format=torch.channels_last)` will calculate state of the Tensor on every call. This functionality going to be updated later.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20455

Differential Revision: D15341577

Pulled By: VitalyFedyunin

fbshipit-source-id: bbb6b4159a8a49149110ad321109a3742383185d",389.0,47.0,"aten/src/ATen/OpaqueTensorImpl.h,aten/src/ATen/SparseTensorImpl.cpp,aten/src/ATen/SparseTensorImpl.h,aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/core/ivalue.h,aten/src/ATen/core/ivalue_inl.h,aten/src/ATen/native/TensorProperties.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/TensorMethods.h,aten/src/ATen/templates/Type.h,c10/core/MemoryFormat.h,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,caffe2/core/tensor.h,test/test_torch.py,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_nn_functions.cpp,tools/autograd/templates/python_variable_methods.cpp,tools/build_variables.py,tools/jit/gen_jit_dispatch.py,torch/CMakeLists.txt,torch/csrc/MemoryFormat.cpp,torch/csrc/MemoryFormat.h,torch/csrc/Module.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/script/function_schema_parser.cpp,torch/csrc/jit/script/schema_type_parser.cpp,torch/csrc/jit/symbolic_script.cpp,torch/csrc/jit/tracer.cpp,torch/csrc/jit/tracer.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_memoryformats.cpp,torch/csrc/utils/tensor_memoryformats.h,torch/onnx/symbolic_opset9.py",41.0,23,6,4.209358461,47.0,35023.0,26.0,841066.4166666666,8700.0,25827.83333,0.0,Feature Addition,0.0,1
pytorch,d49b73bbe692cdf9f50e59086a65395b5873069e,5b81746767d6eec978dc85197083769d13fb319a,Gregory Chanan,gchanan@fb.com,Mon Jun 05 20:06:07 2017 -0700,1496693167.0,Simplify python warning settings and cleanup tests.,198.0,247.0,"test/common_nn.py,test/test_cuda.py,test/test_torch.py,torch/utils/backcompat/__init__.py,torch/utils/backcompat/broadcast/__init__.py,torch/utils/backcompat/broadcast/warning/__init__.py,torch/utils/backcompat/keepdim/__init__.py,torch/utils/backcompat/keepdim/warning/__init__.py",8.0,8,2,0.895761125,31.0,5396.0,5.0,0.0,909.0,11494.94394,0.0,,0.0,1
pytorch,f1b512430619e3b747f63117d2406f070fba2b65,5b86c3af4a6315665e9db1cabaaddbf2d107b52f,sf-wind,sf_wind@hotmail.com,Wed Jun 13 20:10:45 2018 -0700,1528920645.0,"Update from facebook (#8384)

* [fix] fixup the bias multiplier data access issue

Hotfix for failues in conv_transpose

* [D2][Easy]: lint regularizer

lint with black

* [GanH]: Split mu in adaptive weight for diagnose

* [Dper] Add the ability to split FC weights into multiple smaller ones

* fix SumReduceLikeOp for empty blob

as desc.

* add ctc_greedy_decoder for caffe2

ctc_greedy_decoder same as tf's

* Update event callback handling

Allow multiple callbacks per event

* Add WeightedSum layer

The motivation is to do weighted sum in HoNet/crossnet, in the next diff, I'll replace model.Add with model.WeightedSum in
honet: https://fburl.com/f4rmolg2
crossnet: https://fburl.com/v7awn8se, https://fburl.com/63filbnm

* Replicate DAG's behavior

Some callers expect RunAsync to block, replicate that behavior in case of
explicit 'dag' net type

* [dper] layernorm layer

as title

* Override dag, async_dag, async_polling

Overriding dag, async_dag and async_polling with async_scheduling

* Name the thread pools

Caffe thread pools currently inherit the thread names from the thread that starts them, which can be misleading. Give them an explicit name instead.

* [Caffe2] FilleOp should support int64_t dimensions

Change argument type to int64_t for shape argument of FillerOp (used in ConstantFill, XavierFill, etc)

* Remove caffe2/caffe2/contrib/torch/

It's not used anywhere and depends on old lua torch that conflicts with Aten. Given PT1 it's not relevant any more (though it was nice and clever code!)

#accept2ship

* Fix linearWarmup multiplier check

The multiplier needs to be non-negative, not strictly positive.

* Revert D3314316

This is after 2 years and we do not seem to have a use case for this one, so
for the sake of clean API design we should potentially remove this. This would
allow us to potentially pass in arguments to optionally construct an object,
although it is indeed a little bit unclear how we can reuse existing objects if
constructor arguments are passed in. In any case, we may want to remove this
dangling feature.

* Speedup generate proposals by partial_sort.

Speedup generate proposals by partial_sort.

FACEBOOK:
- Saw speed improvement for training with this op.
- Yanghan benchmarked the op on a small dataset and see consistent 100% improvement on speed (6ms -> 3ms) on 420 input resolution. See next diff for details.

* More parallel processing friendly for CPP version of GenerateProposals.

More parallel processing friendly for CPP version of GenerateProposals.

* [DT] [43/n] Lift stop conditions inside reader code back to flow control

1. Split multi_reader function into local_reader and remote_reader
2. Lifted stop conditions inside Limiter back to flow control
3. Split epoch flow building logic into 3 cases:
  - single machine (1 reader, 1 trainer on trainer0 node, no PS)
  - (1 reader + 1 trainer) on trainer0 node, has PS
  - multiple readers, readers do not share nodes with trainers, might have PS or not

* Resolve conflicts for torch/_thnn/utils.py

* [Caffe2] Handle image decoding errors

Image decoding errors can make the whole training fail. This diff is to handle them
1.Catch imdecode exceptions and check if decoded image has zero columns or rows. This is counted as decoding errors.
2.Replace the image with empty in case of error
3.Count the number of errors and throw runtime exception if the rate reaches given number

The empty image data is kept. It might introduce noise in the training data.

* Update MKL exporter to IDEEP ops

TSIA

* [Caffe2] GlobalInit is thread safe, fixing the comment

With the mutex and lock, GlobalInit is thread safe.
Update the comments.

* Back out ""Add support for generating ATen files during fbcode build""

Original commit changeset: 28970ddba353

@override-unit-failures
(Note: this ignores all push blocking failures!)

* [DT]: fix predictor save

similar to D6610058, here we add the fix for distributed online training

* Remove net_singlethread_async_gpu.cc

Closes https://github.com/caffe2/caffe2/pull/2528

This removes net_singlethread_async_gpu.cc as part of our effort to clean
CUDAContext and the net executors.

* Inline DFS task execution

Add a DFS inline task execution mode in executor

* Add c10 folder to fbcode

This adds the c10 folder and its test cases to fbcode. Build flags are mostly taken from aten.

* add dependencies for online trainer

Add some dependencies so that the online model can use DataPipeline and PredictionTransform operators

Relevent post: https://fb.intern.facebook.com/groups/1324375037655677/permalink/1740993462660497/

* Resolve conflicts for tools/jit/gen_jit_dispatch.py

* [Fix] sparse regularization in distributed training

* Support advanced pooling options in sum processor

* support advanced pooling options in sum processor
* remove redundant code
* support attention in sum processor

* Improve shard logging in net tracing code

Make it handle arbitrary shard ids instead of just one digit ids.

* [Caffe2] Call GlobalInit in predictor only in mobile

FACEBOOK:
Calling GlobalInit long after the program starts may not be safe. There are issues if the following happens:

User does not call GlobalInit and initFacebook after program starts
User sets a flag manually: https://fburl.com/mcsumw7d
User calls OSS predictor.
OSS predictor calls GlobalInit
GlobalInit calls initFacebook
initFacebook resets all flags: https://fburl.com/tolszha1
Thus, the user manually set flags are overwritten

This would happen anytime GlobalInit is called long after the program starts.
I suppose the intention of the user in this case is not to call GlobalInit throughout the program,
but use Caffe2 regardless (is that desired?)
But adding GlobalInit in the OSS predictor would automatically call GlobalInit when using Caffe2.

This issue doesn't exist in mobile, since initFacebook is not called on mobile.

For now, guard the GlobalInit in predictor for mobile only.
May want to ensure the GlobalInit is always called at the start of the program. @[3501714:kutta] has seen weird issues when not calling GlobalInit at the start of the program on server side. He has made some progress on this.

* resolve conflicts for caffe2/core/logging_is_google_glog.h and test/test_torch.py

* Add empty fix for SumLikeReduceOp

Add empty fix for SumLikeReduceOp

* Revert D7962948: [caffe2][nomnigraph] Concat elim for sparseNN

This reverts commit f7f434dc5c34ca6058b9765d2ef615453d2276a9

@bypass-lint

An infra SEV is better than not reverting this diff.
If you copy this password, see you in SEV Review!
@cause_a_sev_many_files

* Remove Declarations.yaml

* Include common.h

* Change std::stoi to caffe2::stoi

* Add thread_name.cc to the CMake file

* No need to subtract 1. Fix test segfaults

* Fix NetTest, ObserverTest

Fix tests

(cherry picked from commit 3767e66c3f365596cba3d46d3e7322c933a0ab41)

* CTCGreedyDecoderOp only has CPU implementation, test should only run on CPU

* Add a variable to avoid conversion resizing issue

* [fix] fixup the bias multiplier data access issue

Hotfix for failues in conv_transpose

* [D2][Easy]: lint regularizer

lint with black

* [GanH]: Split mu in adaptive weight for diagnose

* [Dper] Add the ability to split FC weights into multiple smaller ones

* fix SumReduceLikeOp for empty blob

as desc.

* add ctc_greedy_decoder for caffe2

ctc_greedy_decoder same as tf's

* Update event callback handling

Allow multiple callbacks per event

* Add WeightedSum layer

The motivation is to do weighted sum in HoNet/crossnet, in the next diff, I'll replace model.Add with model.WeightedSum in
honet: https://fburl.com/f4rmolg2
crossnet: https://fburl.com/v7awn8se, https://fburl.com/63filbnm

* Replicate DAG's behavior

Some callers expect RunAsync to block, replicate that behavior in case of
explicit 'dag' net type

* [dper] layernorm layer

as title

* Override dag, async_dag, async_polling

Overriding dag, async_dag and async_polling with async_scheduling

* Name the thread pools

Caffe thread pools currently inherit the thread names from the thread that starts them, which can be misleading. Give them an explicit name instead.

* [Caffe2] FilleOp should support int64_t dimensions

Change argument type to int64_t for shape argument of FillerOp (used in ConstantFill, XavierFill, etc)

* Remove caffe2/caffe2/contrib/torch/

It's not used anywhere and depends on old lua torch that conflicts with Aten. Given PT1 it's not relevant any more (though it was nice and clever code!)

#accept2ship

* Fix linearWarmup multiplier check

The multiplier needs to be non-negative, not strictly positive.

* Revert D3314316

This is after 2 years and we do not seem to have a use case for this one, so
for the sake of clean API design we should potentially remove this. This would
allow us to potentially pass in arguments to optionally construct an object,
although it is indeed a little bit unclear how we can reuse existing objects if
constructor arguments are passed in. In any case, we may want to remove this
dangling feature.

* Speedup generate proposals by partial_sort.

Speedup generate proposals by partial_sort.

FACEBOOK:
- Saw speed improvement for training with this op.
- Yanghan benchmarked the op on a small dataset and see consistent 100% improvement on speed (6ms -> 3ms) on 420 input resolution. See next diff for details.

* More parallel processing friendly for CPP version of GenerateProposals.

More parallel processing friendly for CPP version of GenerateProposals.

* [DT] [43/n] Lift stop conditions inside reader code back to flow control

1. Split multi_reader function into local_reader and remote_reader
2. Lifted stop conditions inside Limiter back to flow control
3. Split epoch flow building logic into 3 cases:
  - single machine (1 reader, 1 trainer on trainer0 node, no PS)
  - (1 reader + 1 trainer) on trainer0 node, has PS
  - multiple readers, readers do not share nodes with trainers, might have PS or not

* Resolve conflicts for torch/_thnn/utils.py

* [Caffe2] Handle image decoding errors

Image decoding errors can make the whole training fail. This diff is to handle them
1.Catch imdecode exceptions and check if decoded image has zero columns or rows. This is counted as decoding errors.
2.Replace the image with empty in case of error
3.Count the number of errors and throw runtime exception if the rate reaches given number

The empty image data is kept. It might introduce noise in the training data.

* Update MKL exporter to IDEEP ops

TSIA

* [Caffe2] GlobalInit is thread safe, fixing the comment

With the mutex and lock, GlobalInit is thread safe.
Update the comments.

* Back out ""Add support for generating ATen files during fbcode build""

Original commit changeset: 28970ddba353

@override-unit-failures
(Note: this ignores all push blocking failures!)

* [DT]: fix predictor save

similar to D6610058, here we add the fix for distributed online training

* Remove net_singlethread_async_gpu.cc

Closes https://github.com/caffe2/caffe2/pull/2528

This removes net_singlethread_async_gpu.cc as part of our effort to clean
CUDAContext and the net executors.

* Inline DFS task execution

Add a DFS inline task execution mode in executor

* Add c10 folder to fbcode

This adds the c10 folder and its test cases to fbcode. Build flags are mostly taken from aten.

* add dependencies for online trainer

Add some dependencies so that the online model can use DataPipeline and PredictionTransform operators

Relevent post: https://fb.intern.facebook.com/groups/1324375037655677/permalink/1740993462660497/

* Resolve conflicts for tools/jit/gen_jit_dispatch.py

* [Fix] sparse regularization in distributed training

* Support advanced pooling options in sum processor

* support advanced pooling options in sum processor
* remove redundant code
* support attention in sum processor

* Improve shard logging in net tracing code

Make it handle arbitrary shard ids instead of just one digit ids.

* [Caffe2] Call GlobalInit in predictor only in mobile

FACEBOOK:
Calling GlobalInit long after the program starts may not be safe. There are issues if the following happens:

User does not call GlobalInit and initFacebook after program starts
User sets a flag manually: https://fburl.com/mcsumw7d
User calls OSS predictor.
OSS predictor calls GlobalInit
GlobalInit calls initFacebook
initFacebook resets all flags: https://fburl.com/tolszha1
Thus, the user manually set flags are overwritten

This would happen anytime GlobalInit is called long after the program starts.
I suppose the intention of the user in this case is not to call GlobalInit throughout the program,
but use Caffe2 regardless (is that desired?)
But adding GlobalInit in the OSS predictor would automatically call GlobalInit when using Caffe2.

This issue doesn't exist in mobile, since initFacebook is not called on mobile.

For now, guard the GlobalInit in predictor for mobile only.
May want to ensure the GlobalInit is always called at the start of the program. @[3501714:kutta] has seen weird issues when not calling GlobalInit at the start of the program on server side. He has made some progress on this.

* resolve conflicts for caffe2/core/logging_is_google_glog.h and test/test_torch.py

* Add empty fix for SumLikeReduceOp

Add empty fix for SumLikeReduceOp

* Revert D7962948: [caffe2][nomnigraph] Concat elim for sparseNN

This reverts commit f7f434dc5c34ca6058b9765d2ef615453d2276a9

@bypass-lint

An infra SEV is better than not reverting this diff.
If you copy this password, see you in SEV Review!
@cause_a_sev_many_files

* Remove Declarations.yaml

* Include common.h

* Change std::stoi to caffe2::stoi

* Add thread_name.cc to the CMake file

* No need to subtract 1. Fix test segfaults

* Fix NetTest, ObserverTest

Fix tests

(cherry picked from commit 3767e66c3f365596cba3d46d3e7322c933a0ab41)

* CTCGreedyDecoderOp only has CPU implementation, test should only run on CPU

* Add a variable to avoid conversion resizing issue

* Remove the code per soumith's comments

* Remove the code per soumith's comments

* Remove blank lines in the end of file

* Resolve conflicts for torch/_thnn/utils.py

* Update MKL exporter to IDEEP ops

TSIA

* Back out ""Add support for generating ATen files during fbcode build""

Original commit changeset: 28970ddba353

@override-unit-failures
(Note: this ignores all push blocking failures!)

* add dependencies for online trainer

Add some dependencies so that the online model can use DataPipeline and PredictionTransform operators

Relevent post: https://fb.intern.facebook.com/groups/1324375037655677/permalink/1740993462660497/

* Resolve conflicts for tools/jit/gen_jit_dispatch.py

* Support advanced pooling options in sum processor

* support advanced pooling options in sum processor
* remove redundant code
* support attention in sum processor

* resolve conflicts for caffe2/core/logging_is_google_glog.h and test/test_torch.py

* Revert D7962948: [caffe2][nomnigraph] Concat elim for sparseNN

This reverts commit f7f434dc5c34ca6058b9765d2ef615453d2276a9

@bypass-lint

An infra SEV is better than not reverting this diff.
If you copy this password, see you in SEV Review!
@cause_a_sev_many_files

* Remove Declarations.yaml

* Include common.h

* Change std::stoi to caffe2::stoi

* [caffe2] uprade IDEEP and hotfix for conv op accuracy issue (#8364)

* [IDEEP] Upgrade IDEEP version

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* [IDEEP] Fix accuracy issue in conv op

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Fix build error due to lack of src in CMakeLists

Signed-off-by: Gu, Jinghui <jinghui.gu@intel.com>

* Remove the code per soumith's comments

* [ONNX] Add an ATen fallback pathway for ONNX export (#8273)

* ATen fallback for ONNX export

* Move to enum

* Fix model test

* Add comment

* Address comments

BC interface

* Remove imaginary file (#8415)

* [Caffe2] Enable AMD/MIOPEN ops for Caffe2  (#8306)

* Add hip support for caffe2 core

* Add MIOPEN header/wrapper to caffe2 core

* Add HIP device into caffe2 PB

* top level makefile change for rocm/hip

* makefile scaffolding for AMD/RocM/HIP

* Makefile scafodding for AMD/RocM/HIP; add makefile/utility for HIP files

* caffe2 PB update for AMD/ROCM HIP device

* Add AMD/RocM/Thrust dependency

* HIP threadpool update

* Fix makefile macro

* makefile fix: duplicate test/binary name

* makefile clean-up

* makefile clean-up

* add HIP operator registry

* add utilities for hip device

* Add USE_HIP to config summary

* makefile fix for BUILD_TEST

* merge latest

* Fix indentation

* code clean-up

* Guard builds without HIP and use the same cmake script as PyTorch to find HIP

* Setup rocm environment variables in build.sh (ideally should be done in the docker images)

* setup locale

* set HIP_PLATFORM

* Revert ""set HIP_PLATFORM""

This reverts commit 8ec58db2b390c9259220c49fa34cd403568300ad.

* continue the build script environment variables mess

* HCC_AMDGPU_TARGET

* Cleanup the mess, has been fixed in the lastest docker images

* Assign protobuf field hip_gpu_id a new field number for backward compatibility

* change name to avoid conflict

* Fix duplicated thread pool flag

* Refactor cmake files to not add hip includes and libs globally

* Fix the wrong usage of environment variables detection in cmake

* Add MIOPEN CNN operators

* Revert ""Add MIOPEN CNN operators""

This reverts commit 6e89ad4385b5b8967a7854c4adda52c012cee42a.

* Add MIOPEN pooling operator

* Add MIOPEN activation operator

* Add MIOPEN softmax operator

* Add MIOPEN spatial batch norm operator

* Add MIOPEN loacl response normalization operator

* Add MIOPEN conv operator

* Clean-up LRN ops

* enable fp16 in MIOPEN pool ops

* Enable fp16 for MIOPEN relu op

* Enable fp16 for MIOPEN spatial batch norm op

* code clean-up

* revert float16 support

* Create Caffe2 python binding for AMD/ROCM/HIP

* Add op fallback for HIP operator

* add hip src/test files in cmake

* exclude hip src/test files

* fix python binding for hip backend

* fix MIOPEN pooling op workspace

* hack to compile miopen operators

* fix include path for MIOPEN ops

* Fix include path

* Add HIP math utilities

* Fix path for HIP math utils

* cmake fix

* Cmake fix / hipcc for hip files

* suppress hipcc warning

* cmake fix /replcae USE_HIP with USE_ROCM

* revert LoadHIP.cmake change

* fix include for thrust/cub-hip

* include path fix for conversion.h

* Updated with latest upstream changes

* clang format fixes

* Context_hip updates

* Fixed typo in rocblas handle get function

* Updated hipified math utils

* Updated math hip test util

* Updated context hip test

* Updated common_hip

* Updated net async dag for HIP

* Added MIOPEN in operator hip test

* fix

* C2 dependencies clean-up

* fix include path for building custom protobuf

* Decouple miopen pool op and conv_pool_op base

* cmake refactor

* fix operator_hip_test

* move all hip/miopen ops files into caffe2/operators/hip

* sanitize cmake

* permission issue

* remove extra parenthesis

* remove artifact from resolving merge conflict

* cont. sanitize cmake files

* fix syntax error

* sanitize conversion.h

* .

* Revert "".""

This reverts commit 56020cb0e996a31ae27bf1f8f491955ed0b121b9.

* clang-format

* Enable some reduce operators' ONNX backend tests (#8418)

* fix old comment to point to the right file (#8416)

* Stop pinning nccl version. (#8421)

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* Expose logsumexp docs and mark log_sum_exp in distributions for internal use (#8428)

* Enable some of the ONNX backend test on broadcasting (#8423)

* Enable some of the ONNX backend test on broadcasting

* enable gemm broadcast

* Expose proto utils and ONNX (#8073)

* Expose proto utils and ONNX from PyTorch libcaffe2.so

* Try to use protobuf from _C.so

* Fix ONNX proto header include

* Adjust order of imports for ONNX until nanopb goes away

* Set and use ONNX_NAMESPACE for PyTorch builds

* Show protobuf summary for all builds

* Add ONNX_NAMESPACE for cpp_build

* Statically link libprotobuf.a into libtorch.so

* Set ONNX_NAMESPACE on Windows build

* Move core/dispatch up as well

* Add /MD flag for Windows build of _C

* Potential Windows fix for ONNX and protobuf

* Add direct linkage from _C to ONNX on Windows

* Only include protobuf wrapper for PyTorch

* Pass extra_compile_args to _nvrtc ext build

* Remove installation of .a files

* Rebase creates some weird situations, revert them manually

* Remove more weird changes due to rebase

* Need to add thread_name.cc after merge",1212.0,1642.0,"CMakeLists.txt,c10/CMakeLists.txt,c10/dummy.cpp,c10/dummy.h,c10/dummy_test.cpp,caffe2/CMakeLists.txt,caffe2/contrib/torch/th_ops.cc,caffe2/contrib/torch/th_ops_gpu.cu,caffe2/contrib/torch/th_ops_test.py,caffe2/contrib/torch/torch_op.cpp,caffe2/contrib/torch/torch_op.h,caffe2/contrib/torch/torch_op_gpu.cpp,caffe2/contrib/torch/torch_ops_test.py,caffe2/core/event.cc,caffe2/core/event.h,caffe2/core/event_cpu.h,caffe2/core/init.h,caffe2/core/net.cc,caffe2/core/net_async_base.cc,caffe2/core/net_async_base.h,caffe2/core/net_async_scheduling.cc,caffe2/core/net_async_scheduling.h,caffe2/core/net_async_tracing.cc,caffe2/core/net_async_tracing.h,caffe2/core/net_async_tracing_test.cc,caffe2/core/net_dag.cc,caffe2/core/net_gpu_test.cc,caffe2/core/net_singlethread_async_gpu.cc,caffe2/core/net_test.cc,caffe2/core/observer_test.cc,caffe2/core/predictor.cc,caffe2/ideep/operators/utility_ops.cc,caffe2/image/image_input_op.h,caffe2/operators/conv_transpose_op_impl.h,caffe2/operators/ctc_greedy_decoder_op.cc,caffe2/operators/ctc_greedy_decoder_op.h,caffe2/operators/elementwise_ops.cu,caffe2/operators/filler_op.h,caffe2/operators/generate_proposals_op.cc,caffe2/operators/sparse_normalize_op_gpu.cu,caffe2/opt/converter.cc,caffe2/python/layer_model_helper.py,caffe2/python/layers/adaptive_weight.py,caffe2/python/layers/blob_weighted_sum.py,caffe2/python/layers/fc.py,caffe2/python/layers/layer_normalization.py,caffe2/python/layers_test.py,caffe2/python/mkl/rewrite_graph.py,caffe2/python/onnx/tests/onnx_backend_test.py,caffe2/python/operator_test/ctc_greedy_decoder_op_test.py,caffe2/python/operator_test/elementwise_op_broadcast_test.py,caffe2/python/operator_test/filler_ops_test.py,caffe2/python/predictor/predictor_py_utils.py,caffe2/python/regularizer.py,caffe2/sgd/learning_rate_op.h,caffe2/utils/CMakeLists.txt,caffe2/utils/thread_name.cc,caffe2/utils/thread_name.h,caffe2/utils/thread_pool.h,caffe2/utils/threadpool/ThreadPool.h,caffe2/utils/threadpool/WorkersPool.h",61.0,20,2,4.643866176,75.0,14191.0,20.0,2729627.0,1319.0,3746.305292,0.0,Corrective,1.0,1
pytorch,d7e6ede78465ceddf574840eb844f08a9e870557,5b8fe5cbb5b696d49d1480942f1904f92444857b,Edward Z. Yang,ezyang@mit.edu,Thu Dec 21 16:38:31 2017 -0500,1513874311.0,"Batchnorm in ATen (#4285)

* Batchnorm in ATen

This commit moves BatchNorm derivatives into ATen, eliminating
torch/csrc/autograd/functions/batch_normalization.cpp

Some refactoring along the way:

- Functions got renamed to remove _forward from their names
- CuDNN batchnorm forward was modified to return save_mean/save_std instead of
  take it as parameters. To avoid returning undefined Variables, these return
  (small) uninitialized tensors when they are not used.
- THNN batch normalization takes care of resizing save_mean and save_std on
  forward.
- There are some shenanigans re batchnorm backwards in eval mode. I'm tracking
  that in #4284
- I decided not to introduce buffers as a proper concept in ATen, which means
  that tensors like running_mean/running_var are variables in ATen.  This meant
  there needed to be some adjustments to how we *trace* such variables; the
  new strategy is if we can't find a Value for a variable, we look and see
  if we have a Value for the buffer pointed to by the variable, before
  finally falling back on constant.
- This PR finally reliably triggered OOM on Travis builds; I fixed this by reducing
  the number of parallel jobs.
- Stop using std::string when it's not necessary.
- Remove training parameter from cudnn_batch_norm_backward, because it
  doesn't make sense; cuDNN doesn't implement the math for evaluation mode
  batchnorm backwards.
- batchnorm_double_backward is now in an anonymous namespace, as it
  no longer needs to be called from torch/csrc

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",203.0,481.0,".travis.yml,aten/src/ATen/native/BatchNorm.cpp,aten/src/ATen/native/cudnn/BatchNorm.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/nn.yaml,aten/src/THCUNN/generic/BatchNormalization.cu,aten/src/THNN/generic/BatchNormalization.c,setup.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,tools/autograd/templates/Functions.h,torch/csrc/autograd/functions/batch_normalization.cpp,torch/csrc/autograd/functions/batch_normalization.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/onnx/batch_normalization.cpp,torch/csrc/autograd/symbolic.h,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/tracer.h,torch/nn/functional.py,torch/onnx/symbolic.py",20.0,21,3,3.324309795,40.0,7081.0,12.0,1203092.0526315789,389.0,1234.905869,0.0,Corrective,1.0,1
pytorch,392fc8885cf9ad38d76dfa61eb566703ca753fca,5bbeb55f22e8d23b4bc182d4a4813f2ceb2ffdc1,li-roy,8813817+li-roy@users.noreply.github.com,Tue Feb 27 23:35:50 2018 -0800,1519774550.0,"add reduce=True arg to MultiMarginLoss (#5150)

* add reduce=True arg to MultiMarginLoss

* Change tests to support legacy

* fix flake8

* address comments

* formatting change

* remove free of unallocated tensor

* fix after variable/tensor merge",393.0,88.0,"aten/src/ATen/nn.yaml,aten/src/THCUNN/MultiMarginCriterion.cu,aten/src/THCUNN/generic/MultiMarginCriterion.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/MultiMarginCriterion.c,aten/src/THNN/generic/THNN.h,test/common_nn.py,test/test_legacy_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/legacy/nn/MultiMarginCriterion.py,torch/nn/functional.py,torch/nn/modules/loss.py",13.0,15,4,2.895976561,39.0,17225.0,6.0,2721544.846153846,2412.0,24704.35823,0.0,Corrective,1.0,1
pytorch,27009d6129d1dc8bf5a57ee6735c49f5e566a3ac,5c189946746599d47479c16f097dcc1beeecd7e0,kshitij12345,kshitijkalambarkar@gmail.com,Sun May 30 03:54:15 2021 -0700,1622346855.0,"[special] Add `i1` and `i1e` (#56352)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50345

* [x] Check Docs https://12721710-65600975-gh.circle-artifacts.com/0/docs/special.html
* [x] Investigate fp32 failure on CI?! (Fails on clang. Reproduced locally with clang-11)
* [ ] Kernel vs Composite?
* [x] Autograd for `i0e` for zero?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56352

Reviewed By: anjali411

Differential Revision: D28700888

Pulled By: mruberry

fbshipit-source-id: 91a3cbb94f5b8a3b063589ec38179848c11def83",647.0,61.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_unary_ufuncs.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/special.h,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",17.0,21,5,3.037470603,17.0,30353.0,14.0,1153474.6470588236,12568.0,28515.5,0.0,Feature Addition,0.0,1
pytorch,3a169780e9bca35017478ea4da10d46e01462a7f,5c33400dd36c8da5ea06bfd6f43cb1042166a369,Fritz Obermeyer,fritz.obermeyer@gmail.com,Thu Dec 28 15:54:55 2017 -0800,1514476495.0,Implement OneHotCategorical distribution (#4357),209.0,53.0,"docs/source/distributions.rst,test/common.py,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/distribution.py,torch/distributions/one_hot_categorical.py",6.0,5,3,1.899032538,37.0,1449.0,4.0,416337.4,873.0,6649.172317,0.0,,0.0,1
pytorch,dc24503a89a1aefaab73e44124d4fa563f4616d1,5c3a9f3fdc1a3d69080129d94dad034a473153c1,Mikayla Gawarecki,mikaylagawarecki@gmail.com,Fri Nov 12 16:11:46 2021 -0800,1636733506.0,"adding opinfo for torch.nn.bilinear and torch.nn.glu (#67478)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67478

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D32027807

Pulled By: mikaylagawarecki

fbshipit-source-id: 501057cc9aced19fca26c4294fe81dcbb4b83a26",71.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,13052.0,1.0,45868.0,17052.0,40090.0,0.0,Feature Addition,0.0,1
pytorch,006cfebf3dcc4bc0cba344c25243baebaeeefbb7,5c5abd591d0b452ab8d4d37115fbc37cc8c984c7,Antonio Cuni,anto.cuni@gmail.com,Fri Jan 08 14:40:29 2021 -0800,1610116829.0,"Implement torch.linalg.svd (#45562)

Summary:
This is related to https://github.com/pytorch/pytorch/issues/42666 .
I am opening this PR to have the opportunity to discuss things.
First, we need to consider the differences between `torch.svd` and `numpy.linalg.svd`:

1. `torch.svd` takes `some=True`, while `numpy.linalg.svd` takes `full_matrices=True`, which is effectively the opposite (and with the opposite default, too!)

2. `torch.svd` returns `(U, S, V)`, while `numpy.linalg.svd` returns `(U, S, VT)` (i.e., V transposed).

3. `torch.svd` always returns a 3-tuple; `numpy.linalg.svd` returns only `S` in case `compute_uv==False`

4. `numpy.linalg.svd` also takes an optional `hermitian=False` argument.

I think that the plan is to eventually deprecate `torch.svd` in favor of `torch.linalg.svd`, so this PR does the following:

1. Rename/adapt the old `svd` C++ functions into `linalg_svd`: in particular, now `linalg_svd` takes `full_matrices` and returns `VT`

2. Re-implement the old C++ interface on top of the new (by negating `full_matrices` and transposing `VT`).

3. The C++ version of `linalg_svd` *always* returns a 3-tuple (we can't do anything else). So, there is a python wrapper which manually calls `torch._C._linalg.linalg_svd` to tweak the return value in case `compute_uv==False`.

Currently, `linalg_svd_backward` is broken because it has not been adapted yet after the `V ==> VT` change, but before continuing and spending more time on it I wanted to make sure that the general approach is fine.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45562

Reviewed By: H-Huang

Differential Revision: D25803557

Pulled By: mruberry

fbshipit-source-id: 4966f314a0ba2ee391bab5cda4563e16275ce91f",558.0,255.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/backward_compatibility/check_backward_compatibility.py,test/test_linalg.py,test/test_namedtuple_return_api.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",14.0,15,5,2.162519558,33.0,37587.0,9.0,465352.8571428572,7956.0,17990.0,0.0,,0.0,1
pytorch,5b9f7f7b0e205a6d8d5f2e61f558eee378f0ce40,5c9d1e48242587a9b1958df2d2efea3472072f4f,Xingying Cheng,xcheng16@fb.com,Mon Apr 27 17:16:59 2020 -0700,1588007819.0,"Propagate module lints for mobile scripted module. (#37046)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37046
ghstack-source-id: 102669259

Creating a python api entry to generate mobile model lints which takes a scripted module as argument and returns a map of module lints.

The initial version is to create placeholder which included module bundled input as the first lint instance. More lints will be added in the future.

Test Plan: python test/test_optimizer.py

Reviewed By: dreiss

Differential Revision: D21164648

fbshipit-source-id: 9e8f4e19d74b5464a55cc73b9dc18f358c5947d6",102.0,8.0,"test/test_mobile_optimizer.py,torch/utils/mobile_optimizer.py",2.0,3,2,0.999761525,1.0,106.0,1.0,842325.0,1434.0,3790.5,0.0,Feature Addition,0.0,1
pytorch,3d4b92b171b047ce4c484c37c96bd1f8fc014950,5caa27a3fd75f737db9a059d061469fb2ce0b2d8,Peter Bell,peterbell10@live.co.uk,Tue Dec 06 19:21:21 2022 +0000,1670354481.0,"as_strided: Fix default storage_offset for reference implementation (#89513)

This fixes the default storage_offset to take it from the input. This was
previously untested, so I've also added a new OpInfo which includes samples with
non-zero storage_offsets on the input tensor.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89513
Approved by: https://github.com/ezyang, https://github.com/ngimel",88.0,11.0,"test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",5.0,6,2,1.247974297,7.0,32725.0,4.0,25664.0,10245.0,23441.5,0.0,Corrective,1.0,1
pytorch,9a393b023dcc86b68e660b925702300172dcf09d,5cbf8504ef0649063c0181c3cb07995646f6d245,Gregory Chanan,gchanan@fb.com,Mon Oct 31 17:00:47 2016 -0700,1477933247.0,"Add generic support for VolumetricMaxPooling, VolumetricMaxUnpooling,
VolumetricDilatedMaxPooling.",572.0,564.0,"THCUNN.h,VolumetricDilatedMaxPooling.cu,VolumetricMaxPooling.cu,VolumetricMaxUnpooling.cu,generic/THCUNN.h,generic/VolumetricDilatedMaxPooling.cu,generic/VolumetricMaxPooling.cu,generic/VolumetricMaxUnpooling.cu",8.0,1,1,2.517773862,17.0,1931.0,3.0,259.5,74.0,1019.933333,0.0,Feature Addition,0.0,1
pytorch,30e68988088f23312009208c6730ad9e9a560ed6,5cc26c0c90e771339cf850b29b4b5be38e5e3f55,Tongzhou Wang,SsnL@users.noreply.github.com,Mon Dec 18 07:19:08 2017 -0500,1513581548.0,"Add default PyTorch seeding and worker_init_fn to DataLoader (#4018)

* Add default PyTorch seeding and worker_init_fn to DataLoader

* generate seed using current RNG each time

* worker_seed <- main_proc_RNG_generated_seed + worker_id",87.0,9.0,"test/test_dataloader.py,torch/utils/data/dataloader.py",2.0,4,2,0.979868757,35.0,823.0,2.0,666475.0,2204.0,24233.85823,0.0,Feature Addition,0.0,1
pytorch,789dc6d445136307070486fccf6c9276a722a384,5cd8a77e0154d5b8131b75a0b05011f697cbeca5,76181208+imaginary-person@users.noreply.github.com,76181208+imaginary-person@users.noreply.github.com,Wed Mar 24 04:08:47 2021 -0700,1616558927.0,"Skip inplace autograd test if inplace variant doesn't exist (#54460)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/54413

1. Skip inplace autograd test for an op if its inplace variant does not exist.
2. For ops that don't have an inplace variant, remove redundant `supports_inplace_autograd=False` assignments in their `OpInfo`s.
3. Ops having inplace variants that do not support autograd should not have `supports_inplace_autograd=False` entries removed from their `OpInfo`s.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54460

Reviewed By: ngimel

Differential Revision: D27255938

Pulled By: mruberry

fbshipit-source-id: f15334b09e68995e9f26adc2ff3e59c292689ee8",13.0,82.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.251862848,2.0,5334.0,2.0,12933.5,10021.0,22188.0,0.0,Corrective,1.0,1
pytorch,83450aa11d244e426c9a9d2d6d780efe056d326c,5d00c374ddf03ec8784aec934b7ed28abd3af76f,BowenBao,bowbao@microsoft.com,Tue Jun 15 19:21:31 2021 -0700,1623784891.0,"[ONNX] Sum empty tensor could not be exported to ONNX successfully. (#58141) (#59537)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59537

PyTorch sum over empty tensor gives 0, while ONNX produces an error.

torch.sum will be translated into onnx::ReduceSum op. Per the definition of ReduceSum, update the keepdims attribute for this scenario.

Test Plan: Imported from OSS

Reviewed By: nikithamalgifb, ansley

Differential Revision: D29046604

Pulled By: SplitInfinity

fbshipit-source-id: 6f5f3a66cb8eda8b5114b8474dda6fcdbae73469

Co-authored-by: fatcat-z <jiz@microsoft.com>",21.0,3.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.763911157,3.0,13491.0,2.0,821328.0,13024.0,29501.0,0.0,,0.0,1
pytorch,5b66062f999f5a6669e7d95eb79c66a73ff1b95a,5d130e4232aad952861eeceb4fd6d30f4f5f3049,vishwakftw,cs15btech11043@iith.ac.in,Wed Jul 31 16:53:46 2019 -0700,1564592026.0,"Allowing batching for det/logdet/slogdet operations (#22909)

Summary:
Changelog:
- Add batching for det / logdet / slogdet operations
- Update derivative computation to support batched inputs (and consequently batched outputs)
- Update docs
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22909

Test Plan:
- Add a `test_det_logdet_slogdet_batched` method in `test_torch.py` to test `torch.det`, `torch.logdet` and `torch.slogdet` on batched inputs. This relies on the correctness of `torch.det` on single matrices (tested by `test_det_logdet_slogdet`). A port of this test is added to `test_cuda.py`
- Add autograd tests for batched inputs

Differential Revision: D16580988

Pulled By: ezyang

fbshipit-source-id: b76c87212fbe621f42a847e3b809b5e60cfcdb7a",315.0,95.0,"aten/src/ATen/native/LinearAlgebra.cpp,test/common_methods_invocations.py,test/common_utils.py,test/test_cuda.py,test/test_jit.py,test/test_torch.py,tools/autograd/templates/Functions.cpp,torch/_torch_docs.py",8.0,9,4,2.456961,42.0,45532.0,6.0,546340.75,10324.0,29563.83333,0.0,Corrective,0.0,1
pytorch,74e6d2ce67f28dd96b3909648cb5f7b2d2d5b1ac,5d300e761dc3680937c1928ab84ae886b22c3fe8,Saketh Are,saketh@fb.com,Tue Nov 23 03:59:41 2021 -0800,1637639981.0,"Add OpInfos for parcel Activation Functions I (#68521)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68521

Reviewed By: jbschlosser

Differential Revision: D32606625

Pulled By: saketh-are

fbshipit-source-id: acf98a07c45bce95b1470bf9856577426265f3d1",194.0,10.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.044664601,2.0,15324.0,2.0,288340.5,17279.0,40638.5,0.0,Feature Addition,0.0,1
pytorch,77f98ea5e09d1bf41ae72457fbdd3e93fa9e8533,5d4452937d9094e4198cc0932ca18e40fbf47492,Richard Zou,zou3519@gmail.com,Thu Oct 14 16:11:42 2021 -0700,1634227902.0,"OpInfos for some Tensor dtype conversion methods (#64282)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64282

OpInfos for:
- Tensor.bfloat16, Tensor.bool, Tensor.bypte, Tensor.char
- Tensor.double, Tensor.float, Tensor.half, Tensor.int
- Tensor.short, Tensor.long

None of these are supported by TorchScript. Also, the OpInfo autograd
test runner assumes that the operation is not allowed to change the
dtype of the argument, so only Tensor.double has
`supports_autograd=True` (in theory Tensor.bfloat16, Tensor.float,
Tensor.half should be differentiable).

Test Plan: - run tests

Reviewed By: dagitses

Differential Revision: D31452627

Pulled By: zou3519

fbshipit-source-id: b7f272e558558412c47aefe947af7f060dfb45c5",150.0,0.0,"test/test_fx.py,test/test_fx_experimental.py,test/test_jit_fuser_te.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,1.0144831,2.0,18473.0,3.0,111026.25,16249.0,37574.0,0.0,,0.0,1
pytorch,7cbe255296041adc5656758ceead80f842fa005a,5d5cfe2e5728b0a8a5b49ed9bdbebf9816c66ebe,Luke Yeager,lukeyeager@users.noreply.github.com,Mon Feb 27 20:29:04 2017 -0800,1488227344.0,[Lint] Address E731,7.0,4.0,"test/test_nn.py,test/test_torch.py,tox.ini",3.0,1,1,1.435371391,24.0,4856.0,3.0,70812.0,442.0,3918.075745,0.0,Feature Addition,0.0,1
pytorch,1000403f669a390c1e2c1bf18876e8a28920ccf3,5d64658ce8a385353a013396135b6bd95456ebca,anjali411,chourdiaanjali123@gmail.com,Tue Jan 19 16:15:52 2021 -0800,1611072952.0,"Add complex support for `torch.{acosh, asinh, atanh}` (#50387)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50387

Test Plan: Imported from OSS

Reviewed By: heitorschueroff

Differential Revision: D25947496

Pulled By: anjali411

fbshipit-source-id: c70886a73378501421ff94cdc0dc737f1738bf6f",43.0,21.0,"aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryGeometricKernels.cu,tools/autograd/gen_variable_type.py,torch/testing/_internal/common_methods_invocations.py",4.0,11,3,1.457559958,13.0,4725.0,4.0,1330965.5,8171.0,18481.0,0.0,Feature Addition,0.0,1
pytorch,8fe3d287b2f4019254f65e184ef7d1090574f790,5d6a5cf3a7ea26bcab347e510cc95c38aeead3b3,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Mon Jan 08 22:21:27 2018 +0500,1515450087.0,Implementation of Gumbel Distribution (#4517),98.0,2.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/gumbel.py,torch/distributions/pareto.py",5.0,5,3,1.500954294,8.0,1526.0,3.0,109230.75,902.0,6704.172317,0.0,,0.0,1
pytorch,054a2fd6c2fc361796663eed4772368c287d6c83,5d6e8315630d4e62e5e015c2e4c816be04f1f94e,Peter Bell,peterbell10@live.co.uk,Fri Oct 14 16:06:42 2022 +0100,1665763602.0,"OpInfo: Sample input cleanup (3/n) (#86380)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86380
Approved by: https://github.com/mruberry",27.0,185.0,"test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,torch/testing/_internal/opinfo/definitions/linalg.py",3.0,7,2,0.428725119,1.0,5436.0,3.0,125689.0,8402.0,20088.5,0.0,,0.0,1
pytorch,2f8d6582de96812a9fcc88987bc97510f1100908,5d7770948587bb5f19b929200f20e2eaf7074e5c,Tongzhou Wang,SsnL@users.noreply.github.com,Sat Mar 24 16:21:13 2018 -0400,1521908473.0,"Linearly interpolating upsampling fix (#5927)

* Changes in bilinear upsampling

* Add align_corners option to upsampling module & functional when using linearly interpolating modes
When align_corners=True, it uses the old original upsampling scheme, which gives visually better results,
but doesn't properly align input and output pixels, and thus cause the output vary basing on input.
This PR adds this align_corners option, and changes the default behavior to align_corners=False, with
proper warning if this option is not specified upon using nn.Upsample or nn.functional.upsample to let
be aware of this new change.
Adds tests in test_nn.py for spatial invariance when align_corners=False, and usual module tests for
align_corners=False.

* remove redundant checks and unnecessary variables; fix the cast

* fix negative indices",423.0,186.0,"aten/src/ATen/nn.yaml,aten/src/ATen/nn_parse.py,aten/src/ATen/test/basic.cpp,aten/src/THCUNN/SpatialUpSamplingBilinear.cu,aten/src/THCUNN/TemporalUpSamplingLinear.cu,aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu,aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THCUNN/generic/TemporalUpSamplingLinear.cu,aten/src/THCUNN/generic/VolumetricUpSamplingTrilinear.cu,aten/src/THCUNN/linear_upsampling.h,aten/src/THNN/generic/SpatialUpSamplingBilinear.c,aten/src/THNN/generic/THNN.h,aten/src/THNN/generic/TemporalUpSamplingLinear.c,aten/src/THNN/generic/VolumetricUpSamplingTrilinear.c,aten/src/THNN/generic/linear_upsampling.c,aten/src/THNN/init.cpp,test/test_nn.py,tools/autograd/derivatives.yaml,torch/nn/functional.py,torch/nn/modules/upsampling.py",21.0,14,4,3.942555236,37.0,16435.0,8.0,6523029.368421053,526.0,2454.5,0.0,Corrective,1.0,1
pytorch,a8f9aab84074ceab684da166199cc598afc62d54,5d80a48cef373e22393af1b1f4f4e3f2ad948a76,anjali411,chourdiaanjali123@gmail.com,Wed Sep 01 23:11:38 2021 -0700,1630537898.0,"Add fast path for addmm when the inputs are conjugate (#59380)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/59380

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D28898374

Pulled By: anjali411

fbshipit-source-id: eab0e64d37bb57c18b54cabb8e5c00666338ba04",223.0,59.0,"aten/src/ATen/ConjugateFallback.cpp,aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/native/CPUBlas.cpp,aten/src/ATen/native/CPUBlas.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/NegateFallback.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/cuda/Blas.cpp,test/test_linalg.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",11.0,10,3,2.572041579,43.0,32656.0,9.0,2878145.363636364,15151.0,34707.5,0.0,Feature Addition,0.0,1
pytorch,192df16a4dbb4f092c896c3370013938461e1f87,5dba4ff786415973213f3c270a793698e635be1f,albanD,desmaison.alban@gmail.com,Tue Apr 13 13:17:20 2021 -0700,1618319840.0,"move topk to use OpInfo (#55547)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55547

Test Plan: Imported from OSS

Reviewed By: VitalyFedyunin, mruberry

Differential Revision: D27649412

Pulled By: albanD

fbshipit-source-id: e36a5bb5703681b7f7647ca30d6f4a72faf5ed0e",33.0,9.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,5173.0,1.0,1.0,10701.0,23648.5,0.0,,0.0,1
pytorch,8c09cc64757afbbbefc641ec54a9d186ad4ce829,5dd1568aa3cf6029ddcf93ee6a8c26b24b10fbb1,Jeff Daily,jeff.daily@amd.com,Tue Feb 09 17:12:36 2021 -0800,1612890756.0,"[ROCm] skip more magma tests (#51915)

Summary:
Additional magma tests have been identified as failing after integrating hipMAGMA into the ROCm builds.  Skipping is necessary until they can be fixed properly.  This is blocking migration of ROCm CI to 4.0.1.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51915

Reviewed By: izdeby

Differential Revision: D26326404

Pulled By: malfet

fbshipit-source-id: 558cce66f216f404c0316ab036e2e5637fc99798",28.0,22.0,"test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.242292189,2.0,10026.0,2.0,357038.5,8793.0,19759.0,0.0,Corrective,1.0,1
pytorch,3a6da16a5a8727ff0d44d2eca009e3ab0ecb0ede,5dd1c677768a94ee4c014438369c8a9f832a7e40,Justin Chu,justinchuby@users.noreply.github.com,Thu May 05 00:19:22 2022 +0000,1651709962.0,"[ONNX] Format ONNX python with black

Format all onnx python code with black and isort with

```sh
isort torch/onnx/ test/onnx
black torch/onnx/ test/onnx
```

Updated lintrunner config to include these paths.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76754
Approved by: https://github.com/suo, https://github.com/BowenBao",9113.0,3691.0,".lintrunner.toml,test/onnx/autograd_helper.py,test/onnx/debug_embed_params.py,test/onnx/export_onnx_tests_filter.py,test/onnx/export_onnx_tests_generator.py,test/onnx/model_defs/__init__.py,test/onnx/model_defs/dcgan.py,test/onnx/model_defs/emb_seq.py,test/onnx/model_defs/lstm_flattening_result.py,test/onnx/model_defs/mnist.py,test/onnx/model_defs/op_test.py,test/onnx/model_defs/rnn_model_with_packed_sequence.py,test/onnx/model_defs/squeezenet.py,test/onnx/model_defs/srresnet.py,test/onnx/model_defs/super_resolution.py,test/onnx/model_defs/word_language_model.py,test/onnx/pytorch_helper.py,test/onnx/test_caffe2_common.py,test/onnx/test_custom_ops.py,test/onnx/test_models.py,test/onnx/test_models_onnxruntime.py,test/onnx/test_onnx_common.py,test/onnx/test_onnx_export.py,test/onnx/test_onnx_opset.py,test/onnx/test_operators.py,test/onnx/test_pytorch_common.py,test/onnx/test_pytorch_helper.py,test/onnx/test_pytorch_jit_onnx.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_caffe2_quantized.py,test/onnx/test_pytorch_onnx_no_runtime.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_pytorch_onnx_onnxruntime_cuda.py,test/onnx/test_pytorch_onnx_shape_inference.py,test/onnx/test_utility_funs.py,test/onnx/test_verify.py,test/onnx/verify.py,tools/onnx/update_default_opset_version.py,torch/onnx/__init__.py,torch/onnx/onnx_supported_ops.py,torch/onnx/symbolic_caffe2.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset14.py,torch/onnx/symbolic_opset15.py,torch/onnx/symbolic_opset16.py,torch/onnx/symbolic_opset7.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",53.0,7,3,3.656226413,15.0,31075.0,30.0,19955275.0,2876.0,6915.0,0.0,,0.0,1
pytorch,0185a05ceb257854fc71981cb84a16911f270cc3,5de22d3f6914fd39a63aa910994276cc50b81982,Mike Ruberry,mruberry@devfair044.maas,Sun Dec 06 03:23:50 2020 -0800,1607225030.0,"Removes redundant method_test entries (#48828)

Summary:
Now that Lilyjjo's [stack of OpInfo updates](https://github.com/pytorch/pytorch/pull/48627) is landed, we can port method_test entries to OpInfos. This PR doesn't port any method_test entries, but it removes redundant entries. These entries previously tested both multi-dim and zero-dim tensors, so a new zero-dim tensor input is added to UnaryUfuncInfo's sample inputs.

To recap, this PR:

- removes method_test() entries that are redundant with OpInfo entries
- adds a new sample input to unary ufunc OpInfos that tests them on 0d tensors

cc kshitij12345 as an fyi. Going forward we should have a goal of not only porting all the MathTestMeta objects to use the OpInfo pattern but also all the current method_test entries. For each entry the function needs to be added as an OpInfo and the inputs need to be added as sample inputs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48828

Reviewed By: malfet

Differential Revision: D25336071

Pulled By: mruberry

fbshipit-source-id: 6b3f6c347195233d6b8ad57e2be68fd772663d9b",27.0,73.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,2054.0,1.0,140000.0,7245.0,16364.0,0.0,Feature Addition,0.0,1
pytorch,7f9533e22481e07ae47e6e0b92ebcfd2ceed2aba,5df59f957fc4a5e30ae0b23a3952285f54a118b2,Edward Z. Yang,ezyang@meta.com,Fri Mar 31 16:53:36 2023 -0400,1680281616.0,"Fix G001,G002,G003 in logs to % syntax (#97812)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97812
Approved by: https://github.com/Skylion007, https://github.com/kiukchung, https://github.com/malfet, https://github.com/mlazos",70.0,77.0,".flake8,benchmarks/dynamo/common.py,test/simulate_nccl_errors.py,torch/_dynamo/backends/distributed.py,torch/_dynamo/convert_frame.py,torch/backends/xeon/run_cpu.py,torch/distributed/algorithms/ddp_comm_hooks/post_localSGD_hook.py,torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py,torch/distributed/distributed_c10d.py,torch/distributed/elastic/rendezvous/etcd_rendezvous.py,torch/distributed/fsdp/sharded_grad_scaler.py,torch/distributed/nn/jit/instantiator.py",12.0,16,3,2.7750379,4.0,10574.0,9.0,7460376.416666667,14028.0,32183.5,0.0,Corrective,1.0,1
pytorch,bf517f4092ad8c63e129bc62cfd8dec4c1cfdaa7,5e0ec03a717f8da1f73cbdc682fcf0976ed034be,Shunting Zhang,shunting@fb.com,Tue Aug 29 21:20:20 2023 -0700,1693344020.0,"[inductor][easy] reuse a single is_aligned function (#108135)

Resolve comment: https://github.com/pytorch/pytorch/pull/107722#discussion_r1308117422

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108135
Approved by: https://github.com/jansel
ghstack dependencies: #107722",16.0,19.0,torch/_inductor/codegen/triton_utils.py,1.0,3,1,0,1.0,74.0,1.0,82906.0,19180.0,43531.5,0.0,Non Functional,0.0,1
pytorch,8d2b9a08f42188cce5c4c44c616e7c456eb01030,5e382894be5b7e1906b04a73e469d472b98d218c,Holger Kohr,h.kohr@cwi.nl,Wed Nov 08 14:01:29 2017 +0100,1510149689.0,add numpy() and from_numpy() to HalfTensor (#2953),173.0,66.0,"test/test_torch.py,torch/csrc/Module.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/methods/TensorSerialization.cwrap",4.0,5,2,1.133318292,39.0,7734.0,4.0,482132.0,791.0,6488.672317,0.0,Feature Addition,0.0,1
pytorch,9080942afb0f53be24efd3e213a39f1c270b8e5c,5e462a3ed6ec88a022e1ab4d60f9e2282e96ec44,vishwakftw,cs15btech11043@iith.ac.in,Tue Mar 26 14:49:58 2019 -0700,1553611798.0,"Introduce SobolEngine (#10505)

Summary:
`SobolEngine` is a quasi-random sampler used to sample points evenly between [0,1]. Here we use direction numbers to generate these samples. The maximum supported dimension for the sampler is 1111.

Documentation has been added, tests have been added based on Balandat 's references. The implementation is an optimized / tensor-ized implementation of Balandat 's implementation in Cython as provided in #9332.

This closes #9332 .

cc: soumith Balandat
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10505

Reviewed By: zou3519

Differential Revision: D9330179

Pulled By: ezyang

fbshipit-source-id: 01d5588e765b33b06febe99348f14d1e7fe8e55d",1652.0,0.0,"aten/src/ATen/native/SobolEngineOps.cpp,aten/src/ATen/native/SobolEngineOpsUtils.h,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_torch.py,tools/autograd/gen_variable_type.py,torch/__init__.py,torch/quasirandom.py",8.0,10,5,1.228900564,43.0,17625.0,4.0,1003076.0,7682.0,23309.83333,0.0,Feature Addition,0.0,1
pytorch,bcf93181a0ca5db75bd038db0d5f7e4cee733db7,5e5c31954994274e51c09731ac71a4f824ddb620,Richard Zou,zou3519@gmail.com,Thu Sep 22 13:56:40 2022 -0700,1663855000.0,"Move functorch python bindings to torch/csrc (#85426)

This moves functorch's python bindings to torch/csrc/functorch/init.cpp.
Coming next is the torchdim move. I didn't do torchdim yet because
moving functorch's python bindings unblocks some other things that I
want to do first.

Test Plan:
- tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85426
Approved by: https://github.com/ezyang",532.0,436.0,"build_variables.bzl,functorch/README.md,functorch/_src/aot_autograd.py,functorch/_src/eager_transforms.py,functorch/_src/monkey_patching.py,functorch/_src/vmap.py,functorch/csrc/init.cpp,functorch/csrc/init_dim_only.cpp,functorch/dim/batch_tensor.py,functorch/examples/dp_cifar10/cifar10_transforms.py,functorch/examples/maml_omniglot/maml-omniglot-transforms.py,functorch/test/common_utils.py,functorch/test/test_eager_transforms.py,functorch/test/test_vmap.py,torch/csrc/Module.cpp,torch/csrc/functorch/init.cpp,torch/csrc/functorch/init.h",17.0,11,2,1.634667289,45.0,15826.0,13.0,1583457.5,7581.0,17792.5,0.0,,0.0,1
pytorch,673b35c847ee6ba67367ba27ff8597c8ae382257,5e5c610a587d671044303c4fa56af20f33eee5dd,Andrey Talman,atalman@fb.com,Thu Sep 01 20:24:06 2022 +0000,1662063846.0,"Move slow-grad checks to CUDA-11.6 (#84313)

Mitigates #84192 by skipping two tests

Please note: We tried to increase the tolerance for test_fn_gradgrad_linalg_det_singular_cuda_float64 but this did not help.
Ref:
Increase `test_fn_gradgrad_linalg_det_singular_cuda_float64` error tolerance to  1e-4 as suggested in https://github.com/pytorch/pytorch/issues/84192#issuecomment-1230644574

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84313
Approved by: https://github.com/malfet, https://github.com/huydhn, https://github.com/Lezcano",21.0,16.0,".github/workflows/periodic.yml,torch/testing/_internal/opinfo/definitions/linalg.py",2.0,7,2,0.909022156,1.0,2570.0,2.0,88825.5,7025.0,16471.0,0.0,Non Functional,0.0,1
pytorch,647ce0ab8d47d5334e6d8f8fc31c998b596b6769,5e90f1e61b7bd31d8a2e51909ec5ce23d0a54b22,Richard Zou,zou3519@gmail.com,Mon May 10 13:00:00 2021 -0700,1620651600.0,[functorch] Parameterized testing,176.0,40.0,"functorch/test/common.py,functorch/test/test_vmap.py",2.0,2,1,0.99380674,1.0,2743.0,1.0,0.0,66.0,156.5,0.0,,0.0,1
pytorch,93fbbaab2a36fb9b99344f0eeb1e6e3a68c3e4c2,5e97f251a8de94e9fc0179cce651791d4d16bf12,"Gao, Xiang",qasdfgtyuiop@gmail.com,Tue Sep 01 22:32:15 2020 -0700,1598999535.0,"Enable TF32 support for cuDNN (#40737)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40737

Reviewed By: mruberry

Differential Revision: D22801525

Pulled By: ngimel

fbshipit-source-id: ac7f7e728b4b3e01925337e8c9996f26a6433fd2",758.0,478.0,"aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/autocast_mode.cpp,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/native_functions.yaml,caffe2/serialize/inline_container.h,docs/source/notes/cuda.rst,test/backward_compatibility/check_backward_compatibility.py,test/cpp/jit/test_lite_interpreter.cpp,test/test_cuda.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/_C/__init__.pyi.in,torch/backends/cudnn/__init__.py,torch/csrc/Module.cpp,torch/csrc/jit/passes/fold_conv_bn.cpp,torch/csrc/jit/passes/graph_rewrite_helper.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/onnx/symbolic_opset9.py,torch/testing/_internal/autocast_test_lists.py,torch/testing/_internal/common_cuda.py,torch/testing/_internal/common_nn.py",25.0,27,6,2.805655954,50.0,43158.0,22.0,2151361.52,4749.0,11057.0,0.0,,0.0,1
pytorch,5824a866b72c251ad47a9c16dc652e49cfd7e234,5ec4ad7f54cc35ee40b5991bd65c974056048f84,kshitij12345,kshitijkalambarkar@gmail.com,Sun Jun 20 01:35:11 2021 -0700,1624152911.0,"[special] Add special.ndtri (#58650)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50345

TODO
* [x] Add docs https://13865352-65600975-gh.circle-artifacts.com/0/docs/special.html#torch.special.ndtri
* [x] Add comments on implementation
* [x] Clean-up

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58650

Reviewed By: H-Huang

Differential Revision: D29160170

Pulled By: mruberry

fbshipit-source-id: 50e4ea663920e97b8437d03d5b52bcd9dedc1a8d",246.0,30.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",14.0,19,4,1.961788172,17.0,27282.0,9.0,1367115.4285714286,13165.0,29812.5,0.0,Feature Addition,0.0,1
pytorch,dd91d57c3f5647fb4ac63ac4325a42224f9a3028,5ed3f3347a5684a2b6208546e34c9a13771e77ab,gchanan,gregchanan@gmail.com,Tue Apr 17 03:52:59 2018 -0400,1523937179.0,"Add dtypes (with reasonable defaults) to sum, prod, cumsum, cumprod. (#6573)

* Add dtypes (with reasonable defaults) to sum, prod, cumsum, cumprod.

This adds optional dtypes to torch.sum, torch.prod, torch.cumsum, torch.cumprod.
By default, the dtype is torch.float64 for integral types, and the dtype of the input for floating point types.

* Don't use optional<ScalarType>, because the jit can't handle it yet.

Instead, we manually build the overloads.  This is fairly painful because of default arguments, but should be easy to pull out once the jit can handle optional<ScalarType>.

* Fix keepdim with out parameters.

* Fix _cudnn_rnn_flatten_weight.

* If dtype is provided to an out function, make sure it matches the dtype of the result.

* Fix typo.",325.0,36.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.h,tools/jit/gen_jit_dispatch.py,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h",13.0,13,4,2.194515793,39.0,16430.0,5.0,533766.3076923077,589.0,3488.0,0.0,Corrective,1.0,1
pytorch,d2ff733cb109d32049381884302650463b71d77e,5edf6b203749f3088569ccb5a83fbbe3ece97871,gchanan,gregchanan@gmail.com,Tue Feb 20 16:04:14 2018 -0500,1519142654.0,"Add numpy-style dtypes to Variable factories. (#5245)

* Add numpy-style dtypes to Variable factories.

1) Add numpy-style dtypes corresponding to torch tensor types.  These are:
torch.float16, torch.float32, torch.float64, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64
as well as torch.cuda, torch.sparse, and torch.cuda.sparse equivalents.

2) Adds ""legacy"" names for the above dtypes that correspond more closely to existing tensor names.  These are:
torch.half, torch.float, torch.double, torch.short, torch.int, torch.long.
torch.byte and torch.char don't exist because they either don't match numpy semantics or differ on different architectures.

3) Adds a ""dtype"" parameter to Variable factories (e.g. zeros, ones) that allows the user to specify the type without changing the default tensor type.

4) Adds a ""dtype"" getter to Variables that return the canonical dtype from 1)

This PR is missing the following useful features that should be added in the future:
A) We only add the ""dtype"" parameter to auto-generated factories; hand-written factories like in tensor_new.cpp don't support this yet.

B) We don't allow type conversions to use dtypes; that should be added to type(param) or a new function.

C) We don't yet have a ""device"" parameter for these factories; right now, they will only create Variables on the default device.

* backend_to_string can be private.

* Define python binding argument indexes in a more simple way.

* add all_declared_types, still need to hook it up to THPDType.

* Fix all_declared_types for missing types (it's Sparse + Half).

* Ensure cuda dtypes are created even if compiled with NO_CUDA=1.

* Fix case where dtype is provided but dispatch is via namespace.

This happens in ones_like, empty_like, randn_like.

There is some question if we should do:
1) at::ones_like(tensor).toType(dtype)
2) at::ones_like(tensor.toType(dtype))

I did the former because this matches with the numpy documentation, i.e.:
""Overrides the data type of the result."" and it's easier to implement.

Note that the above causes an extra copy, either of the input or output.
Here's a better implementation:
1) Make zeros_like, ones_like native functions that take an optional type (named dtype?).
2) Match the type argument with the dtype, so we don't have two different parameters.
3) Call at::zeros_like(input, type) -> at::native::zeros_like(input, type) -> type.zeros(input.sizes())

* Don't return from maybe_initialize_cuda.

* Don't leak DType name.

* Address cpp review comments.

* Share code between sparse and non-sparse test_dtypes.

* Rewrite _like functions as native function with explicit type parameter.

* Use type 'Type' instead of 'dtype' for consistency.

* Address review comments.

* Handle arg_idx when there is requires_grad but no dtype in python_binding_arguments.",565.0,54.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/ConvolutionTBC.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,setup.py,test/test_autograd.py,test/test_sparse.py,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/python_torch_functions.cpp,tools/autograd/templates/python_torch_functions_dispatch.h,tools/jit/gen_jit_dispatch.py,torch/__init__.py,torch/csrc/Dtype.cpp,torch/csrc/Dtype.h,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Module.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/utils/wrap_outputs.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_dtypes.cpp,torch/csrc/utils/tensor_dtypes.h,torch/csrc/utils/tensor_types.cpp,torch/csrc/utils/tensor_types.h",28.0,14,4,3.861405145,40.0,20836.0,18.0,1403720.375,536.0,1574.405869,0.0,Corrective,1.0,1
pytorch,e47aeede3249256b867a95a52744a80b15dd1399,5f06dcc4d72487a4091ea124bdf061a89ca28845,Lara Haidar,haidar.lara@gmail.com,Wed Feb 27 22:52:26 2019 -0800,1551307946.0,"ONNX Export Adaptive Pooling

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/17412

Differential Revision: D14247923

Pulled By: houseroad

fbshipit-source-id: 5530cea8f80da7368bff1e29cf89c45ad53accee",71.0,8.0,"test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic.py",2.0,4,2,0.957863024,12.0,2737.0,2.0,592238.0,7251.0,22154.33333,0.0,,0.0,1
pytorch,d3839b624b5f6451a13bd9b5ecbbce4c2a9b1db6,5f3f8fd3c7a3a912a6c7fb1f0a10ebe0b3757757,Jokeren,robinho364@gmail.com,Tue Feb 20 16:58:20 2024 +0000,1708448300.0,"[Inductor] Setting kernel launch and exit callbacks for inductor generated triton kernels (#119450)

`CompiledKernel.launch_enter_hook` and `CompiledKernel.launch_exit_hook` are hooks that allow external tools to monitor the execution of Triton kernels and read each kernel's metadata. Initially, these hooks have a value of `None`.

Triton's kernel launcher passes hooks and kernel metadata by default, while Inductor's launcher doesn't. This PR could unify the parameters passed to both launchers so that tools can get information from both handwritten Triton kernels and Inductor-generated Triton kernels.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/119450
Approved by: https://github.com/soulitzer",35.0,1.0,"test/inductor/test_profiler.py,torch/_inductor/triton_heuristics.py",2.0,4,2,0.764204507,1.0,1577.0,1.0,497639.0,25349.0,57216.5,0.0,,0.0,1
pytorch,5b8fe5cbb5b696d49d1480942f1904f92444857b,5f7c5502b8fd3c94c74a823f6a83d936892eb9dd,Edward Z. Yang,ezyang@mit.edu,Thu Dec 21 18:03:43 2017 -0500,1513879423.0,"Further improvements to ATen convolution (#4287)

- Rename THNN convolution to have thnn_ prefix.
- Propagate CuDNN benchmark and deterministic to at::Context
- Add 'convolution', 'convNd' and 'conv_transposeNd' native wrappers, with defaults
  The conv_transposeNd wrappers are updated to have the same argument
  order as Python.
- torch.nn.functional directly dispatches to the native wrappers
- Make it possible to turn off tracing for some native wrappers, so I don't
  have to write symbolics for all the functions above
- Spectral ops can now make use of CuDNN convolution if possible
- Better commentary on cudnn_batch_norm
- Turn on DCE for all JIT tests.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",337.0,235.0,"aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/nn.yaml,test/expect/TestJit.test_c_function.expect,test/expect/TestJit.test_conv.expect,test/test_jit.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/backends/cudnn/__init__.py,torch/csrc/Module.cpp,torch/nn/functional.py",15.0,13,4,2.476975033,39.0,13171.0,9.0,136045.4,390.0,1242.405869,0.0,Corrective,1.0,1
pytorch,5a7133b87fe2fd7d025d36855ed4cc06539a9299,5f997a7d2fcd81584d1c9f6e173e30c867892ee8,Pavithran Ramachandran,pavithran@fb.com,Fri Aug 20 16:34:53 2021 -0700,1629477293.0,"[PyTorch][Edge] Improve InflatableArgs for Bundled Inputs (#62368)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62368

# Context
The bundled inputs accepts an expression in the form of string InflatableArg.fmt that can be applied on the inputs to inflate. The InflatableArg.fmt provides flexibility to have custom transformation to inflate. When the input arguments to a function are not Tensor type, TorchScript casts the inputs from type T to Optional[T] expects the function to handle Nullable (None) clause as well. This becomes tricky to handle in one line code or lambda functions.

We propose an alternative way which allows InflatableArg to include the text of a TorchScript function that would be defined on the module as a helper, then use that in its inflation expression. This can be provided by InflatableArg.fmt_fn. Please refer to pytorch/test/test_bundled_inputs.py for example on how to use the same.

Also refer JacobSzwejbka comment on the same [here](https://github.com/pytorch/pytorch/pull/62368#issuecomment-892012812)

# Mitigation
Allow InflatedArg to include the text of a TorchScript function that would be defined on the module as a helper, then use that in its inflation expression.
ghstack-source-id: 135158680

Test Plan:
To run `test_dict_args`

```
(base) [pavithran@devvm1803.vll0 /data/users/pavithran/fbsource/fbcode] buck test //caffe2/test:test_bundled_inputs -- test_dict_args
Action graph will be rebuilt because files have been added or removed.
Building: finished in 5.4 sec (100%) 12180/12180 jobs, 0/12180 updated
  Total time: 5.8 sec
More details at https://www.internalfb.com/intern/buck/build/fafcf277-1095-4cba-978d-6022f0d391ad
Tpx test run coordinator for Facebook. See https://fburl.com/tpx for details.
Running with tpx session id: 5ef9de71-c1b1-406b-a6c0-3321c2368b8d
Trace available for this run at /tmp/tpx-20210727-163946.454212/trace.log
Started reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/7036874465805934
    â ListingSuccess: caffe2/test:test_bundled_inputs - main (11.365)
    â Pass: caffe2/test:test_bundled_inputs - test_dict_args (test_bundled_inputs.TestBundledInputs) (12.307)
Summary
  Pass: 1
  ListingSuccess: 1
If you need help understanding your runs, please follow the wiki: https://fburl.com/posting_in_tpx_users
Finished test run: https://www.internalfb.com/intern/testinfra/testrun/7036874465805934
```

To check the py code of TS module:
P433043973

Reviewed By: dreiss

Differential Revision: D29950421

fbshipit-source-id: c819ec5c94429b7fbf6c4beb0259457f169b08ec",180.0,10.0,"test/test_bundled_inputs.py,torch/utils/bundled_inputs.py",2.0,3,2,0.967788463,1.0,735.0,2.0,4440097.5,14804.0,33881.0,0.0,Feature Addition,0.0,1
pytorch,8c5a5a7c4dbd1f30c0839aece16882ab043259f4,5f99ec5ea43a5be9ff3226657b49d8317f29581a,Samantha Andow,samdow@fb.com,Fri May 13 21:17:55 2022 -0400,1652476675.0,[functorch] fix ci (pytorch/functorch#804),103.0,1.0,"functorch/test/test_eager_transforms.py,functorch/test/test_ops.py",2.0,2,1,0.137099479,1.0,4368.0,2.0,0.0,1070.0,1459.0,0.0,Corrective,1.0,1
pytorch,63e16216d8830b6340816c873b035e1a31ad4636,5faa2792fa3c46f2124d1d1c5f7b6a3865d47d7b,Sherlock Huang,bahuang@fb.com,Tue Nov 15 01:06:23 2022 +0000,1668474383.0,"Symintify decomps for split and upsample_bilinear; Fix decomp for _softmax_backward_data and native_dropout_backward (#88761)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88761
Approved by: https://github.com/ezyang",177.0,64.0,"test/dynamo/test_dynamic_shapes.py,test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,test/test_decomp.py,test/test_proxy_tensor.py,torch/_decomp/decompositions.py,torch/testing/_internal/common_methods_invocations.py",9.0,8,2,1.975089596,7.0,31773.0,6.0,94882.0,9502.0,22155.0,0.0,Corrective,1.0,1
pytorch,c6d9ca0c2b4bb124a902db00393372c0fc12b190,5fb1142702320f0d52a3d87a94ab4c93220013c9,Sameer Deshmukh,sameer.deshmukh93@gmail.com,Mon Apr 12 17:07:56 2021 -0700,1618247276.0,"Add CSR (compressed sparse row) layout for sparse tensors (#50937)

Summary:
Implement compressed sparse row format. Derived from the GCS implementation at https://github.com/pytorch/pytorch/pull/44190

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50937

Reviewed By: mrshenli

Differential Revision: D27439865

Pulled By: ezyang

fbshipit-source-id: 3ba3dcb9679505b980ff6a5f513e913bbae2fb1d",2310.0,201.0,"BUILD.bazel,aten/src/ATen/SparseCsrTensorImpl.cpp,aten/src/ATen/SparseCsrTensorImpl.h,aten/src/ATen/SparseCsrTensorUtils.h,aten/src/ATen/core/DeprecatedTypeProperties.h,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/Resize.cpp,aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/native/mkl/SparseCsrLinearAlgebra.cpp,aten/src/ATen/native/mkl/SparseCsrLinearAlgebra.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseCsrTensor.cpp,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/templates/TensorBody.h,benchmarks/README.md,benchmarks/sparse/README.md,benchmarks/sparse/__init__.py,benchmarks/sparse/spmm.py,benchmarks/sparse/spmv.py,benchmarks/sparse/test_csr.sh,benchmarks/sparse/utils.py,c10/core/Backend.h,c10/core/DispatchKey.cpp,c10/core/DispatchKey.h,c10/core/DispatchKeySet.h,c10/core/Layout.h,c10/core/TensorImpl.h,c10/core/TensorOptions.h,docs/source/name_inference.rst,docs/source/sparse.rst,test/test_jit.py,test/test_sparse_csr.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp,tools/build_variables.bzl,tools/codegen/gen.py,tools/codegen/model.py,tools/pyi/gen_pyi.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/csrc/autograd/python_variable.cpp,torch/csrc/jit/frontend/sugared_value.cpp,torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/tensor_layouts.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h,torch/overrides.py",52.0,28,7,4.220936714,45.0,64880.0,32.0,2718339.5789473685,10657.0,23583.5,0.0,Feature Addition,0.0,1
pytorch,f665a7f8a1e071daa548b62d4b2561d3dd723472,60263e0f5acb7c0dbb15c824a711cf7ec8c478e1,Xue Haotian,njxht@foxmail.com,Thu Apr 08 17:13:19 2021 -0700,1617901999.0,"OpInfo porting for torch.maximum / torch.minimum / torch.fmax / torch.fmin (#55129)

Summary:
Related https://github.com/pytorch/pytorch/issues/54261

This PR ports the method_tests() entries of following operators to OpInfo.
- torch.maximum
- torch.minimum
- torch.fmax
- torch.fmin

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55129

Reviewed By: ngimel

Differential Revision: D27562189

Pulled By: mruberry

fbshipit-source-id: 9f25aeb09eb353080af43f25ea2e931474510aca",17.0,4.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,5030.0,1.0,15975.0,10569.0,23368.5,0.0,,0.0,1
pytorch,849188fdab144337ca73ff8dbc4382bb63d4ee2d,6027513574c3ce526c0a3573d1f9516862407173,Adam Paszke,adam.paszke@gmail.com,Thu Oct 27 15:17:01 2016 +0200,1477581421.0,Add support for indexing with numpy types,41.0,8.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.642064289,11.0,3252.0,1.0,103137.0,265.0,2347.64599,0.0,Feature Addition,0.0,1
pytorch,759c37a4f4fb1962c37650bf24d88a7fa0918a5e,60295e3abde373a1ca7ceea518ef65c8d7c7f058,Richard Zou,zou3519@gmail.com,Mon Aug 15 15:42:38 2022 -0700,1660578158.0,"[functorch] Delete functorch_lagging_op_db (#83418)

No need to have a lagging op db because there are no more sync issues
between functorch and pytorch. If someone adds a new OpInfo, then we
should explicitly check if we support it or not.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83418
Approved by: https://github.com/samdow",22.0,719.0,"functorch/codegen/gen_functorch_lagging_op_db.py,functorch/test/common_utils.py,functorch/test/discover_coverage.py,functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",7.0,3,1,0.856240364,2.0,8475.0,5.0,769414.4285714285,6427.0,14934.0,0.0,Feature Addition,0.0,1
pytorch,1df99e541c1c0380d4bef796bc56d1aa31fb4f78,602a09dde7a594cd78d0ce5b7bf13cb42b05c51d,sf-wind,sf_wind@hotmail.com,Mon Mar 12 19:22:59 2018 -0700,1520882579.0,"Update caffe2 from facebook 4f527ef46abf (#2234)

* [GanH]: two_task_discriminator

as titled

and adding label smooth

* [Dper2] Simplified UI options needed for blob magnitude visualization

* [GanH]: fix tags

as titled

* Added type and shape inference for GatherRange operator

This helps with type / shape inference when using this operator in layers.
Also just a nice to have in general.

* Demonstrate Caffe2 exception handling with StoreHandlerTimeoutError in Python

We'd like to catch and recover from certain Caffe2 net exceptions. Use this diff to demonstrate a pattern of registering a pybind exception mapping and catching in Pythonusing caffe2::StoreHandlerTimeoutException.

* Bind Gloo IoException to IoError in Python

Allow peer failure handling and recovery using an exception based mechanism. This diff registers gloo::IoException with pybind.

* [GanH]: add label smoothing to softmax with loss

as titled

* [C2] Enable LARS in Adagrad and hook it to DPER

* [DPER] Don't pass LayerModelHelper in create_trainer_nodes

Since we're planning to get rid of it eventually and I want to get access to
NetDef only interface ASAP - I'm looking towards removing all references to
LMH, where we don't really need them.

* fix bugs in LambdaRankNdcgOp

the loss and gradient in LambdaRankNdcgOp are incorrect. The loss should be negative log of probs instead of log.

* Restrict thread pool on iOS to only big cores

Historically, iPhones exposed only one type of cores, and Caffe2 thread pool used all of them.
However, iPhone 8/iPhone X exposes 2 big + 4 LITTLE cores. As our thread pool doesn't support work stealing or other forms of load balancing, fast cores end up waiting for the slow ones, and it may be better to restrict execution to only 2 fast cores, like we do on Android.

* Remove SparseLength Sum/WeightedSum/Mean operators with fp16 engine

Remove SparseLength Sum/WeightedSum/Mean operators with fp16 engine

* make clang happy and get fewer warnings

make clang happy and get fewer warnings

* [Personalization] Support add_output_schema() in layer_model_helper

Problem:
Currently the output_schema of sparse_nn can only be set once. https://fburl.com/efth5zer.

Solution:
For flexibility, we want to add fields to output_schema incrementally.

Plan:
Wrap the change of `model._output_schema` into a new function `add_output_schema()` for adding additional output_schema.

Callsite:
The add_output_schema() should be called instead at https://fburl.com/efth5zer

Reference:
The newly added `add_output_schema()` will be similar to `add_loss()` in https://fburl.com/t2ii8njh",364.0,75.0,"caffe2/contrib/gloo/gloo_test.py,caffe2/contrib/gloo/py_export.cc,caffe2/distributed/file_store_handler_op_test.py,caffe2/distributed/py_export.cc,caffe2/distributed/redis_store_handler_op_test.py,caffe2/distributed/store_ops_test_util.py,caffe2/operators/listwise_l2r_op.cc,caffe2/operators/utility_ops.cc,caffe2/python/layer_model_helper.py,caffe2/python/layers/batch_lr_loss.py,caffe2/python/layers/batch_softmax_loss.py,caffe2/python/layers/select_record_by_context.py,caffe2/python/layers/sparse_lookup.py,caffe2/python/layers_test.py,caffe2/python/modeling/compute_norm_for_blobs.py,caffe2/python/modeling/compute_norm_for_blobs_test.py,caffe2/python/modeling/compute_statistics_for_blobs.py,caffe2/python/modeling/compute_statistics_for_blobs_test.py,caffe2/python/operator_test/gather_ranges_op_test.py,caffe2/python/operator_test/listwise_l2r_operator_test.py,caffe2/python/optimizer.py,caffe2/python/optimizer_test.py,caffe2/python/pybind_state_mkl.cc,caffe2/utils/threadpool/ThreadPool.cc",24.0,11,1,4.120671291,9.0,7671.0,13.0,4591962.636363637,757.0,1760.305292,0.0,Corrective,1.0,1
pytorch,6701cc7c8e7f5d09fcff6bf56761d01b33c99cc0,6039f007c4d0a985a76c476d6dc8b5ac95cf9104,Edward Z. Yang,ezyang@fb.com,Tue Aug 22 14:39:07 2017 -0700,1503412747.0,"Make assertExpected Python 2 friendly.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",8.0,4.0,test/common.py,1.0,1,1,0,35.0,341.0,1.0,0.0,1593.0,23887.09738,0.0,,0.0,1
pytorch,a152c12d7bc29929660edbc7db272afc5179f78e,604f503d301f9942e2de3f163577400b716a989f,Nikita Shulga,nshulga@fb.com,Wed Jul 21 01:13:32 2021 -0700,1626830012.0,"Revert D29794958 + compilation fix (#61937)

Summary:
This PR un-reverts https://github.com/pytorch/pytorch/issues/61475 + fixes compilation with MSVC, that does not recognize alternative operator spellings (i.e. using `or` instead of `||` )

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61937

Reviewed By: albanD

Differential Revision: D29805941

Pulled By: malfet

fbshipit-source-id: 01e5963c6717c1b44b260300d87ba0bf57f26ce9",177.0,55.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/Integration.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_autograd.py,test/test_binary_ufuncs.py,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",10.0,13,5,1.877116633,44.0,45654.0,1.0,8039.0,13993.0,31504.0,0.0,Corrective,1.0,1
pytorch,7355f5cd8dc52a048d8c367cabfed9e888acd586,605307f8f3c249d9279030502d2aac98d4170b83,Kaiyu Shi,skyisno.1@gmail.com,Mon Apr 02 17:52:33 2018 +0800,1522691553.0,"Add support for printing extra information in Module and refactor redundant codes (#5936)

This PR enables users to print extra information of their subclassed nn.Module.
Now I simply insert the user-defined string at the ending of module name, which should be discussed in this PR.

Before this PR, users should redefine the __repr__ and copy&paste the source code from Module.

* Add support for extra information on Module

* Rewrite the repr method of Module

* Fix flake8

* Change the __repr__ to get_extra_repr in Linear

* Fix extra new-line for empty line

* Add test for __repr__ method

* Fix bug of block string indent

* Add indent for multi-line repr test.

* Address review comments

* Update tutorial for creating nn.Module

* Fix flake8, add extra_repr of bilinear

* Refactor DropoutNd

* Change to extra_repr in some Modules

* Fix flake8

* Refactor padding modules

* Refactor pooling module

* Fix typo

* Change to extra_repr

* Fix bug for GroupNorm

* Fix bug for LayerNorm",402.0,589.0,"docs/source/notes/extending.rst,test/test_nn.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/fold.py,torch/nn/modules/linear.py,torch/nn/modules/module.py,torch/nn/modules/normalization.py,torch/nn/modules/padding.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/modules/upsampling.py",17.0,7,3,2.499150629,38.0,13739.0,2.0,246209.23529411765,560.0,3427.5,0.0,Corrective,1.0,1
pytorch,107e0676540e7cdbe38fadeacbfffdb2d7a1e087,607094c4bf19ca1b9463afbe7d93e63176b702eb,Igor Sugak,sugak@fb.com,Fri Nov 09 18:02:00 2018 -0800,1541786520.0,"fix null-pointer-use in reshape_op.h

Summary:
UndefinedBehaviorSanitizer: null-pointer-use ../fbcode/third-party-buck/gcc-5-glibc-2.23/build/libgcc/include/c++/5.5.0/bits/stl_vector.h:794:16
```
Here we take the address of the first element in the empty vector. Fix the error by guarding against empty source.

Reviewed By: pixelb

Differential Revision: D12989957

fbshipit-source-id: ac5ec366385df835b546bd1756e30cd762f13a7a",5.0,2.0,caffe2/operators/reshape_op.h,1.0,2,1,0,7.0,139.0,1.0,235709.0,5289.0,15986.83333,0.0,Corrective,1.0,1
pytorch,"eea54cc065be04e771a6c6419f1c3ad20aae3506,5ef96aadd9287ef1f0c10d0469097fd9439efcd7",608327b156d5477ba7a73fdcc67a5cd3d3fa3a60,Soumith Chintala,soumith@gmail.com,Thu Sep 07 15:55:26 2017 -0400,1504799726.0,Merge commit '5ef96aadd9287ef1f0c10d0469097fd9439efcd7',4.0,0.0,torch/lib/THCUNN/CMakeLists.txt,1.0,3,1,0,35.0,91.0,1.0,33221.0,1685.0,19378.8043,0.0,Feature Addition,0.0,1
pytorch,4182c1183bf12c9c5af1621dc9f03fdf2b97a30b,60a1efe13835481d56417c4a13556c74a63f5e6e,Owen Anderson,owen.anderson@oculus.com,Fri Oct 25 00:16:14 2019 -0700,1571962574.0,"Eliminate some unnecessary tensor refcount bumps.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28355

Differential Revision: D18129161

fbshipit-source-id: 493cf0c1d754a375ec6c73dd57cd985639c849b7",4.0,3.0,aten/src/ATen/native/Linear.cpp,1.0,4,1,0,7.0,531.0,1.0,10528787.0,12526.0,34883.83333,0.0,,0.0,1
pytorch,ea11c30df62ec436225197e6073f1421694aac2e,60c03bc09c86c8ff15010cd09437d8957f9bb0aa,Sam Gross,colesbury@gmail.com,Thu Dec 07 19:48:56 2017 -0500,1512676136.0,"Implement apply_, map_, and map2_ in Variable (#4057)",227.0,2.0,"aten/src/ATen/templates/Type.cpp,aten/src/ATen/templates/Type.h,setup.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_apply.cpp,torch/csrc/utils/tensor_apply.h",9.0,11,4,1.9419595,39.0,7014.0,4.0,982416.7142857144,365.0,1094.405869,0.0,,0.0,1
pytorch,0c48092b2270d56cdab327bd1ff0ca89f5b7d569,60d606094c75f18876a8ec24e41abaf5d49bc25b,Negin Raoof,neginmr@utexas.edu,Fri Oct 25 23:57:32 2019 -0700,1572047852.0,"Export Meshgrid (#26037)

Summary:
Exporting meshgrid op in opset 9 symbolics
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26037

Reviewed By: hl475

Differential Revision: D17452325

Pulled By: houseroad

fbshipit-source-id: d556b78e46594a232cdefd8c257cccd8b98221d6",380.0,5.0,"test/onnx/expect/TestOperators.test_meshgrid.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/peephole.cpp,torch/onnx/symbolic_opset7.py,torch/onnx/symbolic_opset9.py",7.0,9,2,1.265985342,9.0,7791.0,6.0,1837486.8333333333,12561.0,34964.33333,0.0,,0.0,1
pytorch,58319030bdee120fcb9f57cf1960417ac267efed,60fbba9d0e40ed6905c11a9f957663ed81a725cb,Horace He,horacehe2007@yahoo.com,Sat Jun 26 08:47:01 2021 -0700,1624697221.0,[functorch] Added erf/inverse/isinf/isnan batching rules,15.0,0.0,"functorch/functorch/csrc/BatchRulesUnaryOps.cpp,functorch/test/test_vmap.py",2.0,4,1,0.836640742,1.0,3094.0,1.0,0.0,171.0,320.5,0.0,Feature Addition,0.0,1
pytorch,5248dd1a51b22af84d91acea1e131f8ed74863b6,6100de9b1b911ca5ab4ef9628c588249c72fc758,SsnL,tongzhou.wang.1994@gmail.com,Wed Aug 28 19:21:36 2019 -0700,1567020096.0,"implement bool_tensor.bernoulli_ (#25076)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/25072
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25076

Differential Revision: D17073453

Pulled By: ezyang

fbshipit-source-id: 42410da8c9911c1d7b3543bde740c7e66ae0cc1c",15.0,9.0,"aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Distributions.cu,test/test_cuda.py,test/test_torch.py",5.0,7,2,1.988362071,41.0,17727.0,5.0,859851.4,10967.0,31006.83333,0.0,Corrective,1.0,1
pytorch,05269b582bfaf560cf5cbf93922c1006777c464d,611c771fc8b9940005c0e2431e6a27495dd0ac5c,gchanan,gregchanan@gmail.com,Tue Feb 27 00:10:29 2018 -0500,1519690229.0,"Introduce torch.tensor (was torch.autograd.variable). (#5419)

* Introduce torch.tensor (was torch.autograd.variable).

* Get rid of torch.variable usages.

* Use more precise name.",13.0,13.0,"test/test_distributions.py,test/test_indexing.py,test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp,torch/autograd/__init__.py",5.0,6,3,2.045841641,38.0,9223.0,2.0,816047.8,559.0,1678.905869,0.0,Feature Addition,0.0,1
pytorch,89d556f648e71e2a4e560de1e72db4d7d8ccb183,613c1aca6dcc5c9764d81ddfb1e33f7b7b06bd5f,Mike Ruberry,mruberry@fb.com,Fri Nov 12 03:26:49 2021 -0800,1636687609.0,"Adds support for automated error and warning testing (#67354)

Summary:
Adds a new class `ErrorOrWarningInput` that is a `SampleInput` with some additional metadata for validating that `SampleInput` throws the desired warning or error. The architecture to support these new tests is modeled after the existing reference tests and sample input functions.

Existing invalid input tests for neg and kthvalue are ported to the new scheme to validate it.

There may be a simpler/clearer naming scheme we can use here.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67354

Reviewed By: jbschlosser

Differential Revision: D31989888

Pulled By: mruberry

fbshipit-source-id: 4fa816e1e8d0eef21b81c2f80813d42b2c26714e",113.0,61.0,"test/test_ops.py,test/test_sort_and_select.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,0.979777591,2.0,16626.0,3.0,635108.75,17046.0,40084.5,0.0,Feature Addition,0.0,1
pytorch,7163bfdf589605a96e46f4ec5ff1a5de9c502433,614edfce81ae81897ec8a5a199b86e61c1ebf7ff,Lara Haidar,haidar.lara@gmail.com,Fri Sep 27 05:29:20 2019 -0700,1569562160.0,"Add Support to Dicts and Strings in ONNX for Inputs and Outputs (#25889)

Summary:
ONNX does not support dictionaries for inputs and output. The reason is that the arg flattening and unflattening does not handle Dictionary types.
This PR adds flattening/unflattening support for dictionaries and strings.
However this feature should be handled with caution for input dictionaries; and users need to verify their dict inputs carefully, and keep in mind that dynamic lookups are not available.

This PR will allow exporting cases where models have dictionnary outputs (detection and segmentation models in torchvision), and where dictionary inputs are used for model configurations (MultiScaleRoiAlign in torchvision).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25889

Reviewed By: hl475

Differential Revision: D17613605

Pulled By: houseroad

fbshipit-source-id: c62da4f35e5dc2aa23a85dfd5e2e11f63e9174db",266.0,11.0,"test/onnx/expect/TestOperators.test_dict.expect,test/onnx/expect/TestOperators.test_dict_str.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/python_arg_flatten.cpp,torch/csrc/jit/python_arg_flatten.h,torch/jit/__init__.py,torch/onnx/utils.py",8.0,8,2,2.655510058,15.0,5311.0,6.0,4585824.666666667,11820.0,33118.33333,0.0,Feature Addition,0.0,1
pytorch,dd27997aeb2fe220a00d0731c0af88f6540942b1,61813cfd977b366bfa964d987d5066a3906879b8,peterjc123,peter_jiachen@163.com,Sun Sep 17 03:40:58 2017 +0800,1505619658.0,"Improve Windows Compatibility(for lib/TH) (#2439)

* Win64 support for lib/TH

* Edit codes to clear warnings(for TH)

* fix format string

* revert modulo changes

* change formats for snprintf",941.0,868.0,"torch/lib/TH/THAllocator.c,torch/lib/TH/THAllocator.h,torch/lib/TH/THAtomic.c,torch/lib/TH/THAtomic.h,torch/lib/TH/THDiskFile.c,torch/lib/TH/THFile.c,torch/lib/TH/THFile.h,torch/lib/TH/THFilePrivate.h,torch/lib/TH/THGeneral.c,torch/lib/TH/THGeneral.h.in,torch/lib/TH/THGenerateByteType.h,torch/lib/TH/THGenerateCharType.h,torch/lib/TH/THGenerateIntType.h,torch/lib/TH/THGenerateLongType.h,torch/lib/TH/THGenerateShortType.h,torch/lib/TH/THMemoryFile.c,torch/lib/TH/THRandom.c,torch/lib/TH/THRandom.h,torch/lib/TH/THSize.c,torch/lib/TH/THSize.h,torch/lib/TH/THStorage.c,torch/lib/TH/THStorage.h,torch/lib/TH/THTensorApply.h,torch/lib/TH/THTensorDimApply.h,torch/lib/TH/generic/THBlas.c,torch/lib/TH/generic/THBlas.h,torch/lib/TH/generic/THTensor.c,torch/lib/TH/generic/THTensor.h,torch/lib/TH/generic/THTensorConv.c,torch/lib/TH/generic/THTensorConv.h,torch/lib/TH/generic/THTensorCopy.c,torch/lib/TH/generic/THTensorLapack.c,torch/lib/TH/generic/THTensorMath.c,torch/lib/TH/generic/THTensorMath.h,torch/lib/TH/generic/THTensorRandom.c,torch/lib/TH/generic/THTensorRandom.h,torch/lib/TH/generic/simd/convolve.c,torch/lib/TH/generic/simd/convolve.h,torch/lib/TH/generic/simd/convolve5x5_avx.c,torch/lib/TH/generic/simd/convolve5x5_sse.c,torch/lib/TH/vector/NEON.c",41.0,6,1,4.278218153,37.0,14100.0,1.0,280967.0,1734.0,22367.05562,0.0,Corrective,1.0,1
pytorch,cc7a28d7271ad658aacfe034789b182e8a2ebdc2,61b074581ce1ccf0fb1bf4f1b73f4b99f93fa70c,Nikita Vedeneev,nik@quansight.com,Fri Mar 19 16:42:53 2021 -0700,1616172173.0,"`torch.prod` backward for complex types. (#48125)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/53511
torch.det does depend on torch.prod, which in turn depends on several other functions, and they also depend on torch.prod, so there is a circular relationship, hence this PR will enable complex backward support for several functions at once.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48125

Reviewed By: pbelevich

Differential Revision: D27188589

Pulled By: anjali411

fbshipit-source-id: bbb80f8ecb83a0c3bea2b917627d3cd3b84eb09a",124.0,48.0,"aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/cuda/Indexing.cu,test/test_cuda.py,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",7.0,13,4,1.355152861,43.0,16960.0,6.0,141832.57142857142,9921.0,21979.5,0.0,Corrective,1.0,1
pytorch,748d011c8bf120bc5ac6111b4fc5e0c8a1640f94,61bd5a0643b19700553745c89a532df1684f6cff,Luke Yeager,lukeyeager@users.noreply.github.com,Mon Feb 27 22:45:47 2017 -0800,1488235547.0,[Lint] Address F811,6.0,16.0,"test/test_cuda.py,test/test_torch.py,test/test_utils.py,torch/autograd/variable.py,tox.ini",5.0,3,2,2.071735027,24.0,4941.0,5.0,331690.0,444.0,3921.575745,0.0,Feature Addition,0.0,1
pytorch,5ca4f5b43b63882794951aec8fcb81a7595414d0,61c96811be8051e39b1d5663b2c3538cafb93fd0,Teng Li,teng-li@users.noreply.github.com,Tue Jun 19 20:02:39 2018 -0700,1529438559.0,"[c10d] NCCL python binding and CI test, with bug fixes (#8357)

* [c10d] NCCL python binding and CI test, with bug fixes

* Addressed comments and further bug fix

* Made NCCL build optional, made C10D libc10d.a only

* Fixed tests so that NCCL pg won't run when not neeeded

* Addressed comments",219.0,74.0,"setup.py,test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/CMakeLists.txt,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/lib/c10d/cmake/Def.hpp.in,torch/lib/c10d/example/CMakeLists.txt,torch/lib/c10d/test/CMakeLists.txt,torch/lib/c10d/test/ProcessGroupMPITest.cpp",10.0,10,2,2.356754393,42.0,2365.0,7.0,1020998.6666666666,1367.0,3996.305292,0.0,Corrective,1.0,1
pytorch,4256dbe2d067e009360f23db4b60b9cc9effa58e,6204877cd44594541956d164e324642cae523d59,Sam Gross,colesbury@gmail.com,Wed Feb 14 07:14:40 2018 -0500,1518592480.0,"Allow zero-dim tensors to be bound to at::Scalar (#5142)

* Allow zero-dim tensors to be bound to at::Scalar

This relaxes THPUtils_unpackLong and THPUtils_unpackDouble to allow
values convertable to PyLong and PyFloat objects. This includes NumPy
scalars and zero-dim tensors (Variables).

This is important to maintain backwards compatibility in the Tensor
constructors once scalars are enabled and Variable and Tensor are
merged.

* Add comment and unpack PyInt as int64_t",49.0,17.0,"test/test_torch.py,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/python_numbers.h",4.0,4,2,1.920583387,38.0,6362.0,3.0,1879348.5,444.0,2283.0,0.0,Feature Addition,0.0,1
pytorch,13b1580613022921be478d6324944fcac697fc95,62063b2f62ad21cf8bb6401a4bb03c152045693a,Alykhan Tejani,alykhan.tejani@gmail.com,Wed Mar 15 15:08:05 2017 +0000,1489590485.0,"Fix docs for pointwise ops (#845) (#985)

* add torch.nn.init docs to the source folder",60.0,20.0,torch/_torch_docs.py,1.0,1,1,0,12.0,4325.0,1.0,597499.0,542.0,3540.055248,0.0,Corrective,1.0,1
pytorch,4829dcea0920ba89719a658238a44f7c931d536e,620a1fcb55c0658d57aecc2b68b4128482023669,Richard Zou,zou3519@gmail.com,Tue Dec 14 14:47:17 2021 -0800,1639493237.0,"OpInfos for: normal, bernoulli, multinomial (#66358)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66358

Test Plan: - run tests

Reviewed By: mruberry

Differential Revision: D31551695

Pulled By: zou3519

fbshipit-source-id: cf1b43118a0414a1af9ece9ae8c0598b2701aa0a",140.0,1.0,"test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.209159275,2.0,20686.0,3.0,438696.3333333333,17708.0,41848.5,0.0,,0.0,1
pytorch,bf4c269bee8a7c0687122a2f59a35153432bd9e8,6214487fa7901327dc6d55ee29048235baf82538,Richard Zou,zou3519@users.noreply.github.com,Wed Nov 01 10:33:18 2017 -0400,1509532398.0,"Add reduce keyword to L1Loss (#3366)

* Add reduce keyword to L1Loss

* Fix legacy test for abscriterion

* Address comments",142.0,26.0,"test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/legacy/nn/AbsCriterion.py,torch/lib/ATen/nn.yaml,torch/lib/THCUNN/AbsCriterion.cu,torch/lib/THCUNN/generic/AbsCriterion.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THNN/generic/AbsCriterion.c,torch/lib/THNN/generic/THNN.h,torch/nn/functional.py,torch/nn/modules/loss.py",12.0,15,3,3.066557624,39.0,11807.0,3.0,170547.83333333334,2047.0,23925.35823,0.0,Corrective,1.0,1
pytorch,ba818991afb4ef6189fa4ed57dd204173e9c7b74,621ff0f9735cd8c4c5d6becb291ad050c35e01c0,lezcano,lezcano-93@hotmail.com,Thu May 05 21:21:48 2022 +0000,1651785708.0,"Add linalg.vander

This PR adds `linalg.vander`, the linalg version of `torch.vander`.

We add autograd support and support for batched inputs.

We also take this chance to improve the docs (TODO: Check that they
render correctly!) and add an OpInfo.

**Discussion**: The current default for the `increasing` kwargs is extremely
odd as it is the opposite of the classical definition (see
[wiki](https://en.wikipedia.org/wiki/Vandermonde_matrix)). This is
reflected in the docs, where I explicit both the odd defaults that we
use and the classical definition. See also [this stackoverflow
post](https://stackoverflow.com/a/71758047/5280578), which shows how
people are confused by this defaults.

My take on this would be to correct the default to be `increasing=True`
and document the divergence with NumPy (as we do for other `linalg`
functions) as:

- It is what people expect
- It gives the correct determinant called ""the Vandermonde determinant"" rather than (-1)^{n-1} times the Vandermonde det (ugh).
- [Minor] It is more efficient (no `flip` needed)
- Since it's under `linalg.vander`, it's strictly not a drop-in replacement for `np.vander`.

We will deprecate `torch.vander` in a PR after this one in this stack
(once we settle on what's the correct default).

Thoughts? mruberry

cc kgryte rgommers as they might have some context for the defaults of
NumPy.

Fixes https://github.com/pytorch/pytorch/issues/60197

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76303

Approved by: https://github.com/albanD, https://github.com/mruberry",132.0,2.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/allowlist_for_publicAPI.json,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",7.0,11,4,1.993325054,16.0,43166.0,4.0,48356.71428571428,2923.0,7029.0,0.0,Corrective,1.0,1
pytorch,eca0ef5e425718277062654d30a0060ac274a1a1,6223bfdb1d3273a57b58b2a04c25c6114eaf3911,Orion Reblitz-Richardson,orionr@gmail.com,Wed Apr 18 06:36:40 2018 -0700,1524033400.0,"Update from Facebook (#6692)

* [GanH][Easy]: Add assertion to adaptive weighting layer

0 weight causes numeric instability and exploding ne

* [Easy] Add cast op before computing norm in diagnose options

As LpNorm only takes floats we add a manual casting here.

* Introduce a new caching device allocator

`cudaMalloc` and `cudaFree` calls are slow, and become slower the
more GPUs there are. Essentially, they grab a host-wide (not device-wide) lock
because GPU memory is transparently shared across all GPUs. Normally, this
isn't much of a concern since workloads allocate memory upfront, and reuse it
during later computation.

However, under some computation models (specifically, memory conserving
approaches like checkpoint-and-recompute, see
https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9)
this assumption is no longer true. In these situations, `cudaMalloc` and
`cudaFree` are common and frequent. Furthermore, in data parallel contexts,
these calls happen at nearly the same time from all GPUs worsening lock
contention.

A common solution to this problem is to add a custom allocator. In fact,
nVIDIA provides one out of the box: CUB, which Caffe2 already supports.
Unfortunately, the CUB allocator suffers from very high fragmentation. This is
primarily because it is a ""buddy"" allocator which neither splits nor merges
free cached blocks. Study
https://github.com/NVlabs/cub/blob/1.8.0/cub/util_allocator.cuh#L357 if you
want to convince yourself.

This diff adapts a caching allocator from the Torch codebase
https://github.com/torch/cutorch/blob/master/lib/THC/THCCachingAllocator.cpp
which does splitting and merging and ends up working really well, at least for
workloads like the checkpoint-and-recompute computation models noted above.

I simplified the implementation a little bit, made it a bit more C++-like. I
also removed a bunch of stream synchronization primitives for this diff. I
plan to add them back in subsequent diffs.

* Report reader progress in fblearner workflows

Integrate with fblearner progress reporting API and add support to report training progress from reader nodes.
If reader is constructed with batch limits, report based on finished batch vs total batch. The finished batch may be more than total batch because we evaludate if we should stop processing everytime we dequeue a split.
If no limit for the reader, report based on finished splits (Hive files) vs total splits. This is fairly accurate.

* [GanH][Diagnose]: fix plotting

1. ganh diagnose needs to set plot options
2. modifier's blob name is used for metric field can need to be fixed before
generating net

* Automatic update of fbcode/onnx to 985af3f5a0f7e7d29bc0ee6b13047e7ead9c90c8

* Make CompositeReader stops as soon as one reader finishes

Previously, CompositeReader calls all readers before stopping. It results in flaky test since the last batch may be read by different threads; resulting in dropped data.

* [dper] make sure loss is not nan

as desc.

* [rosetta2] [mobile-vision] Option to export NHWC order for RoIWarp/RoIAlign

Thanks for finding this @stzpz and @wangyanghan. Looks like NHWC is more
optimized. For OCR though it doesn't yet help since NHWC uses more mem b/w but
will soon become important.

* Intra-op parallel FC operator

Intra-op parallel FC operator

* [C2 Proto] extra info in device option

passing extra information in device option

design doc: https://fb.quip.com/yAiuAXkRXZGx

* Unregister MKL fallbacks for NCHW conversions

* Tracing for more executors

Modified Tracer to work with other executors and add more tracing

* Remove ShiftActivationDevices()

* Check for blob entry iff it is present

When processing the placeholders ops, ignore if the blob is not present in the blob_to_device.

* Internalize use of eigen tensor

Move use of eigen tensor out of the header file so we don't get template partial specialization errors when building other libraries.

* feature importance for transformed features.

* - Fix unused parameter warnings

The changes in this diff comments out unused parameters.
This will allow us to enable -Wunused-parameter as error.

#accept2ship

* add opencv dependencies to caffe2

The video input op requires additional opencv packages. This is to add them to
cmake so that it can build

* Add clip_by_value option in gradient clipping

Add clip_by_value option in gradient clipping

when the value is bigger than max or smaller than min, do the clip

* std::round compat",1140.0,567.0,"caffe2/core/THCCachingAllocator.cu,caffe2/core/THCCachingAllocator.h,caffe2/core/common.h,caffe2/core/context_gpu.cu,caffe2/core/context_gpu.h,caffe2/core/net_async_base.cc,caffe2/core/net_async_base.h,caffe2/core/net_async_dag_gpu.cc,caffe2/core/net_async_tracing.cc,caffe2/core/net_async_tracing.h,caffe2/core/net_dag.cc,caffe2/core/net_dag.h,caffe2/core/operator.h,caffe2/core/registry.h,caffe2/operators/dataset_ops.cc,caffe2/operators/enforce_finite_op.cc,caffe2/operators/enforce_finite_op.cu,caffe2/operators/enforce_finite_op.h,caffe2/operators/reduction_ops.h,caffe2/operators/rnn/recurrent_network_executor.cc,caffe2/proto/caffe2.proto,caffe2/python/core.py,caffe2/python/core_test.py,caffe2/python/data_parallel_model_test.py,caffe2/python/data_parallel_model_utils.py,caffe2/python/dataio.py,caffe2/python/dataio_test.py,caffe2/python/examples/resnet50_trainer.py,caffe2/python/layers/adaptive_weight.py,caffe2/python/layers/tags.py,caffe2/python/modeling/compute_norm_for_blobs.py,caffe2/python/modeling/compute_norm_for_blobs_test.py,caffe2/python/modeling/gradient_clipping.py,caffe2/python/modeling/gradient_clipping_test.py,caffe2/python/net_builder.py,caffe2/python/onnx/backend.py,caffe2/python/operator_test/enforce_finite_op_test.py,caffe2/utils/math.h,caffe2/utils/math_cpu.cc,caffe2/utils/thread_pool.h,cmake/Dependencies.cmake",41.0,13,2,3.808398912,20.0,18332.0,14.0,1309335.705882353,24.0,73.5,0.0,Corrective,1.0,1
pytorch,48b6b9221a544f7d5b7d6f93eec2fa304da8af94,627a33125735a0683083f3e2b56e17fee80741b8,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Mon Jan 25 10:53:56 2021 -0800,1611572036.0,"Port CPU torch.orgqr to ATen (#50502)

Summary:
Now we can remove `_th_orgqr`!

Compared to the original TH-based `orgqr`, complex (https://github.com/pytorch/pytorch/issues/33152) and batched inputs are now supported.
CUDA support will be added in a follow-up PR.

Closes https://github.com/pytorch/pytorch/issues/24747

Ref. https://github.com/pytorch/pytorch/issues/49421, https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50502

Reviewed By: mrshenli

Differential Revision: D25953300

Pulled By: mruberry

fbshipit-source-id: f52a74e1c8f51b5e24f7b461430ca8fc96e4d149",289.0,177.0,"aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/LegacyTHFunctionsCPU.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THLapack.cpp,aten/src/TH/generic/THLapack.h,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorLapack.h,test/test_linalg.py",11.0,7,2,2.569557234,12.0,18719.0,6.0,2390013.5454545454,8320.0,18799.0,0.0,Feature Addition,0.0,1
pytorch,70f9f58a711727575cb3509243c1f45850d65132,6285348f06f59b7ed31bd57254dc1b9ccfb4e0b3,Peter Bell,peterbell10@live.co.uk,Thu Sep 30 23:00:42 2021 -0700,1633042842.0,"Implement n-dimensional hermitian FFTs (#63890)

Summary:
Closes https://github.com/pytorch/pytorch/issues/59127

cc mruberry peterbell10 walterddr

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63890

Reviewed By: ngimel

Differential Revision: D30761909

Pulled By: mruberry

fbshipit-source-id: 06e1e4dc65726f35c99a74f18b9fa36eb7d694a5",630.0,91.0,"aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_spectral_ops.py,torch/fft/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",6.0,9,3,2.005596017,12.0,26101.0,5.0,1543843.0,15849.0,36629.0,0.0,,0.0,1
pytorch,acd63940f277973ba16d4e073a4a546be8168962,62868fa3d5b0b5d75e4a71b098b1319f50ae485c,vfdev,vfdev.5@gmail.com,Mon Jun 27 14:10:35 2022 +0200,1656339035.0,"[functorch] Added chunks arg to vmap (pytorch/functorch#774)

* Added chunks arg to vmap

Description:
- Added chunks arg to vmap
- Added a test

* Create chunk_vmap into experimental

* COde formatting

* Updated tests
Refactored common code and fixed random state with randomness = same

* Updated docstring and split tests by randomness",174.0,24.0,"functorch/functorch/_src/vmap.py,functorch/functorch/experimental/__init__.py,functorch/test/test_eager_transforms.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",6.0,5,1,1.094226951,1.0,9693.0,5.0,3.0,1141.0,1539.5,0.0,Corrective,1.0,1
pytorch,9b7eceddc8b526e494024195472c80e5d7ab8577,62af45d99f4e9930ce86342ff1080ae1640ebb71,Sergey Zagoruyko,zagoruyko2@gmail.com,Thu Dec 29 21:53:57 2016 +0100,1483048437.0,Basic functional interface (#354),542.0,171.0,"torch/nn/functional.py,torch/nn/functions/conv.py,torch/nn/functions/rnn.py,torch/nn/functions/thnn/auto.py,torch/nn/functions/thnn/pooling.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/loss.py,torch/nn/modules/pooling.py,torch/tensor.py",12.0,5,1,2.397000091,18.0,3382.0,3.0,100360.36363636365,350.0,3020.196975,0.0,,0.0,1
pytorch,8d35b6cec7146f0cffded101b806eb4dc3d47daf,62b10721fbebf4d8977c19eac8f3c71514393751,Michael Suo,suo@fb.com,Sat Dec 07 01:48:20 2019 -0800,1575683300.0,"Actually make flake8 do something (#30892)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30892

Fixes all outstanding lints and actually installs a properly configured
flake8

Test Plan: Imported from OSS

Differential Revision: D18862825

Pulled By: suo

fbshipit-source-id: 08e9083338a7309272e17bb803feaa42e348aa85",144.0,78.0,".flake8,.github/workflows/lint.yml,aten/src/ATen/native/quantized/cpu/qnnpack/generate-wrapper.py,test/common_methods_invocations.py,test/dist_autograd_test.py,test/jit/_imported_class_test/bar.py,test/jit/_imported_class_test/very/very/nested.py,test/jit/test_class_type.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_caffe2_quantized.py,test/rpc_agent_test_fixture.py,test/test_cuda.py,test/test_jit.py,test/test_quantization.py,test/test_sparse.py,test/test_torch.py,test/test_type_promotion.py,torch/_torch_docs.py,torch/autograd/gradcheck.py,torch/functional.py,torch/jit/__init__.py,torch/onnx/symbolic_caffe2.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py,torch/quantization/_quantize_script.py,torch/tensor.py",27.0,20,4,4.007023144,45.0,60377.0,24.0,1105973.0,13713.0,37433.33333,0.0,Corrective,1.0,1
pytorch,7c993d7f0358ab8eff3c0e0f9d9d2374678b6e70,62e095ab57c020e7b353e91577e2a8a2da40d41e,Richard Zou,zou3519@gmail.com,Fri Aug 05 18:01:22 2022 -0700,1659722482.0,"[functorch] Add vmap in-place testing (#82898)

After this PR
- test_vmap_exhaustive tests that both the inplace variant
and the out-of-place variant for an OpInfo is correct.
- test_op_has_batch_rule tests that both the inplace variant and the
out-of-place variant for an OpInfo have batching rules.

I added a separate denylist for failures in the inplace path for both of
these OpInfos. The separate denylist is not very nice, but I'll get rid
of it soon by fixing the problems (they're mostly low-hanging fruit)
in some future PRs.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82898
Approved by: https://github.com/Chillee",134.0,6.0,"functorch/test/common_utils.py,functorch/test/test_vmap.py",2.0,2,1,0.627430335,2.0,4587.0,1.0,2.0,6209.0,14404.5,0.0,Corrective,1.0,1
pytorch,08227072e27c384863424f0254ba0a5ea7a1319c,62e16934cb35607a0cac795e2f4733b0df825113,Jianyu Huang,jianyuhuang@fb.com,Fri Jul 10 16:03:37 2020 -0700,1594397017.0,"[caffe2] Add the dedup implementation of fused RowWiseAdagrad op on GPUs (#40282)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40282

Test Plan:
```
buck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \(caffe2\.caffe2\.fb\.net_transforms\.tests\.fuse_sparse_ops_test\.TestFuseSparseOps\)' --print-passing-details
```

https://our.intern.facebook.com/intern/testinfra/testrun/4785074632584150

Reviewed By: jspark1105

Differential Revision: D22102737

fbshipit-source-id: fa3fef7cecb1e2cf5c9b6019579dc0f86fd3a3b2",605.0,177.0,"caffe2/sgd/adagrad_fused.cc,caffe2/sgd/adagrad_fused_op_gpu.cu,caffe2/sgd/adagrad_fused_op_gpu.cuh,caffe2/sgd/rowwise_adagrad_fused.cc,torch/utils/hipify/cuda_to_hip_mappings.py",5.0,5,2,0.905424585,2.0,9730.0,3.0,1669031.4,3516.0,8345.0,0.0,Feature Addition,0.0,1
pytorch,d0a56cab0763d9accddc62dab119d8612e6833d2,62ebad4ff9b7f42e5bca77cb6a5233df385095f1,neginraoof,neginmr@utexas.edu,Mon Sep 14 21:45:39 2020 -0700,1600119939.0,"[ONNX] Export new_empty and new_zeros (#43506)

Summary:
Adding symbolic to export new_empty and new_zeros

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43506

Reviewed By: houseroad

Differential Revision: D23674574

Pulled By: bzinodev

fbshipit-source-id: ecfcdbd4845fd3a3c6618a060129fbeee4df5dd7",38.0,4.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.998363673,3.0,7102.0,1.0,849303.0,5107.0,11660.5,0.0,Feature Addition,0.0,1
pytorch,d4cce30573651b5683d7e932b88194425db23758,6305e572ede4aed71af2f988de2a0f88aaae7dc1,BowenBao,bowbao@microsoft.com,Wed Apr 13 19:17:58 2022 +0000,1649877478.0,"[ONNX] Support dynamic scale & zero_point for fake_quantize_per_tensor_affine

Dynamic scale & zero_point requires opset 13 `ONNX::QuantizeLinear`
and `ONNX::DequantizeLinear`.
Improved error message when scale is not constant for opset 10 symbolic function.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75697
Approved by: https://github.com/garymm",43.0,10.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset13.py",3.0,4,2,1.536508825,4.0,11690.0,2.0,1051342.6666666667,2241.0,5320.0,0.0,Perfective,0.0,1
pytorch,ad028e5e09d5ddffee63c49b723594a84b8a90e4,631f0351313da2edeeb72cf8bb963035b11425d9,soulitzer,soulitzer@gmail.com,Sat Apr 02 13:51:18 2022 -0400,1648907478.0,"Update forward AD not supported error message

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75105

Approved by: https://github.com/albanD",10.0,8.0,"test/test_ops_gradients.py,tools/autograd/gen_variable_type.py",2.0,3,2,0.764204507,14.0,1257.0,2.0,147565.5,1927.0,4611.5,0.0,,0.0,1
pytorch,4e2b1543421ce9db6995616ffacb304a21256d1d,6322cf32340f27be85db23a9d3aa67c147db4328,Sam Gross,sgross@fb.com,Thu Dec 01 18:40:08 2016 -0800,1480617608.0,"Allow device=None in Tensor constructor""

Setting device=None is the same as not specifying the device (use the
current active device).",13.0,2.0,"test/test_cuda.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.970950594,17.0,1352.0,1.0,438967.0,309.0,2362.512243,0.0,,0.0,1
pytorch,63e2833cebe6d72e239feb0d5c7096cac188a940,6325b6e44e56f518d423cb46c93cfad892b236d0,Pieter Noordhuis,pietern@fb.com,Sat Apr 20 00:20:37 2019 -0700,1555719637.0,"Make finding unused model parameters optional (#19515)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19515

This is still done by default, but can now be disabled by specifying
`find_unused_parameters=False`. There are use cases where finding
unused parameters results in erroneous behavior, because a subset of
model parameters is used *outside* the `forward` function. One can
argue that doing this is not a good idea, but we should not break
existing use cases without an escape hatch. This configuration
parameter is that escape hatch.

Reviewed By: bddppq

Differential Revision: D15016381

fbshipit-source-id: f2f86b60771b3801ab52776e62b5fd6748ddeed0",125.0,12.0,"test/test_c10d.py,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h,torch/nn/parallel/distributed.py",4.0,7,2,1.426882127,17.0,3402.0,2.0,161974.75,8211.0,24654.33333,0.0,,0.0,1
pytorch,07a181da1df5579e6b0e76a0236031e43cca5003,6335d91c389fa719f472f1bfa5c30c2683d5c81a,Pritam Damania,pritam.damania@fb.com,Tue Oct 22 01:13:08 2019 -0700,1571706788.0,"Disable tsan for test_c10d multiprocess test cases. (#28385)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28385

TSAN doesn't work with multiprocessing with fork() since we end up
forking in a multithreaded environment which is dangerous. As a result, I'm
disabling TSAN in this change.

Similar to https://github.com/pytorch/pytorch/pull/27410 and
https://github.com/pytorch/pytorch/pull/25005
ghstack-source-id: 92319347

Test Plan: waitforbuildbot

Differential Revision: D18047778

fbshipit-source-id: 6c4e251639f74f4c772bd09bc6f2dfa83cf18fad",5.0,1.0,test/test_c10d.py,1.0,1,1,0,2.0,3213.0,1.0,356688.0,12431.0,34676.83333,0.0,,0.0,1
pytorch,2797c8005b5b52ff44e32a1c8c51358329d04f89,634c8315a482f90abab2dac29cb7e7de6e89c6dd,SsnL,SsnL@users.noreply.github.com,Fri Oct 20 14:20:33 2017 -0400,1508509233.0,"isContiguous problems (#3148)

* with the size=1 case, impossible to do single point check, replace with isContiguousRange

* fix stride in desc; fix undef scope

* add test for this case for cudnn

* assertTrue",84.0,49.0,"test/test_cuda.py,test/test_nn.py,test/test_torch.py,torch/csrc/cudnn/Descriptors.h,torch/lib/TH/generic/THTensorRandom.c,torch/lib/THC/THCDeviceTensor-inl.cuh,torch/lib/THC/THCDeviceTensor.cuh,torch/lib/THC/generic/THCTensor.c",8.0,9,2,2.288188553,39.0,12498.0,2.0,83864.75,29.0,45.5,0.0,Corrective,1.0,1
pytorch,50057e560bc0b863dcb0fa67b7dec891eaa4023f,6350fcef830fa21aad2f707c3dc545fb4b083830,kshitij12345,kshitijkalambarkar@gmail.com,Thu Apr 15 14:38:13 2021 -0700,1618497493.0,"[testing] add `broadcasts_input` and verifies the behaviour for inplace_variant. (#55771)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/55595

* Add `broadcasts_input` attribute to SampleInput
* Update test_variant_consistency_eager to verify that sample with `broadcasts_input==True` and inplace variant raises an error.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55771

Reviewed By: jbschlosser, ngimel

Differential Revision: D27760530

Pulled By: mruberry

fbshipit-source-id: feb0658730d4cff483848a5ade9512837a65c24c",73.0,60.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.712813261,2.0,6018.0,2.0,152226.5,10827.0,23913.0,0.0,Corrective,1.0,1
pytorch,5f83c5d8349fb0c3645146efe7ad4b4f20fca457,63585c3b815f107e2f45b940c991fda1e336423c,Junjie Bai,jbai@fb.com,Thu May 23 19:46:08 2019 -0700,1558640768.0,"Add support for save and load mkldnn modules

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/20799

Reviewed By: wanchaol

Differential Revision: D15447891

fbshipit-source-id: e34de946c79282fb934a5c52ff1def41c7993c75",181.0,58.0,"test/common_utils.py,test/test_jit.py,test/test_mkldnn.py,torch/utils/mkldnn.py",4.0,3,2,1.249633803,12.0,17151.0,4.0,1815837.75,8855.0,26112.83333,0.0,Feature Addition,0.0,1
pytorch,a7f6b0ab4fa4e43a2fa7bcb53825277db88b55f0,635bb5ec9d99c53aeded558c54dd4c093fa273bb,Tzu-Wei Huang,huang.dexter@gmail.com,Tue Jul 04 15:04:30 2017 +0800,1499180670.0,corrects typo,1.0,1.0,torch/nn/modules/rnn.py,1.0,3,1,0,28.0,570.0,1.0,52092.0,1085.0,15181.88961,0.0,Corrective,0.0,1
pytorch,d5490c662ef8d2255b8129d77e7d3ac171b17a75,638c4375dec29b1b42780f2d525a88c7f69cf7d7,BowenBao,bowbao@microsoft.com,Thu Sep 26 23:21:12 2019 -0700,1569540072.0,"Export index_fill and index_copy, fix caffe2 scatter

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23052

Reviewed By: hl475

Differential Revision: D16428486

Pulled By: houseroad

fbshipit-source-id: 8c5905052763fd70197c67aba5f28eeff0790721",152.0,18.0,"caffe2/operators/utility_ops.h,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",5.0,6,3,2.086230244,9.0,7365.0,3.0,134558.0,11803.0,33093.33333,0.0,Corrective,1.0,1
pytorch,91af122d43e74f04337c0d61708c2e7b07a94d05,638f0b5d78fe5ff2e484dc573c35b97a4bcf4e82,GÃ¶kÃ§en Eraslan,gokcen.eraslan@gmail.com,Wed Nov 01 12:47:19 2017 +0100,1509540439.0,"Prevent numerical issues with poisson_nll_loss when log_input=False (#3336)

* Prevent numerical issues with poisson_nll_loss when log_input=False

Evaluation of the logarithm of the input variable in poisson negative log likelihood leads to NaN loss if variable being evaluated is zero. Small epsilon is added to prevent this. See equivalent Keras epsilon here: https://github.com/fchollet/keras/blob/master/keras/losses.py#L68

* PEP8 fix

* Add epsilon support to PoissonNLLLoss in nn.modules.loss",11.0,6.0,"torch/nn/functional.py,torch/nn/modules/loss.py",2.0,3,1,0.997502546,36.0,2414.0,1.0,8041.0,2050.0,23927.35823,0.0,Corrective,1.0,1
pytorch,788ef939d837a299eaa0bb4deba20925e20b53ae,639c68b2feb0bf7411d71899db0362e00b2ada67,xiaobingsuper,xiaobing.zhang@intel.com,Mon Mar 30 20:50:36 2020 -0700,1585601436.0,"bfloat16: enable basic math function (#35172)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/35172

Test Plan: Imported from OSS

Differential Revision: D20721068

Pulled By: ngimel

fbshipit-source-id: 7e40bda6683f041f04f78739a950cb2a6ac74571",271.0,0.0,"aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h",2.0,5,1,0.035137192,5.0,229.0,1.0,9068452.0,614.0,1803.5,0.0,,0.0,1
pytorch,240d62fbaa0110f565b4702809014982b82c86e4,63a55d4932318a43e6731d720cfae08be990ad48,BowenBao,semisqg@gmail.com,Thu Jun 06 15:32:36 2019 -0700,1559835156.0,"Support gather export with OneHot + Mul (#21235)

Summary:
This could serve as a alternative solution to export ```torch.gather``` before something similar goes into ONNX spec. The exported model is verified to be correct against onnxruntime backend. We weren't able to test against Caffe2 backend because it doesn't seem to support OneHot opset9.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21235

Differential Revision: D15613039

Pulled By: houseroad

fbshipit-source-id: 7fc097f85235c071474730233ede7d83074c347f",178.0,0.0,"test/onnx/expect/TestOperators.test_gather.expect,test/onnx/test_operators.py,torch/onnx/symbolic_opset9.py",3.0,5,2,0.538054735,3.0,2254.0,2.0,152986.0,9192.0,26823.83333,0.0,Corrective,0.0,1
pytorch,456b5b1642f9ba2021dcac46b6c75e7f55fd099a,63ac3633f53966d038d5983e83254b7524300680,gchanan,gregchanan@gmail.com,Wed Dec 20 18:08:42 2017 -0500,1513793322.0,"Implement torch.where(condition, x, y) CPU Variable. (#4259)

* Implement torch.where(condition, x, y) CPU Variable.

* Get rid of IMPLEMENT_STATELESS for where.",179.0,59.0,"aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Check.cpp,aten/src/ATen/Check.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,test/test_autograd.py,tools/autograd/derivatives.yaml",8.0,7,3,1.869643143,38.0,6182.0,7.0,470373.75,384.0,1192.405869,0.0,,0.0,1
pytorch,79b797ccac43844b05c60c4d0e6620d05f08d795,63c957cd94e4870fcef3a735dec752ddcfd4c4d8,Pritam Damania,pritam.damania@fb.com,Tue Nov 19 23:49:20 2019 -0800,1574207360.0,"Use std::shared_ptr for DistAutogradContext. (#29770)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29770

We were passing around const and non-const references for
DistAutogradContext from DistAutogradContainer. This wasn't safe since the
context could be deleted from the container and a thread might still be using
the reference. This usually would happen when a backward pass fails on the node
driving the backward pass (resulting in delete context messages being sent to
all nodes) but other nodes are still executing code related to that autograd
context.

This was also the reason why `test_backward_autograd_engine_error` was flaky.

Using a std::shared_ptr everywhere ensures we're safe and never crash.

Closes #28928
Closes #26922
ghstack-source-id: 94201446

Differential Revision: D18494814

fbshipit-source-id: 0c925fdbd5755f6d876dad56885e2cbaf41fc5f0",163.0,83.0,"test/cpp/dist_autograd/test_dist_autograd.cpp,test/dist_autograd_test.py,torch/csrc/distributed/autograd/context/container.cpp,torch/csrc/distributed/autograd/context/container.h,torch/csrc/distributed/autograd/context/context.h,torch/csrc/distributed/autograd/engine/dist_engine.cpp,torch/csrc/distributed/autograd/engine/dist_engine.h,torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp,torch/csrc/distributed/autograd/functions/recvrpc_backward.h,torch/csrc/distributed/autograd/init.cpp,torch/csrc/distributed/autograd/utils.cpp,torch/csrc/distributed/autograd/utils.h,torch/csrc/distributed/rpc/request_callback_impl.cpp",13.0,11,2,3.006568654,1.0,2971.0,6.0,424201.3846153846,13297.0,36412.83333,0.0,,0.0,1
pytorch,9323734e0ed18eccb371ffaa24752dfd2c25da57,63ca57247d258cd3aee2918e96e1a053037a8be3,soulitzer,jw3468@fb.com,Mon Nov 29 21:51:20 2021 -0500,1638222680.0,"[functorch] Add per-op tests for jvp and vmap + jvp (pytorch/functorch#232)

* Test

* Address comments",190.0,25.0,"functorch/functorch/csrc/BatchRulesViews.cpp,functorch/functorch/csrc/BatchedTensorImpl.cpp,functorch/functorch/csrc/BatchedTensorImpl.h,functorch/functorch/csrc/BatchingRegistrations.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py",6.0,4,1,0.772410788,1.0,2501.0,4.0,1.8333333333333333,560.0,774.0,0.0,Feature Addition,0.0,1
pytorch,de89261abea0880b2cefb84c229ab3ecc3cc1e83,63d62d3e44a0a4ec09d94f30381d49b78cc5b095,Mike Ruberry,mruberry@fb.com,Tue Sep 08 20:12:34 2020 -0700,1599595954.0,"Skips test_addcmul_cuda if using ROCm (#44304)

Summary:
This test is failing consistently on linux-bionic-rocm3.7-py3.6-test2. Relevant log snippet:

```
03:43:11 FAIL: test_addcmul_cuda_float16 (__main__.TestForeachCUDA)
03:43:11 ----------------------------------------------------------------------
03:43:11 Traceback (most recent call last):
03:43:11   File ""/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 818, in wrapper
03:43:11     method(*args, **kwargs)
03:43:11   File ""/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 258, in instantiated_test
03:43:11     result = test(self, *args)
03:43:11   File ""test_foreach.py"", line 83, in test_addcmul
03:43:11     self._test_pointwise_op(device, dtype, torch._foreach_addcmul, torch._foreach_addcmul_, torch.addcmul)
03:43:11   File ""test_foreach.py"", line 58, in _test_pointwise_op
03:43:11     self.assertEqual(tensors, expected)
03:43:11   File ""/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 1153, in assertEqual
03:43:11     exact_dtype=exact_dtype, exact_device=exact_device)
03:43:11   File ""/var/lib/jenkins/.local/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 1127, in assertEqual
03:43:11     self.assertTrue(result, msg=msg)
03:43:11 AssertionError: False is not true : Tensors failed to compare as equal! With rtol=0.001 and atol=1e-05, found 10 element(s) (out of 400) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 0.00048828125 (-0.46484375 vs. -0.46533203125), which occurred at index (11, 18).
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44304

Reviewed By: malfet, izdeby

Differential Revision: D23578316

Pulled By: mruberry

fbshipit-source-id: 558eecf42677383e7deaa4961e12ef990ffbe28c",4.0,2.0,"test/test_foreach.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,1,2.0,1881.0,2.0,265814.0,4924.0,11370.5,0.0,Feature Addition,0.0,1
pytorch,36501ff5d9f852baf4a8ec18d55f2afdc0f8a6a7,63dc1363e6177694259e6240aba2c55826a4bb24,Nick Gibson,nickg@fb.com,Fri Jun 12 00:05:39 2020 -0700,1591920339.0,"[TensorExpr] Eliminate Cond statements when each branch is a different kind of empty (#39754)

Summary:
Fix another simplification edge case, a Cond statement when one branch is nullptr and the other is a zero stmt block. This happens mostly with an if with no else branch where all statements inside the if are removed (eg via inlining or simplification). Common case is SplitWithMask -> ComputeInline.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39754

Differential Revision: D21962987

Pulled By: nickgg

fbshipit-source-id: 2461415466fbbab88d2329061f90fcfdfa85e243",38.0,0.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",3.0,7,2,0.954894281,2.0,4135.0,3.0,496291.0,2768.0,6756.5,0.0,Corrective,1.0,1
pytorch,ba14a701dc3f58d0027aa385baa3b1347301c484,63e545e0fe1e8ea31a9cd296db8997f135aff49c,Rohan Varma,rvarm1@fb.com,Wed May 27 01:21:26 2020 -0700,1590542486.0,"Revert D21717199: [pytorch][PR] Updates assertEqual to require atol and rtol, removes positional atol

Test Plan: revert-hammer

Differential Revision:
D21717199

Original commit changeset: 9feb856f94ee

fbshipit-source-id: bfde9c39a5ce99f0ca6183a7dde703c65b7c8259",831.0,843.0,"test/cpp_api_parity/functional_impl_check.py,test/cpp_api_parity/module_impl_check.py,test/distributed/test_c10d.py,test/distributed/test_c10d_spawn.py,test/distributed/test_data_parallel.py,test/quantization/test_qat_module.py,test/quantization/test_quantize.py,test/quantization/test_quantized_module.py,test/quantization/test_quantized_op.py,test/quantization/test_workflow_module.py,test/test_autograd.py,test/test_cpp_extensions_aot.py,test/test_cpp_extensions_jit.py,test/test_cuda.py,test/test_distributions.py,test/test_jit.py,test/test_multiprocessing.py,test/test_namedtensor.py,test/test_nn.py,test/test_optim.py,test/test_serialization.py,test/test_sparse.py,test/test_tensorboard.py,test/test_torch.py,test/test_type_promotion.py,test/test_utils.py,torch/testing/_internal/common_distributed.py,torch/testing/_internal/common_nn.py,torch/testing/_internal/common_utils.py",29.0,7,2,2.956860277,48.0,92886.0,6.0,32587.655172413797,2336.0,5920.5,0.0,,0.0,1
pytorch,b546fa3fcd2cfb24d96ec080085726d7079f04bd,63f6c0d69201ec2c64142a69ce4cdeed8e75f02e,Edgar Riba,edgar.riba@gmail.com,Fri Mar 24 15:29:40 2017 +0100,1490369380.0,add Pairwise distance (#835),68.0,2.0,"test/test_nn.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/distance.py",4.0,4,2,1.504840406,25.0,3100.0,2.0,65275.0,566.0,5487.055248,0.0,Feature Addition,0.0,1
pytorch,5ed3be799d8ca063d1b990065521868a83cd4ff1,63f83edcfbaaeea2c4dedb68a13779ef95151140,Xue Haotian,njxht@foxmail.com,Thu Apr 15 20:24:54 2021 -0700,1618518294.0,"OpInfo porting for torch.real & torch.imag (#55134)

Summary:
Related https://github.com/pytorch/pytorch/issues/54298

This PR ports the method_tests() entries of torch.real & torch.imag to OpInfo.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55134

Reviewed By: agolynski

Differential Revision: D27793242

Pulled By: anjali411

fbshipit-source-id: 0e9a987bfef16e78a1cda81ce14970993a59e467",30.0,7.0,"test/test_autograd.py,torch/testing/_core.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.602291043,42.0,14282.0,3.0,226154.66666666663,10855.0,23966.5,0.0,,0.0,1
pytorch,b967119906703faa55b65775cf62082d95e8e48d,6400d27bbbbd7fb4d6d66c8d493fa6fa91d62998,Yi Wang,wayi@fb.com,Sat Nov 21 17:18:10 2020 -0800,1605979090.0,"[Gradient Compression] Define a customized state for PowerSGD comm hook (#48348)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48348

To support the features like error feedback, warm start, PowerSGD comm hook needs to maintain a state besides process group. Currently this state only includes a process group and a matrix approximation rank config.

This diff is a pure refactoring. Plan to add more state fields later.

Original PR issue: Investigate Applying PowerSGD to Communication Hook for Gradient Compression #47202
ghstack-source-id: 117305280

Test Plan:
buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_powerSGD_ddp_comm_hook_nccl

buck test mode/dev-nosan caffe2/test/distributed:c10d --
test_powerSGD_ddp_comm_hook_nccl_grad_is_view

Reviewed By: rohan-varma

Differential Revision: D25137962

fbshipit-source-id: cd72b8b01e20f80a92c7577d22f2c96e9eebdc52",71.0,23.0,"test/distributed/test_c10d.py,torch/distributed/algorithms/ddp_comm_hooks/__init__.py,torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py",3.0,6,2,1.437898183,2.0,4409.0,2.0,176326.33333333334,6982.0,15795.0,0.0,Feature Addition,0.0,1
pytorch,e9d1a5f6d51d45bc67483afb9376ec530470a51b,6404904d8af7d90842a530738f456556fde53f70,Peter Goldsborough,peter@goldsborough.me,Sun Mar 11 00:16:40 2018 -0800,1520727400.0,Fix run_test.py (#5693),22.0,12.0,test/run_test.py,1.0,1,1,0,2.0,230.0,1.0,98078.0,478.0,2352.5,0.0,Corrective,1.0,1
pytorch,2ad4b8e58cd416e69d717a33cf0a942c12e52f5a,6408cbd918629a5885f68607aafa2f1a72649fb4,Peter Bell,peterbell10@live.co.uk,Thu Jun 03 18:42:14 2021 -0700,1622745734.0,"Migrate renorm to ATen (CPU and CUDA) (#59250)

Summary:
Resubmit of https://github.com/pytorch/pytorch/issues/59108, closes https://github.com/pytorch/pytorch/issues/24754, closes https://github.com/pytorch/pytorch/issues/24616

This reuses `linalg_vector_norm` to calculate the norms. I just add a new kernel that turns  the norm into a normalization factor, then multiply the original tensor using a normal broadcasted `mul` operator. The result is less code, and better performance to boot.

#### Benchmarks (CPU):
|     Shape    | Dim |  Before | After (1 thread) | After (8 threads) |
|:------------:|:---:|--------:|-----------------:|------------------:|
| (10, 10, 10) | 0   | 11.6 us |           4.2 us |            4.2 us |
|              | 1   | 14.3 us |           5.2 us |            5.2 us |
|              | 2   | 12.7 us |           4.6 us |            4.6 us |
| (50, 50, 50) | 0   |  330 us |           120 us |           24.4 us |
|              | 1   |  350 us |           135 us |           28.2 us |
|              | 2   |  417 us |           130 us |           24.4 us |

#### Benchmarks (CUDA)
|     Shape    | Dim |  Before |   After |
|:------------:|:---:|--------:|--------:|
| (10, 10, 10) | 0   | 12.5 us | 12.1 us |
|              | 1   | 13.1 us | 12.2 us |
|              | 2   | 13.1 us | 11.8 us |
| (50, 50, 50) | 0   | 33.7 us | 11.6 us |
|              | 1   | 36.5 us | 15.8 us |
|              | 2   | 41.1 us |   15 us |

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59250

Reviewed By: mruberry

Differential Revision: D28820359

Pulled By: ngimel

fbshipit-source-id: 572486adabac8135d52a9b8700f9d145c2a4ed45",191.0,532.0,"BUILD.bazel,aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/LegacyTHFunctionsCPU.h,aten/src/ATen/LegacyTHFunctionsCUDA.h,aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/Normalization.h,aten/src/ATen/native/cpu/RenormKernel.cpp,aten/src/ATen/native/cuda/DistanceKernel.cu,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/PersistentSoftmax.cuh,aten/src/ATen/native/cuda/RenormKernel.cu,aten/src/ATen/native/cuda/TensorModeKernel.cuh,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/CMakeLists.txt,aten/src/THC/THCTensorMath.h,aten/src/THC/THCTensorMathReduce.cuh,aten/src/THC/THCThrustAllocator.cuh,aten/src/THC/generated/THCTensorMathReduceBFloat16.cu,aten/src/THC/generated/THCTensorMathReduceBool.cu,aten/src/THC/generated/THCTensorMathReduceByte.cu,aten/src/THC/generated/THCTensorMathReduceChar.cu,aten/src/THC/generated/THCTensorMathReduceDouble.cu,aten/src/THC/generated/THCTensorMathReduceFloat.cu,aten/src/THC/generated/THCTensorMathReduceHalf.cu,aten/src/THC/generated/THCTensorMathReduceInt.cu,aten/src/THC/generated/THCTensorMathReduceLong.cu,aten/src/THC/generated/THCTensorMathReduceShort.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorMathReduce.h,tools/autograd/gen_variable_type.py,tools/build_variables.bzl,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py",36.0,19,3,4.172994132,17.0,30343.0,8.0,173391.38888888888,12685.0,28828.0,0.0,Feature Addition,0.0,1
pytorch,039dc9085425e18b8c217f6e33a1c3dd072dd48f,641750e33cb3d1ba36d9e9cf0c7cc5490bf0fa81,Gregory Chanan,gchanan@fb.com,Mon Feb 24 15:43:41 2020 -0800,1582559021.0,"Fix NaN handling in torch.mv. (#31666)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31666

List of changes:
1) Fix a case where torch.mv was not handling NaNs correctly.  In particular, with a transposed tensor and expanded vector, NaNs in the output are kept, even if beta = 0.
This is handled in the `out=` case by zero-ing out the passed-in Tensor, but this can happen just the same with the non-out variant if the allocated tensor happens to have a NaN.
Also adds tests for this case.
NOTE: we zero out the output tensor in all cases for mv and mm, even though this is probably overkill.  I didn't find another case where this would be a problem, but the old code at least
attempted to do this for all mv and mm calls and I didn't add comprehensive testing to be sure that it's not a problem.

2) on CPU: move mv, mv_out, mm, mm_out to be direct wrappers on _th_addmv, _th_addmm, rather than having their own wrappers in Declarations.cwrap.
Ths is to remove the magic around cpu_zero from the codegen, which simplifies the codegen and makes testing this easier.

Test Plan: Imported from OSS

Differential Revision: D19239953

Pulled By: gchanan

fbshipit-source-id: 27d0748d215ad46d17a8684696d88f4cfd8a917e",98.0,12.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/BlasWrappersCPU.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THBlas.cpp,test/test_torch.py",7.0,7,2,2.208951908,43.0,25862.0,6.0,1830897.3333333333,14923.0,40020.33333,0.0,Corrective,1.0,1
pytorch,046272f3e5f8f54eac2ab9ce0987b997ee1f61b5,643e58466ee30cd80b61851ca46b3616ae54df70,Hui Guo,huiguo@fb.com,Sat Jul 24 00:17:09 2021 -0700,1627085829.0,"[nnc] Rename IRSimplifierBase with PolynomialBase (#60686)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/60686

Test Plan: Imported from OSS

Reviewed By: navahgar, soulitzer

Differential Revision: D29373316

Pulled By: huiguoo

fbshipit-source-id: bd44bff60455076d1c5291273989e9939a428f9a",11.0,11.0,"torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",2.0,4,1,0.994030211,1.0,3059.0,1.0,1918654.0,14106.0,32243.0,0.0,,0.0,1
pytorch,fb8f9de49878edd66931a0609df5b6ffb3ed937a,643f8d12fffd92e2d20b6dcf217e507204dd9136,Alykhan Tejani,alykhan.tejani@gmail.com,Thu Jul 27 00:28:56 2017 -1000,1501115336.0,"[bugfix] in bce_with_logits logsumexp calculation (#2221)

* fix bug in bce_with_logits logsumexp calculation

* flake8 fix",10.0,2.0,"test/test_nn.py,torch/nn/functional.py",2.0,3,2,0.650022422,33.0,5103.0,1.0,8260.0,1277.0,17148.52883,0.0,Corrective,1.0,1
pytorch,292211baa4ad480de6e5cc7a164e1df920dd50bd,6463eebc7bc3e648b5b17b05e7d461fb026e5c2f,Yangqing Jia,jiayq84@gmail.com,Thu Jul 21 17:16:42 2016 -0700,1469121402.0,chunky sync - build scripts to be written,12048.0,5309.0,"caffe2/binaries/convert_caffe_image_db.cc,caffe2/binaries/convert_db.cc,caffe2/binaries/convert_encoded_to_raw_leveldb.cc,caffe2/binaries/db_throughput.cc,caffe2/binaries/fb_run_plan_mpi.cc,caffe2/binaries/inspect_gpus.cc,caffe2/binaries/make_cifar_db.cc,caffe2/binaries/make_image_db.cc,caffe2/binaries/make_mnist_db.cc,caffe2/binaries/print_registered_core_operators.cc,caffe2/binaries/run_plan.cc,caffe2/binaries/run_plan_mpi.cc,caffe2/binaries/speed_benchmark.cc,caffe2/binaries/split_db.cc,caffe2/binaries/zmq_feeder.cc,caffe2/contrib/nccl/cuda_nccl_gpu.cc,caffe2/contrib/nccl/cuda_nccl_gpu.h,caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,caffe2/contrib/nccl/nccl_ops_test.py,caffe2/contrib/nervana/nervana_fc_op_gpu_test.cc,caffe2/contrib/nervana/nervana_init_gpu.cc,caffe2/contrib/nervana/nervana_math_gpu.cc,caffe2/contrib/warpctc/ctc_op.h,caffe2/contrib/warpctc/ctc_ops_test.py,caffe2/core/blob.h,caffe2/core/blob_gpu_test.cc,caffe2/core/blob_serialization.cc,caffe2/core/blob_serialization.h,caffe2/core/blob_serialization_gpu.cc,caffe2/core/blob_test.cc,caffe2/core/common.h,caffe2/core/common_cudnn.cc,caffe2/core/common_cudnn.h,caffe2/core/common_gpu.cc,caffe2/core/common_gpu.h,caffe2/core/context.h,caffe2/core/context_gpu.cc,caffe2/core/context_gpu.h,caffe2/core/context_gpu_test.cc,caffe2/core/cuda_memorypool_gpu.cc,caffe2/core/cuda_memorypool_gpu.h,caffe2/core/cuda_memorypool_gpu_test.cc,caffe2/core/db.cc,caffe2/core/db.h,caffe2/core/flags.cc,caffe2/core/flags.h,caffe2/core/init.cc,caffe2/core/init.h,caffe2/core/logging.cc,caffe2/core/logging.h,caffe2/core/logging_is_google_glog.h,caffe2/core/logging_is_not_google_glog.h,caffe2/core/net.cc,caffe2/core/net.h,caffe2/core/net_test.cc,caffe2/core/operator.cc,caffe2/core/operator.h,caffe2/core/operator_gradient.h,caffe2/core/operator_schema.cc,caffe2/core/operator_schema.h,caffe2/core/operator_schema_test.cc,caffe2/core/operator_test.cc,caffe2/core/parallel_net_test.cc,caffe2/core/registry.h,caffe2/core/registry_test.cc,caffe2/core/tensor.cc,caffe2/core/tensor.h,caffe2/core/typeid.cc,caffe2/core/typeid.h,caffe2/core/typeid_test.cc,caffe2/core/types.h,caffe2/core/workspace.cc,caffe2/core/workspace.h,caffe2/core/workspace_test.cc,caffe2/cuda_rtc/common_rtc.h,caffe2/cuda_rtc/elemenntwise_rtc_gpu.cc,caffe2/cuda_rtc/pool_op_rtc_gpu.cc,caffe2/db/create_db_op.cc,caffe2/db/db_test.cc,caffe2/db/leveldb.cc,caffe2/db/lmdb.cc,caffe2/db/protodb.cc,caffe2/db/rocksdb.cc,caffe2/db/zmqdb.cc,caffe2/image/image_input_op.h,caffe2/mpi/mpi_common.cc,caffe2/mpi/mpi_common.h,caffe2/mpi/mpi_gpu_test.cc,caffe2/mpi/mpi_ops.cc,caffe2/mpi/mpi_ops.h,caffe2/mpi/mpi_ops_gpu.cc,caffe2/mpi/mpi_test.cc,caffe2/operators/accumulate_op.cc,caffe2/operators/accumulate_op.h,caffe2/operators/accuracy_op.cc,caffe2/operators/accuracy_op.cu,caffe2/operators/accuracy_op.h,caffe2/operators/clip_op.cc,caffe2/operators/clip_op.cu,caffe2/operators/clip_op.h,caffe2/operators/conv_op.cc,caffe2/operators/conv_op.h,caffe2/operators/conv_op_cudnn.cc,caffe2/operators/conv_op_impl.h,caffe2/operators/conv_pool_op_base.h,caffe2/operators/cross_entropy_op.cc,caffe2/operators/cross_entropy_op.cu,caffe2/operators/cross_entropy_op.h,caffe2/operators/depth_split_op.cc,caffe2/operators/depth_split_op.h,caffe2/operators/depth_split_op_gpu.cc,caffe2/operators/dropout_op.cc,caffe2/operators/dropout_op.cu,caffe2/operators/dropout_op.h,caffe2/operators/elementwise_op.cc,caffe2/operators/elementwise_op.h,caffe2/operators/elementwise_op_gpu.cc,caffe2/operators/filler_op.cc,caffe2/operators/filler_op.h,caffe2/operators/fully_connected_op.cc,caffe2/operators/fully_connected_op.h,caffe2/operators/fully_connected_op_gpu_test.cc,caffe2/operators/fully_connected_op_test.cc,caffe2/operators/half_float_ops.cu,caffe2/operators/l2_distance_op.cc,caffe2/operators/l2_distance_op.cu,caffe2/operators/l2_distance_op.h,caffe2/operators/load_save_op.cc,caffe2/operators/load_save_op.h,caffe2/operators/load_save_op_gpu.cc,caffe2/operators/local_response_normalization_op.cc,caffe2/operators/local_response_normalization_op.cu,caffe2/operators/local_response_normalization_op.h,caffe2/operators/loss_op.cc,caffe2/operators/loss_op.h,caffe2/operators/loss_op_gpu.cc,caffe2/operators/operator_fallback_gpu.h,caffe2/operators/operator_fallback_gpu_test.cc,caffe2/operators/order_switch_ops.cc,caffe2/operators/order_switch_ops.cu,caffe2/operators/order_switch_ops.h,caffe2/operators/pool_op.cc,caffe2/operators/pool_op.cu,caffe2/operators/pool_op.h,caffe2/operators/pool_op_cudnn.cc,caffe2/operators/prefetch_op.h,caffe2/operators/recurrent_op.h,caffe2/operators/recurrent_op_cudnn.cc,caffe2/operators/reducer_functors.h,caffe2/operators/relu_op.cc,caffe2/operators/relu_op.cu,caffe2/operators/relu_op.h,caffe2/operators/relu_op_cudnn.cc,caffe2/operators/relu_op_fp16.cu,caffe2/operators/segment_reduction_op.cc,caffe2/operators/sigmoid_op.cc,caffe2/operators/sigmoid_op.cu,caffe2/operators/softmax_op.cc,caffe2/operators/softmax_op.cu,caffe2/operators/softmax_op.h,caffe2/operators/softmax_op_cudnn.cc,caffe2/operators/spatial_batch_norm_op.cc,caffe2/operators/spatial_batch_norm_op.h,caffe2/operators/spatial_batch_norm_op_cudnn.cc,caffe2/operators/summarize_op.cc,caffe2/operators/summarize_op.cu,caffe2/operators/summarize_op.h,caffe2/operators/tanh_op.cc,caffe2/operators/tanh_op.cu,caffe2/operators/tensor_protos_db_input.cc,caffe2/operators/tensor_protos_db_input.h,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_gpu.cc,caffe2/proto/BREW,caffe2/proto/caffe2.proto,caffe2/proto/caffe2_legacy.proto,caffe2/python/_import_c_extension.py,caffe2/python/caffe2_python.cc,caffe2/python/caffe2_python.h,caffe2/python/caffe2_python_gpu.cc,caffe2/python/caffe_translator.py,caffe2/python/cnn.py,caffe2/python/convnet_benchmarks.py,caffe2/python/convnet_benchmarks_test.py,caffe2/python/core.py,caffe2/python/core_gradients.py,caffe2/python/core_gradients_test.py,caffe2/python/core_test.py,caffe2/python/gradient_check_test.py,caffe2/python/gradient_checker.py,caffe2/python/hypothesis_test.py,caffe2/python/hypothesis_test_util.py,caffe2/python/model_device_test.py,caffe2/python/muji_test.py,caffe2/python/net_drawer.py,caffe2/python/operator_test/sparse_ops_test.py,caffe2/python/test_util.py,caffe2/python/toy_regression_test.py,caffe2/python/utils.py,caffe2/python/workspace.py,caffe2/python/workspace_gpu_test.py,caffe2/python/workspace_test.py,caffe2/sgd/adagrad_op.cc,caffe2/sgd/adagrad_op.h,caffe2/sgd/adagrad_op_gpu.cu,caffe2/sgd/adam_op.cc,caffe2/sgd/adam_op.h,caffe2/sgd/adam_op_gpu.cu,caffe2/sgd/iter_op.cc,caffe2/sgd/learning_rate_op.h,caffe2/sgd/rmsprop_op.cc,caffe2/sgd/rmsprop_op.h,caffe2/sgd/rmsprop_op_gpu.cu,caffe2/utils/math.h,caffe2/utils/math_cpu.cc,caffe2/utils/math_gpu.cu,caffe2/utils/math_test.cc,caffe2/utils/proto_utils.cc,caffe2/utils/proto_utils.h,caffe2/utils/simple_queue.h,caffe2/utils/simple_queue_test.cc,caffe2/utils/zmq_helper.h",223.0,17,1,6.489653277,4.0,30761.0,15.0,7056556.417040358,234.0,1703.833333,0.0,,0.0,1
pytorch,40ecdc57ff8c46b050ac1b859c30cee1ac5c8567,6466ddbd868b3f56b0cb1c00dc276e01e463ae11,Edward Yang,ezyang@fb.com,Tue Mar 12 20:34:40 2019 -0700,1552422880.0,"Fix lint in test_torch.py (#17807)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17807

Lint also detected a bug in test_linspace where we weren't
actually testing the CUDA case.

Differential Revision: D14388241

fbshipit-source-id: e219e46400f4952c6b384bca3baa0724ef94acde",16.0,16.0,test/test_torch.py,1.0,1,1,0,40.0,10543.0,1.0,17949.0,7459.0,22857.33333,0.0,Corrective,1.0,1
pytorch,20fc7b6ec7cb7b43f66526abf9d5b4fa44ffefb7,646cb6157df948643ec98b8947026ccaee5c7415,Ilia Cherniavskii,iliacher@fb.com,Tue Apr 16 07:13:50 2019 -0700,1555398830.0,"Move OMP/MKL thread initialization into ATen/Parallel (#19011)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19011
ghimport-source-id: 432e31eccfd0e59fa21a790f861e6b2ff4fdbac6

Differential Revision: D14846034

Pulled By: ilia-cher

fbshipit-source-id: d9d03c761d34bac80e09ce776e41c20fd3b04389",140.0,159.0,"aten/src/ATen/ATen.h,aten/src/ATen/CPUGeneral.cpp,aten/src/ATen/CPUGeneral.h,aten/src/ATen/Context.h,aten/src/ATen/Parallel.cpp,aten/src/ATen/Parallel.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIteratorReduce.cpp,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/tbb_init_test.cpp,aten/src/ATen/test/thread_init_test.cpp,aten/src/TH/THGeneral.cpp,aten/src/TH/THGeneral.h.in,torch/_torch_docs.py,torch/csrc/Module.cpp,torch/csrc/autograd/engine.cpp",18.0,10,2,3.368710239,43.0,10709.0,13.0,6467244.05882353,8089.0,24413.33333,0.0,,0.0,1
pytorch,f4c37e6b32407b2b84d117c89877218a66629d28,646e214706fd940ae36c51cdc8d66789962203ff,Pritam Damania,pritam.damania@fb.com,Fri Oct 04 20:27:19 2019 -0700,1570220839.0,"ProcessGroupNCCL should respect timeout passed in to init_process_group. (#27224)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27224

As part of adding error handling to NCCL, we are now able to specify a
timeout for operations using ProcessGroupNCCL. Although, this timeout had a
default of 10 seconds and didn't respect the timeout specified in
init_process_group.

In this change, I've ensured we pass the appropriate timeout to
ProcessGroupNCCL.
ghstack-source-id: 91283548

Test Plan:
Added unit test to verify timeout passed in to init_process_group is
respected.

Differential Revision: D17717992

fbshipit-source-id: c73320187f1f3b2693ba1e177d80646e282d01a2",92.0,42.0,"test/test_c10d.py,torch/distributed/distributed_c10d.py",2.0,3,2,0.382957672,2.0,4642.0,2.0,430967.0,12044.0,33798.83333,0.0,Feature Addition,0.0,1
pytorch,25e530083ee0b3746e3ad051a29e4808b2c4ef61,647b8f8e3e6e971304162e7a6ea34e0e81d03239,Kurt Mohler,kmohler@quansight.com,Thu Jan 19 21:04:09 2023 +0000,1674162249.0,"Add TORCH_CHECK_TENSOR_ALL (#89097)

`TORCH_CHECK_TENSOR_ALL(cond, ...)` is a wrapper around `TORCH_CHECK` which allows the condition argument to be a tensor, batched or unbatched. `cond` can be a boolean tensor of any size. If any element is False, or if `cond.numel() == 0`, then `TORCH_CHECK_TENSOR_ALL` raises an error

Part of #72948
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89097
Approved by: https://github.com/zou3519",145.0,0.0,"aten/src/ATen/TensorUtils.h,aten/src/ATen/functorch/BatchRulesDecompositions.cpp,aten/src/ATen/functorch/BatchRulesReduceOps.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/TestOps.cpp,aten/src/ATen/native/native_functions.yaml,test/functorch/test_vmap.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/overrides.py",10.0,10,4,2.047971633,48.0,36516.0,9.0,666666.5,11458.0,26252.5,0.0,Feature Addition,0.0,1
pytorch,ea98256e96d8e0baa807bd228be5dd1a3fde14e8,64834f6fb8055cf2d633353992288a8823b915ed,Edward Z. Yang,ezyang@mit.edu,Thu May 10 17:28:33 2018 -0700,1525973313.0,"Split libATen.so into libATen_cpu.so and libATen_cuda.so  (#7275)

* Split libATen.so into libATen_cpu.so and libATen_cuda.so

Previously, ATen could be built with either CPU-only support, or
CPU/CUDA support, but only via a compile-time flag, requiring
two separate builds.  This means that if you have a program which
indirectly uses a CPU-only build of ATen, and a CPU/CUDA-build of
ATen, you're gonna have a bad time.  And you might want a CPU-only
build of ATen, because it is 15M (versus the 300M of a CUDA build).

This commit splits libATen.so into two libraries, CPU/CUDA, so
that it's not necessary to do a full rebuild to get CPU-only
support; instead, if you link against libATen_cpu.so only, you
are CPU-only; if you additionally link/dlopen libATen_cuda.so,
this enables CUDA support.  This brings ATen's dynamic library
structure more similar to Caffe2's.  libATen.so is no more
(this is BC BREAKING)

The general principle for how this works is that we introduce
a *hooks* interface, which introduces a dynamic dispatch indirection
between a call site and implementation site of CUDA functionality,
mediated by a static initialization registry.  This means that we can continue
to, for example, lazily initialize CUDA from Context (a core, CPU class) without
having a direct dependency on the CUDA bits.  Instead, we look up
in the registry if, e.g., CUDA hooks have been loaded (this loading
process happens at static initialization time), and if they
have been we dynamic dispatch to this class.  We similarly use
the hooks interface to handle Variable registration.

We introduce a new invariant: if the backend of a type has not
been initialized (e.g., it's library has not been dlopened; for
CUDA, this also includes CUDA initialization), then the Type
pointers in the context registry are NULL.  If you access the
registry directly you must maintain this invariant.

There are a few potholes along the way.  I document them here:

- Previously, PyTorch maintained a separate registry for variable
  types, because no provision for them was made in the Context's
  type_registry.  Now that we have the hooks mechanism, we can easily
  have PyTorch register variables in the main registry.  The code
  has been refactored accordingly.

- There is a subtle ordering issue between Variable and CUDA.
  We permit libATen_cuda.so and PyTorch to be loaded in either
  order (in practice, CUDA is always loaded ""after"" PyTorch, because
  it is lazily initialized.)  This means that, when CUDA types are
  loaded, we must subsequently also initialize their Variable equivalents.
  Appropriate hooks were added to VariableHooks to make this possible;
  similarly, getVariableHooks() is not referentially transparent, and
  will change behavior after Variables are loaded.  (This is different
  to CUDAHooks, which is ""burned in"" after you try to initialize CUDA.)

- The cmake is adjusted to separate dependencies into either CPU
  or CUDA dependencies.  The generator scripts are adjusted to either
  generate a file as a CUDA (cuda_file_manager) or CPU file (file_manager).

- I changed all native functions which were CUDA-only (the cudnn functions)
  to have dispatches for CUDA only (making it permissible to not specify
  all dispatch options.)  This uncovered a bug in how we were handling
  native functions which dispatch on a Type argument; I introduced a new
  self_ty keyword to handle this case.  I'm not 100% happy about it
  but it fixed my problem.

  This also exposed the fact that set_history incompletely handles
  heterogenous return tuples combining Tensor and TensorList.  I
  swapped this codegen to use flatten() (at the possible cost of
  a slight perf regression, since we're allocating another vector now
  in this code path).

- thc_state is no longer a public member of Context; use getTHCState() instead

- This PR comes with Registry from Caffe2, for handling static initialization.
  I needed to make a bunch of fixes to Registry to make it more portable

  - No more ##__VA_ARGS__ token pasting; instead, it is mandatory to pass at
    least one argument to the var-args. CUDAHooks and VariableHooks pass a nullary
    struct CUDAHooksArgs/VariableHooksArgs to solve the problem. We must get rid of
    token pasting because it does not work with MSVC.

  - It seems MSVC is not willing to generate code for constructors of template
    classes at use sites which cross DLL boundaries. So we explicitly instantiate
    the class to get around the problem. This involved tweaks to the boilerplate
    generating macros, and also required us to shuffle around namespaces a bit,
    because you can't specialize a template unless you are in the same namespace as
    the template.
  - Insertion of AT_API to appropriate places where the registry must be exported

- We have a general problem which is that on recent Ubuntu distributions,
  --as-needed is enabled for shared libraries, which is (cc @apaszke who was
  worrying about this in #7160 see also #7160 (comment)). For now, I've hacked
  this up in the PR to pass -Wl,--no-as-needed to all of the spots necessary to
  make CI work, but a more sustainable solution is to attempt to dlopen
  libATen_cuda.so when CUDA functionality is requested.

    - The JIT tests somehow manage to try to touch CUDA without loading libATen_cuda.so. So
      we pass -Wl,--no-as-needed when linking libATen_cuda.so to _C.so

- There is a very subtle linking issue with lapack, which is solved by making sure libATen_cuda.so links against LAPACK. There's a comment in aten/src/ATen/CMakeLists.txt about htis as well as a follow up bug at #7353

- autogradpp used AT_CUDA_ENABLED directly. We've expunged these uses and added
  a few more things to CUDAHooks (getNumGPUs)

- Added manualSeedAll to Generator so that we can invoke it polymorphically (it
  only does something different for CUDAGenerator)

- There's a new cuda/CUDAConfig.h header for CUDA-only ifdef macros (AT_CUDNN_ENABLED, most prominently)

- CUDAHooks/VariableHooks structs live in at namespace because Registry's
  namespace support is not good enough to handle it otherwise (see Registry
  changes above)

- There's some modest moving around of native functions in ReduceOps and
  UnaryOps to get the CUDA-only function implementations into separate files, so
  they are only compiled into libATen_cuda.so. sspaddmm needed a separate CUDA
  function due to object linkage boundaries.

- Some direct uses of native functions in CUDA code has to go away, since these
  functions are not exported, so you have to go through the dispatcher
  (at::native::empty_like to at::empty_like)

- Code in THC/THCS/THCUNN now properly use THC_API macro instead of TH_API
  (which matters now that TH and THC are not in the same library)

- Added code debt in torch/_thnn/utils.py and other THNN parsing code to handle
  both TH_API and THC_API

- TensorUtils.h is now properly exported with AT_API

- Dead uses of TH_EXPORTS and co expunged; we now use ATen_cpu_exports and
  ATen_cuda_exports (new, in ATenCUDAGeneral.h) consistently

- Fix some incorrect type annotations on _cudnn_rnn_backward, where we didn't
  declare a type as possibly undefined when we should have. We didn't catch this
  previously because optional annotations are not tested on ""pass-through"" native
  ATen ops (which don't have dispatch). Upstream issue at #7316

- There's a new cmake macro aten_compile_options for applying all of our
  per-target compile time options. We use this on the cpu and cuda libraries.

- test/test_cpp_extensions.py can be run directly by invoking in Python,
  assuming you've setup your PYTHONPATH setup correctly

- type_from_string does some new funny business to only query for all valid CUDA
  types (which causes CUDA initialization) when we see ""torch.cuda."" in the
  requested string

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* Last mile libtorch fixes

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* pedantic fix

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",1691.0,862.0,".gitignore,.jenkins/pytorch/test.sh,aten/CMakeLists.txt,aten/contrib/data/CMakeLists.txt,aten/contrib/meter/CMakeLists.txt,aten/src/ATen/ATenGeneral.h,aten/src/ATen/AccumulateType.h,aten/src/ATen/CMakeLists.txt,aten/src/ATen/CPUGenerator.cpp,aten/src/ATen/CUDAGenerator.cpp,aten/src/ATen/Config.h.in,aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/Generator.h,aten/src/ATen/PinnedMemoryAllocator.cpp,aten/src/ATen/PinnedMemoryAllocator.h,aten/src/ATen/Registry.h,aten/src/ATen/TensorUtils.h,aten/src/ATen/common_with_cwrap.py,aten/src/ATen/copy_wrapper.py,aten/src/ATen/cuda/ATenCUDAGeneral.h,aten/src/ATen/cuda/CUDAConfig.h.in,aten/src/ATen/cuda/CUDAGenerator.cpp,aten/src/ATen/cuda/CUDAHalf.cu,aten/src/ATen/cuda/CUDAHalf.cuh,aten/src/ATen/cuda/PinnedMemoryAllocator.cpp,aten/src/ATen/cuda/PinnedMemoryAllocator.h,aten/src/ATen/cuda/detail/CUDAHooks.cpp,aten/src/ATen/cuda/detail/CUDAHooks.h,aten/src/ATen/cudnn/Utils.h,aten/src/ATen/detail/CUDAHooksInterface.cpp,aten/src/ATen/detail/CUDAHooksInterface.h,aten/src/ATen/detail/VariableHooksInterface.cpp,aten/src/ATen/detail/VariableHooksInterface.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/gen.py,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/Memory.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/TensorProperties.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/CUDAReduceOps.cpp,aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,aten/src/ATen/native/cuda/Distributions.cu,aten/src/ATen/native/cuda/Gesv.cu,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/SparseMM.cu,aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,aten/src/ATen/native/cudnn/BatchNorm.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/cudnn/GridSampler.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/stub/CombinedStub.cpp,aten/src/ATen/templates/GeneratorDerived.h,aten/src/ATen/templates/RegisterCUDA.cpp,aten/src/ATen/templates/RegisterCUDA.h,aten/src/ATen/templates/StorageDerived.cpp,aten/src/ATen/templates/TensorDerived.cpp,aten/src/ATen/templates/Type.cpp,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeDerived.cpp,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/verify_api_visibility.cpp,aten/src/TH/THGeneral.h.in,aten/src/THC/THCGeneral.h.in,aten/src/THCS/THCSTensor.h,aten/src/THCS/THCSparse.h,aten/src/THCS/generic/THCSTensor.h,aten/src/THCS/generic/THCSTensorMath.h,aten/src/THCUNN/generic/SpatialGridSamplerBilinear.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THCUNN/generic/VolumetricGridSamplerBilinear.cu,aten/src/THS/THSTensor.h,setup.py,test/test_cpp_extensions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/build_pytorch_libs.bat,tools/cpp_build/libtorch/CMakeLists.txt,torch/_thnn/utils.py,torch/backends/cudnn/rnn.py,torch/csrc/DynamicTypes.cpp,torch/csrc/api/src/detail.cpp,torch/csrc/autograd/aten_variable_hooks.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/tensor_types.cpp,torch/utils/cpp_extension.py",90.0,42,5,5.140540004,47.0,18080.0,28.0,1950486.6805555555,1113.0,2907.805292,0.0,Corrective,1.0,1
pytorch,064fef1313a4660e6d490afd5aeb3c60b6de644e,648d1b101a049cb3809a0e6ce8054e34b8f3be8f,Yangqing Jia,jiayq84@gmail.com,Mon Oct 12 06:14:06 2015 -0700,1444630446.0,"A consolidation of a couple random weekend work.

(1) various bugfixes.
(2) Tensor is now a class independent from its data type. This allows us
    to write easier type-independent operators.
(3) code convention changes a bit: dtype -> T, Tensor<*Context> -> Tensor* alias.
(4) ParallelNet -> DAGNet to be more consistent with what it does.
(5) Caffe's own flags library instead of gflags.
(6) Caffe's own logging library instead of glog, but glog can be chosen with
    compile-time definition -DCAFFE2_USE_GOOGLE_GLOG. As a result, glog macros
    like CHECK, DCHECK now have prefix CAFFE_, and LOG(*) now becomes
    CAFFE_LOG_*.
(7) an optional protobuf inclusion, which can be chosen with USE_SYSTEM_PROTOBUF
    in build_env.py.",3398.0,2549.0,"brewery.py,build_env.py,caffe2/binaries/BREW,caffe2/binaries/convert_caffe_image_db.cc,caffe2/binaries/convert_db.cc,caffe2/binaries/convert_encoded_to_raw_leveldb.cc,caffe2/binaries/db_throughput.cc,caffe2/binaries/inspect_gpus.cc,caffe2/binaries/make_cifar_db.cc,caffe2/binaries/make_image_db.cc,caffe2/binaries/make_mnist_db.cc,caffe2/binaries/print_registered_core_operators.cc,caffe2/binaries/run_client.cc,caffe2/binaries/run_plan.cc,caffe2/binaries/run_plan_mpi.cc,caffe2/binaries/split_db.cc,caffe2/binaries/zmq_feeder.cc,caffe2/core/BREW,caffe2/core/blob.h,caffe2/core/blob_gpu_test.cc,caffe2/core/blob_serialization.cc,caffe2/core/blob_serialization.h,caffe2/core/blob_serialization_gpu.cc,caffe2/core/blob_test.cc,caffe2/core/client.cc,caffe2/core/common.h,caffe2/core/common_cudnn.h,caffe2/core/common_gpu.cc,caffe2/core/common_gpu.h,caffe2/core/context.h,caffe2/core/context_gpu.h,caffe2/core/cuda_memorypool.cc,caffe2/core/cuda_memorypool.h,caffe2/core/cuda_memorypool_test.cc,caffe2/core/flags.cc,caffe2/core/flags.h,caffe2/core/init.cc,caffe2/core/init.h,caffe2/core/init_test.cc,caffe2/core/logging.cc,caffe2/core/logging.h,caffe2/core/logging_is_google_glog.h,caffe2/core/logging_is_not_google_glog.h,caffe2/core/minidb.cc,caffe2/core/net.cc,caffe2/core/net.h,caffe2/core/operator.cc,caffe2/core/operator.h,caffe2/core/operator_test.cc,caffe2/core/parallel_net_test.cc,caffe2/core/registry.h,caffe2/core/registry_test.cc,caffe2/core/typeid.cc,caffe2/core/typeid.h,caffe2/core/typeid_test.cc,caffe2/core/workspace.cc,caffe2/db/BREW,caffe2/db/leveldb.cc,caffe2/db/lmdb.cc,caffe2/db/protodb.cc,caffe2/db/zmqdb.cc,caffe2/end_to_end_test/end_to_end_test.cc,caffe2/image/image_input_op.h,caffe2/mpi/BREW,caffe2/mpi/mpi_common.h,caffe2/mpi/mpi_gpu_test.cc,caffe2/mpi/mpi_ops.cc,caffe2/mpi/mpi_ops.h,caffe2/mpi/mpi_ops_fallback.h,caffe2/mpi/mpi_test.cc,caffe2/operators/BREW,caffe2/operators/accumulate_op.h,caffe2/operators/accuracy_op.cc,caffe2/operators/accuracy_op.cu,caffe2/operators/accuracy_op.h,caffe2/operators/averagepool_op.cc,caffe2/operators/averagepool_op.cu,caffe2/operators/averagepool_op.h,caffe2/operators/clip_op.cc,caffe2/operators/clip_op.cu,caffe2/operators/clip_op.h,caffe2/operators/conv_op.cu,caffe2/operators/conv_op.h,caffe2/operators/conv_op_cudnn.cu.working,caffe2/operators/conv_op_gpu.cc,caffe2/operators/conv_op_impl.h,caffe2/operators/conv_pool_op_base.h,caffe2/operators/cross_entropy_op.cc,caffe2/operators/cross_entropy_op.cu,caffe2/operators/cross_entropy_op.h,caffe2/operators/depth_split_op.cc,caffe2/operators/depth_split_op.cu,caffe2/operators/depth_split_op.h,caffe2/operators/depth_split_op_gpu.cc,caffe2/operators/dropout_op.cc,caffe2/operators/dropout_op.cu,caffe2/operators/dropout_op.h,caffe2/operators/elementwise_op.cc,caffe2/operators/elementwise_op.h,caffe2/operators/filler_op.cc,caffe2/operators/filler_op.cu,caffe2/operators/filler_op.h,caffe2/operators/fully_connected_op.h,caffe2/operators/fully_connected_op_test.cc,caffe2/operators/l2_distance_op.cc,caffe2/operators/l2_distance_op.cu,caffe2/operators/l2_distance_op.h,caffe2/operators/load_save_op.cu,caffe2/operators/load_save_op.h,caffe2/operators/load_save_op_gpu.cc,caffe2/operators/local_response_normalization_op.cc,caffe2/operators/local_response_normalization_op.cu,caffe2/operators/local_response_normalization_op.h,caffe2/operators/loss_op.h,caffe2/operators/maxpool_op.cc,caffe2/operators/maxpool_op.cu,caffe2/operators/maxpool_op.h,caffe2/operators/order_switch_ops.cc,caffe2/operators/order_switch_ops.cu,caffe2/operators/order_switch_ops.h,caffe2/operators/prefetch_op.h,caffe2/operators/relu_op.cc,caffe2/operators/relu_op.cu,caffe2/operators/relu_op.h,caffe2/operators/softmax_op.cc,caffe2/operators/softmax_op.cu,caffe2/operators/softmax_op.h,caffe2/operators/softmax_op_cudnn.cc,caffe2/operators/summarize_op.cc,caffe2/operators/summarize_op.cu,caffe2/operators/summarize_op.h,caffe2/operators/tensor_protos_db_input.h,caffe2/operators/tensor_protos_db_input_test.cc,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_gpu.cc,caffe2/proto/caffe2_legacy.proto,caffe2/sgd/iter_op.cc,caffe2/sgd/learning_rate_functors.h,caffe2/sgd/learning_rate_op.cc,caffe2/sgd/learning_rate_op.h,caffe2/utils/BREW,caffe2/utils/math.h,caffe2/utils/math_cpu.cc,caffe2/utils/math_gpu.cu,caffe2/utils/math_test.cc,caffe2/utils/mkl_alternate.h,caffe2/utils/proto_utils.cc,caffe2/utils/proto_utils.h,caffe2/utils/simple_queue.h,caffe2/utils/simple_queue_test.cc,gtest/BREW,gtest/caffe2_gtest_main.cc,gtest/gtest-all.cpp,pycaffe2/BREW,pycaffe2/caffe2_python.cc,pycaffe2/cnn.py,pycaffe2/gradient_check_test.py,pycaffe2/model_device_test.py,third_party/cnmem/cnmem.cpp,third_party/gflags/BREW,third_party/glog/BREW,third_party/openmpi/BREW,third_party/python_libs/BREW",164.0,19,4,6.605848117,4.0,26685.0,33.0,6099198.397350993,106.0,934.8333333,0.0,Corrective,1.0,1
pytorch,d1fda539b79e7fd1ee912a5d257bf3558e346bf5,64a15928d7178347edfcd42dccece9e9257c27ca,Adam Paszke,adam.paszke@gmail.com,Fri Sep 16 03:23:54 2016 -0700,1473996234.0,Fix tests on python 3.3,9.0,1.0,test/common_nn.py,1.0,1,1,0,5.0,642.0,1.0,3634.0,175.0,3747.032937,0.0,Corrective,1.0,1
pytorch,a3e3cbfbbe093d9046d704738c52212f7e76b11c,64a3fbae5e6e4fe5a5b71d065b30549ca7a03847,Richard Zou,zou3519@gmail.com,Tue Aug 16 15:04:27 2022 -0700,1660662267.0,"[functorch] Classify some vmap failures with comments (#83517)

The silent incorrectness issues are hi-pri

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83517
Approved by: https://github.com/samdow",32.0,27.0,functorch/test/test_vmap.py,1.0,2,1,0,2.0,4447.0,1.0,82088.0,6465.0,14981.5,0.0,Corrective,0.0,1
pytorch,967bceb16bf93e28833ced13adf917df8db6f1e9,64a9ecae020fe528921bc47d6b4ee67f2bb2c456,Tongzhou Wang,SsnL@users.noreply.github.com,Mon Jan 29 00:18:17 2018 -0500,1517185097.0,"Dataloader issues (#4643)

* EINTR and kill by loader fix

* addressed @apaszke 's comments

* remove EINTR handling and add test if we are in main thread before setting SIGCHLD",125.0,41.0,"test/common.py,test/test_dataloader.py,torch/csrc/DataLoader.cpp,torch/utils/data/dataloader.py",4.0,5,2,1.637654067,37.0,1537.0,4.0,1854897.5,2313.0,24432.35823,0.0,Corrective,1.0,1
pytorch,0bf1260795445622adc3ab58e3c35e18baf24fee,64aec8d2cac444319275b0f3a59053d19b86ec73,kshitij12345,kshitijkalambarkar@gmail.com,Thu Jun 17 00:15:59 2021 -0700,1623888959.0,"[testing] OpInfoHelper tool (#58698)

Summary:
Fixes: https://github.com/pytorch/pytorch/issues/57577

Usage:
Add OpInfo entry to `common_methods_invocations` with `dtypes=_DYNAMIC_DYTPES`
Eg.
```
OpInfo('atan2',
        dtypes=_DYNAMIC_DTYPES,
        sample_inputs_func=sample_inputs_atan2,)
```

Run the helper with `python -m torch.testing._internal.opinfo_helper`

Output
```
OpInfo(atan2,
       # hint: all_types + (torch.bool,),
       dtypes=[torch.float32, torch.float64, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64, torch.bool],
       # hint: all_types + (torch.bool, torch.bfloat16, torch.float16),
       dtypesIfCUDA=[torch.float32, torch.float64, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64, torch.bool, torch.bfloat16, torch.float16],
       sample_inputs_func=sample_inputs_atan2)
```

Output without CUDA (run with `$ CUDA_VISIBLE_DEVICES=-1 python -m torch.testing._internal.opinfo_helper`)
```
UserWarning: WARNING: CUDA is not available, information pertaining to CUDA could be wrong
  warnings.warn(""WARNING: CUDA is not available, information pertaining to CUDA could be wrong"")
OpInfo(atan2,
       # hint: all_types + (torch.bool,),
       dtypes=[torch.float32, torch.float64, torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64, torch.bool],
       sample_inputs_func=sample_inputs_atan2)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58698

Reviewed By: H-Huang

Differential Revision: D29160668

Pulled By: mruberry

fbshipit-source-id: 707370a83b451b02ad2fe539775c8c50ecf90be8",193.0,3.0,"test/test_ops.py,test/test_testing.py,torch/testing/_core.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo_helper.py",5.0,4,2,1.392197456,2.0,9973.0,4.0,1073078.25,13075.0,29605.5,0.0,Corrective,1.0,1
pytorch,9ac56ef0fca614f9fdd95e60a5fa0f3ca1cf19d0,64c54f92ca36c98856f8d8b0b18ba2c1a5a156a1,kshitij12345,kshitijkalambarkar@gmail.com,Fri Aug 06 00:06:51 2021 -0700,1628208411.0,"[opinfo] nn.functional.unfold (#62705)

Summary:
Reference: facebookresearch/functorch#78

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62705

Reviewed By: H-Huang

Differential Revision: D30138807

Pulled By: zou3519

fbshipit-source-id: 1d0b0e58feb13aec7b231c9f632a6d1694b9d272",28.0,10.0,"test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.831474388,43.0,26834.0,2.0,14147.5,14445.0,33025.0,0.0,,0.0,1
pytorch,2b7943c47c8561a46103488b0fe9a592b87dc5bb,64c6a89bd608ab76beda029ca81a18b5a68348ff,Mike Ruberry,mruberry@fb.com,Fri May 13 13:12:04 2022 +0000,1652447524.0,"[primTorch] reshape and view (#77220)

This PR makes the following changes...

Prims
- adds as_strided
- fixes errors in flatten meta

Testing
- enables view consistency checking (which can be opted out of, see issues below)
- adds reference inputs for view, reshape, and flatten
- adds error inputs for reshape

Refs
- adds as_strided, reshape, and view
- fixes an error in the flatten ref where it was not returning self on no-op
- fixes a bug in transpose where it was not retuning a view when the transposed tensor has 1 or fewer dims

Issues
- https://github.com/pytorch/pytorch/issues/77218
- https://github.com/pytorch/pytorch/issues/77216
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77220
Approved by: https://github.com/ngimel",492.0,50.0,"test/test_ops.py,torch/_prims/__init__.py,torch/_prims/utils.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",5.0,6,2,1.953596697,5.0,23708.0,3.0,59136.0,3176.0,7635.5,0.0,Corrective,1.0,1
pytorch,9a22cb9f49e2d4a76733da4eef101749627e2861,64f06d4964d0d4d5b7a67c4679f21fa8aaa2bd92,Iurii Zdebskyi,iuriiz@fb.com,Thu May 30 23:01:24 2019 -0700,1559257284.0,"Enable all and any for bool tensors (#21033)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21033
ghimport-source-id: 35fdcf27b0bde8ec3e5b3051cf0d730f20f94783

Differential Revision: D15530497

Pulled By: izdeby

fbshipit-source-id: 9c15cc960055f59a05ce0276f9d51c567626d966",28.0,12.0,"aten/src/ATen/native/ReduceOps.cpp,test/test_torch.py",2.0,5,2,0.970950594,40.0,12493.0,2.0,110533.0,9025.0,26526.83333,0.0,,0.0,1
pytorch,4f5ea5983ab697032639778bc48c8abfebed2f6f,6502fb89dd9a6e56aed5a6e24e1db1793beffd68,Jiewen Tan,jwtan@fb.com,Fri Oct 01 05:39:41 2021 -0700,1633066781.0,"Make JIT Aliasing Test Less Brittle (#65493)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65493

Added a last resolve to use whatever ATen operator that has Tensor outputs in the graph as the operator node to check alias annotation.

Test Plan: python test/test_ops.py -k test_variant_consistency_jit

Reviewed By: mrshenli

Differential Revision: D31321221

Pulled By: alanwaketan

fbshipit-source-id: f4a5cbfd36bd0867d8c1bf9de9a65365ee7c35d6",214.0,154.0,"torch/csrc/jit/passes/utils/check_alias_annotation.cpp,torch/testing/_internal/common_methods_invocations.py",2.0,7,1,0.39795216,2.0,10674.0,2.0,53324.5,15867.0,36658.5,0.0,Feature Addition,0.0,1
pytorch,0e11454d19e106ba6d5819c1147ca540cbce2943,6512838fabcbbf2854597790c62599af1e7f74ed,BowenBao,bowbao@microsoft.com,Wed Sep 15 19:56:33 2021 -0700,1631735793.0,"[ONNX] Enhance shape (two changes merged) (#64585)

Summary:
Enhanced shape inference by introducing typeReliableMap.
[ONNX] exporter changes for torch hub models (https://github.com/pytorch/pytorch/issues/62856)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64585

Reviewed By: ezyang

Differential Revision: D30870418

Pulled By: msaroufim

fbshipit-source-id: 87a294799cb87d649d1d13b6114a5cfbac9be15c

Co-authored-by: jiafatom <jiafa@microsoft.com>",1245.0,46.0,"aten/src/ATen/core/interned_strings.h,docs/source/onnx.rst,test/onnx/expect/TestOperators.test_aten_embedding_1.expect,test/onnx/expect/TestOperators.test_aten_embedding_2.expect,test/onnx/expect/TestOperators.test_c2_op.expect,test/onnx/expect/TestOperators.test_dynamic_axes_add.expect,test/onnx/expect/TestOperators.test_dynamic_axes_matmul.expect,test/onnx/expect/TestOperators.test_dynamic_axes_reduce_mean.expect,test/onnx/expect/TestOperators.test_dynamic_axes_unchange.expect,test/onnx/expect/TestOperators.test_lstm_none_sequence_lens.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx/constant_fold.cpp,torch/csrc/jit/passes/onnx/constant_map.cpp,torch/csrc/jit/passes/onnx/constant_map.h,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.h,torch/onnx/__init__.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",22.0,15,4,3.353747679,14.0,17744.0,14.0,6606994.266666667,15467.0,35362.5,0.0,Perfective,0.0,1
pytorch,89d0e31fe582696c018fdfde9166b187ee62538e,652d911f81c1028df63d28d3c9ec5e16c7402e5b,mingfeima,mingfei.ma@intel.com,Tue Jun 29 21:05:50 2021 -0700,1625000750.0,"add BFloat16 support for LayerNorm CPU (#55210)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55210

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D28836793

Pulled By: VitalyFedyunin

fbshipit-source-id: 998298deedd7a18e45fb761a0a4e0d88b65f2e0c",35.0,26.0,"aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/layer_norm_kernel.cpp,aten/src/ATen/native/cpu/moments_utils.h,test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,1.364365774,43.0,26599.0,4.0,760209.6,13459.0,30475.0,0.0,Feature Addition,0.0,1
pytorch,cd3bbc9dfdb3dcab2d3fac4fb1819f84e982c107,655c22569e9f0e355d063c18441e69da0fb29504,Martin Raison,raison@fb.com,Fri Mar 24 14:21:15 2017 -0700,1490365275.0,CPU hspmm + more efficient reorder,103.0,91.0,"test/test_sparse.py,torch/lib/THCS/THCSTensor.h,torch/lib/THCS/generic/THCSTensorMath.cu,torch/lib/THS/THSTensor.h,torch/lib/THS/generic/THSTensor.c,torch/lib/THS/generic/THSTensorMath.c",6.0,7,2,1.284902251,16.0,2036.0,4.0,241.66666666666663,336.0,15031.107,0.0,,0.0,1
pytorch,a51a094200e82a73059528f8994242fdc6037f11,658d4c7ea8bbd4b6adc825e08c200b6bde76696b,SsnL,tongzhou.wang.1994@gmail.com,Mon Dec 18 23:26:17 2017 -0500,1513639577.0,allow optional int tensor,116.0,13.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/preprocess_declarations.py,docs/source/torch.rst,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_torch_docs.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorMath.cwrap",9.0,13,5,2.244469022,39.0,20055.0,8.0,606591.2222222222,2220.0,24285.35823,0.0,,0.0,1
pytorch,fc4f8b5edec69c0ddc63e5210d3f34724d4bd672,65a8f8f62ec47e11e718cc437159bd14983a9905,anjali411,chourdiaanjali123@gmail.com,Tue May 10 15:40:17 2022 +0000,1652197217.0,"Add __all__ for torch.autograd.{anomaly_mode, gradcheck, forward_ad}

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76492

Approved by: https://github.com/albanD, https://github.com/soulitzer",19.0,24.0,"test/allowlist_for_publicAPI.json,torch/autograd/anomaly_mode.py,torch/autograd/forward_ad.py,torch/autograd/gradcheck.py",4.0,3,2,1.720551636,30.0,5429.0,4.0,12637813.75,3031.0,7291.5,0.0,Feature Addition,0.0,1
pytorch,90d6112a948644dac77120cfcf1de9ac5566ab79,65e887c041943bf5d1ae2c515cc7a89e3b89b588,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu Sep 01 07:18:42 2022 +0000,1662016722.0,"Remove unnecessary copy from torch._refs.to, add OpInfo for torch.Tensor.to (#84270)

This PR removes unnecessary copy from `torch._refs.to`, adds OpInfo for `torch.Tensor.to`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84270
Approved by: https://github.com/ngimel",113.0,0.0,"functorch/test/test_ops.py,functorch/test/test_vmap.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",4.0,6,2,1.156548147,7.0,27492.0,4.0,301335.0,6998.0,16428.5,0.0,Feature Addition,0.0,1
pytorch,4b74c848aa5a4b05f36a21c36d469d5cc60c116e,6620d7d688c9482a00b1a9827187f135b72b7862,kshitij12345,kshitijkalambarkar@gmail.com,Thu Jun 03 15:24:36 2021 -0700,1622733876.0,"OpInfo: norm (#59259)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

EDIT:
~~Test takes whooping 4 mins to run :sweat:~~ (Filtered tests also included linalg norm)

Newly added tests take around 2 mins.
```
==================================================== 193 passed, 224 skipped, 27224 deselected, 5 warnings in 138.87s (0:02:18) ====================================================
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59259

Reviewed By: jbschlosser

Differential Revision: D28833962

Pulled By: mruberry

fbshipit-source-id: 40b24d6a8cb8b7d231b2f6b34b87cee4f136c5f9",174.0,34.0,"test/test_fx.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.043940679,2.0,10226.0,2.0,150281.5,12679.0,28800.5,0.0,Feature Addition,0.0,1
pytorch,ab7606702cca34c06f0bf08d6b109402c574563c,665feda15bc45d0f50326596ecde6f2d96ac6644,Mike Ruberry,mruberry@devfair044.maas,Thu Sep 03 09:48:53 2020 -0700,1599126533.0,"Adds opinfo-based autograd tests and (un)supported dtype tests (#43451)

Summary:
This PR adds a new test suite, test_ops.py, designed for generic tests across all operators with OpInfos. It currently has two kinds of tests:

- it validates that the OpInfo has the correct supported dtypes by verifying that unsupported dtypes throw an error and supported dtypes do not
- it runs grad and gradgrad checks on each op and its variants (method and inplace) that has an OpInfo

This is a significant expansion and simplification of the current autogenerated autograd tests, which spend considerable processing their inputs. As an alternative, this PR extends OpInfos with ""SampleInputs"" that are much easier to use. These sample inputs are analogous to the existing tuples in`method_tests()`.

Future PRs will extend OpInfo-based testing to other uses of `method_tests()`, like test_jit.py, to ensure that new operator tests can be implemented entirely using an OpInfo.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43451

Reviewed By: albanD

Differential Revision: D23481723

Pulled By: mruberry

fbshipit-source-id: 0c2cdeacc1fdaaf8c69bcd060d623fa3db3d6459",301.0,108.0,"test/run_test.py,test/test_ops.py,test/test_unary_ufuncs.py,torch/autograd/gradcheck.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",7.0,5,2,2.301822203,28.0,5708.0,5.0,625605.6666666666,4808.0,11167.0,0.0,Corrective,0.0,1
pytorch,94a8a8aa3273468448909ffb22a188d5c22fa730,667607b3abf7854127b74b8c8b2cb2237dd67dcb,Kulin Seth,kulin_seth@apple.com,Wed Jul 13 05:54:32 2022 +0000,1657691672.0,"[MPS] Reduce the number of command_buf created and improve performance (#81338)

The PR improves performance and reduces the CPU overhead by reducing the number of command buffers created. It uses commit and continue feature in MPS.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81338
Approved by: https://github.com/razarmehr",110.0,111.0,"aten/src/ATen/mps/MPSStream.h,aten/src/ATen/mps/MPSStream.mm,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/mps/operations/Distributions.mm",6.0,7,1,2.019853584,1.0,2037.0,3.0,2317500.5,5273.0,12429.5,0.0,Feature Addition,0.0,1
pytorch,53fdd60635710a7a9f1c2a3eb1115f51b1247e94,66907e7262da6d6ef3d471e4c90a1c48a8f39a76,Richard Zou,zou3519@gmail.com,Mon Sep 19 21:29:41 2022 -0700,1663622981.0,"[functorch] Fix dangling impls (#85299)

Our dangling impls were:
- positive_ (the in-place op just never existed)
- unique (something happened to this op, maybe it was renamed)

Test Plan:
- `import functorch; torch._C._dispatch_find_dangling_impls`
- It's difficult to write a test for this because the number of dangling
impls depends on if `test_dispatch` has been run already or not
(test_dispatch adds a dangling impl)
- Can't remove the torchdynamo skip for this yet either
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85299
Approved by: https://github.com/ezyang",11.0,2.0,"functorch/csrc/BatchRulesDecompositions.cpp,functorch/csrc/BatchRulesDynamic.cpp,functorch/csrc/BatchRulesUnaryOps.cpp,functorch/test/test_vmap.py,test/test_dispatch.py",5.0,4,2,2.038159682,3.0,6085.0,3.0,551469.4,7482.0,17525.0,0.0,Corrective,1.0,1
pytorch,ae45dab57e22e3d04516e7dd81ef8dbefd51bfe3,66979fbfaa2af227a6834157fa6f532979b2d23b,Peter Bell,peterbell10@live.co.uk,Thu Oct 13 17:42:11 2022 +0000,1665682931.0,"Improve complex lerp performance (#84844)

The complex lerp kernel uses `std::abs(z) < 0.5` which involves
computing a sqrt. Instead compare the square against 0.25 has much
lower latency and so performs much better overall.

In a simple timeit benchmark I see more than 10x speedup on CPU for a 4096
element complex lerp, from 84 us to 6.7 us.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84844
Approved by: https://github.com/ngimel",43.0,43.0,"aten/src/ATen/native/Lerp.h,aten/src/ATen/native/cpu/LerpKernel.cpp,aten/src/ATen/native/cuda/Lerp.cu",3.0,6,1,1.542056918,3.0,191.0,3.0,14515269.0,8346.0,19769.0,0.0,Perfective,0.0,1
pytorch,b3cdec88e364e56bc69bf42cd6d8583fcfd62a18,66d50060ebb091a5186acd0c066439a3b80f8ea6,anjali411,chourdiaanjali123@gmail.com,Sun Apr 05 14:19:18 2020 -0700,1586096358.0,"Temporary methods for real and imag values of complex tensors (#35879)

Summary:
Notes:
1. didn't name them as _copy_real and _copy_imag because it's desirable (but not necessary) to have these methods as tensor methods.
2. replaced old .real() and .imag() instances with _copy_real() and _copy_imag() methods
3. didn't add documentation because we plan to remove these methods when we add real and imag as tensor attributes.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35879

Differential Revision: D20841760

Pulled By: anjali411

fbshipit-source-id: 7267e6fbaab9a5ce426e9396f12238994666b0dd",54.0,7.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_complex.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_overrides.py",9.0,9,4,2.752344991,43.0,29152.0,7.0,658294.6666666666,795.0,2253.0,0.0,Feature Addition,0.0,1
pytorch,74c7c736720e1c5cb35729d7b6a55430f8e36a3b,66ee9e96a3b2ecb833884051a4999dd70bbfd90a,Richard Zou,zou3519@gmail.com,Fri May 21 21:52:02 2021 -0700,1621633922.0,[functorch] Add opinfo based testing for grad transform,111.0,0.0,functorch/test/test_grad.py,1.0,2,1,0,1.0,0.0,0.0,0.0,107.0,213.5,0.0,Feature Addition,0.0,1
pytorch,0d667489480fe3a1ee4bf555c55cc7f0613940f1,671c8a459abf07fa8beb87b757010cf2de5233ac,shubhambhokare1,shubhambhokare@gmail.com,Sat Feb 19 00:15:16 2022 +0000,1645229716.0,"[ONNX] Add pixel_unshuffle support in opset 9

Current we are unable to utilize ONNX's SpaceToDepth operator due to the lack of the mode_s attribute, hence we add an alternative symbolic in opset 9 to support pixel_unshuffle

- Adds support for pixel_unshuffle in opset9
- Adds support for dynamic input shapes for pixel_shuffle and pixel_unshuffle
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72449",80.0,14.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.68193532,4.0,14218.0,1.0,20700.0,861.0,2139.5,0.0,Feature Addition,0.0,1
pytorch,c48e1679f9fe758abe8e3dd126ae87d4daef3af0,6732358bf92939789978abac29d18aab6ebc8c2d,Shen Li,shenli@fb.com,Thu Apr 18 04:18:49 2019 -0700,1555561129.0,"Allow DDP to wrap multi-GPU modules (#19271)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19271

allow DDP to take multi-gpu models

Reviewed By: pietern

Differential Revision: D14822375

fbshipit-source-id: 1eebfaa33371766d3129f0ac6f63a573332b2f1c",254.0,31.0,"test/test_c10d.py,torch/nn/parallel/distributed.py",2.0,4,2,0.874761781,17.0,2379.0,1.0,203836.0,8129.0,24494.83333,0.0,,0.0,1
pytorch,9dd5d51b01e6fc4ca50afd03aa64a30e40cec203,67608cc018b4548fcccfad2d9969c7d546224736,Dmytro Dzhulgakov,dzhulgakov@fb.com,Wed Mar 04 19:14:56 2020 -0800,1583349296.0,"Fix MKLDNN conv2d 5d weight handling (#34115)

Summary:
Effectively backporting https://github.com/pytorch/pytorch/pull/32422/commits/c5c00c119fd2631e8500747a04ea2ca9c9933741 before that PR lands

The bug didn't manifesting itself earlier because MkldnnConv2d constructor didn't reorder the weights. So the issue was arising only on second serialization/deserialization. This also fixes the constructor to deliver better perf right away.

Note, that I still serialize 5d tensor - it was the previous behavior, we have to handle it anyway and with https://github.com/pytorch/pytorch/issues/32422 the output of `mkldnn_reorder_conv2d_weight` will always be 4d.

cc pinzhenx
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34115

Reviewed By: wanchaol

Differential Revision: D20224685

Pulled By: dzhulgakov

fbshipit-source-id: 24ca9227c4eb4c139096a64ae348808d7478d7dc",45.0,1.0,"aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp,test/test_mkldnn.py,torch/utils/mkldnn.py",3.0,8,3,1.342884346,1.0,695.0,3.0,10733012.666666666,15215.0,40838.33333,0.0,Corrective,1.0,1
pytorch,b8776e143f11f92268b639a574dee79f50365be0,679fc90cdb126223e159a1ba3baeeb30fcc34b03,BowenBao,bowbao@microsoft.com,Wed May 04 20:19:46 2022 -0700,1651695586.0,"[ONNX] Support optional type (#68793) (#73284)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73284

Some important ops won't support optional type until opset 16,
so we can't fully test things end-to-end, but I believe this should
be all that's needed. Once ONNX Runtime supports opset 16,
we can do more testing and fix any remaining bugs.

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D34625646

Pulled By: malfet

fbshipit-source-id: 537fcbc1e9d87686cc61f5bd66a997e99cec287b

Co-authored-by: BowenBao <bowbao@microsoft.com>
Co-authored-by: neginraoof <neginmr@utexas.edu>
Co-authored-by: Nikita Shulga <nshulga@fb.com>
(cherry picked from commit 822e79f31ae54d73407f34f166b654f4ba115ea5)",1098.0,487.0,"aten/src/ATen/core/interned_strings.h,caffe2/python/onnx/tests/onnx_backend_test.py,test/onnx/test_models.py,test/onnx/test_pytorch_common.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_no_runtime.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_pytorch_onnx_onnxruntime_cuda.py,test/onnx/test_utility_funs.py,torch/_C/__init__.pyi.in,torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp,torch/csrc/jit/passes/onnx/peephole.h,torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.h,torch/csrc/jit/python/python_arg_flatten.cpp,torch/csrc/jit/python/python_ir.cpp,torch/csrc/jit/python/script_init.cpp,torch/csrc/jit/serialization/export.cpp,torch/csrc/jit/serialization/onnx.cpp,torch/csrc/onnx/init.cpp,torch/onnx/symbolic_opset15.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",24.0,20,4,2.992876551,14.0,31752.0,20.0,7845388.826086956,2857.0,6863.0,0.0,Corrective,1.0,1
pytorch,53839ac9d740da3cd4ab162cfe2b4628cce4bd6c,67c1dc65a342358c050f26107f885d62785c8c41,James Reed,jamesreed@fb.com,Wed Oct 28 00:49:29 2020 -0700,1603846169.0,"[FX] Fix handling of `inf` and `nan` literals (#46894)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/46894

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D24555136

Pulled By: jamesr66a

fbshipit-source-id: 22765a4d9d373711e9e6d7b1d3898080ecbcf2f5",25.0,2.0,"test/test_fx.py,torch/fx/graph.py",2.0,3,2,0.876716289,1.0,1598.0,2.0,453472.5,6260.0,14356.0,0.0,Corrective,1.0,1
pytorch,54c0f37646b8e7483519c4246a826ea7cbc6f695,67d979098567fd61dfdb918d837426535eb9883b,Aaron Gokaslan,aaronGokaslan@gmail.com,Sun Feb 12 01:01:21 2023 +0000,1676163681.0,"[BE] Apply almost all remaining flake8-comprehension checks (#94676)

Applies the remaining flake8-comprehension fixes and checks. This changes replace all remaining unnecessary generator expressions with list/dict/set comprehensions which are more succinct, performant, and better supported by our torch.jit compiler. It also removes useless generators such as 'set(a for a in b)`, resolving it into just the set call.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94676
Approved by: https://github.com/ezyang",500.0,526.0,".flake8,benchmarks/distributed/ddp/diff.py,scripts/release_notes/namespace_check.py,test/ao/sparsity/test_sparsifier.py,test/distributed/fsdp/test_fsdp_ignored_modules.py,test/distributed/fsdp/test_fsdp_state_dict.py,test/distributed/fsdp/test_utils.py,test/distributed/pipeline/sync/test_pipe.py,test/distributed/test_c10d_common.py,test/distributed/test_c10d_gloo.py,test/distributed/test_c10d_nccl.py,test/dynamo/test_optimizers.py,test/dynamo/test_repros.py,test/functorch/discover_coverage.py,test/jit/test_builtins.py,test/jit/test_list_dict.py,test/jit/test_misc.py,test/jit/test_save_load.py,test/jit/test_slice.py,test/lazy/test_ts_opinfo.py,test/package/test_dependency_hooks.py,test/package/test_digraph.py,test/quantization/core/test_quantized_op.py,test/quantization/eager/test_model_numerics.py,test/quantization/eager/test_quantize_eager_ptq.py,test/quantization/fx/test_model_report_fx.py,test/quantization/jit/test_quantize_jit.py,test/test_binary_ufuncs.py,test/test_bundled_inputs.py,test/test_cpp_extensions_aot.py,test/test_dataloader.py,test/test_datapipe.py,test/test_decomp.py,test/test_foreach.py,test/test_fx.py,test/test_fx_experimental.py,test/test_jit_cuda_fuser.py,test/test_jit_fuser.py,test/test_jit_fuser_te.py,test/test_modules.py,test/test_ops.py,test/test_optim.py,test/test_reductions.py,tools/autograd/gen_trace_type.py,tools/autograd/gen_variable_type.py,torch/_dynamo/symbolic_convert.py,torch/_dynamo/utils.py,torch/_dynamo/variables/torch.py,torch/_functorch/partitioners.py,torch/_inductor/codegen/triton.py,torch/_inductor/graph.py,torch/_inductor/ir.py,torch/_inductor/scheduler.py,torch/_inductor/utils.py,torch/_prims_common/__init__.py,torch/_refs/__init__.py,torch/ao/nn/intrinsic/qat/modules/conv_fused.py,torch/ao/nn/quantized/dynamic/modules/rnn.py,torch/ao/ns/_numeric_suite.py,torch/ao/ns/fx/mappings.py,torch/ao/ns/fx/n_shadows_utils.py,torch/ao/quantization/fx/_model_report/detector.py,torch/ao/quantization/fx/_model_report/model_report.py,torch/ao/quantization/fx/graph_module.py,torch/ao/quantization/quantization_mappings.py,torch/autograd/gradcheck.py,torch/cuda/__init__.py,torch/cuda/_memory_viz.py,torch/distributed/_composable/_ddp.py,torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/linear.py,torch/distributed/_tensor/dispatch.py,torch/distributed/_tensor/ops/tensor_ops.py,torch/distributed/_tensor/ops/view_ops.py,torch/distributed/fsdp/_init_utils.py,torch/distributed/fsdp/_optim_utils.py,torch/distributed/fsdp/_runtime_utils.py,torch/distributed/fsdp/flat_param.py,torch/distributed/fsdp/fully_sharded_data_parallel.py,torch/distributed/rendezvous.py,torch/fx/experimental/accelerator_partitioner.py,torch/fx/experimental/unification/core.py,torch/fx/experimental/unification/match.py,torch/fx/experimental/unification/multipledispatch/conflict.py,torch/fx/experimental/unification/utils.py,torch/fx/passes/dialect/common/cse_pass.py,torch/fx/passes/reinplace.py,torch/fx/passes/splitter_base.py,torch/jit/_builtins.py,torch/jit/_recursive.py,torch/jit/annotations.py,torch/jit/unsupported_tensor_ops.py,torch/masked/_ops.py,torch/masked/maskedtensor/core.py,torch/nn/modules/rnn.py,torch/nn/parallel/distributed.py,torch/nn/utils/_named_member_accessor.py,torch/onnx/_internal/diagnostics/infra/engine.py,torch/onnx/verification.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/composite_compliance.py,torch/testing/_internal/distributed/distributed_test.py,torch/testing/_internal/distributed/nn/api/remote_module_test.py,torch/testing/_internal/distributed/rpc/faulty_agent_rpc_test.py,torch/testing/_internal/distributed/rpc/rpc_test.py,torch/testing/_internal/opinfo/core.py,torch/utils/benchmark/utils/compare.py,torch/utils/checkpoint.py,torch/utils/cpp_extension.py,torch/utils/data/datapipes/_typing.py,torchgen/api/python.py,torchgen/gen.py,torchgen/gen_executorch.py,torchgen/native_function_generation.py",113.0,87,6,5.138568434,52.0,179127.0,70.0,4411397.230088496,12391.0,29176.0,0.0,Corrective,1.0,1
pytorch,61bd5a0643b19700553745c89a532df1684f6cff,67f94557ff26428ac911d8c08c7f9b619a41950e,Adam Paszke,adam.paszke@gmail.com,Sun Feb 26 12:49:08 2017 -0800,1488113348.0,Expose torch.HalfTensor,197.0,21.0,"test/test_torch.py,tools/cwrap/plugins/THPPlugin.py,torch/__init__.py,torch/csrc/Module.cpp,torch/csrc/Storage.cpp,torch/csrc/Storage.h,torch/csrc/Tensor.cpp,torch/csrc/Tensor.h,torch/csrc/byte_order.cpp,torch/csrc/byte_order.h,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/Tensor.h,torch/csrc/generic/methods/Tensor.cwrap,torch/csrc/generic/methods/TensorApply.cwrap,torch/csrc/generic/methods/TensorRandom.cwrap,torch/csrc/generic/utils.cpp,torch/csrc/generic/utils.h,torch/csrc/serialization.cpp,torch/csrc/serialization.h,torch/csrc/utils.cpp,torch/csrc/utils.h",23.0,8,3,4.046253607,25.0,8688.0,5.0,569235.7826086957,445.0,3933.075745,0.0,,0.0,1
pytorch,13ef8432b638bdc9f01f7df613495e1acc641ced,67ff50c30d3f08abec22193a7a52504eefbcd98c,gchanan,gregchanan@gmail.com,Tue Feb 06 16:11:18 2018 -0500,1517933478.0,"Run test_nn criterion tests over Variables, add a scalar test (#5058)

* test_nn working.

* Fix some incorrect scalar assumptions.

* Don't use Variables when we don't have to.

* Use Variable Mixin.

* Fix NLLLoss reference function when WITH_SCALARS not enabled.

* Allow device to be optional in cuda().

* Fix multilabelmarginloss_reference.",101.0,62.0,"test/common_nn.py,test/test_nn.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/python_variable_methods.cpp,torch/nn/modules/loss.py",5.0,7,3,1.320819745,37.0,9257.0,4.0,126682.6,502.0,1500.405869,0.0,Corrective,1.0,1
pytorch,e2f33eb6a2216aec79a76bc1aea57f3371f0d0d7,68116d7f8447ae1650c762c11a40aa81ef507f86,avmgithub,mendoza1@us.ibm.com,Mon Nov 06 23:51:02 2017 -0600,1510012262.0,Fix test_torch.py test for Power see issue #3277 (#3517),1.0,1.0,test/test_torch.py,1.0,1,1,0,38.0,4780.0,1.0,10233.0,2078.0,24011.35823,0.0,Corrective,1.0,1
pytorch,07513cfd1d284a058810efca9107f0697bba55ab,681baa92544a651cb40273a18dc0168a35a8b586,gchanan,gregchanan@gmail.com,Thu May 03 01:53:00 2018 -0400,1525312380.0,"Restore warning to torch.range. (#7194)

Also, get rid of warning specification in Declarations.cwrap, which currently has no effect.",50.0,5.0,"aten/src/ATen/Declarations.cwrap,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp",4.0,7,3,1.096015851,39.0,12078.0,4.0,65025.5,1052.0,2723.305292,0.0,,0.0,1
pytorch,be7c618fd7fadb9db58aae8bef16638912748cfa,68251fb93196a4c28a0c9f9ce37896b21c6c04e0,Francisco Massa,fvsmassa@gmail.com,Wed Nov 28 14:11:08 2018 -0800,1543414268.0,"Fix half tensor printing plus speedup large tensor printing (#14418)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/14344 and https://github.com/pytorch/pytorch/issues/6863

The slowdown was due to the fact that we were only summarizing the tensor (for computing the number of digits to print) if its first dimension was larger than the threshold. It now goes over all the dimensions.

Some quick runtime analysis:

Before this PR:
```python
In [1]: import torch; a = torch.rand(1, 1700, 34, 50)

In [2]: %timeit str(a)
13.6 s Â± 84.5 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

After this PR

```python
In [1]: import torch; a = torch.rand(1, 1700, 34, 50)

In [2]: %timeit str(a)
2.08 ms Â± 395 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)

In [3]: b = a.cuda()

In [4]: %timeit str(b)
8.39 ms Â± 45.9 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14418

Reviewed By: weiyangfb

Differential Revision: D13226950

Pulled By: soumith

fbshipit-source-id: 19eb4b855db4c8f891d0925a9c56ae8a2824bb23",17.0,4.0,"test/test_torch.py,torch/_tensor_str.py",2.0,2,2,0.998363673,40.0,9790.0,2.0,1509432.5,5659.0,17192.33333,0.0,Corrective,1.0,1
pytorch,90261945b71d2ac2a24bd59cbaf823a84ef3b8d2,686555b663077b40f28bc88adc049e64035046b4,George Qi,georgeqi94@gmail.com,Mon Sep 26 21:03:05 2022 +0000,1664226185.0,"[maskedtensor] port torch/_masked into torch/masked (#85515)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85515
Approved by: https://github.com/cpuhrsch",3078.0,3042.0,"functorch/op_analysis/public_api,functorch/test/test_ops.py,test/test_masked.py,test/test_maskedtensor.py,test/test_mps.py,test/test_proxy_tensor.py,test/test_reductions.py,test/test_sparse.py,test/test_sparse_csr.py,tools/update_masked_docs.py,torch/__init__.py,torch/_masked/__init__.py,torch/_masked/_docs.py,torch/masked/__init__.py,torch/masked/_docs.py,torch/masked/_ops.py,torch/masked/maskedtensor/core.py,torch/masked/maskedtensor/reductions.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/core.py,torch/testing/_internal/opinfo/definitions/_masked.py",21.0,13,4,2.301766669,46.0,48674.0,15.0,2541679.3157894737,7687.0,18065.5,0.0,,0.0,1
pytorch,b8caa2b08f2b692ff11ea398a8356d0faaf2e68b,686d7e4c484fb677c9b1b4c6c09771b40446ed01,Yanbo Liang,ybliang8@gmail.com,Wed Jun 07 17:10:54 2023 +0000,1686157854.0,"[Inductor] Fix x.view(dtype) decomp and make inductor support it  (#102920)

Fixes #99804

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102920
Approved by: https://github.com/jansel, https://github.com/ngimel",56.0,1.0,"test/expect/HasDecompTest.test_has_decomposition.expect,test/inductor/test_cpu_repro.py,test/inductor/test_torchinductor.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/triton.py,torch/_inductor/lowering.py,torch/_meta_registrations.py,torch/_refs/__init__.py",8.0,7,2,2.291455981,4.0,28843.0,5.0,97497.75,16593.0,37428.5,0.0,Corrective,1.0,1
pytorch,e594c30bc20d859c909d75914e89ee2bd689e727,686e281bcf06cac5a8e35fafc0595a0b9ad587b6,Mike Ruberry,mruberry@devfair044.maas,Mon Sep 14 22:43:21 2020 -0700,1600123401.0,"Updates div to perform true division (#42907)

Summary:
This PR:

- updates div to perform true division
- makes torch.true_divide an alias of torch.div

This follows on work in previous PyTorch releases that first deprecated div performing ""integer"" or ""floor"" division, then prevented it by throwing a runtime error.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42907

Reviewed By: ngimel

Differential Revision: D23622114

Pulled By: mruberry

fbshipit-source-id: 414c7e3c1a662a6c3c731ad99cc942507d843927",236.0,394.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/PointwiseOps.cpp,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseTensorMath.cpp,test/jit/test_save_load.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_foreach.py,test/test_jit.py,test/test_op_aliases.py,test/test_sparse.py,test/test_torch.py,test/test_type_promotion.py,tools/autograd/derivatives.yaml,torch/_torch_docs.py,torch/csrc/jit/passes/normalize_ops.cpp,torch/testing/_internal/common_methods_invocations.py",20.0,17,4,3.516692708,46.0,73475.0,12.0,851502.6,5114.0,11687.5,0.0,Preventative,0.0,1
pytorch,efb6feb242d04179ec56b3f8fc2698c0abd68d41,689ef9cba3c3d14000c5795437ffa769bc84517e,Sam Gross,colesbury@gmail.com,Wed Dec 20 20:12:07 2017 -0500,1513800727.0,Move upsampling to ATen (#4264),159.0,521.0,"aten/src/ATen/nn.yaml,aten/src/ATen/nn_parse.py,aten/src/THNN/generic/THNN.h,test/test_nn.py,tools/autograd/derivatives.yaml,torch/nn/_functions/thnn/__init__.py,torch/nn/_functions/thnn/upsampling.py,torch/nn/functional.py,torch/onnx/symbolic.py",9.0,13,4,1.694794448,37.0,11149.0,5.0,289364.22222222225,386.0,1215.905869,0.0,,0.0,1
pytorch,d28a882319d92bae17827101787adf838a05df0a,68a6113248ac25841b524d59f9dc0f298b389ba2,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Mon Oct 03 15:03:08 2022 +0000,1664809388.0,"Add nvFuser support for torch.native_batch_norm (#85562)

This PR adds nvFuser's implementation for batch_norm as there's no reference yet (https://github.com/pytorch/pytorch/pull/81191) and no in-place copy support (https://github.com/pytorch/pytorch/pull/84545).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85562
Approved by: https://github.com/kevinstephano, https://github.com/ngimel",371.0,4.0,"functorch/test/test_ops.py,functorch/test/test_vmap.py,test/test_prims.py,torch/_prims/context.py,torch/_prims/nvfuser_executor.py,torch/_prims/nvfuser_prims.py,torch/csrc/jit/codegen/cuda/ops/normalization.cpp,torch/csrc/jit/codegen/cuda/python_frontend/fusion_interface.cpp,torch/csrc/jit/codegen/cuda/python_frontend/fusion_record.h,torch/csrc/jit/codegen/cuda/python_frontend/python_bindings.cpp,torch/testing/_internal/common_methods_invocations.py",11.0,13,3,2.852037515,8.0,30118.0,8.0,748140.3636363636,7964.0,18870.0,0.0,Feature Addition,0.0,1
pytorch,2f40c8850875a61da71c5f2d6b19255e9dae4fab,68aed0779d2b4957fa6a1b542e1226198a2bce68,li-roy,8813817+li-roy@users.noreply.github.com,Thu Feb 15 20:29:44 2018 -0800,1518726584.0,"add reduce=True arg to MultiLabelSoftMarginLoss (#5097)

* add reduce=True arg to MultiLabelSoftMarginLoss

* Move some tests to new_criterion_tests

* fix flake8

* fix multilabelsoftmarginloss weights test",72.0,14.0,"test/common_nn.py,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/loss.py",4.0,4,2,1.576666762,37.0,10101.0,3.0,161815.0,2375.0,24621.85823,0.0,Corrective,1.0,1
pytorch,cbabd8f9f8e850051fb7a059ad7f8c8d743b7f70,68b18666a92d174d23f960a4e8584ced83524775,Richard Zou,zou3519@gmail.com,Fri Apr 08 14:17:58 2022 +0000,1649427478.0,"forward-mode AD formula for F.dropout

Here's what native_dropout does:
- it randomly drops out things in the input with probability p by
multiplying the input with a random mask
- it scales the output with `(p == 1 ? 0.0 : 1.0 / (1.0 - p))`

Further, native_dropout returns two things: the output and the mask
used.

Derivation of formula:
- dropout(x, mask) = mask * x * (p == 1 ? 0.0 : 1.0 / (1.0 - p))
- therefore the formula for `x` is: x_tangent * mask * (p == 1 ? 0.0 : 1.0 / (1.0 - p))

Test Plan:
- OpInfo

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75288
Approved by: https://github.com/soulitzer",2.0,7.0,"tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",2.0,5,2,0.503258335,16.0,19118.0,1.0,33417.0,2105.0,5074.0,0.0,Corrective,1.0,1
pytorch,cd0bab8d8d75cb6ed866be9ae5fa93902f8c724b,68b9daa9bf1694bc481c48b5bffdc5100e0c278c,Kurt Mohler,kmohler@quansight.com,Sat Aug 29 01:26:41 2020 -0700,1598664401.0,"Add `torch.linalg.norm` (#42749)

Summary:
Adds `torch.linalg.norm` function that matches the behavior of `numpy.linalg.norm`.

Additional changes:
* Add support for dimension wrapping in `frobenius_norm` and `nuclear_norm`
* Fix `out` argument behavior for `nuclear_norm`
* Fix issue where `frobenius_norm` allowed duplicates in `dim` argument
* Add `_norm_matrix`

Closes https://github.com/pytorch/pytorch/issues/24802

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42749

Reviewed By: ngimel

Differential Revision: D23336234

Pulled By: mruberry

fbshipit-source-id: f0aba3089a3a0bf856aa9c4215e673ff34228fac",928.0,10.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,test/test_torch.py,tools/autograd/templates/python_linalg_functions.cpp,torch/csrc/api/include/torch/linalg.h,torch/linalg/__init__.py",9.0,17,5,1.864123428,43.0,29613.0,5.0,873384.2222222222,4632.0,10770.0,0.0,Corrective,1.0,1
pytorch,5f4e8c0a4de7fa2a537e2c7fb0dfd25b5b346ca6,68bd687297501b89b672c5d41f71d35c0139c9ac,Richard Zou,zou3519@gmail.com,Sun Jul 24 23:14:13 2022 -0700,1658704453.0,"Add more functorch shards to PR CI (#82013)

This includes a configuration for linux CUDA, which will give us enough
test coverage for functorch to confidently begin accepting PRs to it again.

NB: Previously it turns out that some tests were not being skipped, even
though we added a skip decorator.

Test Plan:
- wait for CI
- check that the tests being skipped with a skip decorator are actually
skipped via reading test logs
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82013
Approved by: https://github.com/janeyx99",8.0,4.0,".github/workflows/pull.yml,functorch/test/test_ops.py",2.0,4,2,0.650022422,2.0,1659.0,2.0,102401.5,5701.0,13288.0,0.0,Feature Addition,0.0,1
pytorch,3c47d41f86a39db8c7d8cadb621bc5da36762af9,68cfc52452802f918003fcc17d72c42a8d869c11,Aapo Kyrola,akyrola@fb.com,Wed Dec 07 19:49:24 2016 -0800,1481140164.0,"MomemtumSGDUpdate -- version of MomentumSGD with update.

Summary:
It gives a significant perf boost to do the parameter update inside MomentumSGD, instead of with a separate WeightedSum op.
To ensure backwards compatibility, I made it a separate op.

Also added an unit test.

Reviewed By: prigoyal

Differential Revision: D4262446

fbshipit-source-id: 38e7ee6d7677b398658ac7fe9b7a59b569e033f4",166.0,25.0,"caffe2/python/examples/resnet50_trainer.py,caffe2/python/operator_test/momentum_sgd_test.py,caffe2/sgd/momentum_sgd_op.cc,caffe2/sgd/momentum_sgd_op.h,caffe2/sgd/momentum_sgd_op_gpu.cu",5.0,5,1,2.183506392,4.0,475.0,2.0,9398032.25,91.0,527.4,0.0,Feature Addition,0.0,1
pytorch,461aafe389b7b5b415c3b4878abaa0ba1769f132,68d438c9dade66073b3f9657bc077623c22001b9,Joel Schlosser,jbschlosser@fb.com,Wed Dec 23 04:12:40 2020 -0800,1608696760.0,"Add PixelUnshuffle (#49334)

Summary:
Adds an implementation of `torch.nn.PixelUnshuffle` as the inverse operation of `torch.nn.PixelShuffle`. This addresses https://github.com/pytorch/pytorch/issues/2456

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49334

Test Plan:
```
# Unit tests.
python test/test_nn.py TestNN.test_pixel_shuffle_unshuffle

# Module test.
python test/test_nn.py TestNN.test_PixelUnshuffle

# C++ API tests.
build/bin/test_api

# C++ / python parity tests.
python test/test_cpp_api_parity.py

# JIT test.
python test/test_jit.py TestJitGeneratedFunctional.test_nn_pixel_unshuffle

# Override tests.
python test/test_overrides.py

# Type hint tests.
python test/test_type_hints.py
```

Screenshots of rendered docs:
<img width=""876"" alt=""Screen Shot 2020-12-18 at 12 19 05 PM"" src=""https://user-images.githubusercontent.com/75754324/102642255-6b07bb00-412b-11eb-88fa-e53e7e8ba720.png"">
<img width=""984"" alt=""Screen Shot 2020-12-18 at 12 19 26 PM"" src=""https://user-images.githubusercontent.com/75754324/102642276-70fd9c00-412b-11eb-8548-445082a2db02.png"">
<img width=""932"" alt=""Screen Shot 2020-12-18 at 12 19 34 PM"" src=""https://user-images.githubusercontent.com/75754324/102642704-19abfb80-412c-11eb-9546-95bdd1c3cf22.png"">
<img width=""876"" alt=""Screen Shot 2020-12-22 at 12 51 36 PM"" src=""https://user-images.githubusercontent.com/75754324/102918259-986aa680-4454-11eb-99e7-a0b4c8b3e283.png"">
<img width=""869"" alt=""Screen Shot 2020-12-22 at 12 51 44 PM"" src=""https://user-images.githubusercontent.com/75754324/102918274-9ef91e00-4454-11eb-94bb-91b58aff47d3.png"">

Reviewed By: mruberry

Differential Revision: D25401439

Pulled By: jbschlosser

fbshipit-source-id: 209d92ce7295e51699e83616d0c62170a7ce75c8",371.0,56.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/PixelShuffle.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/nn.functional.rst,docs/source/nn.rst,test/cpp/api/functional.cpp,test/cpp/api/modules.cpp,test/cpp_api_parity/parity-tracker.md,test/test_nn.py,tools/pyi/gen_pyi.py,torch/csrc/api/include/torch/nn/functional/pixelshuffle.h,torch/csrc/api/include/torch/nn/modules/pixelshuffle.h,torch/csrc/api/include/torch/nn/options/pixelshuffle.h,torch/csrc/api/src/nn/modules/pixelshuffle.cpp,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pixelshuffle.py,torch/overrides.py,torch/testing/_internal/common_nn.py,torch/testing/_internal/jit_metaprogramming_utils.py",20.0,29,5,3.290139752,46.0,46923.0,16.0,7344763.15,7736.0,17448.5,0.0,Feature Addition,0.0,1
pytorch,d60d81b5a79885514b32b7727c96df68e642ff82,68d690ffbd64d0fb697dc3da1635216366649787,leslie-fang-intel,leslie.fang@intel.com,Mon Jun 14 14:52:52 2021 -0700,1623682372.0,"Vectorize the softmax calculation when not along the last dim (#59195)

Summary:
Currently, if we do softmax which are not along the last dim, the calculation will fall to a [scalar version](https://github.com/pytorch/pytorch/blob/d417a094f398f1c4efd7f818b14b8471a597fbcc/aten/src/ATen/native/SoftMax.cpp#L14-L64).  And we find actually we have the chance to vectorize the calculation along the inner_size dim.

Changes we made:

- Use vectorized softmax_kernel instead of host_softmax when not along the last dim.

Performance data on 28 cores' Intel 8280 CPU when the Input size is [32, 81, 15130] and do softmax along the second dim(81).

- FP32 Baseline: 24.67 ms
- FP32 optimized: 9.2 ms

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59195

Reviewed By: ailzhang

Differential Revision: D28854796

Pulled By: cpuhrsch

fbshipit-source-id: 18477acc3963754c59009b1794f080496ae16c3d",123.0,4.0,"aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/cpu/SoftMaxKernel.cpp,aten/src/ATen/native/cpu/SoftmaxKernel.h",3.0,5,1,0.498285609,7.0,625.0,3.0,32519743.0,12986.0,29418.5,0.0,Perfective,0.0,1
pytorch,f6c275f55ddfd22f8c0558efad3068b0e91f1560,690bc1c54d180f0989297f6cdf1de005c2f82d49,Thiago Crepaldi,thiago.crepaldi@microsoft.com,Wed Apr 20 17:53:34 2022 +0000,1650477214.0,"[ONNX] Raise exception for unimplemented ops for non-caffe2 builds

Currently, when an operator symbolic hits an unimplemented scenario, the symbolic may print a warning and return, allowing a non-ONNX operator be emitted into the graph

This PRs maintains this behavior for 1) Caffe2 builds or 2) non-caffe2 builds with `operator_export_type != ONNX`. If none of the conditions above are met, the converter raises a `RuntimeError` exception otherwise. This is needed so that exporter can detect detect unsupported ONNX operators when ATEN fallback is used (for non-caffe2 scenarios)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75468
Approved by: https://github.com/BowenBao",6.0,5.0,"torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",2.0,2,1,0.994030211,4.0,4994.0,2.0,296114.5,2473.0,5890.5,0.0,,0.0,1
pytorch,74a23c5aba86b5da1edebf8abf2227e329353d01,69287250d1953a898d8b3a38423ef1b30bab7e68,Gregory Chanan,gchanan@fb.com,Wed May 31 20:00:50 2017 -0700,1496260850.0,"Add a broadcast parameter to copy_, use it in the library in cases where there is non-broadcasting calls exposed by the tests.",79.0,33.0,"test/test_cuda.py,test/test_torch.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/autograd/gradcheck.py,torch/backends/cudnn/rnn.py,torch/csrc/copy_utils.h,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/methods/Tensor.cwrap,torch/csrc/utils.cpp,torch/cuda/comm.py,torch/legacy/nn/CDivTable.py,torch/legacy/nn/PairwiseDistance.py,torch/legacy/nn/SpatialSubtractiveNormalization.py",15.0,11,2,2.531306957,32.0,10733.0,8.0,3.2,901.0,11477.94394,0.0,Feature Addition,0.0,1
pytorch,e6101f55079b3d6075f94e91543ca397e035540f,6954ae1278a31e77ffbc1bd8deb078afeb4ae5a0,Abdelrauf,quickwritereader@gmail.com,Wed Sep 16 18:40:20 2020 -0700,1600281620.0,"Vec256 Test cases (#42685)

Summary:
[Tests for Vec256 classes https://github.com/pytorch/pytorch/issues/15676](https://github.com/pytorch/pytorch/issues/15676)

Testing
Current list:

- [x] Blends
- [x] Memory: UnAlignedLoadStore
- [x] Arithmetics: Plus,Minu,Multiplication,Division
- [x] Bitwise: BitAnd, BitOr, BitXor
- [x] Comparison: Equal, NotEqual, Greater, Less, GreaterEqual, LessEqual
- [x] MinMax: Minimum, Maximum, ClampMin, ClampMax, Clamp
- [x] SignManipulation: Absolute, Negate
- [x] Interleave: Interleave, DeInterleave
- [x] Rounding: Round, Ceil, Floor, Trunc
- [x] Mask: ZeroMask
- [x] SqrtAndReciprocal: Sqrt, RSqrt, Reciprocal
- [x] Trigonometric: Sin, Cos, Tan
- [x] Hyperbolic: Tanh, Sinh, Cosh
- [x] InverseTrigonometric: Asin, ACos, ATan, ATan2
- [x] Logarithm: Log, Log2, Log10, Log1p
- [x] Exponents: Exp, Expm1
- [x] ErrorFunctions: Erf, Erfc, Erfinv
- [x] Pow: Pow
- [x] LGamma: LGamma
- [x] Quantization: quantize, dequantize, requantize_from_int
- [x] Quantization: widening_subtract, relu, relu6
Missing:
- [ ] Constructors, initializations
- [ ] Conversion , Cast
- [ ] Additional: imag, conj, angle (note: imag and conj only checked for float complex)

#### Notes on tests and testing framework
- some math functions are tested within domain range
- mostly testing framework randomly tests against std implementation within the domain or within the implementation domain for some math functions.
- some functions are tested against the local version. ~~For example, std::round and vector version of round differs. so it was tested against the local version~~
- round was tested against pytorch at::native::round_impl. ~~for double type on **Vsx  vec_round failed  for  (even)+0 .5 values**~~ . it was solved by using vec_rint
- ~~**complex types are not tested**~~  **After enabling complex testing due to precision and domain some of the complex functions failed for vsx and x86 avx as well. I will either test it against local implementation or check within the accepted domain**
- ~~quantizations are not tested~~  Added tests for quantizing, dequantize, requantize_from_int, relu, relu6, widening_subtract functions
- the testing framework should be improved further
- ~~For now  `-DBUILD_MOBILE_TEST=ON `will be used for Vec256Test too~~
Vec256 Test cases will be built for each CPU_CAPABILITY

Fixes: https://github.com/pytorch/pytorch/issues/15676

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42685

Reviewed By: malfet

Differential Revision: D23034406

Pulled By: glaringlee

fbshipit-source-id: d1bf03acdfa271c88744c5d0235eeb8b77288ef8",2513.0,1.0,"aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/vec256_test_all_types.cpp,aten/src/ATen/test/vec256_test_all_types.h,caffe2/CMakeLists.txt,cmake/Codegen.cmake",8.0,8,3,1.124609644,21.0,2707.0,5.0,1643621.3333333333,5164.0,11780.5,0.0,Corrective,1.0,1
pytorch,75563674c4c9133f190726d5e06a2cd59904cb64,695d40efc28bde1f01e29c9041f99a595072bd67,Pieter Noordhuis,pcnoordhuis@gmail.com,Fri Jun 08 19:59:51 2018 -0700,1528487991.0,"Create initial Python bindings for c10d (#8119)

* Build and install c10d from tools/build_pytorch_libs.sh

* Create initial Python bindings for c10d

* clang-format

* Switch link order to include more symbols

* Add bindings and tests for ProcessGroupGloo

* Add broadcast test

* Separate build flag for c10d

* Explicit PIC property

* Skip c10d tests if not available

* Remove c10d from Windows blacklist

Let it skip by itself because it won't be available anyway.

* Make lint happy

* Comments

* Move c10d module into torch.distributed

* Close tempfile such that it is deleted",384.0,10.0,"setup.py,test/run_test.py,test/test_c10d.py,tools/build_pytorch_libs.sh,tools/setup_helpers/dist_check.py,torch/csrc/Module.cpp,torch/csrc/distributed/c10d/c10d.h,torch/csrc/distributed/c10d/init.cpp,torch/distributed/c10d/__init__.py,torch/lib/c10d/CMakeLists.txt",10.0,11,3,2.116678894,43.0,2531.0,6.0,653805.5,1290.0,3641.805292,0.0,Feature Addition,0.0,1
pytorch,6402a4278b25e0307a09ab7fda9df496bf268adb,695fd981924bd805704ecb5ccd67de17c56d7308,gchanan,gregchanan@gmail.com,Wed Jun 20 15:00:25 2018 -0400,1529506825.0,"Compatibility: write nDimension/_nDimension corresponding to dim()/_dim(). (#8676)

Currently, THTensor_(nDimension) goes to _dim(), which makes it difficult to move individual usages over to the new API.
Instead, let's create a THTensor_(_nDimension) going to _dim() and THTensor_(nDimension) going to _dim().  To do this, we will redirect all current
calls and move them over as we did for _dim() and dim().",362.0,338.0,"aten/src/TH/THTensor.hpp,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensor.h,aten/src/TH/generic/THTensorCopy.cpp,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorRandom.cpp,aten/src/THC/THCApply.cuh,aten/src/THC/THCDeviceTensorUtils-inl.cuh,aten/src/THC/THCDeviceTensorUtils.cuh,aten/src/THC/THCReduce.cuh,aten/src/THC/THCReduceAll.cuh,aten/src/THC/THCReduceApplyUtils.cu,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensor.hpp,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorMathReduce.cuh,aten/src/THC/THCTensorSort.cu,aten/src/THC/THCTensorTypeUtils.cuh,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensor.h,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMathBlas.cu,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorMathScan.cu,aten/src/THC/generic/THCTensorMode.cu,aten/src/THC/generic/THCTensorRandom.cu,aten/src/THC/generic/THCTensorScatterGather.cu,aten/src/THC/generic/THCTensorSort.cu,aten/src/THC/generic/THCTensorTopK.cu,aten/src/THCS/generic/THCSTensor.cpp,aten/src/THCS/generic/THCSTensor.h,aten/src/THCS/generic/THCSTensorMath.cu,aten/src/THCUNN/common.h,aten/src/THCUNN/generic/BatchNormalization.cu,aten/src/THCUNN/generic/ClassNLLCriterion.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/FeatureLPPooling.cu,aten/src/THCUNN/generic/FusedRNNKernel.cu,aten/src/THCUNN/generic/Im2Col.cu,aten/src/THCUNN/generic/IndexLinear.cu,aten/src/THCUNN/generic/LookupTable.cu,aten/src/THCUNN/generic/LookupTableBag.cu,aten/src/THCUNN/generic/PReLU.cu,aten/src/THCUNN/generic/SparseLinear.cu,aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu,aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu,aten/src/THCUNN/generic/SpatialFractionalMaxPooling.cu,aten/src/THCUNN/generic/SpatialGridSamplerBilinear.cu,aten/src/THCUNN/generic/SpatialReflectionPadding.cu,aten/src/THCUNN/generic/SpatialReplicationPadding.cu,aten/src/THCUNN/generic/TemporalReflectionPadding.cu,aten/src/THCUNN/generic/TemporalReplicationPadding.cu,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu,aten/src/THCUNN/generic/VolumetricGridSamplerBilinear.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricReplicationPadding.cu,aten/src/THNN/generic/ClassNLLCriterion.c,aten/src/THNN/generic/Col2Im.c,aten/src/THNN/generic/FeatureLPPooling.c,aten/src/THNN/generic/Im2Col.c,aten/src/THNN/generic/IndexLinear.c,aten/src/THNN/generic/Linear.c,aten/src/THNN/generic/LookupTable.c,aten/src/THNN/generic/PReLU.c,aten/src/THNN/generic/SpatialClassNLLCriterion.c,aten/src/THNN/generic/SpatialFractionalMaxPooling.c,aten/src/THNN/generic/VolumetricFractionalMaxPooling.c,aten/src/THNN/init.cpp,aten/src/THS/generic/THSTensor.cpp,aten/src/THS/generic/THSTensor.h",75.0,14,1,5.347916023,6.0,27790.0,17.0,1972929.5066666664,1375.0,4050.805292,0.0,,0.0,1
pytorch,e4f74f0891da9e49c5c82df05794f7723b05cbac,69728d7dd9b6a05a503e25759ed589754741ff01,Richard Zou,zou3519@gmail.com,Thu Aug 18 13:57:21 2022 -0700,1660831041.0,"[functorch] annotate test_jvpvjp (#83530)

Most of these are just ""forward-mode Ad formula not implemented""
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83530
Approved by: https://github.com/samdow",32.0,32.0,functorch/test/test_ops.py,1.0,2,1,0,2.0,1413.0,1.0,176794.0,6558.0,15188.0,0.0,,0.0,1
pytorch,397388ba9518722bf58130069b5210d49fc32f72,69a8cc19204089e5194839fe88ff6680562afcde,Samantha Andow,samdow@fb.com,Thu Feb 24 21:07:26 2022 -0500,1645736846.0,"[functorch] normal with one tensor (pytorch/functorch#511)

[ghstack-poisoned]",128.0,15.0,"functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/test/test_vmap.py",3.0,4,1,1.140079816,1.0,3848.0,2.0,0.0,827.0,1155.0,0.0,,0.0,1
pytorch,23fcc409d533cb8b5b6b2696be976e2eaf875eda,69e343f2cc26f7a090fa0a4c4f8cf5064d4df1c9,vishwakftw,vishwaks@cs.cmu.edu,Fri Nov 15 19:15:02 2019 -0800,1573845302.0,"Expose is_signed for dtype (#29511)

Summary:
Changelog:
- Expose is_signed for torch.dtype by modifying torch/csrc/Dtype.cpp
- Allow half, bfloat16 and bool to also been ""known"" by the isSignedType function
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29511

Test Plan:
- Add tests in test/test_torch.py

Closes https://github.com/pytorch/pytorch/issues/29475

Differential Revision: D18439030

Pulled By: albanD

fbshipit-source-id: 4b1f9da70c1c8dfd0a5bc028b6936acd1c64af47",23.0,5.0,"c10/core/ScalarType.h,test/test_torch.py,torch/csrc/Dtype.cpp",3.0,5,3,1.577406283,40.0,14951.0,3.0,569189.6666666666,13171.0,36196.83333,0.0,Feature Addition,0.0,1
pytorch,38e6b9c7e7f11afe455c2d368865bac5808d718e,69e38ee82138561c05882f64c5aeb203fe705804,albanD,desmaison.alban@gmail.com,Thu Jun 15 12:10:13 2017 +0100,1497528613.0,"clean test code, no functional change",32.0,38.0,"test/test_nn.py,torch/lib/THPP/tensors/generic/THCTensor.cpp",2.0,6,2,0.187176257,30.0,5140.0,2.0,0.0,1000.0,10963.3472,0.0,,0.0,1
pytorch,e2ccd6e7abe67707d8911610afa29bed44b066f3,6a0c636d4ecec8c4b5ad1842caffa98da9e16143,Sam Gross,colesbury@gmail.com,Thu Jan 04 00:22:50 2018 -0500,1515025370.0,"Don't special case NN functions in gen_variable_type.py (#4395)

This modifies NN binding in ATen so that the xxx_forward functions now
return buffers instead of taking them as inputs. The NN functions with
no suffix are implemented in Type.cpp. They call the xxx_forward
variants and discard any returned buffers.

This simplifies derivatives for NN functions. The derivatives are now
defined on the xxx_forward functions and buffers are treated as any
other input.",199.0,190.0,"aten/src/ATen/function_wrapper.py,aten/src/ATen/nn_parse.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py",5.0,5,2,1.946671169,10.0,3729.0,3.0,235121.2,415.0,1308.405869,0.0,Corrective,1.0,1
pytorch,3e18d3958be3dfcc36d3ef3c481f064f98ebeaf6,6a12f10b0832ea02d825597b52554502f55bedfe,vfdev-5,vfdev.5@gmail.com,Wed May 03 19:02:07 2023 +0000,1683140527.0,"Publicly exposing `torch.backends.cpu.get_cpu_capability()` (#100164)

Description:

- As suggested by Nikita, created `torch.backends.cpu` submodule and exposed `get_cpu_capability`.

- In torchvision Resize method we want to know current cpu capability in order to pick appropriate codepath depending on cpu capablities

Newly coded vectorized resize of uint8 images on AVX2 supported CPUs is now faster than older way (uint8->float->resize->uint8). However, on non-avx hardware (e.g. Mac M1) certain configs are slower using native uint8.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/100164
Approved by: https://github.com/albanD, https://github.com/malfet",55.0,17.0,"aten/src/ATen/Version.cpp,aten/src/ATen/Version.h,docs/source/backends.rst,test/test_torch.py,torch/_C/__init__.pyi.in,torch/__init__.py,torch/backends/cpu/__init__.py,torch/csrc/Module.cpp",8.0,11,4,2.187343588,48.0,14824.0,7.0,5002578.142857143,15375.0,34694.5,0.0,,0.0,1
pytorch,862f67454ff1dc99c57a0e5e10d5e72ed8378911,6a37e0df97777ff1fbd46a27dcd56371f3680ec0,Richard Zou,zou3519@gmail.com,Tue Apr 05 20:38:59 2022 +0000,1649191139.0,"Split SILU OpInfo

Motivation
==========

We would like to test autograd support (forward-mode AD and reverse-mode
AD) of SILU in functorch. Unfortunately the OpInfo for
nn.functional.silu has supports_autograd and supports_forward_ad as
False. This is due to nn.functional.silu not supporting complex
autograd.

Solution
========

This PR splits the OpInfo for nn.functional.silu into two. One OpInfo
tests non-complex dtypes and the other ones test complex dtypes.

Alternatives
============

- We could manually add tests in functorch
- We can add complex autograd support for SILU (I don't know how to do
this but if this is easy to do I'm happy to try)

Test Plan
=========

Run tests

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75205
Approved by: https://github.com/soulitzer",24.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,16367.0,1.0,1209.0,1989.0,4737.0,0.0,Corrective,1.0,1
pytorch,7254104cfca28946b5bea7b7c8a8cd4ffa35f04d,6a4ec4f9a8ddb815bae898797143867768f454de,SsnL,tongzhou.wang.1994@gmail.com,Sat Sep 23 07:55:52 2017 -0700,1506153352.0,VolumetricAdaptiveAveragePool,869.0,2.0,"docs/source/nn.rst,test/test_nn.py,torch/lib/THCUNN/VolumetricAdaptiveAveragePooling.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu,torch/lib/THNN/generic/THNN.h,torch/lib/THNN/generic/VolumetricAdaptiveAveragePooling.c,torch/lib/THNN/init.c,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py",13.0,13,3,2.419416993,38.0,12342.0,6.0,285490.6,1822.0,24888.05562,0.0,,0.0,1
pytorch,da023611d756d95914085a321a35b574903f4d2e,6a4fa860262951ca14db0c53fc7d60c3dbc48af4,Saketh Are,saketh@fb.com,Sat Dec 04 01:01:11 2021 -0800,1638579671.0,"Add OpInfos for misc nn.functional operators (#68922)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68922

Reviewed By: Chillee

Differential Revision: D32842301

Pulled By: saketh-are

fbshipit-source-id: b7166faefb64668fc76cca6c528501b0d360c43b",288.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.033368446,2.0,16164.0,1.0,70806.0,17520.0,41174.5,0.0,Feature Addition,0.0,1
pytorch,3b337e7892a7b54758f273477aa092053f02b410,6a6983ed7f14f8335a5b5614928713cb79658281,Thomas Viehmann,tv@beamnet.de,Tue Jan 29 19:19:51 2019 -0800,1548789591.0,"create type hint stub files for module torch (#12500)

Summary:
We have:

- This is an initial stab at creating a type stub `torch/__init__.pyi` .
- This is only tested on Python 3, since that's the only Python version mypy
  works on.
- So far, we only aim at doing this for torch functions and torch.Tensor.
- Quite a few methods and functions have to be typed manually. These are
  done in `torch/__init__.pyi.in`

For me, PyCharm (the non-paid one) didn't seem to indicate errors in the .pyi when opening and seemed to be able to get the type hint for the few functions I tried, but I don't use PyCharm for my usual PyTorch activities, so I didn't extensively try this out.

An example of a generated PYI is at [this gist](https://gist.github.com/ezyang/bf9b6a5fa8827c52152858169bcb61b1).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12500

Differential Revision: D13695553

Pulled By: ezyang

fbshipit-source-id: 4566c71913ede4e4c23ebc4a72c17151f94e8e21",910.0,21.0,".gitignore,.jenkins/pytorch/test.sh,setup.py,test/run_test.py,test/test_type_hints.py,tools/autograd/gen_python_functions.py,tools/autograd/utils.py,tools/pyi/__init__.py,tools/pyi/gen_pyi.py,torch/CMakeLists.txt,torch/__init__.py,torch/__init__.pyi.in,torch/_six.py,torch/_utils.py,torch/csrc/utils/python_arg_parser.cpp,torch/functional.py,torch/serialization.py,torch/tensor.py",18.0,9,4,2.017134419,46.0,6647.0,13.0,2382747.571428572,6692.0,20715.83333,0.0,,0.0,1
pytorch,3669e45736176fc119c66717474378e9031cac3c,6a75f650dd3bf6d3fdaafffe7298d141c0377489,Paul Shao,pshao@fb.com,Sat Jun 06 06:03:25 2020 -0700,1591423405.0,"Implement Quantized Version of Threshold Function (#39352)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39352

In this task, the quantized backend of the kernel is implemented for the threshold function, which clamps the entries in a tensor less than or equal to  a given threshold to be a specified value.

The corresponding Python implementation and unit test are also added.

Test Plan:
1. On a devserver, build PyTorch from source by running the command `buck build mode/dev //caffe2:torch`
2. Run the unit test throught the command
`buck test mode/dev //caffe2/test:quantization -- test_qthreshold`

Reviewed By: z-a-f

Differential Revision: D21822446

fbshipit-source-id: e8c869664e6d4c664f0e7fa3957762992118c082",169.0,0.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qthreshold.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,aten/src/ATen/native/quantized/library.cpp,test/quantization/test_quantized_op.py,torch/nn/quantized/functional.py",7.0,12,3,2.085157965,12.0,13232.0,6.0,1331227.5,2615.0,6448.5,0.0,Feature Addition,0.0,1
pytorch,bb9ef8fc2ece34c5aec2af796f55e574ceada5c3,6a85b133d3694e56e1f5bf50dfa413769e24ce06,li-roy,8813817+li-roy@users.noreply.github.com,Thu Jun 14 06:57:16 2018 -0700,1528959436.0,"Improve number formatting in tensor print (#7632)

* Improve number formatting in tensor print

* fix bad rebase

* address comments

* fix test

* fix test

* use assertExpected for tests

* address comments

* address comments",167.0,107.0,"test/expect/TestTorch.test_print-bigint.expect,test/expect/TestTorch.test_print-default_device.expect,test/expect/TestTorch.test_print-default_dtype.expect,test/expect/TestTorch.test_print-device.expect,test/expect/TestTorch.test_print-dtype.expect,test/expect/TestTorch.test_print-negint.expect,test/expect/TestTorch.test_print-nonfinite.expect,test/expect/TestTorch.test_print-posint.expect,test/expect/TestTorch.test_print-requires_grad.expect,test/expect/TestTorch.test_print-scimode.expect,test/expect/TestTorch.test_print-summary.expect,test/test_torch.py,torch/_tensor_str.py",13.0,3,2,1.195910747,39.0,7754.0,2.0,300609.5,1327.0,3767.805292,0.0,Corrective,1.0,1
pytorch,b34965435d3982502bbb1cedc0be0f6c9660c0f1,6a87e8d087b1b0ea84798de7775e4b672405a8e9,Weiqiang Wu,wqwu@fb.com,Tue Jun 22 19:37:15 2021 -0700,1624390635.0,"Implement erfcx() (#58194)

Summary:
Implement erfcx() https://github.com/pytorch/pytorch/issues/31945

Reference: https://github.com/pytorch/pytorch/issues/50345

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58194

Reviewed By: ngimel

Differential Revision: D29285979

Pulled By: mruberry

fbshipit-source-id: 5bcfe77fddfabbeb8c8068658ba6d9fec6430399",656.0,4.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_core.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",15.0,18,4,1.480877156,17.0,30398.0,9.0,135042.33333333334,13235.0,29962.0,0.0,,0.0,1
pytorch,021e1e20c10a07d43e3d7e62bc015270c5ae9e3d,6abfa9ad8a9f882b15932dbde23b619c8be85b41,Zafar Takhirov,cc.rafaz@zafar.cc,Fri Jan 10 00:13:19 2020 -0800,1578615199.0,"Quantized H Tangent function (#31031)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31031

This activation will be needed for the LSTM implementation.
Also includes the QNNPack implementation.

Test Plan: Imported from OSS

Differential Revision: D19334280

Pulled By: z-a-f

fbshipit-source-id: ae14399765a47afdf9b1e072d3967c24ff473e8d",215.0,8.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qnnpack/src/tanh.c,aten/src/ATen/native/quantized/cpu/qnnpack/test/tanh-operator-tester.h,aten/src/ATen/native/quantized/cpu/qtanh.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py",7.0,11,2,2.047663993,10.0,9940.0,2.0,28567.42857142857,14157.0,38418.33333,0.0,,0.0,1
pytorch,4e9fe7f1682d5610dd5f535c97d1dfbd0fdceeae,6ae0d477ea120e59815dbfcdcc1bedd7a1d00e68,Adam Paszke,adam.paszke@gmail.com,Wed Nov 29 15:16:34 2017 +0100,1511968594.0,"Fix cuBLAS arguments for fp16 dot (#3660)

* Fix cuBLAS arguments for fp16 dot

* Enable FloatTensor <-> CUDA HalfTensor checks in test_cuda.py",42.0,26.0,"aten/src/THC/THCBlas.cu,aten/src/THC/THCBlas.h,aten/src/THC/generic/THCTensorMathBlas.cu,test/test_cuda.py",4.0,5,2,1.810109263,37.0,2551.0,2.0,2170878.5,2161.0,24151.35823,0.0,Corrective,1.0,1
pytorch,821b5f138a987807032a2fd908fe10a5be5439d9,6b0ca8eae5d663ad3db560b428abcef465f09dbb,Shen Li,cs.shenli@gmail.com,Thu Apr 11 03:30:46 2019 -0700,1554953446.0,"Fix flaky store timeout test (#19114)

Summary:
~Sometimes, `init_process_group()`, `store.get()`, and `destory_process_group()` can take more than a few seconds. Hence, removing thread join timeout.~

The error was due to `Address already in use` when starting TPC backend. The solution is to catch the error and report it to the `retry_on_address_already_in_use_error` decorator.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19114

Reviewed By: ezyang

Differential Revision: D14872680

Pulled By: mrshenli

fbshipit-source-id: fc504d02853ca73f76288c0ade564ab20bc01f7e",23.0,12.0,test/test_c10d.py,1.0,1,1,0,2.0,1930.0,1.0,115148.0,8032.0,24225.83333,0.0,Corrective,1.0,1
pytorch,dfcd90783c37075631e873bed72013fe4917d66f,6b3a4637d6e87a543b3df6edb825c07d7dabe611,Sam Gross,colesbury@gmail.com,Tue Apr 03 20:29:25 2018 -0400,1522787365.0,"Make the tensor type torch.Tensor instead of torch.autograd.Variable (#5785)

This changes type(tensor) to return `torch.Tensor` instead of
`torch.autograd.Variable`.

This requires a few implementation changes:

 - torch.Tensor is now a regular Python class instead of a
   pseudo-factory like torch.FloatTensor/torch.DoubleTensor
 - torch.autograd.Variable is just a shell with a __new__ function.
   Since no instanes are constructed it doesn't have any methods.
 - Adds torch.get_default_dtype() since torch.Tensor.dtype returns
   <attribute 'dtype' of 'torch._C._TensorBase' objects>",646.0,578.0,"setup.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/__init__.py,torch/_tensor_docs.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/autograd/init.cpp,torch/csrc/autograd/python_legacy_variable.cpp,torch/csrc/autograd/python_legacy_variable.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/tensor/python_tensor.h,torch/functional.py,torch/nn/parameter.py,torch/tensor.py",16.0,10,3,2.631856861,42.0,12641.0,5.0,421599.0714285714,817.0,1924.305292,0.0,Feature Addition,0.0,1
pytorch,dcf5f8671c4a3239af453f0630f79d0ca73eea65,6b4ed52f10daa3781a726014a2cde56c4d2119d0,Soumith Chintala,soumith@fb.com,Mon Jan 02 08:15:54 2017 -0500,1483344954.0,"adding docs for some torch.* functions, removing all, any stateless methods",255.0,16.0,"docs/source/torch.rst,torch/csrc/Module.cpp,torch/docs.py",3.0,4,2,0.173625552,22.0,1814.0,2.0,77425.33333333333,291.0,6375.724559,0.0,Feature Addition,0.0,1
pytorch,d506951937270bfc0586900c1567ea749d8fe35c,6b59f1ad78ba62fb043b4b3da34afb8ce6ec3416,Richard Zou,zou3519@gmail.com,Wed Jul 21 17:47:30 2021 -0700,1626889650.0,"[functorch] Add `functorch_additional_op_db`

PyTorch OpInfo coverage doesn't include nn.functional.* ops. We can use
functorch_additional_op_db as a staging ground where we implement
nn.functional.* OpInfos and then later upstream them.

Why not just upstream them immediately? I can write OpInfos a few times
faster if I don't have to worry about figuring out what the correct
flags are to pass PyTorch tests...",220.0,7.0,"functorch/test/functorch_additional_op_db.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,2,1,0.46728234,1.0,3391.0,2.0,0.0,218.0,366.5,0.0,Corrective,0.0,1
pytorch,0f458ee3c498444c3519841b7300cb9e7e6f67f1,6b84dc26f03a58a38c3a9f7c9e5b374421da8adc,Adam Paszke,adam.paszke@gmail.com,Mon May 15 17:12:54 2017 +0200,1494868374.0,Add F.cosine_similarity (#1502),37.0,3.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/functional.py",3.0,5,3,1.522336807,28.0,4669.0,3.0,302625.6666666667,762.0,11400.06049,0.0,Feature Addition,0.0,1
pytorch,614edfce81ae81897ec8a5a199b86e61c1ebf7ff,6b9bcd06065eb69a9f7c3f139060c4684174cca5,Negin Raoof,neginmr@utexas.edu,Fri Sep 27 05:51:47 2019 -0700,1569563507.0,"export baddbmm (#26901)

Summary:
Adding symbolic for baddbmm export
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26901

Reviewed By: hl475

Differential Revision: D17620967

Pulled By: houseroad

fbshipit-source-id: 3931dff5a4afdcb4a45d967fb0efaf84029c16e5",196.0,0.0,"test/onnx/expect/TestOperators.test_baddbmm.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",5.0,5,2,1.14513479,4.0,6546.0,3.0,33277.0,11821.0,33120.83333,0.0,Feature Addition,0.0,1
pytorch,93e71cc2f536e330e8ce2c3e9222adf2b5c28da5,6bc62a6392916eaa039e6d44cb971869bb43055b,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Sun Jan 22 03:43:57 2023 +0000,1674359037.0,"Revert ""[inductor] run CPU and CUDA tests with dynamic shapes (#92667)""

This reverts commit 425e506ffe41fc9fd16a18175c992f9d01eef08b.

Reverted https://github.com/pytorch/pytorch/pull/92667 on behalf of https://github.com/kit1980 due to test_topk_dynamic_shapes_cpu failing after this PR",13.0,202.0,"test/dynamo/test_dynamic_shapes.py,test/inductor/test_torchinductor.py,torch/_dynamo/testing.py",3.0,5,2,0.264861091,1.0,7058.0,2.0,18193.666666666668,11548.0,26464.5,0.0,,0.0,1
pytorch,4ef854b4b4108fa03d9d5081d4bf25bdf0717252,6bdb59539fcfa9111e411e9dd7f958c1348ba6ac,albanD,desmaison.alban@gmail.com,Tue Feb 25 15:34:53 2020 -0800,1582644893.0,"follow-up test_torch .data removal (#33696)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33696

This changes two tests:
- The batchnorm inference cannot change the memory format of the weights as they are 1D. So this is removed.
- The batchnorm test now run both in affine and not affine mode.
- I added back the test for type errors using .data. In particular, `.data` allows to change the type of a Tensor inplace (very bad, never do it!) but since it is possible, we should test it until .data is removed.

cc Enealor who did the first version of the PR.

Test Plan: Imported from OSS

Differential Revision: D20069241

Pulled By: albanD

fbshipit-source-id: a0348f40c44df38d654fb2a2b2b526d9d42f598a",32.0,19.0,test/test_torch.py,1.0,1,1,0,40.0,15324.0,1.0,2812.0,14962.0,40092.83333,0.0,Feature Addition,0.0,1
pytorch,b0bdc82a0006439af1c7794f70c64699555d74c9,6bdb871d47f8810fb627d908c4fa2408fa6a632f,James Reed,jamesreed@fb.com,Tue Sep 29 05:50:49 2020 -0700,1601358649.0,"[FX] Lint pass for Graphs (#44973)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44973

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D23792631

Pulled By: jamesr66a

fbshipit-source-id: d8faef0c311d8bd611ba0a7e1e2f353e3e5a1068",110.0,4.0,"test/test_fx.py,torch/fx/graph.py,torch/fx/node.py",3.0,3,2,1.184916007,1.0,1029.0,2.0,358151.0,5546.0,13009.0,0.0,,0.0,1
pytorch,f1420adfe3797ff46719d4365ebd85ee703f4af1,6bdbad93b96ecd943fe5091725ccd70fad9a6057,Edward Yang,ezyang@fb.com,Sun Aug 19 00:25:26 2018 -0700,1534638326.0,"Refactor Device to not depend on Backend. (#10478)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10478

- Removed Backend constructor from Device, and fixed all
  use-sites to use DeviceType::CPU instead of kCPU, or
  use a new function backendToDeviceType to perform
  the conversion.
- New method device_type() on Type; it gives you the
  underlying device type, e.g., CPU for SparseCPU.
- We add backward compatibility for kCPU/kCUDA uses,
  by introducing a new special type which is implicitly
  convertible to both DeviceType and Backend.  As long as
  you don't define a function that's overloaded on both
  DeviceType and Backend (but not on BackendOrDeviceType),
  the implicit conversions will ensure that uses
  of at::Device(at::kCPU) keep working. We fixed use-sites in
  the library, but did NOT fix sites in the test code, so that
  we can exercise this BC code.

Reviewed By: Yangqing

Differential Revision: D9301861

fbshipit-source-id: 9a9d88620500715c7b37e655b4fd761f6dd72716",290.0,202.0,"aten/src/ATen/Allocator.h,aten/src/ATen/Backend.h,aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/Device.h,aten/src/ATen/Formatting.cpp,aten/src/ATen/TensorOptions.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LossCTC.cpp,aten/src/ATen/native/Memory.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Gesv.cu,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/templates/SparseTypeDerived.cpp,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/TensorMethods.h,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeDerived.cpp,aten/src/ATen/test/apply_utils_test.cpp,aten/src/ATen/test/atest.cpp,aten/src/ATen/test/basic.cpp,aten/src/ATen/test/broadcast_test.cpp,aten/src/ATen/test/cudnn_test.cpp,aten/src/ATen/test/dlconvertor_test.cpp,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/scalar_tensor_test.cpp,aten/src/ATen/test/scalar_test.cpp,aten/src/ATen/test/tbb_init_test.cpp,aten/src/ATen/test/test_parallel.cpp,aten/src/ATen/test/test_seed.h,aten/src/ATen/test/undefined_tensor_test.cpp,aten/src/ATen/test/wrapdim_test.cpp,aten/src/TH/THAllocator.cpp,aten/src/THC/THCAllocator.cpp,aten/src/THC/THCBlas.cu,aten/src/THC/THCCachingAllocator.cpp,aten/src/THC/THCCachingHostAllocator.cpp,aten/src/THC/THCGeneral.cpp,aten/src/THC/THCStorage.cpp,caffe2/contrib/aten/aten_op.cc,caffe2/contrib/aten/aten_op_cuda.cc,test/cpp/api/integration.cpp,test/cpp/api/serialization.cpp,test/cpp/api/tensor_options.cpp,test/cpp/api/tensor_options_cuda.cpp,test/cpp_extensions/cudnn_extension.cpp,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/DynamicTypes.cpp,torch/csrc/api/include/torch/serialization.h,torch/csrc/api/src/utils.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/cuda/comm.cpp,torch/csrc/cuda/nccl.cpp,torch/csrc/jit/export.cpp,torch/csrc/jit/fusion_compiler.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/python_arg_flatten.h,torch/csrc/tensor/python_tensor.cpp,torch/csrc/torch.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_apply.cpp,torch/csrc/utils/tensor_list.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_numpy.cpp,torch/csrc/utils/tensor_types.cpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp,torch/lib/c10d/test/ProcessGroupGlooTest.cpp,torch/lib/libshm/core.cpp",79.0,36,5,5.528567122,42.0,20977.0,49.0,2359084.5569620254,3547.0,9659.833333,0.0,Corrective,1.0,1
pytorch,d578e8cfa2db71e45c3565b42ff2b10d13643402,6c37788cb14fcde418eefc3b4389ae382e838e2d,Serhat Yilmaz,serhaty@fb.com,Tue Apr 27 18:27:36 2021 -0700,1619548056.0,"[torch] Add cuda support for segment reduction 'max' (#56704)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56704

This is re submit of PR: https://github.com/pytorch/pytorch/pull/54175

Main changes compared to original PR:
- Switch to importing ""<ATen/cuda/cub.cuh>""
- Use CUB_WRAPPER to reduce boiler plate code.

Test Plan:
Will check CI status to make sure a

Added unit test

Reviewed By: ngimel

Differential Revision: D27941257

fbshipit-source-id: 24a0e0c7f6c46126d2606fe42ed03dca15684415",168.0,51.0,"BUILD.bazel,aten/src/ATen/cuda/cub.cuh,aten/src/ATen/native/SegmentReduce.cpp,aten/src/ATen/native/SegmentReduce.h,aten/src/ATen/native/cuda/SegmentReduce.cu,aten/src/ATen/native/native_functions.yaml,test/test_segment_reductions.py,torch/utils/hipify/cuda_to_hip_mappings.py",8.0,10,3,1.945777362,12.0,19494.0,5.0,1236299.75,11338.0,24984.5,0.0,Feature Addition,0.0,1
pytorch,ba3b05fdf37ddbc3c301294d6a560a816335e717,6c79299a35dad02d2bec382ca204caf8e1e12e87,Aaron Orenstein,aorenste@fb.com,Fri May 24 22:12:03 2024 -0700,1716588723.0,"Improve score_fusion_memory() (#127060)

Related to #98467

The tacotron2 benchmark creates a lot of nodes which fusion then checks. This
improves some of the perf of that checking.

`score_fusion_memory` is called O(n^2) times - so by moving the set union, `has_unbacked_symbols` check, and `numbytes_hint` out of the loop we call them O(n) times and the O(n^2) call gets cheaper.

Testing:
```
time python benchmarks/dynamo/torchbench.py --accuracy --inference --amp --backend inductor --disable-cudagraphs --device cuda --only tacotron2
```

Before this change: 12m33s
After this change: 10m15s

Pull Request resolved: https://github.com/pytorch/pytorch/pull/127060
Approved by: https://github.com/peterbell10, https://github.com/jansel",30.0,8.0,torch/_inductor/scheduler.py,1.0,2,1,0,1.0,2876.0,1.0,63739.0,29125.0,70882.5,0.0,Perfective,0.0,1
pytorch,ea3c36b822a7940bad6f0f7bad67f04afa8f5aa2,6c7fb1582f6d60f2b83b3380b8cb2f3520670b09,Thomas Viehmann,tv@beamnet.de,Mon Jul 30 21:37:21 2018 -0700,1532986641.0,"Introduce __array_priority__ on torch.Tensor (#9651)

Summary:
This causes numpy to yield to the torch functions,
e.g. instead of numpy array/scalar __mul__ converting the tensor to
an array, it will now arrange for the Tensor __rmul__ to be called.

Fixes case 2 of #9468
I also makes case 3 and 4 equivalent but does not fix them.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9651

Differential Revision: D8948079

Pulled By: ezyang

fbshipit-source-id: bd42c04e96783da0bd340f37f4ac3559e9bbf8db",14.0,1.0,"test/test_distributions.py,test/test_torch.py,torch/tensor.py",3.0,2,2,1.103307409,40.0,12880.0,3.0,251916.66666666663,3190.0,8440.833333,0.0,Corrective,1.0,1
pytorch,adc21f1966fcb364fc81dedf38c58d80925b09b1,6c985b57ff9f4e463651a6f26c371365797fb085,kshitij12345,kshitijkalambarkar@gmail.com,Mon Oct 25 15:00:56 2021 -0700,1635174056.0,"OpInfo : nn.functional.embedding (#66997)

Summary:
Adds OpInfo for `nn.functional.embedding`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/66997

Reviewed By: mrshenli

Differential Revision: D31859799

Pulled By: zou3519

fbshipit-source-id: bbca860df4fbc243751f5fa81658231866c31d2e",95.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.084262429,2.0,13065.0,2.0,259978.0,16536.0,38781.5,0.0,Feature Addition,0.0,1
pytorch,1669fffb8d2614193240d3af092d429074b5dbac,6caa7e0fff8de2857ff6c481bbb4a01fb0533a1a,Gregory Chanan,gchanan@fb.com,Tue Oct 25 19:44:15 2016 -0700,1477424655.0,Add generic support for MultiLabelMarginCriterion.,182.0,157.0,"MultiLabelMarginCriterion.cu,MultiMarginCriterion.cu,THCUNN.h,generic/MultiLabelMarginCriterion.cu,generic/THCUNN.h",5.0,1,1,1.531098623,17.0,1485.0,2.0,0.25,65.0,999.4333333,0.0,Feature Addition,0.0,1
pytorch,171cf153d2ffb49350c9e2554f4287dc70347287,6cb128c8dd05245e152038301398a60c5c8210fd,lezcano,lezcano-93@hotmail.com,Thu Jan 27 23:07:54 2022 -0800,1643324874.0,"Generalize noncontiguous tests to several outputs (#67996)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67996

This is necessary for most matrix decompositions in `linalg`.

cc mruberry

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D33774418

Pulled By: mruberry

fbshipit-source-id: 576f2dda9d484808b4acf0621514c0ffe26834e6
(cherry picked from commit fb07c50aa9c143aa9dafab57936a8a8a7d3b4ec4)",57.0,40.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",3.0,4,2,1.498938896,3.0,20243.0,3.0,5495.666666666667,266.0,541.5,0.0,,0.0,1
pytorch,0fd176fea45abb092c903b40008c7119e21ee9f5,6d2b3cc869d2d39832800e8127cb66abf3589311,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Fri Nov 02 02:04:17 2018 -0700,1541124257.0,"Fix pytest, make it work with run_test.py (#13416)

Summary:
Fixes #13326

Also now you can use `run_test.py` with `pytest`. E.g.,
```
python run_test.py -vci distributed -pt
```

Yes it works with `distributed` and `cpp_extension`.

cc zou3519 vishwakftw
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13416

Differential Revision: D12895622

Pulled By: SsnL

fbshipit-source-id: 2d18106f3a118d642a666bfb1318f41c859c3df7",117.0,89.0,"test/common_utils.py,test/run_test.py,test/test_cuda.py,test/test_torch.py",4.0,1,1,1.478070046,41.0,12654.0,4.0,186481.25,5102.0,15180.83333,0.0,Corrective,1.0,1
pytorch,86b1f4e9f2ee61eafd646800daab79aa3d5250fa,6d2bf76bba9bf1f42c6289958410ee7876a323c9,Rong Rong (AI Infra),rongr@fb.com,Mon Mar 29 14:19:04 2021 -0700,1617027544.0,"Using latest windows CUDA exe (#54596)

Summary:
Using latest cuda_11.2.2_461.33_win10 to fix cu112 test failures.
this should fix https://github.com/pytorch/pytorch/issues/51980.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54596

Reviewed By: seemethere

Differential Revision: D27365008

Pulled By: walterddr

fbshipit-source-id: 682e79888d9f10c0a5b227d66165ea50c47ba0f9",125.0,1.0,".circleci/config.yml,.circleci/scripts/windows_cuda_install.sh,.circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml",3.0,4,1,1.10172165,3.0,9441.0,3.0,1501173.3333333333,10151.0,22441.5,0.0,Corrective,1.0,1
pytorch,b7038f7c37e955f7400459bbfc9382a77b16377d,6d6655e6be914e04a3ea4bdb7e7990f9ba9e3c2f,Adam Paszke,adam.paszke@gmail.com,Wed Sep 05 13:31:28 2018 -0700,1536154288.0,"Port PackedSequences functions to C++ (#11224)

Summary:
zdevito
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11224

Differential Revision: D9652703

Pulled By: apaszke

fbshipit-source-id: 558e39457e590cad07516e5bb2ecb12789564950",220.0,171.0,"aten/src/ATen/native/PackedSequence.cpp,aten/src/ATen/native/native_functions.yaml,test/test_jit.py,tools/autograd/derivatives.yaml,torch/nn/_functions/packing.py,torch/nn/utils/rnn.py,torch/onnx/symbolic.py",7.0,12,4,2.132587024,30.0,12473.0,6.0,2493438.833333333,3852.0,10748.83333,0.0,,0.0,1
pytorch,23b556ef771774d27dde7ce22f4cd6c4a8b61b0b,6d693fe413a564bd12ffece61261243b95d01b1e,Sergey Zagoruyko,zagoruyko2@gmail.com,Sun May 07 11:54:16 2017 +0400,1494158056.0,Add F.normalize (#1467),27.0,0.0,"test/test_nn.py,torch/nn/functional.py",2.0,3,2,0.691289869,28.0,3749.0,2.0,133111.0,602.0,5239.672317,0.0,Feature Addition,0.0,1
pytorch,38f87cc9c42838cac976ec52ea89b4c5e99acaf4,6dc67aef1765328169027afe1cba9b3906dc50b1,SsnL,SsnL@users.noreply.github.com,Sat Oct 14 08:44:35 2017 -0400,1507970675.0,doc (#3110),124.0,103.0,"torch/nn/functional.py,torch/nn/modules/conv.py",2.0,3,1,0.718389836,36.0,2000.0,1.0,13218.0,1988.0,23826.85823,0.0,Non Functional,0.0,1
pytorch,d2917f705ac42da6f4b3a70f3033ea0f3cd8feb1,6de9f0fc94623da617decba21edee8031657e1c9,Peter Bell,peterbell10@live.co.uk,Thu Dec 09 16:35:53 2021 -0800,1639067753.0,"OpInfo: Allow sample_inputs_func to be any iterable (#69256)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69256

Closes #52486

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D32942008

Pulled By: mruberry

fbshipit-source-id: f5b01b0298c0160b0bec6e86e2b6db8cfe746206",77.0,96.0,"test/jit/test_dtype_analysis.py,test/test_ops.py,test/test_sparse.py,test/test_sparse_csr.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",6.0,5,2,2.140934007,29.0,24747.0,6.0,212575.66666666663,17638.0,41510.5,0.0,,0.0,1
pytorch,affe742d31a98be938ea9d2536b8dd9b532a01a6,6e0d0f08a9daf099eccd57f2d85bad4bd219f5f2,Tongzhou Wang,SsnL@users.noreply.github.com,Thu Feb 08 20:34:30 2018 -0500,1518122070.0,"Improves Conv*d(Transposed) docs to have correct newline and formatting (#5139)

Improves CUDA matmul error message by basically copying the CPU error message",137.0,90.0,"aten/src/THC/generic/THCTensorMathBlas.cu,torch/nn/modules/conv.py",2.0,7,2,0.39969074,36.0,1640.0,2.0,1935770.5,2354.0,24567.85823,0.0,Corrective,0.0,1
pytorch,02e4ca9cabf0f8d128e4519a6c60f1f5ba46e797,6e3e453ad273d0450e83a47e0f8d9a6b91c8636b,Kai Arulkumaran,Kaixhin@users.noreply.github.com,Tue May 23 16:32:33 2017 +0100,1495557153.0,Tidy up convs docs (#1602),63.0,26.0,torch/nn/modules/conv.py,1.0,3,1,0,28.0,621.0,1.0,688326.0,622.0,5264.672317,0.0,Non Functional,0.0,1
pytorch,36d64760d97fd7c13681d92738dacff7cca48899,6e46f47227a7e0c03e1f05ba55a37da70c8234ee,Nikita Karetnikov,nikita@karetnikov.org,Thu Mar 23 17:41:31 2023 +0100,1679593291.0,"[inductor] xfail tests by default (#97331)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97331
Approved by: https://github.com/ezyang",29.0,15.0,"test/inductor/test_torchinductor.py,test/inductor/test_torchinductor_dynamic_shapes.py",2.0,2,1,0.998509099,1.0,8001.0,2.0,70533.0,13688.0,31603.0,0.0,,0.0,1
pytorch,9f6e0de5485900eed3d5e2438b4797e8744ce088,6e4746c1ac04a8bc81c29ba75e4d7933677e9887,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Jan 29 00:21:38 2021 -0800,1611879698.0,"Port cholesky_inverse to ATen (#50269)

Summary:
Now we can remove `_th_potri`!

Compared to the original TH-based `cholesky_inverse`, complex (https://github.com/pytorch/pytorch/issues/33152) and batched inputs (https://github.com/pytorch/pytorch/issues/7500) are now supported both on CPU and CUDA.

Closes https://github.com/pytorch/pytorch/issues/24685.
Closes https://github.com/pytorch/pytorch/issues/24543.

Ref. https://github.com/pytorch/pytorch/issues/49421, https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50269

Reviewed By: bdhirsh

Differential Revision: D26047548

Pulled By: anjali411

fbshipit-source-id: e4f191e39c684f241b7cb0f4b4c025de082cccef",413.0,294.0,"aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/LegacyTHFunctionsCPU.h,aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THLapack.cpp,aten/src/TH/generic/THLapack.h,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorLapack.h,aten/src/THC/generic/THCTensorMathMagma.cu,aten/src/THC/generic/THCTensorMathMagma.h,test/test_autograd.py,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",17.0,14,3,3.502405683,44.0,35383.0,10.0,438271.5882352941,8453.0,19067.0,0.0,,0.0,1
pytorch,a3c165f9d240f0250216a4f4c5f6085860d0acef,6e4a83ab577887fc3c692e106433a121c3165130,Vitaly Fedyunin,vitalyf@fb.com,Mon Aug 05 18:42:48 2019 -0700,1565030568.0,"Channels last stored in tensor (#23391)

Summary:
Define 4D tensor as stored in channels last memory format, when dimensions order is NCHW and C-strides < W-strides < H-strides < N-strides (If size of any dimension is equal to 1, this dimension strides value is not taken into account).

Channels last contiguous tensor is channel last tensor which occupies contiguous memory block. So x.is_contiguous(memory_format=torch.channels_last) checks if tensor is channels last contiguous.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23391

Differential Revision: D16601414

Pulled By: VitalyFedyunin

fbshipit-source-id: 8d098e7eec2f00fb1d12261bc240b3645d4f5b73",97.0,23.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/templates/Tensor.h,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,test/test_torch.py",5.0,8,3,1.998489519,42.0,15814.0,4.0,387803.2,10398.0,29704.83333,0.0,,0.0,1
pytorch,6b50874cb70f27020b88d16ce8e7fc6a186549ee,6e4f501f1ae2814b1f6801c95b6cab2f297a3cf6,Yael Dekel,yaeld@microsoft.com,Tue Jul 07 03:22:43 2020 -0700,1594092163.0,"Improve error message for Pad operator (#39651)

Summary:
In issue https://github.com/pytorch/pytorch/issues/36997 the user encountered a non-meaningful error message when trying to export the model to ONNX. The Pad operator in opset 9 requires the list of paddings to be constant. This PR tries to improve the error message given to the user when this is not the case.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/39651

Reviewed By: hl475

Differential Revision: D21992262

Pulled By: houseroad

fbshipit-source-id: b817111c2a40deba85e4c6cdb874c1713312dba1",53.0,4.0,"test/onnx/test_pytorch_common.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.767981807,4.0,6820.0,4.0,4479179.75,3412.0,8122.0,0.0,Feature Addition,0.0,1
pytorch,0c8a1c4d85dd959ad22dd5c3d2ece144155e3c28,6ea422dd0be544140e3cc2bb8546159d103a4b0d,Huy Do,huydhn@gmail.com,Mon Jul 25 22:42:21 2022 +0000,1658788941.0,"Format torch/onnx with ufmt (#82137)

This is the last batch for the new ufmt (black + usort) linter. After this, black linter can finally be replaced. The previous PR to format ONNX tests was #81335
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82137
Approved by: https://github.com/kit1980, https://github.com/AllenTiTaiWang",26.0,31.0,".lintrunner.toml,torch/onnx/__init__.py,torch/onnx/symbolic_caffe2.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset15.py,torch/onnx/symbolic_opset7.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",12.0,2,1,3.358017165,15.0,12296.0,10.0,2452121.333333333,5736.0,13362.5,0.0,,0.0,1
pytorch,dd95bf65b6a6c97455efdcfce0aca46e493db916,6eaa324c9fff2e5cb0ff8ddb71665ef1e594d623,mfkasim91,firman.kasim@gmail.com,Thu Oct 29 18:38:18 2020 -0700,1603996698.0,"Implement torch.igamma (#46183)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/41637
This is regularized lower incomplete gamma function, equivalent to scipy's `gammainc` and tensorflow `igamma`.

cc fritzo mruberry

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46183

Reviewed By: gchanan

Differential Revision: D24479126

Pulled By: mruberry

fbshipit-source-id: fdf8ea289fe4ca1b408810732192411e948fcdfe",1590.0,8.0,"NOTICE,aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/native_functions.yaml,c10/util/math_compat.h,docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py",26.0,17,6,2.39140464,46.0,64806.0,15.0,1784136.6153846155,6316.0,14450.0,0.0,Corrective,1.0,1
pytorch,7332c21f7a0b653dd6b1c322cb02f48dc23964c5,6ebc0504cabf6d2f20f2a836c0d59eb000d10537,Darius Tan,dariustan97@gmail.com,Fri Aug 07 20:07:28 2020 -0700,1596830848.0,"BAND, BOR and BXOR for NCCL (all_)reduce should throw runtime errors (#42669)

Summary:
cc rohan-varma
Fixes https://github.com/pytorch/pytorch/issues/41362 #39708

# Description
NCCL doesn't support `BAND, BOR, BXOR`. Since the [current mapping](https://github.com/pytorch/pytorch/blob/0642d17efc73041e5209e3be265d9a39892e8908/torch/lib/c10d/ProcessGroupNCCL.cpp#L39) doesn't contain any of the mentioned bitwise operator, a default value of `ncclSum` is used instead.

This PR should provide the expected behaviour where a runtime exception is thrown.

# Notes
- The way I'm throwing exceptions is derived from [ProcessGroupGloo.cpp](https://github.com/pytorch/pytorch/blob/0642d17efc73041e5209e3be265d9a39892e8908/torch/lib/c10d/ProcessGroupGloo.cpp#L101)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42669

Reviewed By: ezyang

Differential Revision: D22996295

Pulled By: rohan-varma

fbshipit-source-id: 83a9fedf11050d2890f9f05ebcedf53be0fc3516",35.0,9.0,"test/distributed/test_c10d.py,torch/lib/c10d/ProcessGroupNCCL.cpp",2.0,5,2,0.845350937,4.0,4772.0,2.0,515256.0,4162.0,9700.5,0.0,Corrective,1.0,1
pytorch,61418aa069e0ad5b009b7b0247f8ebdc593002ee,6ec71ed4f97d6b3f0ceb5b39cd924b76ecf40ac8,Edward Yang,ezyang@fb.com,Thu Apr 15 15:48:00 2021 -0700,1618501680.0,"Replace all direct cdata access with THPVariable_Unpack (#55799)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55799

I'm going to change the implementation of cdata soon so I need to
abstract over cdata access with a function.  Additionally, many
users are casting manually casting to THPVariable to access
the member so I can remove these unsafe casts in the client code
(the implementation, of course, is still doing an unsafe cast.)

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D27712130

Pulled By: ezyang

fbshipit-source-id: 95fcc013bf3913d67f2c634068eb5b3aab144cb3",183.0,181.0,"aten/src/ATen/core/Tensor.cpp,aten/src/ATen/core/VariableHooksInterface.h,aten/src/ATen/native/VariableMethodStubs.cpp,aten/src/ATen/templates/TensorBody.h,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_variable_methods.cpp,tools/codegen/api/python.py,torch/csrc/Generator.cpp,torch/csrc/Size.cpp,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_hook.cpp,torch/csrc/autograd/python_legacy_variable.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/python/python_arg_flatten.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/pybind.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_new.cpp",24.0,20,3,3.358020314,44.0,14176.0,16.0,2220245.583333333,10839.0,23946.0,0.0,Feature Addition,0.0,1
pytorch,159a2404bdc7fc54918c78d4bd290e5fa830dca7,6eec730a73796b7d8d9bcb8cbab68cd0d1c14eb9,kshitij12345,kshitijkalambarkar@gmail.com,Fri May 07 08:47:36 2021 -0700,1620377256.0,"[testing] atan2: Enable cases where self broadcasts (#57608)

Summary:
Just a follow-up

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57608

Reviewed By: albanD

Differential Revision: D28249409

Pulled By: mruberry

fbshipit-source-id: a1ce2cd736ac5547cecb3e21aaa50637917284bc",8.0,8.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,6446.0,1.0,1092.0,11765.0,26544.5,0.0,,0.0,1
pytorch,c141f28b648ee3c6cb0a7286f0aa100297417e74,6eeeb8817229e7df054db38337cd944b6e2daaad,Peter Bell,peterbell10@live.co.uk,Wed Oct 19 16:00:52 2022 +0100,1666195252.0,"OpInfo: Sample input cleanup (4/n) (#86324)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86324
Approved by: https://github.com/mruberry",64.0,56.0,"test/functorch/test_ops.py,test/test_decomp.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/_masked.py,torch/testing/_internal/opinfo/definitions/linalg.py",5.0,7,2,1.565485289,7.0,23901.0,3.0,136839.6,8544.0,20384.5,0.0,,0.0,1
pytorch,f0054e1a6e04feb2a1a791d5c02da17dadf6d904,6f08ddfc28e8cfc9bf663834f4efbb08a4c772ea,BowenBao,bowbao@microsoft.com,Wed Jul 21 22:00:36 2021 -0700,1626904836.0,"[ONNX] Enable aten:normal op and add tests for aten:uniform op. (#60441) (#61560)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61560

1. Add a new symbolic function broadcast_tensors() to support exporting torch.broadcast_tensors() function. This is required by exporting torch.distribution.normal() function.
2. Add a new symbolic function normal() to support exporting torch.distribution.normal() function.
3. Add relative tests for normal and uniform ops as well.

Test Plan: Imported from OSS

Reviewed By: nikithamalgifb

Differential Revision: D29767995

Pulled By: SplitInfinity

fbshipit-source-id: acfe5e7801d00c0df8ca46966bbd6015fed0045e

Co-authored-by: Jay Zhang <jiz@microsoft.com>",122.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",3.0,4,2,0.978570722,3.0,13706.0,2.0,372823.6666666667,14026.0,31568.5,0.0,Feature Addition,0.0,1
pytorch,4cbe140ec50e02696377f78d39a83f6202900064,6f473c80a5b20093b1a9c39b59b6d984c903be32,Shirong Wu,shirong@fb.com,Wed Jan 05 17:27:12 2022 -0800,1641403632.0,"Enable fx2trt CI test (#70658)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70658

Config '--exclude-distributed-test' was intended to disabled fx2trt test on normal docker test suite, but test is auto disabled now. Remove config.

Test Plan:
CI
https://github.com/pytorch/pytorch/actions/runs/1656375648

Reviewed By: houseroad

Differential Revision: D33417803

fbshipit-source-id: 9dfb4cbd6fa9ad18a4be989ee86d1f8a298347f9",2.0,2.0,.jenkins/pytorch/test.sh,1.0,2,1,0,6.0,589.0,1.0,1179610.0,18099.0,42820.5,0.0,Non Functional,0.0,1
pytorch,10c29c8970a1468cd157d371a0d0b9879cb703a9,6f6b03566ba3c4828f6ee87a772f9d161be0bae7,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Mon Sep 17 03:30:14 2018 -0700,1537155014.0,"Vectorize grid sample 2d CPU kernels (#10980)

Summary:
This PR vectorizes the CPU grid sample 2d forward and backward kernels. Specifically,

 1. add `.data()` in `TensorAccessor`
 2. support non-void return value for declaring CPU kernel stub
 2. add `bool at:: geometry_is_contiguous(IntList sizes, IntList strides)`
1. The following vectorized CPU primitives are added:

    + `gather<scale>(baseaddr, vindex)`: `result[i] = baseaddr[vindex[i] * scale]`
    + `mask_gather<scale>(src, baseaddr, vindex, mask)`: `result[i] = mask[i] ? baseaddr[vindex[i] * scale] : src[i]`.
    + comparison ops
    + binary logical ops
    + `min(a, b)`
    + `cast<dst_t, src_t>(src_vec)`: changing dtype but keeping the bit representation
    + `blendv(a, b, mask)`: `result[i] = mask[i] ? b[i] : a[i]`.
    + ctor with multiple values (i.e., `setr`)
    + `arange(start = 0, step = 1)`: constructs a vector with values specified by the arange parameters
    + `convert_to_int_of_same_size(vec)`: convert floating point vector to corresponding integral type of same size
    + `interleave2(a, b)` & `deinterleave2(x, y)`: interleave or deinterleaves two vectors. E.g., for `interleave`:
        ```
        inputs:
          {a0, a1, a2, a3, a4, a5, a6, a7}
          {b0, b1, b2, b3, b4, b5, b6, b7}
        outputs:
          {a0, b0, a1, b1, a2, b2, a3, b3}
          {a4, b4, a5, b5, a6, b6, a7, b7}
        ```

  2. Grid sample CPU kernel implementations are described in the following note (also in `GridSampleKernel.cpp`:

  ```
   NOTE [ Grid Sample CPU Kernels ]

   Implementation of vectorized grid sample CPU kernels is divided into three
   parts:

   1. `ComputeLocation` struct
      Transforms grid values into interpolation locations of the input tensor
      for a particular spatial dimension, basing on the size of that dimension
      in input tensor, and the padding mode.
```
```cpp
      template<typename scalar_t, GridSamplerPadding padding>
      struct ComputeLocation {
        using Vec = Vec256<scalar_t>;

        // ctor
        ComputeLocation(int64_t size);

        // Given grid values `in`, return the interpolation locations after
        // un-normalization and padding mechanism (elementwise).
        Vec apply(const Vec &in) const;

        // Similar to `apply`, but also returns `d apply(in) / d in`
        // (elementwise).
        // this is often used in gradient computation.
        std::pair<Vec, Vec> apply_get_grad(const Vec &in) const;
      };
```
```
   2. `ApplyGridSample` struct
      Owns N `ComputeLocation` structs, where N is the number of spatial
      dimensions. Given N input grid vectors (one for each spatial dimension)
      and spatial offset, it gets the interpolation locations from
      `ComputeLocation`s, applies interpolation procedure, and then writes to
      the output (or grad_input & grad_grid in backward).
```
```cpp
      template<typename scalar_t, int spatial_dim,
               GridSamplerInterpolation interp,
               GridSamplerPadding padding>
      struct ApplyGridSample {

        // ctor
        ApplyGridSample(const TensorAccessor<scalar_t, 4>& input);

        // Applies grid sampling (forward) procedure:
        //   1. computes interpolation locations from grid values `grid_x` and
        //      `grid_y`,
        //   2. interpolates output values using the locations and input data
        //      in `inp_slice`, and
        //   3. writes the first `len` values in the interpolated vector to
        //      `out_slice` with spatial offset being `offset`.
        //
        // This assimes that `grid_x` and `grid_y` all contain valid grid
        // values \in [-1, 1], even at indices greater than `len`.
        //
        // The `*_slice` argument namess mean samples within a batch (i.e.,
        // with the batch dimension sliced out).
        void forward(TensorAccessor<scalar_t, 3>& out_slice,
                     const TensorAccessor<scalar_t, 3>& inp_slice,
                     int64_t offset, const Vec& grid_x, const Vec& grid_y,
                     int64_t len) const;

        // Applies grid sampling (backward) procedure. Arguments semantics
        // and strategy are similar to those of `forward`.
        void backward(TensorAccessor<scalar_t, 3>& gInp_slice,
                      TensorAccessor<scalar_t, 3>& gGrid_slice,
                      const TensorAccessor<scalar_t, 3>& gOut_slice,
                      const TensorAccessor<scalar_t, 3>& inp_slice,
                      int64_t offset, const Vec& grid_x, const Vec& grid_y,
                      int64_t len) const;
      }
```
```
   3. `grid_sample_2d_grid_slice_iterator` function
      Among the tensors we work with, we know that the output tensors are
      contiguous (i.e., `output` in forward, and `grad_input` & `grad_grid` in
      backward), we need to randomly read `input` anyways, and `grad_output`
      usually comes from autograd and is often contiguous. So we base our
      iterating strategy on the geometry of grid.
      `grid_sample_2d_grid_slice_iterator` function provides an abstract to
      efficiently iterates through a `grid` slice (without batch dimension).
      See comments of that function on the specific cases and strategies used.
```
```cpp
      template<typename scalar_t, typename ApplyFn>
      void grid_sample_2d_grid_slice_iterator(
        const TensorAccessor<scalar_t, 3>& grid_slice,
        const ApplyFn &apply_fn);

      // `apply_fn` is a function/lambda that can be called as if it has
      // declaration:
      //   void apply_fn(const Vec256<scalar_t>& grid_x,
      //                 const Vec256<scalar_t>& grid_y,
      //                 int64_t spatial_offset, int64_t len);
```
```
      `apply_fn` will be called multiple times, and together cover the entire
      output spatial space. Therefore, e.g., to implement forward 2d grid
      sample, we can do
```
```cpp
      ApplyGridSample<scalar_t, 2, interp, padding> grid_sample(input_accessor);

      for (int n = 0; n < input_accessor.size(0); n++) {
        grid_sample_2d_grid_slice_iterator(
          grid_accessor[n],
          [&](const Vec256<scalar_t>& grid_x, const Vec256<scalar_t>& grid_y,
              int64_t spatial_offset, int64_t len) {
            grid_sample.forward(out_accessor[n], input_accessor[n],
                                spatial_offset, grid_x, grid_y, len);
          });
      }
   ```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10980

Differential Revision: D9564867

Pulled By: SsnL

fbshipit-source-id: 5b7c3c7ea63af00eec230ae9ee1c3e6c6c9679b4",1721.0,371.0,"aten/src/ATen/Parallel.h,aten/src/ATen/TensorGeometry.cpp,aten/src/ATen/TensorUtils.cpp,aten/src/ATen/TensorUtils.h,aten/src/ATen/core/TensorAccessor.h,aten/src/ATen/cpu/vec256/intrinsics.h,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/GridSampler.cpp,aten/src/ATen/native/GridSampler.h,aten/src/ATen/native/cpu/GridSamplerKernel.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.h,test/test_nn.py",17.0,9,2,2.736787735,42.0,11733.0,12.0,3620296.0,4146.0,11586.83333,0.0,Feature Addition,0.0,1
pytorch,347ab5d8b86a69c4ab0a163b01d41cd0d0734f59,6f8328ef44f0b80f201342bcb3005af1c298e7a7,kshitij12345,kshitijkalambarkar@gmail.com,Thu Mar 25 01:43:16 2021 -0700,1616636596.0,"[special] Add special.entr (#53500)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50345

TODO:

* [x] Verfiy docs rendering (https://11397990-65600975-gh.circle-artifacts.com/0/docs/special.html)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53500

Reviewed By: ngimel

Differential Revision: D27287096

Pulled By: mruberry

fbshipit-source-id: 6b3dfd53e811a0f023ee444a0b56176f825d39e9",131.0,2.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",12.0,19,4,2.900885964,17.0,20416.0,7.0,352257.9166666667,10048.0,22252.0,0.0,Feature Addition,0.0,1
pytorch,9a941ec2c27885cba39e60b348874b5aec199660,6fa7efaafe7c8efe54717f9faae97a4c257f45a4,Yukio Siraichi,yukio.siraichi@gmail.com,Sat May 21 08:23:36 2022 +0900,1653121416.0,"`linalg_ldl_factor`: change to new `MetaBase` API.

This PR modifies `linalg_ldl_factor` to the new `MetaBase` API. Instead of checking the
output strides manually, it now uses the appropriate `set_output_strided` and
`set_output_contiguous`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78038

Approved by: https://github.com/Lezcano, https://github.com/ezyang",9.0,26.0,aten/src/ATen/native/BatchLinearAlgebra.cpp,1.0,4,1,0,3.0,4712.0,1.0,312.0,3528.0,8366.5,0.0,,0.0,1
pytorch,f03f9ad62137da2eb6df75225351c80138089aff,6fb5ce5569e54c74077f626bd877de7e35d774b4,Nick Gibson,nickg@fb.com,Thu Aug 13 16:45:33 2020 -0700,1597337133.0,"[NNC] Fix some bugs in Round+Mod simplification (#42934)

Summary:
When working on the Cuda Codegen, I found that running the IRSimplifier before generating code lead to test fails. This was due to a bug in Round+Mod simplification (e.g. (x / y * y) + (x % y) => x) to do with the order in which the terms appeared. After fixing it and writing a few tests around those cases, I found another bug in simplification of the same pattern and have fixed it (with some more test coverage).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42934

Reviewed By: zhangguanheng66

Differential Revision: D23085548

Pulled By: nickgg

fbshipit-source-id: e780967dcaa7a5fda9f6d7d19a6b7e7b4e94374b",90.0,6.0,"test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",2.0,7,2,0.842657877,1.0,3915.0,2.0,626244.0,4278.0,10005.0,0.0,Corrective,1.0,1
pytorch,37a5819665bd37a2af148a42dfcc5255e3194f3e,6fb6dc42ff9b96776e020567a8e4e8d2aa007307,lezcano,lezcano-93@hotmail.com,Fri Jul 01 12:04:48 2022 +0000,1656677088.0,"Improve heuristics for linalg_lu_solve when B is a matrix (#79838)

When linalg_lu_solve was added in
https://github.com/pytorch/pytorch/pull/72935 I made the big mistake of
assuming that the choice of backend would not depend on number of
columns of B. This turned out to be false by a large margin.

This PR amends this and provides a heuristic that takes the number of
columns of B into account. The heuristic is not simple and it was
crafted by hand, but as the results show, it is effective.

@xwang233 the cusolver team should look into this one, as I was able to
outperform both cublas and cusolvers algorithms by using triangular
solves...

The benchmarks for the heuristics are here: https://github.com/pytorch/pytorch/pull/79838#issuecomment-1163802792

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79838
Approved by: https://github.com/IvanYashchuk, https://github.com/albanD",69.0,31.0,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,1.0,6,1,0,1.0,3239.0,1.0,1771362.0,4942.0,11689.5,0.0,Feature Addition,0.0,1
pytorch,6f6a1f2d638f5f989786e45b5850b8d3ac4b4048,6fc75eadf087987ee0d634ee248bb249e40bf765,Xiang Gao,qasdfgtyuiop@gmail.com,Wed Aug 01 14:46:03 2018 -0700,1533134763.0,"Add CELU activation to pytorch (#8551)

Summary:
Also fuse input scale multiplication into ELU

Paper:
https://arxiv.org/pdf/1704.07483.pdf
Pull Request resolved: https://github.com/pytorch/pytorch/pull/8551

Differential Revision: D9088477

Pulled By: SsnL

fbshipit-source-id: 877771bee251b27154058f2b67d747c9812c696b",229.0,34.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/nn.yaml,aten/src/THCUNN/ELU.cu,aten/src/THCUNN/generic/ELU.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/ELU.c,aten/src/THNN/generic/THNN.h,docs/source/nn.rst,docs/source/scripts/build_activation_images.py,test/common_nn.py,test/cpp/api/modules.cpp,test/onnx/expect/TestOperators.test_elu.expect,test/onnx/test_operators.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/legacy/nn/ELU.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py,torch/onnx/symbolic.py",21.0,24,5,3.649475767,46.0,23040.0,12.0,2607661.6,3238.0,8552.833333,0.0,Feature Addition,0.0,1
pytorch,c96f0762481d2cbdd5f776ff055b062562985694,6fd20a8dea316c44b362b695dff6e0477383f142,Bassam Yassin,bassam45@fb.com,Wed Apr 07 11:32:47 2021 -0700,1617795167.0,"Back out ""[pytorch][PR] [fix] torch.frac : Handle inf correctly""

Summary: Original commit changeset: 92c7309558ee

Test Plan: reverting D27566407 (https://github.com/pytorch/pytorch/commit/ece075195d49c25213c96b9d53fcf7077215f44a)

Differential Revision: D27618949

fbshipit-source-id: 7930251f4bc88e7991805d77a617a181d68a4880",16.0,75.0,"aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryFractionKernels.cu,torch/testing/_internal/common_methods_invocations.py",7.0,11,2,2.666295132,8.0,8343.0,1.0,7571.0,10510.0,23295.5,0.0,Corrective,1.0,1
pytorch,655c22569e9f0e355d063c18441e69da0fb29504,701e63107fb3c177e83850a0389f0d6580d8fe45,Martin Raison,raison@fb.com,Fri Mar 24 17:17:06 2017 -0700,1490375826.0,"speed improvements, fix tests",979.0,733.0,"test/common.py,test/test_nn.py,test/test_sparse.py,test/test_torch.py,torch/csrc/generic/methods/SparseTensor.cwrap,torch/csrc/generic/methods/TensorMath.cwrap,torch/lib/TH/generic/THTensorMath.c,torch/lib/TH/generic/THTensorMath.h,torch/lib/THC/THCTensorMathPointwise.cuh,torch/lib/THC/generic/THCStorage.cu,torch/lib/THCS/THCSTensor.cu,torch/lib/THCS/THCSTensor.h,torch/lib/THCS/THCSparse.cu,torch/lib/THCS/THCSparse.h,torch/lib/THCS/generic/THCSTensor.c,torch/lib/THCS/generic/THCSTensor.cu,torch/lib/THCS/generic/THCSTensor.h,torch/lib/THCS/generic/THCSTensorMath.cu,torch/lib/THPP/tensors/THCTensor.cpp,torch/lib/THPP/tensors/generic/THCTensor.cpp,torch/lib/THPP/tensors/generic/THTensor.cpp,torch/lib/THS/generic/THSTensor.c,torch/lib/THS/generic/THSTensor.h,torch/lib/THS/generic/THSTensorMath.c,torch/nn/_functions/thnn/sparse.py,torch/sparse/__init__.py",26.0,21,2,2.751875048,29.0,18330.0,5.0,557.6923076923077,337.0,15044.107,0.0,Corrective,1.0,1
pytorch,ba8bbeced316a6406ada879199b11059af818be9,702a7f386432108ec28fcabce2958e4ab11eae5b,Peter Goldsborough,peter@goldsborough.me,Wed Feb 21 21:37:52 2018 -0800,1519249072.0,"Improve Function interface (#5221)

* Improve Function interface

* Undo tracer changes

* Fix bug in VariableType.set_history

* Rename function_counter and sequence_number to sequence_nr

* Clarify Function documentation

* Replace swap_next_edges with next_edges() getter

* Bring back set_gradient_edge

* Simplify special.cpp

* add_gradient_edge -> create_gradient_edge

* Add mutable getters for pre/post hooks

* Use make_variable with Edge

* Remove remove_gradient_edge in favor of detach_

* Fix documentation and remove create_gradient_edge friend method

* Canonicalize some includes",652.0,419.0,"tools/autograd/gen_autograd_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/engine.h,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/functions/accumulate_grad.cpp,torch/csrc/autograd/functions/accumulate_grad.h,torch/csrc/autograd/functions/basic_ops.cpp,torch/csrc/autograd/functions/basic_ops.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/special.cpp,torch/csrc/autograd/functions/special.h,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/functions/tensor.h,torch/csrc/autograd/functions/utils.cpp,torch/csrc/autograd/functions/utils.h,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_engine.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/jit/autodiff.h,torch/csrc/jit/graph_executor.cpp,torch/csrc/jit/interpreter.cpp,torch/csrc/jit/interpreter_autograd_function.cpp,torch/csrc/jit/interpreter_autograd_function.h,torch/csrc/jit/python_compiled_function.cpp,torch/csrc/jit/python_compiled_function.h",33.0,8,2,3.59777167,35.0,7728.0,12.0,1594113.4545454546,2392.0,24659.35823,0.0,Corrective,1.0,1
pytorch,360c1bbd5b36fd84670292b2ea93170c31cbd5cf,7050d83dd728858cb1095788285316d1c3297b86,Thomas Viehmann,tv.code@beamnet.de,Tue Jul 24 19:35:10 2018 -0700,1532460910.0,"Make logsumexp_out inplace (#9755)

Summary:
Fixes: #9754

Maybe this could also make its way into 0.4.1, it  is a severe debugging headache if you hit this...
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9755

Reviewed By: ezyang

Differential Revision: D8967178

Pulled By: zou3519

fbshipit-source-id: 151ed24e3a15a0c67014e411ac808fb893929a42",12.0,11.0,"aten/src/ATen/native/ReduceOps.cpp,test/test_torch.py",2.0,5,2,0.755375413,40.0,9108.0,2.0,42331.5,3080.0,7484.833333,0.0,Corrective,1.0,1
pytorch,b9fc656cf26d60127bd695e4e5a7d27622f2563d,70a3210ecaa0162b4673f53faa17675a9d3ca8de,Philip Meier,github.pmeier@posteo.de,Fri Aug 20 18:43:07 2021 -0700,1629484987.0,"Add `BinaryUfuncOpInfo` and broadcasting tests (#61964)

Summary:
As proof of concept, this PR uses the new `BinaryUfuncOpInfo` in broadcasting tests for `add`, `sub`, `mul`, `div`, `floor_div`, and `true_div`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61964

Reviewed By: ngimel

Differential Revision: D30407734

Pulled By: mruberry

fbshipit-source-id: ada28994f43b0635f279f45a02ecba18bc8ee033",287.0,128.0,"test/test_binary_ufuncs.py,test/test_jit_fuser_te.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.749769348,2.0,14143.0,3.0,263340.3333333333,14811.0,33911.5,0.0,Feature Addition,0.0,1
pytorch,b6df043f1fa31f37c4b205eb4083918b919751ff,70a5113e03fda75ce0e8980d9fdcc50246d00d5e,Jagadish Krishnamoorthy,jagdish.krishna@gmail.com,Thu Oct 21 14:04:00 2021 -0700,1634825040.0,"[ROCm] update Magma for 4.3 release (#65203)

Summary:
Upstream magma fixes the cholesky issues.
Refer https://bitbucket.org/icl/magma/issues/48/parameter-4-was-incorrect-on-entry-to

Signed-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>

Fixes #{issue number}

cc jeffdaily sunway513 jithunnair-amd ROCmSupport

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65203

Reviewed By: anjali411

Differential Revision: D31766608

Pulled By: malfet

fbshipit-source-id: 3829b89314d25d8aa14be57ead879a811ab3f098",4.0,7.0,.circleci/docker/common/install_rocm.sh,1.0,3,1,0,1.0,128.0,1.0,6727566.0,16449.0,38574.0,0.0,Corrective,1.0,1
pytorch,564efd3521d53967509704440071d51323afc9f5,70b8f0ed4783201cf03fcc22e3790a1db9e47ef9,Sam Gross,colesbury@gmail.com,Tue Nov 14 03:41:16 2017 -0500,1510630876.0,"Fix elu double-backwards when applied in-place (#3687)

* Fix elu double-backwards when applied in-place

Removed unused ""input"" argument to elu_backwards. Also removed 'inplace'
argument from backwards functions, since we don't ever want to use it.

* Fix up additional calls to ELU_updateGradInput",37.0,20.0,"aten/src/ATen/nn_parse.py,aten/src/THCUNN/generic/ELU.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/ELU.c,aten/src/THNN/generic/THNN.h,test/test_nn.py,tools/autograd/derivatives.yaml,torch/legacy/nn/ELU.py,torch/nn/_functions/thnn/activation.py",9.0,16,4,2.224723821,39.0,9441.0,5.0,608276.3333333334,2117.0,24074.35823,0.0,Corrective,1.0,1
pytorch,471dfe9791a49767dc823c697d92c720c58fd711,70c33777a6729107c44a24bc80f05e6628aef1bc,Gregory Chanan,gchanan@fb.com,Mon May 01 21:32:50 2017 -0700,1493674370.0,"pow, fmod, remainder also should fallback.

This behavior isn't listed in the docs, but the tests depend on it.",28.0,15.0,"test/test_torch.py,torch/csrc/generic/methods/TensorMath.cwrap",2.0,5,2,0.854180205,31.0,5464.0,2.0,0.0,877.0,11424.44394,0.0,Non Functional,0.0,1
pytorch,fbffd959ca3d7234aeef26be99d1240d58be8892,70d34718b8e7b8c1cd87860639c46005530ee371,Zachary DeVito,zdevito@fb.com,Mon Nov 09 18:33:08 2020 -0800,1604946788.0,"[fx] add missing modules for type annoations (#47537)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/47537

When a module only appears in a type constructor List[torch.Tensor],
it previously didn't get added to the list of used modules. This fixes it
by introspecting on the type constructor.

Test Plan: Imported from OSS

Reviewed By: jamesr66a

Differential Revision: D24806317

Pulled By: zdevito

fbshipit-source-id: 263391af71e1f2156cbefaab95b9818c6b9aaae1",13.0,1.0,"test/test_fx.py,torch/fx/graph.py",2.0,3,2,0.940285959,1.0,1646.0,2.0,429476.0,6575.0,14960.0,0.0,Corrective,1.0,1
pytorch,b7cdeb3fc3cdc71813c5f2b639fbf9bee5c6b843,70dd44f6a8a20a52b8d800e08a8aa7ef98ee6352,Brennan Vincent,btv@fb.com,Fri Jan 11 16:09:06 2019 -0800,1547222946.0,"Match NumPy by considering NaNs to be larger than any number when sorting (#15886)

Summary:
Fixes #15764
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15886

Differential Revision: D13612971

Pulled By: umanwizard

fbshipit-source-id: 91f552a25d1fd108f2f0b10e09a0ce0364f8c21e",50.0,30.0,"aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/THCSortUtils.cuh,aten/src/THC/THCTensorSort.cuh,aten/src/THC/generic/THCTensorSort.cu,test/test_torch.py",5.0,7,2,2.094488361,40.0,12767.0,5.0,662191.2,6409.0,19849.33333,0.0,Corrective,1.0,1
pytorch,224e62bbec2b00719f38a2a2fb2191e40120bc4a,70e68e755a8fdd94218e9b8e0abfb6b805354a08,David Riazati,davidriazati@fb.com,Fri Sep 14 20:29:06 2018 -0700,1536956946.0,"Casting for binary ops (#11708)

Summary:
Fixes #11663

`TensorIterator` was replacing the op tensors with type casted tensors
which ended up producing side effects in binary ops like `a.float() * b`
where `a` and `b` are `LongTensor`s.

colesbury ezyang apaszke
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11708

Differential Revision: D9834016

Pulled By: driazati

fbshipit-source-id: 4082eb9710b31dfc741161a0fbdb9a8eba8fe39d",17.0,1.0,"aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,test/test_torch.py",3.0,5,2,0.944488534,40.0,9736.0,3.0,1186817.3333333333,4131.0,11545.83333,0.0,Corrective,1.0,1
pytorch,cac3026b35b1499a31e4e12d0873a6db33f15bb9,70e71391d2c39cae115940af52530cc3fb053507,Richard Zou,zou3519@users.noreply.github.com,Thu Feb 15 22:41:19 2018 -0500,1518734479.0,"Fix THCTensor_(max) and THCTensor_(min) inits (#5265)

Their cuda kernels should be initialized with (min_value, 0) and
(max_value, 0), respectively, where the second number is a default index
value. However, they were being initialized with (max, 1) and (min, 1)
instead, probably a remnant from the lua torch days.

This caused bugs in torch.max() and torch.min() when the input is at the
extreme values, and the max value (or min value) occurs at index 0. For example,

  import torch
  x = torch.ByteTensor([[0]])
  x.cuda().max(dim=0)  # returns (0, 1) but the expected result is (0, 0)",15.0,2.0,"aten/src/THC/generic/THCTensorMathReduce.cu,test/test_cuda.py",2.0,5,2,0.787126586,37.0,1858.0,2.0,750566.5,446.0,2284.5,0.0,Corrective,1.0,1
pytorch,0e4c12157c41906ca283286381e65ad5b1c7584a,71067631c2fe18d1cf399dd4969ed10142deeb60,Bin Bao,binbao@meta.com,Wed Jul 19 23:21:02 2023 -0700,1689808862.0,"[inductor] Fix an AOTInductor missing output issue (#105496)

Summary: When an output buffer is reused instead of directly referring to the passed-in output, we need to explictly make a copy

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105496
Approved by: https://github.com/jansel",149.0,24.0,"test/inductor/test_aot_inductor.py,torch/_inductor/codegen/wrapper.py",2.0,5,2,0.914393972,1.0,1417.0,1.0,155455.0,17846.0,40330.0,0.0,Corrective,1.0,1
pytorch,c537fd7432b1cf2d11552e485dc00dd9eea0e03c,711e5a6ceb46fcfe90dc8ca176c94c4f44dfbc17,Edward Z. Yang,ezyang@mit.edu,Fri Jun 15 21:52:21 2018 -0400,1529099541.0,"Port THS to ATen. (#8409)

* Port THS to ATen.

The basic structure of the patch:

- All kernels in aten/src/THS got rewritten as native
  functions in aten/src/ATen/native/sparse

  I took the liberty to rename some of the kernels,
  opting for a longer, more transparent names than
  things like 'spaddcmul'.

- Instead of holding fields for sparse tensor in the TH
  C struct THSTensor, they are now held in a C++ class
  SparseTensorImpl (this explains why I had to do this
  all in one go; I can't have *two* reps for sparse
  tensors!)

  Along the way, we change a key internal representation
  invariant: an ""empty"" sparse tensor has dimI == 1 and
  dimV == 0 (this is different from dimI == 0 and dimV == 0
  we had before); this ensures that we maintain the invariant
  that dim == dimI + dimV.  ""Scalar"" sparse tensors are
  made illegal, because there really is no way to properly
  express them in COO format.

- Because we haven't ported THCS or any of the traditional
  dense TH implementations, there is a new set of adapter
  functions in native/LegacyBridge.cpp exclusively devoted
  to deciding whether or not to go to the new native implementation
  or back to the legacy TH binding (prefixed with th_).
  The intent is that when everything gets ported, we can
  delete this file.

- I've kept the stubs for all the THS functions, but they now all
  error if you try to actually call them.  Eventually, we should
  replace these with calls to ATen so that everything keeps
  working.

- I gobbled up SparseMM (SparseMM.cpp is no more). It was tasty.

There are some miscellaneous improvements which were needed for other
changes in this patch:

- There is now AT_FORALL_SCALAR_TYPES_EXCEPT_HALF, which does what
  it says on the tin.

- axpy templated function moved to TH/BlasUtils.h, there's a new macro
  which lets you easily forward to all of the TH functions. We also expose
  THBlas_copy.  I'm not terribly pleased with these functions but
  they seem to serve a purpose they need.

- New method on Tensor to get TensorImpl*, unsafeGetTensorImpl

- accessor() is now this-const, since const-correctness on Tensor is a lie

- New toSparse()/toDense() methods on Type; now you can call these
  directly without having to manually apply at::toSparse/toDense
  on the Backend and then running toBackend yourself.

Changes to the kernels:

- Previously, the whole body of all kernels was compiled for
  every supported scalar type.  In our new implementation,
  the scalar dispatch has been pushed into the smallest extent
  which (1) is not in a type loop and (2) requires statically
  knowing the scalar type.  These sites all use
  AT_DISPATCH_ALL_TYPES.  I tried to use lambdas as much as
  possible, but sometimes it was not possible when a OpenMP
  pragma was used.

- Anywhere we tested if the nDimension of a tensor was zero,
  we replaced with a test that numel is zero.  Because, as we
  known, nDimension of zero-size tensors in TH is zero, and
  that's wrong wrong wrong (and not done this way in ATen).

Some subtleties:

- Places where previously fastget1d was used, I now use a
  TensorAccessor.  However, you have to be careful about grabbing
  the accessor, because sometimes you will be accessor'ing
  indices/values and they are empty, which means they will
  be *1D* (""oh, aren't indices always 2D?"" Nope. Nyet.)
  So, essentially, it is only safe to grab an accessor *after*
  you have checked that nnz != 0.  All of these shenanigans
  will go away when we properly support zero-size dimensions.

  A few places, we test for this case just by wrapping the loop
  in a conditional on nnz.  Some other places this is not so easy,
  so we instead short-circuit the function with a special case for
  when nnz == 0 (usually, these implementations are degenerate).

- There is a very subtle but important difference between
  _sparse_get_impl(self)->indices() and self._indices();
  the latter may return a view!  This is because nnz is
  not guaranteed to match the dimensions of indices/values;
  you can ""truncate"" a sparse tensor by setting the nnz.
  Actually, I think this is not a good idea and we should
  enforce a stronger invariant, but for this patch I slavishly
  adhere to the old ways, and as such I have to be very
  careful if I want to resize something, I had better use
  the former and not the latter.

- I had to reimplement broadcasting by hand (thus the s_
  and non-s_ functions in the sparse native files).  There
  is a very important distinction between foo_out and foo_,
  so it is important that the LegacyBridge function always
  call to the lower layer, and not try to avoid boilerplate
  by calling to another LegacyBridge function first.
  I did NOT put broadcasting in LegacyBridge (even though,
  ultimately, that's where it must live), because the th_
  functions which are invoked from LegacyBridge handle
  broadcasting themselves, and I don't want to broadcast
  twice.

- Sparse function MUST explicitly specify the Type they
  dispatch from, otherwise Variable wrapping/unwrapping will
  not work correctly.  If you use _get_sparse_impl, that is
  sufficient to levy this requirement.

- The ""has native"" tests in LegacyBridge.cpp are not 100%,
  because some of the functions are mixed dense-sparse functions,
  and so you can't just say, ""Oh, if it's sparse and CPU, call
  the native sparse implementation.""  This is handled on a
  case by case basis.  There is some especially complex
  logic for add(), which has dense-dense, sparse-sparse
  and dense-sparse implementations.

- I added some uses of SparseTensorRef in native_functions.yaml,
  but you will notice that these are all on native_* functions,
  and not the actual, top-level functions.  So the SparseTensorRef
  is purely documentary (helping you not call the wrong overload)
  but there is no magic; we do the wrapping ourselves the hard
  way. (This is in constrast to the TH binding code which is magical.)
  Except for _sparse_mask; _sparse_mask is magical.

- There is a raw_copy_sparse_ method, which is really my way of
  getting around the fact that copy_ has never been implemented
  for sparse tensors (even before this patch), but there IS a
  super secret, internal way of doing these copies that the THS
  code used, and which I needed to get my hands on when I did this
  port.  We should refactor so that either (a) copy_ does support
  sparse-sparse copy natively, or (b) we do this other ways.

- Irritatingly, I must explicitly resize_as_ before copy_ into
  a tensor.  This was not the case with THTensor_(copy) but I don't
  have any direct binding that doesn't have this requirement.

- For some reason, the sparse tensor constructor accepts a scalar
  tensor for the values tensor.  This is kind of weird because
  you always need an nnz-dimension.  However, the old code supported
  this and just expanded it into a 1D size 0 tensor; so we need some
  explicit code to do this.

There are maybe a bit more AT_ASSERTs in some of the kernels
than is wise.  I added them all when I was debugging and was
loathe to remove them.

Some last mile fixes after this commit went into PR

- Move expand outside of dispatch so autograd works (it used to be inside and then we lost all of the recorded broadcasts).
- Hack to duplicate the derivatives for our now two definitions TH and native. Mercifully the derivatives are short.
- Apparently, TH has a special case to make foo_ functions method only, and if you don't do this the Python arg parsing is wrong. We carefully work around this in the native bindings
- Apply DCE to a test_jit case, fixes wobbling due to DCE trick in tracing
- Update test_function's output
- Some last mile fixes for dispatch confusion in sparse_coo_tensor functions.
- New simplified regression test based on failures I saw in ONNX
- Increase tolerance on super resolution test
- More robust dynamic_type normalization, fixes ONNX bug.
  The dynamic_type situation is very delicate; probably need
  to stop having both Scalar and real.
- Make new_with_tensor_sparse more CUDA safe
- Note about CUDA-safety in SparseTensorImpl
- Rename dimI/dimV to sparseDims/denseDims.
- Make localScalar on SparseTensorImpl work.
- Make numel uniformly supported on all types, not just dense
  types
- Add tests for is_nonzero() method (which exercises localScalar)
- Disable constant JIT autogenerated tests, which are fragile and broken
  by this change, but being fixed in a parallel track.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",2870.0,1168.0,"aten/src/ATen/CMakeLists.txt,aten/src/ATen/Declarations.cwrap,aten/src/ATen/ScalarType.h,aten/src/ATen/SparseTensorImpl.cpp,aten/src/ATen/SparseTensorImpl.h,aten/src/ATen/TensorImpl.h,aten/src/ATen/copy_wrapper.py,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/LegacyBridge.cpp,aten/src/ATen/native/README.md,aten/src/ATen/native/SparseMM.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/Type.h,aten/src/TH/THBlasUtils.h,aten/src/THS/generic/THSTensor.cpp,aten/src/THS/generic/THSTensorMath.c,test/expect/TestAutograd.test_function-x_grad_desc.expect,test/expect/TestAutograd.test_function-y_grad_desc.expect,test/expect/TestTorch.test_is_nonzero-empty.expect,test/expect/TestTorch.test_is_nonzero-multiple.expect,test/onnx/test_caffe2.py,test/test_autograd.py,test/test_jit.py,test/test_sparse.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/jit/gen_jit_dispatch.py,torch/csrc/utils/python_arg_parser.h",36.0,18,4,3.568257418,44.0,29504.0,19.0,1035585.8888888888,1343.0,3855.805292,0.0,Corrective,1.0,1
pytorch,d0c742134d0b24749d30c24da3c9e79a9b0771df,71260b98e2b215b166d5515b496ceee4a36dd86d,Iurii Zdebskyi,iuriiz@fb.com,Mon May 20 15:01:47 2019 -0700,1558364507.0,"Fixed histc return type for CUDA (#20369)

Summary:
Fixing reported [issue](https://github.com/pytorch/pytorch/issues/20208).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20369

Reviewed By: zou3519

Differential Revision: D15300959

Pulled By: izdeby

fbshipit-source-id: 219692f99a66ea433112dfc226132eb6867122cf",56.0,60.0,"aten/src/ATen/native/cuda/SummaryOps.cu,test/test_cuda.py,test/test_torch.py",3.0,6,2,0.427874286,41.0,14933.0,3.0,1615455.0,8763.0,25957.83333,0.0,Corrective,1.0,1
pytorch,518864a7e094f32044ef4c0de82c0f20f4ed99c2,712686ce91cac3142c6eecd4d74902939db59aa8,Sam Gross,sgross@fb.com,Thu Feb 02 21:44:10 2017 -0800,1486071850.0,"Add cat, contiguous, squeeze, and unsqueeze to THPP

Use unsqueeze and view from TH/THC",271.0,109.0,"test/test_cuda.py,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/Tensor.cwrap,torch/lib/THPP/Tensor.hpp,torch/lib/THPP/tensors/THCTensor.hpp,torch/lib/THPP/tensors/THSTensor.hpp,torch/lib/THPP/tensors/THTensor.hpp,torch/lib/THPP/tensors/generic/THCTensor.cpp,torch/lib/THPP/tensors/generic/THSTensor.cpp,torch/lib/THPP/tensors/generic/THTensor.cpp,torch/tensor.py",14.0,9,2,3.205903148,23.0,13856.0,1.0,159453.0,456.0,4366.116645,0.0,Feature Addition,0.0,1
pytorch,a46217d2ef5512041d009f9c78571647e3e535f1,71632d4d24616ddad6685814aae4ae54c981c0d2,"Liao, Xuan",xuan.liao@intel.com,Sat Aug 19 16:14:37 2023 +0000,1692461677.0,"[cpu] add sdpa choice and UT (#105131)

Feature RFC: https://github.com/pytorch/rfcs/pull/56.

Write an SDPA selecting function for CPU to automatically choose one SDPA implementation among several ones. There are two CPU implementations which could be chosen: the unfused SDPA and flash attention. In general, flash attention has a higher priority than the unfused SDPA. For cases where flash attention is not applicable, such as manually disabling flash attention or the inputs not 4 dimensional, the unfused SDPA is chosen.

## Performance of the stack

### NanoGPT's SDPA kernel
Using benchmark [repo](https://github.com/mingfeima/bench_sdpa/blob/main/README.md), with one socket.
Shape: Batch size 1, Sequence length 1024, Head number 25, Head size 64.
Machine: SPR.

| Dtype    | Causal   | Mode      | SDPA            | Time (ms per iter) | Speedup |
| -------- | -------- | -------   | -------         | -------            | ------- |
| float32  | FALSE    | Inference | Unfused         | 3.081              |         |
|          |          |           | Flash attention | 1.665              | **1.85045** |
| float32  | TRUE     | Inference | Unfused         | 3.463              |         |
|          |          |           | Flash attention | 1.662              | **2.083634**|
| bfloat16 | FALSE    | Inference | Unfused         | 1.203              |         |
|          |          |           | Flash attention | 1.154              | **1.042461**|
| bfloat16 | TRUE     | Inference | Unfused         | 1.543              |         |
|          |          |           | Flash attention | 1.154              | **1.337088**|
| float32  | FALSE    | Training  | Unfused         | 54.938             |         |
|          |          |           | Flash attention | 23.029             | **2.385601**|
| float32  | TRUE     | Training  | Unfused         | 58.266             |         |
|          |          |           | Flash attention | 17.835             | **3.266947**|
| bfloat16 | FALSE    | Training  | Unfused         | 18.924             |         |
|          |          |           | Flash attention | 18.886             | **1.002012**|
| bfloat16 | TRUE     | Training  | Unfused         | 21.08              |         |
|          |          |           | Flash attention | 14.172             | **1.48744** |

### Stable Diffusion
Following model's [BKM](https://github.com/intel-innersource/frameworks.ai.models.intel-models/blob/develop/quickstart/diffusion/pytorch/stable_diffusion/inference/cpu/README.md).
Mode: Inference; Machine: SPR.

| Dtype    | SDPA                    | Throughput (fps) | Speedup SDPA | Total Time (ms) | Speedup |
| -------- | --------                | -------          | -------      | -------         | ------- |
| float32  | Unfused                 | 1.63             |              | 1139            |         |
|          | Flash attention         | 1.983            | 1.216564     | 547.488         | **2.080411**|
| bfloat16 | Flash attention in IPEX | 4.784            |              | 429.051         |         |
|          | Flash attention         | 4.857            | 1.015259     | 408.823         | **1.049479**|

### LLM models of Torchbench

Dtype: float32; Mode: Inference, single socket; Machine: CPX.
Model   name | SDPA | Inductor_new | Inductor_old | Inductor   Ratio(old/new)
-- | -- | -- | -- | --
hf_Albert | Unfused -> Flash attention | 0.048629309 | 0.05591545 | **1.14983024**
hf_Bert | Unfused -> Flash attention | 0.053156243 | 0.060732115 | **1.142520841**
hf_Bert_large | Unfused -> Flash attention | 0.141089502 | 0.155190077 | **1.099940636**
llama | Unfused -> Flash attention | 0.033250106 | 0.033720745 | **1.01415451**

Dtype: bfloat16; Mode: Inference, single socket; Machine: SPR.
Model   name | SDPA | Inductor_new | Inductor_old | Inductor   Ratio(old/new)
-- | -- | -- | -- | --
hf_Albert | Unfused -> Flash attention | 0.020681298 | 0.020718282 | **1.001788324**
hf_Bert | Unfused -> Flash attention | 0.019932816 | 0.019935424 | **1.000130842**
hf_Bert_large | Unfused -> Flash attention | 0.047949174 | 0.048312502 | **1.007577355**
llama | Unfused -> Flash attention | 0.018528057 | 0.01861126 | **1.0044907**

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105131
Approved by: https://github.com/drisspg
ghstack dependencies: #104583, #104584, #103826, #104693, #104863, #107128",697.0,415.0,"aten/src/ATen/native/cpu/FlashAttentionKernel.cpp,aten/src/ATen/native/transformers/attention.cpp,aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,aten/src/ATen/native/transformers/cuda/sdp_utils.h,aten/src/ATen/native/transformers/sdp_utils_cpp.cpp,aten/src/ATen/native/transformers/sdp_utils_cpp.h,build_variables.bzl,test/functorch/test_ops.py,test/test_transformers.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_modules.py,torch/testing/_internal/jit_metaprogramming_utils.py",12.0,12,3,2.373189702,9.0,33975.0,9.0,1436835.181818182,18772.0,42513.0,0.0,Feature Addition,0.0,1
pytorch,33017948553d7f9fe684d350148388ef709adb64,717274c001509ffbc14189edef0f7d2ee2e5b777,Alban Desmaison,albandes@fb.com,Wed Dec 11 17:44:34 2019 -0800,1576086274.0,"Add useful warnings for t.grad when it won't be populated for known reasons (#30531)

Summary:
Fix https://github.com/pytorch/pytorch/issues/2362 and https://github.com/pytorch/pytorch/issues/19778

To avoid issues with frozen model, we only consider warning for Tensors that require gradients and are neither leafs nor retain gradients.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30531

Differential Revision: D18832767

Pulled By: albanD

fbshipit-source-id: 743e863dc14ab57713e66da78b2e4d759dfba0ff",48.0,14.0,"test/test_jit.py,test/test_torch.py,torch/_tensor_docs.py,torch/autograd/gradcheck.py,torch/csrc/autograd/python_variable.cpp,torch/tensor.py",6.0,5,2,1.953043922,43.0,37644.0,5.0,185611.66666666663,13785.0,37587.83333,0.0,Corrective,1.0,1
pytorch,4fc29e9c43b35092c091227c394712e1c345b964,719d29dab5e915437f026a01feb45fef62828047,Muthu Arivoli,ma381@duke.edu,Sun Sep 06 06:09:43 2020 -0700,1599372583.0,"Implement torch.i0 and torch.kaiser_window (#43132)

Summary:
Related to https://github.com/pytorch/pytorch/issues/38349

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43132

Reviewed By: smessmer

Differential Revision: D23479072

Pulled By: mruberry

fbshipit-source-id: 4fb1de44830771c6a7222cf19f7728d9ac7c043b",449.0,32.0,"aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py",23.0,15,5,2.739347208,45.0,53094.0,18.0,1203041.0869565215,4897.0,11327.0,0.0,,0.0,1
pytorch,5b86c3af4a6315665e9db1cabaaddbf2d107b52f,71a3633e3f25b0f974fd30d1a9d74751397a93e3,Wei Yang,38509346+weiyangfb@users.noreply.github.com,Wed Jun 13 20:22:50 2018 -0700,1528921370.0,"change tensor.set_() argument names to match descriptions in doc (#8403)

Replaced args name `storage` and `sourceStorage` to `source` in tensor.set_() to match the descriptions in docs.",20.0,7.0,"aten/doc/Tensor.h,aten/doc/Type.h,aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,test/test_torch.py",5.0,5,2,2.010231342,39.0,14423.0,5.0,1610899.8,1320.0,3748.805292,0.0,Non Functional,0.0,1
pytorch,66bbe5d75a55df31792a272553204bc8a2b4af9b,71ce3448d959ed97f77460d200dbdbdecd28cf12,Sam Gross,sgross@fb.com,Thu Jul 20 23:38:36 2017 -0700,1500593916.0,"Fix torch.inverse when magma is not available

Fixes #2156",1.0,0.0,test/test_cuda.py,1.0,1,1,0,33.0,936.0,1.0,173329.0,1230.0,15037.69861,0.0,Corrective,1.0,1
pytorch,8fbe003d4ed946804d67a6d3bcd84eb6c3df9a4a,71d731fb57d261c64bf700f8de2729ee1d2ee0cc,Ozan Caglayan,ozancag@gmail.com,Mon Oct 30 11:35:31 2017 +0100,1509363331.0,"Fix documentation inconsistencies for some loss classes

- The actual parameter is weight not weights
- Unify all mentions about batch_size -> N
- Unify all mentions about n_classes -> C",27.0,29.0,torch/nn/modules/loss.py,1.0,3,1,0,35.0,708.0,1.0,175822.0,2019.0,23882.35823,0.0,Corrective,1.0,1
pytorch,8ab101ccee8f87fb9fd9d35dd541002438cfd34a,71d73211f4b94ae00f88cfea8c8517831e94bbc8,Tongzhou Wang,SsnL@users.noreply.github.com,Fri Mar 09 04:02:38 2018 -0500,1520568158.0,"[ready] torch.* doc update for Variable/Tensor merge, and other improvements (#5443)

* 1. Update doc to reflect changes in Variable/Tensor merge, and new printing style
2. Remove functions in torch/functional.py that are already implemented with native_function
3. Add set_detault_tensor_type doc

* fix torch.split

* py2 unicode string fix

* update torch.gels doc

* address @fmassa 's comments

* double-colon",1061.0,831.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_autograd.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/__init__.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/autograd/variable.py,torch/functional.py",11.0,12,5,1.62415077,40.0,14763.0,7.0,127192.36363636365,2436.0,24755.85823,0.0,Corrective,1.0,1
pytorch,5816a95ca93112fb88c98a54551acb698c9a132a,723ba4e31d8aeab94f360f74fcbcab6e2622388e,Peter Bell,peterbell10@live.co.uk,Sat Mar 05 00:39:20 2022 -0800,1646440760.0,"CUDA Kernels: Use per-operator headers (1/4) (#71212)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71212

Splitting this into multiple PRs to keep the diffs more managable.

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D33949896

Pulled By: malfet

fbshipit-source-id: b11e5effa44d660932b8c21ccab6ece3e48e848c
(cherry picked from commit b866a2b5dafb8d8af061190080c367099c12b178)",280.0,45.0,"aten/src/ATen/TensorIterator.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/Bucketization.cpp,aten/src/ATen/native/BucketizationUtils.h,aten/src/ATen/native/TypeProperties.h,aten/src/ATen/native/cuda/Activation.cpp,aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu,aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu,aten/src/ATen/native/cuda/AmpKernels.cu,aten/src/ATen/native/cuda/AveragePool2d.cu,aten/src/ATen/native/cuda/AveragePool3d.cu,aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu,aten/src/ATen/native/cuda/Blas.cpp,aten/src/ATen/native/cuda/Bucketization.cu,aten/src/ATen/native/cuda/CUDAScalar.cu,aten/src/ATen/native/cuda/Col2Im.cu,aten/src/ATen/native/cuda/ComplexKernel.cu,aten/src/ATen/native/cuda/ConvolutionMM2d.cu,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/CrossKernel.cu,aten/src/ATen/native/cuda/CuFFTPlanCache.h,aten/src/ATen/native/cuda/CuFFTUtils.h,aten/src/ATen/native/cuda/DepthwiseConv2d.cu,aten/src/ATen/native/cuda/DepthwiseConv3d.cu,aten/src/ATen/native/cuda/DilatedMaxPool2d.cu,aten/src/ATen/native/cuda/DilatedMaxPool3d.cu,aten/src/ATen/native/cuda/DistanceKernel.cu,aten/src/ATen/native/cuda/im2col.cuh,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h,aten/src/ATen/native/im2col_shape_check.h",34.0,6,1,4.72790981,6.0,16768.0,25.0,9051963.38235294,1202.0,2949.0,0.0,,0.0,1
pytorch,2631da0822149c19214dba94510e29abb5b25f75,72822ee6b251118aefbe9af75ac0fcb4ee56a0ee,Edward Yang,ezyang@fb.com,Fri Sep 14 15:55:39 2018 -0700,1536940539.0,"Fix #11430 (CPU only builds raise opaque error message when calling .â¦ (#11533)

Summary:
â¦cuda())

While I was at it, I audited all other ways I know how we might get a CUDA
type from PyTorch and fixed more constructors which don't work.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11533

Differential Revision: D9775786

Pulled By: ezyang

fbshipit-source-id: cd07cdd375fdf74945539ec475a48bf08cbc0c17",50.0,19.0,"test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp,tools/autograd/templates/python_torch_functions_dispatch.h,torch/csrc/DynamicTypes.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/cuda_lazy_init.cpp,torch/csrc/utils/cuda_lazy_init.h",8.0,8,3,2.798935777,41.0,10882.0,8.0,4115148.875,4121.0,11528.33333,0.0,Corrective,1.0,1
pytorch,c1744a6c394178e7c05afdb68e809efe46d75c8a,7284f448ba6207a2416a9abceda0d38f057590ad,Brian Vaughan,bvaughan@fb.com,Fri Jun 14 17:44:22 2019 -0700,1560534262.0,"Fix handling of kwargs from common method invocations (#21499)

Summary:
When kwargs are specified in a test defined via common_method_invocations, it doesn't work if there isn't also a positional argument (`{'foo':'foo'}` without a positional arg generates a python call like: `self.method(, foo=foo)`, erroring on the `,`). I wanted to test something in a different PR and noticed I couldn't.

Also fixed some flake8 warnings I was seeing locally.

I replaced `lambda x: x` with `ident` since it seems a bit cleaner to me, but happy to revert that if others don't agree?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21499

Differential Revision: D15826974

Pulled By: nairbv

fbshipit-source-id: a3f37c80ba2303c7d9ae06241df06c7475b64e36",57.0,47.0,"test/common_methods_invocations.py,test/onnx/test_pytorch_onnx_caffe2.py,test/test_autograd.py,test/test_jit.py,test/test_jit_fuser.py",5.0,2,1,1.214079942,42.0,23790.0,5.0,528595.6,9423.0,27344.83333,0.0,Corrective,1.0,1
pytorch,3a56758e1fdbc928be3754d5b60e63c7fc55ea45,729f7cd52f3549e2065ecaeb7138e406f610bd98,Saketh Are,saketh@fb.com,Tue Jun 22 17:04:51 2021 -0700,1624381491.0,"Implement histogram operator on CPU (#58780)

Summary:
The existing [torch.histc](https://pytorch.org/docs/stable/generated/torch.histc.html) operator is limited in comparison to [numpy.histogram](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html). This PR adds torch.histogram on CPU. The new operator replicates numpy.histogram's behavior, including support for caller-specified bin edges and weights. It was motivated by previous community requests for histogram.

The implementation was [benchmarked](https://docs.google.com/spreadsheets/d/1xCR0jODchVvwdVSAjiLsNCkmyictA6j1LNfDpWOafjw/edit?usp=sharing) against numpy.histogram as well as torch.histc. This implementation is weakly faster than numpy.histogram across all types of inputs tested, and performs in line with torch.histc for the limited inputs histc supports.

mruberry

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58780

Test Plan:
Added unit tests, OpInfo for the new torch.histogram operator.

Tested execution time on a variety of input sizes and compared to numpy.histogram performance: https://docs.google.com/spreadsheets/d/1xCR0jODchVvwdVSAjiLsNCkmyictA6j1LNfDpWOafjw/edit?usp=sharing

Reviewed By: ezyang

Differential Revision: D29134626

Pulled By: saketh-are

fbshipit-source-id: f2773085de1697f6bc6ffdeffe9a81267f51bdfc",727.0,136.0,"aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/LegacyTHFunctionsCPU.h,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/Col2Im.cpp,aten/src/ATen/native/Histogram.cpp,aten/src/ATen/native/Histogram.h,aten/src/ATen/native/Im2Col.cpp,aten/src/ATen/native/LegacyNNDefinitions.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/HistogramKernel.cpp,aten/src/ATen/native/im2col.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/vol2col.h,docs/source/tensors.rst,docs/source/torch.rst,test/test_namedtuple_return_api.py,test/test_reductions.py,tools/build_variables.bzl,tools/codegen/api/python.py,tools/codegen/gen.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",26.0,15,5,2.882607339,37.0,50120.0,16.0,5236449.956521739,13217.0,29907.0,0.0,Feature Addition,0.0,1
pytorch,84cf3372d16626f19c7dd95ed08b8ca91d661667,72bc3d9de4f8baa28ebc8f9399ad86cefe89d9f5,Michael Carilli,mcarilli@gmail.com,Thu Oct 01 14:48:56 2020 -0700,1601563736.0,"Use MTA for amp grad unscaling, enforce op math type in MTA functors, and allow op lambdas (#44778)

Summary:
Amp gradient unscaling is a great use case for multi tensor apply (in fact it's the first case I wrote it for).  This PR adds an MTA unscale+infcheck functor.  Really excited to have it for `torch.cuda.amp`. izdeby your interface was clean and straightforward to use, great work!

Labeled as bc-breaking because the native_functions.yaml exposure of unscale+infcheck changes from [`_amp_non_finite_check_and_unscale_` to `_amp_foreach_non_finite_check_and_unscale_`]( https://github.com/pytorch/pytorch/pull/44778/files#diff-f1e4b2c15de770d978d0eb77b53a4077L6289-L6293).

The PR also modifies Unary/Binary/Pointwise Functors to
- do ops' internal math in FP32 for FP16 or bfloat16 inputs, which improves precision ([and throughput, on some architectures!](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions)) and has no downside for the ops we care about.
- accept an instantiated op functor rather than an op functor template (`template<class> class Op`).  This allows calling code to pass lambdas.

Open question:  As written now, the PR has MTA Functors take care of pre- and post-casting FP16/bfloat16 inputs to FP32 before running the ops.  However, alternatively, the pre- and post-math casting could be deferred/written into the ops themselves, which gives them a bit more control.  I can easily rewrite it that way if you prefer.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44778

Reviewed By: gchanan

Differential Revision: D23944102

Pulled By: izdeby

fbshipit-source-id: 22b25ccad5f69b413c77afe8733fa9cacc8e766d",498.0,176.0,"aten/src/ATen/native/ForeachUtils.h,aten/src/ATen/native/cuda/AmpKernels.cu,aten/src/ATen/native/cuda/ForeachBinaryOpList.cu,aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu,aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu,aten/src/ATen/native/cuda/ForeachFunctors.cuh,aten/src/ATen/native/cuda/ForeachPointwiseOp.cu,aten/src/ATen/native/cuda/ForeachUnaryOp.cu,aten/src/ATen/native/cuda/MultiTensorApply.cuh,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py,test/test_cuda.py,test/test_foreach.py,tools/codegen/model.py,torch/cuda/amp/grad_scaler.py",15.0,12,4,3.000807308,44.0,15037.0,7.0,1894777.8,5641.0,13250.5,0.0,Feature Addition,0.0,1
pytorch,cc8f6f56dc8b948896a84f87a56c2298de1288d9,72e58a756c536600f8725aadd38e71e177fb88b7,rzou,zou3519@gmail.com,Fri Dec 08 02:05:20 2023 -0800,1702001120.0,"Set markDynamoStrictTest in functorch/test_vmap.py (#115274)

We set markDynamoStrictTest in most of functorch/test_vmap.py. This
revealed many existing failing tests, so we mark those all as expected
failures or skip them.

Test Plan:
- CI
Pull Request resolved: https://github.com/pytorch/pytorch/pull/115274
Approved by: https://github.com/guilhermeleobas, https://github.com/kshitij12345
ghstack dependencies: #115267, #115276, #115268",67.0,1.0,test/functorch/test_vmap.py,1.0,2,1,0,1.0,5339.0,1.0,572953.0,22844.0,51876.0,0.0,,0.0,1
pytorch,f0006315a9205d51e7e92a67fb1fd0a39c7fe483,7328710cbce459bbd77dfe424acc94010725d979,Scott Wolchok,swolchok@fb.com,Tue Feb 02 07:09:26 2021 -0800,1612249766.0,"[PyTorch][codemod] Replace immediately-dereferenced cast calls w/castRaw (#50229)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50229

`fastmod -m 'cast(<((at|c10)::)?\w+Type>\(\)\s*)->' 'castRaw${1}->'` Presuming it builds, this is a safe change: the
result of `cast()` wasn't being saved anywhere, so we didn't need
it, so we can use a raw pointer instead of a new `shared_ptr`.
ghstack-source-id: 120769170

Test Plan: CI

Reviewed By: SplitInfinity

Differential Revision: D25837494

fbshipit-source-id: 46319100dc0dfc78f6d2b45148207f83481f2ada",44.0,41.0,"aten/src/ATen/core/function_schema.h,aten/src/ATen/core/ivalue.cpp,aten/src/ATen/core/jit_type.h,aten/src/ATen/core/jit_type_base.h,aten/src/ATen/core/type.cpp,test/cpp/jit/test_gpu.cpp,torch/csrc/distributed/rpc/rref_impl.cpp,torch/csrc/jit/frontend/function_schema_parser.cpp,torch/csrc/jit/frontend/schema_matching.cpp,torch/csrc/jit/ir/alias_analysis.cpp,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx/fold_if_node.cpp,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp,torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/passes/tensorexpr_fuser.cpp,torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp,torch/csrc/jit/serialization/unpickler.cpp,torch/csrc/jit/tensorexpr/kernel.cpp",20.0,19,3,3.929050684,12.0,25318.0,14.0,1018373.55,8543.0,19250.5,0.0,,0.0,1
pytorch,eb314f9b1ac2aa2a9fa11e75ccbe7ff7b25fe299,73422512817f3b0c9b9c946eced5f2ab6fae1e99,Richard Zou,zou3519@gmail.com,Wed Dec 07 22:14:56 2022 -0800,1670451296.0,"functorch.grad support for autograd.Function (#89860)

Happy to split this PR more if it helps.

This PR adds functorch.grad support for autograd.Function. There's a lot
going on; here is the high level picture and there are more details as
comments in the code.

Mechanism (PyOperator)
- Somehow, autograd.Function needs to dispatch with functorch. This is
necessary because every layer of functorch needs to see the
autograd.Function; grad layers need to preserve the backward pass.
- The mechanism for this is via PyOperator. If functorch transforms are
active, then we wrap the autograd.Function in a `custom_function_call`
PyOperator where we are able to define various rules for functorch
transforms.
- `custom_function_call` has a rule for the functorch grad transform.

autograd.Function changes
- I needed to make some changes to autograd.Function to make this work.
- First, this PR splits autograd.Function into a _SingleLevelFunction
(that works with a single level of functorch transform) and
autograd.Function (which works with multiple levels). This is necessary
because functorch's grad rule needs some way of specifying a backward
pass for that level only.
- This PR changes autograd.Function's apply to eitehr call
`custom_function_call` (if functorch is active) or super().apply (if
functorch isn't active).

Testing
- Most of this PR is just testing. It creates an autograd.Function
OpInfo database that then gets passed to the functorch grad-based tests
(grad, vjp, vjpvjp).
- Since functorch transform tests are autogenerated from OpInfo tests,
this is the easiest way to test various autograd.Function with
functorch.

Future
- jvp and vmap support coming next
- better error message (functorch only supports autograd.Function that
have the optional setup_context staticmethod)
- documentation to come when we remove the feature flag

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89860
Approved by: https://github.com/soulitzer",715.0,78.0,"aten/src/ATen/functorch/ADInterpreters.cpp,aten/src/ATen/functorch/DynamicLayer.cpp,aten/src/ATen/functorch/DynamicLayer.h,test/functorch/common_utils.py,test/functorch/test_eager_transforms.py,test/functorch/test_ops.py,torch/_C/_functorch.pyi,torch/_dynamo/variables/builder.py,torch/_functorch/autograd_function.py,torch/_functorch/utils.py,torch/autograd/function.py,torch/csrc/autograd/python_function.cpp,torch/csrc/functorch/init.cpp,torch/testing/_internal/autograd_function_db.py",14.0,17,3,2.381005518,41.0,10335.0,6.0,1371662.25,10338.0,23667.5,0.0,Feature Addition,1.0,1
pytorch,5136ed0e44c65cb3747a1f22f77ccf09d54c125c,735463f210584d44ca3460d7319f5eaa213842c4,Lara,lahaidar@microsoft.com,Wed Oct 16 18:16:56 2019 -0700,1571249816.0,"ONNX Export Scripted Interpolate Op (#27566)

Summary:
We currently support exporting traced interpolate ops to ONNX.

Scripting interpolate op invokes aten::__interpolate in the Torch IR (instead of aten::upsample_[mode][dim]d), which we do not support yet.
This PR implements the ONNX symbolic for __interpolate() to support exporting interpolate in scripting scenarios.

Related open issue: https://github.com/pytorch/pytorch/issues/25807
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27566

Reviewed By: hl475

Differential Revision: D17817731

Pulled By: houseroad

fbshipit-source-id: e091793df503e2497f24821cf2954ff157492c75",171.0,24.0,"test/onnx/expect/TestOperators.test_upsample_nearest.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",8.0,5,2,2.463725603,6.0,5732.0,7.0,516423.625,12337.0,34463.33333,0.0,,0.0,1
pytorch,16cac6442a1b9f6328e39ccd283694cb7163fc10,7355c63845de41a0468970a0403728a4dc0e16b9,soumith,soumith@fb.com,Tue Nov 01 04:26:11 2016 -0700,1477974371.0,adding multiple types for dist,56.0,52.0,"THCTensorMath.h,THCTensorMath2.cu,THCTensorMathReduce.cuh,THCTensorMathScan.cu,generic/THCTensorMathReduce.cu,generic/THCTensorMathReduce.h",6.0,1,1,2.16620416,28.0,1327.0,3.0,185556.8333333333,141.0,1820.655843,0.0,Feature Addition,0.0,1
pytorch,fe810edc805f5a377bdf71e71ac5eab2bb3d3566,7363736c50b0349728daa9b073bb37cfdb421243,Tongzhou Wang,SsnL@users.noreply.github.com,Wed Feb 14 20:00:55 2018 -0500,1518638455.0,"Fix THC multinomial stride usage; (#5238)

Improve multinomial test",29.0,13.0,"aten/src/THC/THCReduceApplyUtils.cuh,aten/src/THC/THCTensorRandom.cuh,test/test_torch.py",3.0,4,2,0.946372936,38.0,6099.0,3.0,3227655.333333333,2370.0,24616.35823,0.0,Corrective,1.0,1
pytorch,78994d13c083cbc7a04f368f0805de6edd6b7238,7397683b575f2feea78219d174a0e1db1d5e1dd2,soulitzer,soulitzer@gmail.com,Mon Jan 10 21:40:07 2022 -0800,1641850807.0,"Add forward AD formulas for mv, scatter_add, _s_where (#70468)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/70468

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D33405364

Pulled By: soulitzer

fbshipit-source-id: 7681c33fb264a7a3ec6436ebb7c5bb07cd5ffc3d",54.0,34.0,"test/test_autograd.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",3.0,6,3,0.526263714,43.0,27508.0,2.0,679603.0,18242.0,43221.0,0.0,Feature Addition,0.0,1
pytorch,c4eafb056915f14ca064d2c429b1918c5cfd4878,73acfe7efec16d7430f9c4cd7aa68f07aacbd13e,Richard Zou,zou3519@users.noreply.github.com,Mon Jul 11 18:37:00 2022 -0400,1657564620.0,"[functorch] Remove removable randomness skips (pytorch/functorch#953)

The jvp and vjp transforms should not change randomness behavior; e.g.
dropout under vjp and with regular PyTorch autograd should produce the
same values. vmap however does change randomness behavior.

This PR removes a bunch of randomness skips from jvp and vjp only tests
and also fixes our implementation of dropout such that it maintains the
above property.

Test Plan:
- run tests",13.0,42.0,"functorch/functorch/csrc/PyTorchOperatorHacks.cpp,functorch/test/test_ops.py",2.0,4,1,0.721928095,1.0,1734.0,2.0,0.5,1154.0,1553.5,0.0,Corrective,1.0,1
pytorch,a6789074fce4589ef6b81817b56663b585f377ec,73f009a2aae598d5cb6aba371f93a489a3233fd3,albanD,desmaison.alban@gmail.com,Wed Sep 02 16:18:14 2020 -0700,1599063494.0,"refactor manual function definitions (#43711)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/43711

this makes them available in forward if needed

No change to the file content, just a copy-paste.

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D23454146

Pulled By: albanD

fbshipit-source-id: 6269a4aaf02ed53870fadf8b769ac960e49af195",3089.0,2867.0,".github/workflows/lint.yml,caffe2/CMakeLists.txt,tools/autograd/templates/Functions.cpp,tools/autograd/templates/VariableType.cpp,tools/build_variables.bzl,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h",7.0,9,4,1.209375279,23.0,5284.0,5.0,633628.8,4778.0,11115.0,0.0,Perfective,0.0,1
pytorch,7b61b458b1ff46a62e7b2687ba4a694be674778c,74043b69c23113e399917ef047c255f31cb82bda,Richard Zou,zou3519@users.noreply.github.com,Sat Mar 10 04:46:42 2018 -0500,1520657202.0,"Alias torch.diagonal, torch.diagflat (#5622)

* Alias torch.diagonal, torch.diagflat

* Address comments; Add sanity tests for torch.diagonal and torch.diagflat",179.0,1.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/test_cuda.py,test/test_torch.py,torch/_torch_docs.py",5.0,6,3,1.499461772,38.0,14037.0,4.0,59775.6,2438.0,24759.85823,0.0,Feature Addition,0.0,1
pytorch,20c516ac182c0176bc821a36cc2bf05a91e3f6b6,742912512c3fdb9ae08022ff7cf05639cc4ae7b9,Tongzhou Wang,SsnL@users.noreply.github.com,Fri Jun 08 15:37:46 2018 -0400,1528472266.0,"Move signal window functions to ATen; add Blackman window (#8130)

* Move signal window functions to ATen; add Blackman window

* fix cuda test not checking scipy",313.0,194.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_cuda.py,test/test_torch.py,torch/_torch_docs.py,torch/functional.py",7.0,8,4,2.181920233,40.0,16935.0,6.0,1188539.142857143,2710.0,25186.85823,0.0,Corrective,1.0,1
pytorch,87b67028337a08bb2d9bfe92e7244a01c9562b3d,748285ccd76b67db7ebc0bc099789bb09c23c038,kshitij12345,kshitijkalambarkar@gmail.com,Tue Mar 02 05:55:34 2021 -0800,1614664534.0,"[complex] add autograd support for torch.polar (#52488)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/33152

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52488

Reviewed By: zou3519

Differential Revision: D26711841

Pulled By: anjali411

fbshipit-source-id: b8538fb8cb44456b832e4f993cf41954b3ddd2e8",34.0,4.0,"tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",5.0,7,2,1.90949057,14.0,10108.0,5.0,967055.0,9323.0,20675.5,0.0,Feature Addition,0.0,1
pytorch,8995ddda05c4bdda8912d63aa4bdc18123e13606,749d51414af11da48bdb195375f702822450aded,gchanan,gregchanan@gmail.com,Thu Apr 12 18:05:44 2018 -0400,1523556344.0,"Separate cuda-ness from dtype. (#6470)

* Separate cuda-ness from dtype.

There are no longer torch.cuda.int64, etc; only torch.int64 that correspond to at::ScalarType.
At the python arg parser level, the corresponding ATen type is selected from the combination of (ScalarType, Layout, Device).

There is also currently unused code in here for support ScalarType in native_functions; this will be used for specifying aggregate types
on reduction functions.

* Fix test_autograd.

* Add defaults to randint_like.

* Track is_cuda in py tensor types.

* Fix test_sparse.

* Fix multiprocessing.

* Fix rnn.

* Fix test_nn.

* Fix flake8.",298.0,253.0,"aten/src/ATen/native/native_functions.yaml,test/test_autograd.py,test/test_cuda.py,test/test_multiprocessing.py,test/test_nn.py,test/test_sparse.py,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_torch_functions.cpp,tools/autograd/templates/python_variable_methods.cpp,tools/jit/gen_jit_dispatch.py,torch/backends/cudnn/rnn.py,torch/csrc/Device.cpp,torch/csrc/Dtype.cpp,torch/csrc/Dtype.h,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Module.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/device.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_devices.h,torch/csrc/utils/tensor_dtypes.cpp,torch/csrc/utils/tensor_new.cpp,torch/nn/_functions/rnn.py,torch/testing/__init__.py",30.0,19,4,3.994299791,41.0,27126.0,13.0,603923.6551724138,872.0,2093.305292,0.0,Corrective,1.0,1
pytorch,9b527b35bb54689df80422ec0278ea357db654b4,74a0663afdb4b0f24f5e125a38fc7f596ef2dab8,Edward Yang,ezyang@fb.com,Tue Mar 03 19:44:37 2020 -0800,1583264677.0,"In torch_test, mark every test that takes >5s on a DEBUG CPU-only build as slow test (#33901)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33901

After this change, the pytest profile looks like:

4.83s call     test/test_torch.py::TestTorch::test_fft_ifft_rfft_irfft
4.23s call     test/test_torch.py::TestTorch::test_var_dim
4.22s call     test/test_torch.py::TestTorch::test_std_dim
4.19s call     test/test_torch.py::TestTorch::test_max
4.06s call     test/test_torch.py::TestTorch::test_min
3.60s call     test/test_torch.py::TestTorchDeviceTypeCPU::test_cdist_norm_batch_cpu
2.62s call     test/test_torch.py::TestTorchDeviceTypeCPU::test_pow_cpu
2.60s call     test/test_torch.py::TestTorch::test_matmul_small_brute_force_1d_Nd

And the entire CPU-only test suite can be run in 88s on my Intel(R) Xeon(R) CPU
E5-2650 v4 @ 2.20GHz

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Differential Revision: D20222288

Pulled By: ezyang

fbshipit-source-id: 4224a9117f42566e290ae202881d76f1545cebec",12.0,0.0,test/test_torch.py,1.0,1,1,0,40.0,15491.0,1.0,10424.0,15158.0,40744.33333,0.0,Corrective,1.0,1
pytorch,21b7af1e7b986e3f0e19b15390bb89e2b9314f56,74c00b1f6977f2f6b01e6f218f051d43f8163b02,Michael Ranieri,mranieri@fb.com,Wed Apr 29 04:36:18 2020 -0700,1588134978.0,"move to explicit avx2 switching (#37207)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37207

The main idea here is to try and give build system more flexibility on when various AVX instruction are defined, previously it was based solely on compiler defined preprocessor flags.

Here we re-use `CPU_CAPABILITY` which already needs to be defined for each pass in `[""DEFAULT"", ""AVX"", ""AVX2""]` over the source files.

To give a slightly more concrete reason this is needed, is that we have not found a way to override `/arch` flags previously specified on the command line from visual studio (causing us to duplicate symbols in some cases).

Test Plan: CI green

Differential Revision: D21218512

fbshipit-source-id: f628153f5f3d83cd6bd4a5283fb0dc751a58ebf9",78.0,78.0,"aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/cpu/DistributionTemplates.h,aten/src/ATen/native/cpu/avx_mathfun.h,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",13.0,10,1,2.897212535,8.0,9125.0,10.0,1848021.0,1497.0,3961.0,0.0,,0.0,1
pytorch,c2bdda1224c999326e6b7a4bc76e6bbaeb312176,74d1bb54e67e32f423a4c1f9521b08f7edb67e14,Dhanton,dhantonrevolution@gmail.com,Mon Nov 06 17:26:04 2017 +0100,1509989164.0,Add single argument version of torch.arange (#3494),37.0,8.0,"aten/src/ATen/Declarations.cwrap,test/test_torch.py,torch/_torch_docs.py,torch/csrc/generic/methods/Tensor.cwrap",4.0,8,3,1.731517801,38.0,14868.0,3.0,379531.75,166.0,12854.22461,0.0,Feature Addition,0.0,1
pytorch,0e106fce9ceae146193f57192a6b7cba02013447,7513455c743d3d644b45a804902c1a0d14b69f45,Winston Smith,76181208+imaginary-person@users.noreply.github.com,Mon Apr 19 11:19:06 2021 -0700,1618831146.0,"Make tensordot resize output tensor's size if out= argument is specified & make it safely cast & copy output (#56286)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/56022.
Fixes https://github.com/pytorch/pytorch/issues/56316

For `torch.tensordot`,
1. `tensordot`'s out variant now resizes the output tensor provided as the `out` argument if necessary.
2. Added a check to verify if the output tensor provided as the argument for `out` is on the same device as the input tensors.
3. Added a check to verify if the dtype of the result is castable to the dtype of the output tensor provided as an argument for `out`.
4. Because of (2) & (3), `tensordot`'s out variant now [safely casts & copies output](https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch).
5. `test_tensordot` in `test_linalg.py` had a bug - the output tensor wasn't being defined to be on the same device as the input tensors. It was fixed by simply using a `device` argument in its definition.
6. Added an `OpInfo` for `tensordot` and modified the `OpInfo` for `inner`.

cc heitorschueroff mruberry

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56286

Reviewed By: ngimel

Differential Revision: D27845980

Pulled By: mruberry

fbshipit-source-id: 134ab163f05c31a6900dd65aefc745803019e037",55.0,15.0,"aten/src/ATen/native/Linear.cpp,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",3.0,8,3,1.069406651,8.0,13779.0,3.0,675791.6666666666,10973.0,24194.5,0.0,Corrective,1.0,1
pytorch,08c90d9e55919e3158c4b8351f887a167dcd00a3,7526e38cd3000efe3290a46597bc25f3215c1c65,Xinyu Li,lixinyu@fb.com,Wed Jan 20 02:18:34 2021 -0800,1611109114.0,"Revert ""Stable sort for CPU (#50052)"" (#50752)

Summary:
This reverts commit c99f35605105f7366bcf4709df534da3ceab9a15.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50752

Reviewed By: zou3519

Differential Revision: D25958146

Pulled By: glaringlee

fbshipit-source-id: f4068d038f9bd337bac8b673eaeb46a4646f6c77",32.0,106.0,"aten/src/ATen/LegacyTHFunctionsCUDA.h,aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp,aten/src/ATen/native/CompositeRandomAccessorCommon.h,aten/src/ATen/native/NamedTensor.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/Sorting.h,aten/src/ATen/native/cpu/SortingKernel.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/TensorCompare.cpp,test/backward_compatibility/check_backward_compatibility.py,test/test_sort_and_select.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",16.0,14,4,3.534401491,45.0,38415.0,6.0,194197.9375,8190.0,18513.5,0.0,,0.0,1
pytorch,d7aaa3327bf9ad8757897a76879230da92bf607f,752f433a2484db25f076b2fc85c40ab191656bd9,Sinan Nasir,sinannasir@fb.com,Mon Aug 10 20:53:46 2020 -0700,1597092826.0,"DDP communication hook: skip dividing grads by world_size if hook registered. (#42400)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42400

mcarilli spotted that in the original DDP communication hook design described in [39272](https://github.com/pytorch/pytorch/issues/39272), the hooks receive grads that are already predivided by world size.

It makes sense to skip the divide completely if hook registered. The hook is meant for the user to completely override DDP communication. For example, if the user would like to implement something like GossipGrad, always dividing by the world_size would not be a good idea.

We also included a warning in the register_comm_hook API as:
> GradBucket bucket's tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.
ghstack-source-id: 109548696

**Update:** We discovered and fixed a bug with the sparse tensors case. See new unit test called `test_ddp_comm_hook_sparse_gradients` and changes in `reducer.cpp`.

Test Plan: python test/distributed/test_c10d.py and perf benchmark tests.

Reviewed By: ezyang

Differential Revision: D22883905

fbshipit-source-id: 3277323fe9bd7eb6e638b7ef0535cab1fc72f89e",99.0,38.0,"test/distributed/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/nn/parallel/distributed.py",4.0,8,2,1.2745315,19.0,6621.0,1.0,241627.0,4194.0,9835.5,0.0,Corrective,1.0,1
pytorch,e17cf93b9a1cbf922b46e87be10109c8dfb127dc,7539ea0207859d5e20f6e836818bfafa77f3045e,Nick Gibson,nickg@fb.com,Thu Apr 16 06:54:22 2020 -0700,1587020062.0,"[TensorExpr] Add simplification of length 0 and 1 For loops to IR Simplifier (#36348)

Summary:
Simplifies loops which can be collapsed down into a single block or removed entirely. E.g.

```
For 0..1 {
  Statements...
}
```

Is now just `Block({Statements...})`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36348

Differential Revision: D21057959

Pulled By: nickgg

fbshipit-source-id: 2f95a19a965c4a6e023680e2cea9ea846e82d62e",281.0,0.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/stmt.h",5.0,7,2,0.811192636,2.0,4360.0,3.0,343162.8,1112.0,2911.5,0.0,Feature Addition,0.0,1
pytorch,02e7eba309f42c65c77945c5c7ca793859c50bc5,7592e965037122fc3edb3dc20c8bc128ab7ffe4f,Zhi-Qiang Zhou,zzq_890709@hotmail.com,Tue Jan 02 03:14:41 2018 +0800,1514862881.0,"More detailed documentation. (#4428)

* More detailed documentation.

* More detailed documentation.

* Fixed W291

* minor bug fixes",91.0,24.0,torch/nn/modules/loss.py,1.0,3,1,0,35.0,795.0,1.0,1232908.0,2235.0,24315.85823,0.0,Corrective,1.0,1
pytorch,19943aafe90558a209857a188191df161fd32a6f,75995e4bf621a8ae079ba0faa12e25a4ad4fa002,BowenBao,bowbao@microsoft.com,Wed Apr 21 05:58:06 2021 -0700,1618984686.0,"[ONNX] Add support for hann_window operator. (#54587) (#56163)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56163

* [ONNX] Improve index_put symbolic to handle singular Bool updates (#53690)

Adds support for cases where the updates to the index_put node is a single Bool value, such as the case shown below

```
mask[indices] = True
```

Fixes #53507

* [ONNX] Support primitive type input/outputs and attributes (#53550)

Support primitive type attributes. Needed for Silero model.

* [ONNX] Fix if output shape mismatch error & Fix graph input directly used as output (#53219)

Fix if output shape mismatch error & Fix graph input directly used as output

* Add support for hann_window operator.

* [ONNX] Replace decomposeLinear pre process pass with a symbolic (#53077)

Replace decomposeLinear pre process pass with a symbolic

* Add a test case for dtype is None.

* Resolve flake8 issue.

* Remove one unused test case.

* Add support for hann_window operator.

* Add a test case for dtype is None.

* Remove one unused test case.

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D27866145

Pulled By: SplitInfinity

fbshipit-source-id: e0b43df9ecd1a95cd7ac297213aba453bbaf2913

Co-authored-by: Shubham Bhokare <32080845+shubhambhokare1@users.noreply.github.com>
Co-authored-by: Negin Raoof <neginmr@utexas.edu>
Co-authored-by: Bowen Bao <bowbao@microsoft.com>
Co-authored-by: Ksenija Stanojevic <KsenijaS@users.noreply.github.com>",78.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.117656214,3.0,11818.0,3.0,1375493.3333333333,11059.0,24396.0,0.0,Corrective,1.0,1
pytorch,afe6b4c8ee7c494c259bbcf8585ffb30a416d924,75a2a92b02e53efdeb7d21adeafa47cf37b603b6,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Sun May 02 01:45:19 2021 -0700,1619919919.0,"Add torch.linalg.cholesky_ex without checking for errors by default (#56724)

Summary:
The new function has the following signature `cholesky_ex(Tensor input, *, bool check_errors=False) -> (Tensor L, Tensor infos)`. When `check_errors=True`, an error is thrown if the decomposition fails; `check_errors=False` - responsibility for checking the decomposition is on the user.

When `check_errors=False`, we don't have host-device memory transfers for checking the values of the `info` tensor.

Rewrote the internal code for `torch.linalg.cholesky`. Added `cholesky_stub` dispatch. `linalg_cholesky` is implemented using calls to `linalg_cholesky_ex` now.

Resolves https://github.com/pytorch/pytorch/issues/57032.

Ref. https://github.com/pytorch/pytorch/issues/34272, https://github.com/pytorch/pytorch/issues/47608, https://github.com/pytorch/pytorch/issues/47953

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56724

Reviewed By: ngimel

Differential Revision: D27960176

Pulled By: mruberry

fbshipit-source-id: f05f3d5d9b4aa444e41c4eec48ad9a9b6fd5dfa5",440.0,134.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/backward_compatibility/check_backward_compatibility.py,test/test_linalg.py,test/test_namedtuple_return_api.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",16.0,15,5,2.825120973,16.0,38343.0,12.0,265414.4375,11561.0,26171.5,0.0,Feature Addition,1.0,1
pytorch,5c809de4b4663c688ebc2cd389c2c866aa22f6e5,75c11d62b7c3f4a04b954eaa0c9fc6e21a3d80f3,Sam Gross,colesbury@gmail.com,Fri Dec 08 18:05:51 2017 -0500,1512756351.0,Implement Variable.__invert__ (#4082),23.0,1.0,"test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp",2.0,4,2,0.870864469,38.0,5511.0,2.0,76964.5,368.0,1108.405869,0.0,,0.0,1
pytorch,5000a05724136e259e0977a5e74edf08196d2f7e,75cf0faf4cca86270d90f4a13aaa91499bc0bc94,Ailing,ailzhang@users.noreply.github.com,Sun May 20 12:59:02 2018 +0800,1526821142.0,Implement __reduce__ for torch.dtype (#7699),21.0,1.0,"test/test_torch.py,torch/csrc/Dtype.cpp",2.0,3,2,0.845350937,39.0,7413.0,2.0,1710019.5,1072.0,7003.172317,0.0,,0.0,1
pytorch,e221536ad81241e8c46ee38ce351276a2d462ba4,75f1989bec555ea21836ad1906ac373f6434199c,Uridah Sami Ahmed,11beseuahmed@seecs.edu.pk,Mon Mar 27 00:37:01 2017 +0500,1490575021.0,Add nn.Bilinear and tests,148.0,4.0,"test/test_nn.py,torch/nn/_functions/linear.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/linear.py",5.0,5,2,1.884366772,28.0,3717.0,1.0,7611.0,663.0,8050.211758,0.0,Feature Addition,0.0,1
pytorch,912d3626c8f834dc7d66ed6fd2d3b27ec77e39ed,75f49befeb922a577a1e0e38f26e963b57c1f8ea,Roy Li,royboy@fb.com,Thu Sep 13 19:25:20 2018 -0700,1536866720.0,"move instance_norm to aten (#10792)

Summary:
This also removes the usage of torch.onnx.symbolic_override in instance_norm. Fixes #8439.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10792

Differential Revision: D9800643

Pulled By: li-roy

fbshipit-source-id: fa13a57de5a31fbfa2d4d02639d214c867b9e1f1",73.0,60.0,"aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/native_functions.yaml,test/test_jit.py,torch/nn/functional.py,torch/onnx/symbolic.py",5.0,8,3,2.019914584,36.0,14161.0,4.0,95631.6,4093.0,11425.83333,0.0,Corrective,1.0,1
pytorch,e1cd220b901c7e1bb37bf822c356ba3f8e6b3134,760679352e8a6fda08b55c7e0cf2c916c7ed6d53,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Fri Sep 14 01:19:24 2018 -0700,1536887964.0,"Move Pixel Shuffle to ATen (#9721)

Summary:
<del>#9692 </del>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9721

Differential Revision: D8955829

Pulled By: SsnL

fbshipit-source-id: 4f4d1c7720b6f757fbef9a10f70209ae76f61399",57.0,34.0,"aten/src/ATen/native/PixelShuffle.cpp,aten/src/ATen/native/native_functions.yaml,torch/nn/functional.py,torch/nn/modules/pixelshuffle.py",4.0,7,2,1.616537243,35.0,4675.0,3.0,1228521.6666666667,4107.0,11488.83333,0.0,,0.0,1
pytorch,4c1d9e58c24fece936cf87aae6c1a57eda1cddaf,76147b897cd1ec1486430a6bc7a9ef8521c2da86,BowenBao,bowbao@microsoft.com,Fri Mar 12 10:42:06 2021 -0800,1615545726.0,"[ONNX] Update assign output shape for nested structure and dict output (#52893) (#53311)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53311

Fixes dict output & nested tuple.

Test Plan: Imported from OSS

Reviewed By: pbelevich, malfet

Differential Revision: D26922426

Pulled By: SplitInfinity

fbshipit-source-id: c2c6b71c8d978b990181e0b025626dbf6ef2199e",151.0,111.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/shape_type_inference.cpp",2.0,7,2,0.644080442,3.0,9087.0,2.0,10.0,9711.0,21498.0,0.0,Corrective,1.0,1
pytorch,49df8993c4ad4f87352c32f4d11854983e7fcecc,76214bb4641fd75fe4ab0e87b05f90462de85fdc,anjali411,chourdiaanjali123@gmail.com,Thu Apr 22 13:59:45 2021 -0700,1619099985.0,"Add OpInfo for torch.baddbmm (#56502)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56502

Reviewed By: heitorschueroff

Differential Revision: D27890939

Pulled By: anjali411

fbshipit-source-id: 072647a05cf93aedb76df0367af71b534be77258",44.0,21.0,"test/test_autograd.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.779349837,42.0,14375.0,2.0,80568.5,11143.0,24662.0,0.0,Feature Addition,0.0,1
pytorch,5dbcd5638b055aeeb060ada7b1b37efcbe292787,7630f407cccf3069764c143d60f2311d871529e1,Philip Meier,github.pmeier@posteo.de,Thu Aug 05 17:42:43 2021 -0700,1628185363.0,"add `OpInfo` for `torch.nn.functional.grid_sample` (#62311)

Summary:
Addresses facebookresearch/functorch#78.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62311

Reviewed By: malfet

Differential Revision: D30013388

Pulled By: zou3519

fbshipit-source-id: 0887ae9935923d928bfeb59054afe1aab954b40b",48.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8526.0,1.0,902.0,14436.0,33009.5,0.0,Feature Addition,0.0,1
pytorch,c5fdcd85c7570b654eec45b6cba7cc75b0cf8f6b,7646f3c77f01f6d5439a7c056b5160fa97f8bc39,Zhijian Liu,zhijianliu.cs@gmail.com,Thu Jul 23 22:52:41 2020 -0700,1595544761.0,"Fix type annotation for CosineAnnealingLR (#41866)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/41866

Reviewed By: izdeby

Differential Revision: D22703576

Pulled By: mrshenli

fbshipit-source-id: 10a0f593ffaaae82a2923a42815c36793a9043d5",1.0,1.0,torch/optim/lr_scheduler.pyi,1.0,2,1,0,2.0,40.0,1.0,23178.0,3801.0,9011.5,0.0,Corrective,1.0,1
pytorch,e4fba752cb6ab11e8212ddd4d81778fea3cbf264,764bf826e33e667cde84060b7ea25154d0fe4d50,Dmytro Dzhulgakov,dzhulgakov@fb.com,Fri Sep 27 20:43:12 2019 -0700,1569616992.0,"Remove fbgemm_is_cpu_supported in favor of torch.backends.quantized.supported_qengines (#26840)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26840

Cleaning up top-level namespace. Also cosmetic changes to torch.backends.quantized

Test Plan: Imported from OSS

Differential Revision: D17604403

Pulled By: dzhulgakov

fbshipit-source-id: c55af277ea7319d962a82a6120f65ccd47a60abc",125.0,137.0,"aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/core/OpsAlreadyMovedToC10.cpp,aten/src/ATen/native/QuantizedLinear.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/qadd.cpp,aten/src/ATen/native/quantized/cpu/qpool.cpp,aten/src/ATen/native/quantized/cpu/qrelu.cpp,test/backward_compatibility/check_backward_compatibility.py,test/common_quantized.py,test/test_jit.py,test/test_nn.py,test/test_quantization.py,test/test_quantized.py,test/test_quantized_models.py,test/test_quantized_nn_mods.py,test/test_quantizer.py,test/test_torch.py,torch/backends/quantized/__init__.py",19.0,12,3,3.525663619,46.0,57170.0,16.0,317560.5789473684,11847.0,33175.33333,0.0,,0.0,1
pytorch,88ebdbc97c103271766203df6662240e95a09b42,764eae9c4e9b3187cda61482642703a2828923f9,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Tue Mar 19 17:14:28 2024 +0000,1710868468.0,"Revert ""Add Flash Attention support on ROCM (#121561)""

This reverts commit a37e22de7059d06b75e4602f0568c3154076718a.

Reverted https://github.com/pytorch/pytorch/pull/121561 on behalf of https://github.com/huydhn due to Sorry for reverting your change but this needs more work to be able to land in fbcode because https://github.com/ROCm/aotriton is not available there atm.  We are working to reland this change before 2.3 release ([comment](https://github.com/pytorch/pytorch/pull/121561#issuecomment-2007717091))",326.0,264.0,"CMakeLists.txt,aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip,caffe2/CMakeLists.txt,cmake/Dependencies.cmake,cmake/External/aotriton.cmake,cmake/External/oort.cmake,test/test_transformers.py,torch/testing/_internal/common_cuda.py",9.0,15,5,1.431353923,73.0,10510.0,4.0,455510.2222222223,26365.0,62613.0,0.0,Feature Addition,0.0,1
pytorch,35d4fa444b67cbcbe34a862782ddf2d92f5b1ce7,7656ef73f1ae73798ae965da6dedd260b7cb4f01,Peter Bell,peterbell10@live.co.uk,Mon Aug 22 13:23:55 2022 +0100,1661174635.0,"Move `torch.special` OpInfos into opinfo/definitions/special.py (#83762)

Ref #82518

As with `linalg` this doesn't include ops with an alias in special,
only the ones where `special.foo` is the actual name of the opinfo.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83762
Approved by: https://github.com/albanD",760.0,595.0,"torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/core.py,torch/testing/_internal/opinfo/definitions/__init__.py,torch/testing/_internal/opinfo/definitions/special.py,torch/testing/_internal/opinfo/utils.py",5.0,5,1,1.218685575,6.0,21330.0,4.0,277572.5,6676.0,15519.0,0.0,,0.0,1
pytorch,7cd951c21e0d93222d547353d4074e6e093f9e23,766ebf4441b4e8e8e50f84fa92b83ad9320bba81,Andrew M. James,andrew.m.james2@gmail.com,Fri Jan 06 18:20:10 2023 -0600,1673029210.0,"Remove hard numpy dependency introduced by inductor (#90796)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90796
Approved by: https://github.com/ngimel, https://github.com/cpuhrsch",22.0,24.0,"torch/_inductor/ir.py,torch/_inductor/mkldnn.py,torch/_inductor/scheduler.py",3.0,2,1,1.386890907,1.0,6140.0,3.0,123437.33333333331,11078.0,25278.0,0.0,Feature Addition,0.0,1
pytorch,d102f9ea181c133e21d10a5f39654eed88346c6a,769f5f7cfe6432ac8fcd84e54c24bb6ffd7b877b,Richard Zou,zou3519@users.noreply.github.com,Wed May 30 21:50:32 2018 -0400,1527717032.0,"Handling of scalars in torch.Size (#5676)

* Handling of scalars in torch.Size

torch.Size() constructor uses python_arg_parser

IntList in python_arg_parser can take iter/range

Have IntList take python iterables and ranges.

Address comments: don't use python_arg_parser and instead call __index__ in THPSize_pynew

Address comments

Address comments

* Rebased

* Address nit",31.0,3.0,"test/test_torch.py,torch/csrc/Size.cpp",2.0,3,2,0.959686894,39.0,7507.0,2.0,1210163.0,1215.0,3346.805292,0.0,Feature Addition,0.0,1
pytorch,37059ba0ec173aaf512d070a4be2d4e78cbf693a,76a283db409dd47e92b42225da565c891e0ca3a5,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Tue Mar 13 13:47:43 2018 +0500,1520948863.0,"[ready] General Documentation Improvements - 2 (#5685)

* Fix some minor errors in existing docs.

* Fix Convolution and Pooling docs in torch.nn.functional

* Cleaned up torch.nn.functional docs

* Address @SsnL 's comments

* Add multiplication sign missing in docs

* Fix more typos, and clear some warnings

* Change infinity symbol in LPPool2d

* Revert some changes in torch.nn.functional

* Few more minor changes",294.0,266.0,"torch/_torch_docs.py,torch/functional.py,torch/nn/functional.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/instancenorm.py,torch/nn/modules/linear.py,torch/nn/modules/loss.py,torch/nn/modules/normalization.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/modules/upsampling.py,torch/nn/utils/rnn.py,torch/serialization.py",18.0,4,1,2.245900001,37.0,15032.0,12.0,362248.2222222223,485.0,2376.0,0.0,Corrective,1.0,1
pytorch,f93ead6d37b476576b50a2f550c5898415a1fe35,76c185dccaca99b753d51a5c3eae6f8c67e61f82,Alex Suhan,asuhan@fb.com,Thu Sep 24 00:03:48 2020 -0700,1600905828.0,"[TensorExpr] When lanes differ, insert Broadcast instead of Cast (#45179)

Summary:
We need to check if dtypes differ in scalar type or lanes to decide between
Cast and Broadcast.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45179

Test Plan: test_tensorexpr --gtest_filter=TensorExprTest.SimplifyBroadcastTermExpander

Reviewed By: bwasti

Differential Revision: D23873316

Pulled By: asuhan

fbshipit-source-id: ca141be67e10c2b6c5f2ff9c11e42dcfc62ac620",44.0,3.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",3.0,7,2,1.115227265,2.0,6463.0,2.0,43999.0,5374.0,12252.0,0.0,,0.0,1
pytorch,122587dcb41427f473b7833eaf254384919e06fc,76c964dfb0955e459047eddceea27b0301737a42,Supriya Rao,supriyar@fb.com,Thu May 07 05:32:45 2020 -0700,1588829565.0,"Reland [quant][tests] Enable tests to run on all qengine backends (#37943)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37943

Refactor tests to use supported_qengines

Test Plan:
python test/test_quantization.py

Imported from OSS

Differential Revision: D21435514

fbshipit-source-id: 8004ef2535e1cc65036f331c00af27ded1c04a6b",536.0,600.0,"test/quantization/test_backward_compatibility.py,test/quantization/test_quantized_module.py,test/quantization/test_quantized_op.py,torch/testing/_internal/common_quantized.py",4.0,5,2,1.392818016,2.0,4200.0,4.0,173539.5,1794.0,4646.5,0.0,Perfective,0.0,1
pytorch,d081de67cf6d89ec53f30f0b60d914f86b3f655c,76d262d4b7b5a86d8bdc7533c41e2060fecb1626,neginraoof,neginmr@utexas.edu,Wed Oct 23 22:11:48 2019 -0700,1571868708.0,"export group_norm (#27071)

Summary:
Updated group_norm symbolic
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27071

Reviewed By: hl475

Differential Revision: D17792249

Pulled By: houseroad

fbshipit-source-id: 08be6071952ca2c256d2c6a0a6bbc19a8442f1fe",78.0,8.0,"test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.450123116,2.0,6120.0,1.0,79491.0,12490.0,34778.33333,0.0,,0.0,1
pytorch,fbd50bbfb969cd861cd91e9684004a498e9f2cb4,76d8979afececd2813463ea5edb536fdda02a3a7,Junjie Bai,jbai@fb.com,Wed Nov 14 00:39:43 2018 -0800,1542155983.0,"Revert D13007287: [codemod][caffe2] Tensor construction: combine Resize+mutable_data - 3/4

Differential Revision:
D13007287

Original commit changeset: c89a24458e04

fbshipit-source-id: 74d3fe310f1f551e2f52c6e3d9a744a47767b4b1",97.0,107.0,"caffe2/operators/locally_connected_op_impl.h,caffe2/operators/lpnorm_op.cc,caffe2/operators/map_ops.h,caffe2/operators/merge_id_lists_op.h,caffe2/operators/moments_op.h,caffe2/operators/multi_class_accuracy_op.cc,caffe2/operators/ngram_ops.h,caffe2/operators/one_hot_ops.cc,caffe2/operators/order_switch_ops.h,caffe2/operators/pack_rnn_sequence_op.h,caffe2/operators/pack_segments.cc,caffe2/operators/pad_op.cc,caffe2/operators/perplexity_op.cc,caffe2/operators/rank_loss_op.cc,caffe2/operators/reduce_front_back_max_ops.h,caffe2/operators/reduce_front_back_sum_mean_ops.h,caffe2/operators/reduce_ops.h,caffe2/operators/reduction_ops.h,caffe2/operators/reshape_op.h,caffe2/operators/resize_op.cc,caffe2/operators/reverse_packed_segs_op.h,caffe2/operators/rmac_regions_op.cc,caffe2/operators/rnn/recurrent_network_blob_fetcher_op.h,caffe2/operators/roi_align_op.cc",24.0,3,1,4.257565929,11.0,5022.0,1.0,15200.0,5366.0,16236.33333,0.0,,0.0,1
pytorch,f619ac6ac9346d2ef974711637cc3b116a35e1f0,76ee014d10ea17897a5aa7edf641b9c00d6b7fd0,Francisco Massa,fvsmassa@gmail.com,Mon Jun 19 21:21:04 2017 -0400,1497907264.0,Add documentation to SELU and AlphaDropout,33.0,2.0,"docs/source/nn.rst,torch/nn/functional.py,torch/nn/modules/activation.py",3.0,5,2,1.308793415,30.0,2553.0,2.0,244170.66666666663,1003.0,10965.8472,0.0,Feature Addition,0.0,1
pytorch,fdc9a1404eeae460bbcbc46a1f4d6c9247e20f1f,76f7b3e5600282d0c4f89656fc148e2a66bf81e2,Jiong Gong,jiong.gong@intel.com,Tue Jul 23 14:58:03 2024 -0400,1721746683.0,"[inductor][cpp][gemm] improve thread blocking heuristics (#131024)

This PR improves the thread blocking heuristics to favor full occupancy as much as possible. Also, the ""m x n"" block size is made as squared as possible for better data reuse.

Take the shape M=20000, N=64, K=128 as an example, the original heuristics couldn't use up all the threads when the number of threads is large, say 60:
AUTOTUNE linear_unary(200000x128, 64x128, 64)
  _linear_pointwise 0.1010 ms 100.0%
  cpp_packed_gemm_0 0.8303 ms 12.2%
0722 02:26:39.220660 302553 torch/_inductor/codegen/cpp_gemm_template.py:503] [0/0] Register blocking: GemmBlocking(block_m=32, block_n=32, block_k=32)
V0722 02:26:39.221042 302553 torch/_inductor/codegen/cpp_gemm_template.py:507] [0/0] Cache blocking: GemmBlocking(block_m=625, block_n=1, block_k=4)
V0722 02:26:39.221118 302553 torch/_inductor/codegen/cpp_gemm_template.py:509] [0/0] Thread blocking: GemmBlocking(block_m=625, block_n=1, block_k=4)
V0722 02:26:39.221252 302553 torch/_inductor/codegen/cpp_gemm_template.py:526] [0/0] Number of threads: 60, occupancy: (10, 2, 1)

After this PR:
AUTOTUNE linear_unary(200000x128, 64x128, 64)
  _linear_pointwise 0.1143 ms 100.0%
  cpp_packed_gemm_0 0.1228 ms 93.1%
V0722 02:29:49.261794 304201 torch/_inductor/codegen/cpp_gemm_template.py:309] [0/0] Register blocking: GemmBlocking(block_m=32, block_n=32, block_k=32)
V0722 02:29:49.262860 304201 torch/_inductor/codegen/cpp_gemm_template.py:313] [0/0] Cache blocking: GemmBlocking(block_m=64, block_n=1, block_k=8)
V0722 02:29:49.262951 304201 torch/_inductor/codegen/cpp_gemm_template.py:315] [0/0] Thread blocking: GemmBlocking(block_m=69, block_n=79, block_k=8)
V0722 02:29:49.263075 304201 torch/_inductor/codegen/cpp_gemm_template.py:332] [0/0] Number of threads: 60, occupancy: (15, 4, 1)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/131024
Approved by: https://github.com/leslie-fang-intel, https://github.com/chunyuan-w",96.0,37.0,"torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_prefix.h",2.0,3,1,0.915769153,2.0,1171.0,2.0,1445009.5,31585.0,79464.0,0.0,Perfective,0.0,1
pytorch,fe89e21b02140a230e1114353495633bf03cb62f,771fcb3455cbfe69c2abcc4cb3bd7ef92d59af24,Paul Jesse Hellemn,jesse.hellemn@gmail.com,Mon Apr 02 23:35:27 2018 -0700,1522712127.0,"[caffe2] Fbcode to GitHub sync (#6208)

* [easy] allow empty tensor in cuda relu op

The diff has not enabled unit test of empty tensor, because MLKVersion of ReluOp need extra work to support

* Make blob norm plotting work with distributed trainer when the old framework is used",179.0,82.0,"caffe2/operators/relu_op.cu,caffe2/python/layer_model_helper.py,caffe2/python/layer_model_instantiator.py,caffe2/python/modeling/compute_histogram_for_blobs.py,caffe2/python/modeling/compute_histogram_for_blobs_test.py,caffe2/python/modeling/compute_norm_for_blobs.py,caffe2/python/modeling/compute_norm_for_blobs_test.py,caffe2/python/modeling/compute_statistics_for_blobs.py,caffe2/python/modeling/compute_statistics_for_blobs_test.py,caffe2/python/modeling/gradient_clipping.py,caffe2/python/modeling/net_modifier.py,caffe2/python/operator_test/relu_op_test.py",12.0,5,1,3.116795551,7.0,1505.0,5.0,524102.25,72.0,851.5,0.0,,0.0,1
pytorch,a17cd226d6836e77310c5af2712e66a0270ec546,773ae817f7bd1d428bc1a0bc83ca56f84e0d6a87,andrewor14,andrewor14@gmail.com,Mon Mar 18 14:27:27 2024 -0700,1710772047.0,"Batch Norm Consolidation (#116092)

**Summary:**

This commit simplifies the existing decomposition hierarchy
of batch norm ops by adding a single, backend agnostic op:
`batch_norm_with_update`. The existing hierarchy looks like:

```
aten.batch_norm ->
aten._batch_norm_impl_index ->
[
  aten.native_batch_norm ->
  aten._native_batch_norm_legit (export only) ->
  _batch_norm_legit_cpu/cuda (kernels, export only) ->
  _batch_norm_cpu/cuda (kernels)
] OR
[ aten.cudnn_batch_norm ] OR
[ aten.miopen_batch_norm ]
```

Aside from complexity, an important problem with the
above decomposition hierarchy is cuda numerics in
export flows. We observed significantly worse convergence
when training a mobilenetv2-like model when using the
`_batch_norm_cuda` kernel instead of the `cudnn_batch_norm`
kernel. This means users who export their models on CPU
first then move the models to cuda later may silently
see worse accuracies even when cudnn is installed,
because they are using the worse kernel. This issue is
summarized in https://github.com/pytorch/pytorch/issues/111384.

Instead, the new hierarchy proposed by consolidating
existing batch norm ops will look like:

```
aten.batch_norm ->
aten.batch_norm_with_update ->
[ _batch_norm_cpu (kernel) ] OR
[ _batch_norm_cuda (kernel) ] OR
[ cudnn_batch_norm (kernel) ] OR
[ miopen_batch_norm (kernel) ]
```

The new op `batch_norm_with_update` hides backend
implementation details and automatically picks the right
kernel based on what is installed. This commit also adds
the following variants to this op:

```
batch_norm_with_update_functional
batch_norm_with_update.out
batch_norm_no_update
batch_norm_no_update.out
batch_norm_backward
```

Note that this commit only adds this op and its variants,
but does not actually change the decomps to produce these
ops in the graph. This will be done after the 2 week FC
window, and the ops used in the old stack is planned to
be removed after the 6 month BC window.

Test Plan: `OpInfo` tests for `batch_norm_with_update`.

Reviewers: albanD, bdhirsh

Subscribers: albanD, bdhirsh, supriyar

Tasks: https://github.com/pytorch/pytorch/issues/111384

Differential Revision: [D54805279](https://our.internmc.facebook.com/intern/diff/D54805279)
Co-authored-by: Tugsbayasgalan Manlaibaatar <tmanlaibaatar@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/116092
Approved by: https://github.com/bdhirsh, https://github.com/albanD",779.0,72.0,"aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/Normalization.h,aten/src/ATen/native/cuda/Normalization.cu,aten/src/ATen/native/cudnn/BatchNorm.cpp,aten/src/ATen/native/cudnn/BatchNorm.h,aten/src/ATen/native/mkldnn/Normalization.cpp,aten/src/ATen/native/mps/operations/Normalization.mm,aten/src/ATen/native/native_functions.yaml,test/distributed/_tensor/test_dtensor_ops.py,test/dynamo_skips/TestProxyTensorOpInfoCPU.test_make_fx_exhaustive_batch_norm_with_update_cpu_float32,test/dynamo_skips/TestProxyTensorOpInfoCPU.test_make_fx_fake_exhaustive_batch_norm_with_update_cpu_float32,test/dynamo_skips/TestProxyTensorOpInfoCPU.test_make_fx_symbolic_exhaustive_batch_norm_with_update_cpu_float32,test/dynamo_skips/TestProxyTensorOpInfoCPU.test_make_fx_symbolic_exhaustive_inplace_batch_norm_with_update_cpu_float32,test/dynamo_skips/TestProxyTensorOpInfoCPU.test_make_fx_symbolic_exhaustive_out_batch_norm_with_update_cpu_float32,test/expect/HasDecompTest.test_aten_core_operators.expect,test/expect/HasDecompTest.test_has_decomposition.expect,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,test/onnx/test_fx_op_consistency.py,test/test_jit_fuser_te.py,test/test_meta.py,test/test_mps.py,test/test_proxy_tensor.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,torch/_C/__init__.pyi.in,torch/_decomp/decompositions.py,torch/_decomp/decompositions_for_jvp.py,torch/_dynamo/trace_rules.py,torch/_functorch/partitioners.py,torch/_inductor/decomposition.py,torch/csrc/Module.cpp,torch/csrc/jit/runtime/serialized_shape_function_registry.cpp,torch/jit/_shape_functions.py,torch/testing/_internal/common_methods_invocations.py",36.0,31,4,3.824658787,50.0,96570.0,6.0,538069.0833333334,26324.0,62450.0,0.0,Feature Addition,0.0,1
pytorch,fe8e5eb260e3e6af3fb55c2eb2bd7319afc34099,773cfae93b4c3b3d6a56c743244fecd950552ee3,Edward Yang,ezyang@fb.com,Fri May 21 01:15:21 2021 -0700,1621559721.0,"Tag PyObject on TensorImpl per torchdeploy interpreter (#57985)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57985

Fixes https://github.com/pytorch/pytorch/issues/57756

This PR introduces a new `pyobj_interpreter_` field on TensorImpl which tracks what Python interpreter (if any) owns the TensorImpl. This makes it illegal to bind a TensorImpl from multiple Python interpreters, and means that we can now directly store PyObject pointer on TensorImpl even in the presence of multiple Python interpreters, as is the case in torchdeploy. This is a necessary step for PyObject preservation, which cannot be easily implemented when there are multiple Python interpreters.

Although the PR is not that long, there is a very subtle portion of the implementation devoted to ensuring that the tagging process is thread safe, since multiple threads can concurrently try to tag a PyObject. Check Note [Python interpreter tag] and Note [Memory ordering on Python interpreter tag] for detailed discussion of how this is handled. You will have to check this code carefully in code review; I did not torture test the multithreaded paths in any meaningful way.

In a follow up PR, I will pack the interpreter and PyObject fields into single atomic word on 64-bit.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: wconstab

Differential Revision: D28390242

Pulled By: ezyang

fbshipit-source-id: a6d9b244ee6b9c7209e1ed185e336297848e3017",372.0,60.0,"c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/deploy/test_deploy.cpp",4.0,6,2,1.548094724,29.0,3913.0,4.0,682080.75,12298.0,27837.5,0.0,Corrective,1.0,1
pytorch,c3b499227d6867a45aa5ffc16046cf58761f913a,77484d91db052dfcfa22a38408349853b6246f8a,Sam Gross,sgross@fb.com,Thu Jun 28 04:12:13 2018 -0700,1530159133.0,"Add AT_WARN to issue warnings from ATen (#8967)

Summary:
Use AT_WARN from python_anomaly_mode instead of printing to stdout.
Closes https://github.com/pytorch/pytorch/pull/8967

Reviewed By: ezyang

Differential Revision: D8670654

Pulled By: colesbury

fbshipit-source-id: 3f7aee8ea06914d7d4381feec086e95f0b194752",76.0,12.0,"aten/src/ATen/Error.cpp,aten/src/ATen/Error.h,test/test_autograd.py,torch/csrc/Module.cpp,torch/csrc/autograd/python_anomaly_mode.cpp,torch/csrc/autograd/python_anomaly_mode.h",6.0,7,3,2.438554305,43.0,4282.0,4.0,1217229.6666666667,2747.0,6237.833333,0.0,Feature Addition,0.0,1
pytorch,c02eda8166068400a9e5d82343108cd8a524095c,774ae0851d98829b412e46dde85e716dad065a06,Heitor Schueroff,heitorschueroff@fb.com,Thu Aug 26 13:05:28 2021 -0700,1629983128.0,"[OpInfo] Added ReductionOpInfo subclass of OpInfo and ported sum test (#62737)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62737

ReductionOpInfo is a specialization of OpInfo for reduction operators. For now, it is designed to work with reductions that return a single tensor and that reduce all elements along one or more dimensions to a single value. In particular this excludes operators such as `max` and `min` that return multiple tensors and `quantile` that can return multiple values.

fixes https://github.com/pytorch/pytorch/issues/49746

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D30406568

Pulled By: heitorschueroff

fbshipit-source-id: 218b1da1902f67bcf4c3681e2a0f0029a25d51f1",208.0,72.0,"test/test_ops.py,test/test_reductions.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.505066859,2.0,12887.0,3.0,464799.6666666667,14955.0,34260.0,0.0,Corrective,1.0,1
pytorch,cd11109c2eedbe58ada092b1b8fc274006385c43,7793ab0871d25d3c39911f9eedd327dffe466cd6,Edward Yang,ezyang@fb.com,Tue Jul 16 21:43:35 2019 -0700,1563313415.0,"More documentation about the pyobj field.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22885

Test Plan: Imported from OSS

Differential Revision: D16283076

Pulled By: ezyang

fbshipit-source-id: 4f6a87d900c4d430eedc90661de89e0f6916347e",26.0,1.0,"c10/core/TensorImpl.h,torch/csrc/autograd/python_variable.cpp",2.0,5,2,0.999010271,27.0,2274.0,2.0,102715.0,9968.0,28884.83333,0.0,Non Functional,0.0,1
pytorch,e426020c87751ce6b600f774f7acd9c60cb0471b,77c792ec276ee8bf9e279ce34ecb8dac5ecbf472,Peter Goldsborough,peter@goldsborough.me,Wed Jan 03 21:30:55 2018 -0800,1515015055.0,Vectorize normal_ (#4312),869.0,7.0,"aten/src/TH/generic/THTensorRandom.c,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.c,aten/src/TH/generic/THVectorDispatch.c,aten/src/TH/vector/AVX2.c,aten/src/TH/vector/AVX2.h,aten/src/TH/vector/avx_mathfun.h,test/common.py,test/test_nn.py",9.0,6,2,1.119059346,38.0,6701.0,7.0,2262921.25,890.0,6683.172317,0.0,,0.0,1
pytorch,63189698ec19e56e05192cda41e8dbe695079d11,77c7a50d46cbe2267cfde07684a43c8301105f5b,CaoE,e.cao@intel.com,Mon Apr 04 15:54:57 2022 -0700,1649087697.0,"Add BFloat16 support for logsigmoid, hardsigmoid, hardshrink, softshrink, hardswish and softplus on CPU (#63134)

Summary:
Add BFloat16 support for logsigmoid, hardsigmoid, hardshrink, softshrink,  hardswish and softplus  on CPU,  and optimize the performance of softshrink.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63134

Reviewed By: yinghai

Differential Revision: D34897992

Pulled By: frank-wei

fbshipit-source-id: 4c778f5271d6fa54dd78158258941def8d9252f5
(cherry picked from commit decda0e3debf56cc5c4d7faea41b1165a7cabe12)",391.0,48.0,"aten/src/ATen/native/cpu/Activation.cpp,test/test_nn.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",4.0,9,3,0.967682473,45.0,39038.0,4.0,1186591.5,1947.0,4649.5,0.0,Feature Addition,0.0,1
pytorch,db3f5f86b2b21228ffe84155fa50e0d3932b867c,77ddd5130b45891dd71f928aa7b91c50dcf1c2ad,Richard Zou,zou3519@users.noreply.github.com,Tue Nov 07 13:57:11 2017 -0500,1510063031.0,Add reduce keyword for KLDivLoss (#3330),169.0,27.0,"aten/src/ATen/nn.yaml,aten/src/THCUNN/DistKLDivCriterion.cu,aten/src/THCUNN/generic/DistKLDivCriterion.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/DistKLDivCriterion.c,aten/src/THNN/generic/THNN.h,test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/legacy/nn/DistKLDivCriterion.py,torch/nn/functional.py,torch/nn/modules/loss.py",13.0,16,4,3.327372735,39.0,12798.0,7.0,386052.0,2082.0,24022.85823,0.0,Feature Addition,0.0,1
pytorch,c902f1cf980eef27541f3660c685f7b59490e744,77dfdbf96c1e3576ad333f06cfdb8c310219b7cf,Richard Zou,zou3519@users.noreply.github.com,Mon Dec 11 11:00:54 2017 -0500,1512990054.0,"Ensure RNNCell variants don't broadcast (#4074)

* Ensure RNNCell variants don't broadcast

* Fix lint

* Add test for hidden_size=1 in RNNCell no broadcasting test

* Prevent broadcasting for hidden_size and input_size

* Isolate input checking from hidden size checking",52.0,0.0,"test/test_nn.py,torch/nn/modules/rnn.py",2.0,4,2,0.995727452,37.0,5641.0,2.0,965061.5,834.0,6569.672317,0.0,Corrective,1.0,1
pytorch,7e46eb1613e1b242cb755ed1599d55f76aaf1d07,77fbc12f2334ad219e3c94301fd7237c4a3a86f3,Sam Gross,colesbury@gmail.com,Fri Mar 17 22:28:39 2017 -0400,1489789719.0,"Fix some deadlocks when torch_shm_manager is not found (#1030)

- Add additional timeouts to test_multiprocessing to reduce chances of
   hanging indefintely on failure
 - Add missing header guards
 - Fix typo
 - Check that torch_shm_manager exists in torch/__init__.py",60.0,39.0,"test/test_multiprocessing.py,torch/__init__.py,torch/lib/libshm/alloc_info.h,torch/lib/libshm/core.cpp,torch/lib/libshm/err.h,torch/lib/libshm/manager.cpp,torch/lib/libshm/socket.h",7.0,4,2,2.307738067,25.0,1225.0,2.0,780265.4285714285,548.0,3548.555248,0.0,Corrective,1.0,1
pytorch,bf14e42a076aa969fd27992fa46c0c1c1ce65c02,7820bab01a05ed3c83a27dfd749976facd893a84,Richard Zou,zou3519@users.noreply.github.com,Fri Jun 10 14:20:37 2022 -0400,1654870837.0,"[functorch] Fix index.Tensor, index_put batching rules (pytorch/functorch#862)

Fixes https://github.com/pytorch/functorch/issues/859

Start reading at `NOTE: [advanced indexing (index.Tensor) batch rule]`
in the code for details. This PR rewrites the index.Tensor and index_put
batching rules.

The TL;DR is:
- advanced indexing has different behavior depending on if the ""advanced
indices are adjacent"":
https://numpy.org/doc/stable/user/basics.indexing.html#combining-advanced-and-basic-indexing
- we have to take this into account in our batching rules, because
index.Tensor and index_put handle these internally.

Test Plan
- I added new test cases for getitem and aten.ops.index_put via OpInfo
testing.

Future
- primtorch should have a sane decomposition that we can use
- We haven't fixed the index_put_ batching rule yet. TODO later...
- Upstream our test cases (see next section) into pytorch/pytorch",383.0,71.0,"functorch/functorch/csrc/BatchRulesScatterOps.cpp,functorch/test/functorch_additional_op_db.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,0.911761309,1.0,6931.0,3.0,0.5,1103.0,1496.5,0.0,Corrective,1.0,1
pytorch,8d6479725ac7004334be184f034e91b69fef683c,7827ae2864afa1955bc9ce04d168b274700d24e5,Huy Do,huydhn@gmail.com,Mon Oct 02 18:01:49 2023 +0000,1696269709.0,"Increase job timeout limit when running with memory leak check (#110193)

This fixes the daily timeout of ROCm jobs when running with memory leak check turning on.  I want to use something like `inputs.timeout-minutes * 2` but that syntax, unfortunately, isn't supported in GitHub action YAML.  So I decide to just x2 the current timeout value of 300 minutes to make it 600 minutes.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/110193
Approved by: https://github.com/clee2000",9.0,3.0,".github/workflows/_linux-test.yml,.github/workflows/_rocm-test.yml,.github/workflows/_win-test.yml",3.0,2,1,1.251629167,2.0,842.0,3.0,3310780.6666666665,20290.0,46456.5,0.0,Corrective,1.0,1
pytorch,13e78a2cb9f203a71b165b31c39e074394ab57f3,785b13af664e5de702958eeedc6595623fa9a947,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Sat Aug 06 21:06:06 2022 +0000,1659819966.0,"Revert ""Use UnaryUfuncInfo for type conversion functions (#82349)""

This reverts commit 13e78a2cb9f203a71b165b31c39e074394ab57f3.

Reverted https://github.com/pytorch/pytorch/pull/82349 on behalf of https://github.com/peterbell10 due to This stack broke macos tests on trunk",145.0,156.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,6.0,20129.0,1.0,3866.0,6183.0,14359.0,0.0,,0.0,1
pytorch,8f9f12c068977b8ec8d888553b5243e85de35dca,78c9b2948ad00b1c853838b95b51e1c4f5a035aa,Yang Chen,yangche@fb.com,Thu Feb 15 22:43:36 2024 -0800,1708037016.0,"[aot_inductor] move CudaWrapperCodeGen into a separate file (#119870)

This reverts commit 3ab08946d5052eaeda11d683d6a58e801a032755.

Differential Revision: [D53817852](https://our.internmc.facebook.com/intern/diff/D53817852)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/119870
Approved by: https://github.com/khabinov",306.0,290.0,"tools/amd_build/build_amd.py,torch/_inductor/codegen/cpp_wrapper_cuda.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/graph.py",4.0,5,2,1.101895784,5.0,4824.0,4.0,1117675.0,25262.0,57009.0,0.0,,0.0,1
pytorch,d371a9f9c5fe94884688bf54d143433a6319c739,796be045bb8248770c81734fe5af98e7376f4191,Jeffrey Wan,jw3468@fb.com,Wed Mar 24 21:30:58 2021 -0700,1616621458.0,"Refactor gradcheck (#53857)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53857

This PR basically just factors a lot of the logic out from the main gradcheck function into their own individual functions. It aims to avoid any behavior change (but we may not have enough tests to actually verify this). Refactorings that lead to any behavior chang are done in the next PR in this stack.

The rationale for this change is 1) to make the main gradcheck function cleaner to read, and 2) also allow us to reuse the same pieces when we add the fast gradcheck.

Maybe this PR is also a good place to add some tests for gradcheck, i.e., make sure gradcheck fails when it should fail, as to make sure that we are indeed not changing any logic. This will also help us make sure our fast_gradcheck does all the necessary checks:
So far existing tests are:
- test_gradcheck_fail_when_no_differentiable_outputs_and_num_grad_not_zero` (test_autograd)
- test_gradcheck_single_input (test_autograd)
- test_gradcheck_sparse_input (test_autograd)
- test_gradcheck_nondeterministic (test_autograd)
- test_gradcheck (test_overrides)

Full coverage would potentially require adding the following missing tests (for each test for both raise_exception=True/False) - Methodology for getting the list below is that for every type of error message we spit out, we make sure we can hit it:
- complex:
  - when numerical != analytical when tested with imag grad_out
- check_inputs
  - ~when inputs are not dense, but check_sparse_nnz is false~
  - ~when none of the inputs require grad~
  - ~(warning) when inputs are not double precision~
  - ~when layout is not mkldnn(aka has strides) and input has a dimension with stride 0.~
- check_no_differentiable_outputs:
  - ~when none of the outputs are differentiable, but numerical gradient is not zero~
- check_outputs:
  - ~when sparse outputs (always raise)~
  - ~when mkldnn outputs (always raise)~
- test_batched_grad
  - ~when encounter runtime error while computing batched grad (print big message)~
  - when not allclose (print out big message)
- test_backward_mul_by_grad_output
  - ~when layout of grad_input is not the same as input~
  - ~when grad_input is sparse and has incorrect sparse_dim/dense_dim~
  - ~when backward not multiplied by grad_output (sparse/non-sparse case)~
  - when grad is incorrect type/size
- test_undefined_grad
  - ~when encounter runtime error while running backward~
  - when we complete backward but grad inputs (the output of .grad()) is not none
- check_analytical_jacobian_attributes (for both complex/non complex)
  - when grad input is incorrect dtype/size

Test Plan: Imported from OSS

Reviewed By: heitorschueroff

Differential Revision: D27201571

Pulled By: soulitzer

fbshipit-source-id: 86670a91e65740d57dd6ada7c6b4512786d15962",321.0,197.0,"test/test_autograd.py,torch/autograd/gradcheck.py",2.0,3,2,0.800392208,42.0,8810.0,2.0,2103619.5,10042.0,22241.0,0.0,Corrective,0.0,1
pytorch,d81c6bde3b32454bde336146ce0314f2a70ba076,79c27ba4ef609c56907c504e15737fbaf8cc8f87,Lara,lahaidar@microsoft.com,Thu Dec 12 03:36:57 2019 -0800,1576121817.0,"Add ONNX Export Support to floor_divide (#31081)

Summary:
Adding support for the new ATen op floor_divide which was introduced in https://github.com/pytorch/pytorch/pull/30493/files.

This operation is used in Torchvision/FasterRCNN-MaskRCNN, which are now failing after the new op was introduced.
This PR fixes the failure.

cc: neginraoof
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31081

Reviewed By: houseroad

Differential Revision: D18945316

Pulled By: eellison

fbshipit-source-id: 09919c237d618ce7db293c7770f48f7304949dcf",51.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.997502546,2.0,4601.0,1.0,71715.0,13807.0,37612.83333,0.0,Corrective,1.0,1
pytorch,1110dd1f8f3fb1e2b8ebd82f26bd95f060d397f8,79c3ebc040c4bac896477030d8af4ac94bc6f440,Subhash Mullapudi,32626926+subcomputes@users.noreply.github.com,Tue Apr 10 10:18:01 2018 -0500,1523355481.0,adds correct precision to test_noncontig_conv_grad (#6440),1.0,1.0,test/test_nn.py,1.0,1,1,0,38.0,7185.0,1.0,121933.0,1031.0,6951.172317,0.0,Corrective,0.0,1
pytorch,322dff475c0fa4a21288acc6de33b3fdac2621ea,79c5e33349df10648ce586af118f09f2ccfd9c86,Justin Chu,justinchu@microsoft.com,Fri Jul 21 04:56:13 2023 -0700,1689915373.0,"[BE] Enable ruff's UP rules and autoformat nn/ mps/ and torch/ (#105436)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105436
Approved by: https://github.com/malfet, https://github.com/albanD",296.0,320.0,"test/nn/test_convolution.py,test/nn/test_module_hooks.py,test/nn/test_multihead_attention.py,test/nn/test_pooling.py,test/onnx/dynamo/test_exporter_api.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/__init__.py,torch/_custom_op/impl.py,torch/_inductor/codegen/wrapper.py,torch/_linalg_utils.py,torch/_lobpcg.py,torch/_logging/_internal.py,torch/_lowrank.py,torch/_ops.py,torch/_python_dispatcher.py,torch/_tensor.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/_utils.py,torch/autograd/function.py,torch/autograd/functional.py,torch/autograd/gradcheck.py,torch/autograd/graph.py,torch/autograd/profiler_util.py,torch/backends/_nnapi/serializer.py,torch/backends/cudnn/rnn.py,torch/backends/mps/__init__.py,torch/backends/opt_einsum/__init__.py,torch/backends/quantized/__init__.py,torch/backends/xeon/run_cpu.py,torch/contrib/_tensorboard_vis.py,torch/cuda/__init__.py,torch/cuda/_memory_viz.py,torch/cuda/_utils.py,torch/cuda/amp/grad_scaler.py,torch/cuda/graphs.py,torch/cuda/memory.py,torch/cuda/streams.py,torch/hub.py,torch/jit/_decompositions.py,torch/jit/_recursive.py,torch/jit/_script.py,torch/jit/_serialization.py,torch/jit/_state.py,torch/jit/_trace.py,torch/jit/annotations.py,torch/jit/frontend.py,torch/jit/mobile/__init__.py,torch/jit/quantized.py,torch/jit/supported_ops.py,torch/library.py,torch/linalg/__init__.py,torch/masked/_docs.py,torch/masked/_ops.py,torch/masked/maskedtensor/core.py,torch/mps/__init__.py,torch/nn/_reduction.py,torch/nn/functional.py,torch/nn/init.py,torch/nn/modules/_functions.py,torch/nn/modules/activation.py,torch/nn/modules/adaptive.py,torch/nn/modules/batchnorm.py,torch/nn/modules/channelshuffle.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/flatten.py,torch/nn/modules/fold.py,torch/nn/modules/instancenorm.py,torch/nn/modules/lazy.py,torch/nn/modules/module.py,torch/nn/modules/padding.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/transformer.py,torch/nn/modules/utils.py,torch/nn/parallel/distributed.py,torch/nn/parallel/parallel_apply.py,torch/nn/utils/init.py,torch/nn/utils/parametrizations.py,torch/nn/utils/prune.py,torch/nn/utils/rnn.py,torch/nn/utils/spectral_norm.py,torch/overrides.py,torch/serialization.py,torch/storage.py,torch/torch_version.py",89.0,30,2,5.821102942,55.0,100563.0,72.0,9522338.04494382,17893.0,40488.5,0.0,,0.0,1
pytorch,94e52e1d1745003fa3a434ed74c1fe87cf8ef349,79ead42ade84ad6e09131cbe8672ee7d8b539471,Sam Gross,colesbury@gmail.com,Tue Oct 18 16:15:57 2016 -0400,1476807357.0,Add CUDA Stream and Event API (#133),347.0,1.0,"setup.py,test/test_cuda.py,torch/csrc/Module.cpp,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Stream.cpp,torch/csrc/cuda/Stream.h,torch/csrc/cuda/THCP.h,torch/cuda/__init__.py,torch/cuda/streams.py",9.0,5,2,2.381084713,11.0,2113.0,2.0,80922.83333333333,47.0,52.15263348,0.0,Feature Addition,0.0,1
pytorch,3ed720079e01c1b2e1c9f95e5bb43da56d6fc2c3,79f5bf84e54d57a4c81912aeedb0b8a27f97c27e,Luke Yeager,lukeyeager@users.noreply.github.com,Fri Jan 27 21:49:15 2017 -0800,1485553755.0,[pep8] Potentially breaking docstring changes,117.0,71.0,"torch/_torch_docs.py,torch/nn/modules/conv.py,torch/nn/modules/loss.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py",5.0,3,1,1.348676934,21.0,5924.0,2.0,0.0,384.0,3829.196975,0.0,Non Functional,0.0,1
pytorch,3dd266304c8deaee840f9d427ad32f38e0cb3ff7,79f8582289a2967fbf512e722bfd3f2f932aea53,Ksenija Stanojevic,ksenija.stanojevic@gmail.com,Tue Nov 10 02:00:52 2020 -0800,1604973652.0,"[ONNX] Add export of aten::is_floating point (#46442)

Summary:
Add export of aten::is_floating point

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46442

Reviewed By: mrshenli

Differential Revision: D24566156

Pulled By: bzinodev

fbshipit-source-id: 91ea95e2c4d4866e2ef51bffe07461de2e31c110",62.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",3.0,4,2,0.659459212,3.0,8424.0,2.0,17667.666666666668,6593.0,14980.5,0.0,Feature Addition,0.0,1
pytorch,03f7289fcf8161177742cfbc09d50ce8d3248aee,7a048cdcd77dfee7d65c7624831663036fff9449,cpuhrsch,cpuhrsch@googlemail.com,Tue Jun 19 20:56:49 2018 -0400,1529441809.0,"Vectorize non-contiguous unary operations (#8488)

* Vectorize non-contiguous unary operations

All builds pass. Manual Windows rerun is here:
https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-win-ws2016-cuda9-cudnn7-py3-trigger/9714/",207.0,141.0,"aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Declarations.cwrap,aten/src/ATen/cpu/vec256/functional.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h",10.0,7,1,2.168649621,8.0,5449.0,5.0,562072.2,1370.0,4004.305292,0.0,,0.0,1
pytorch,1c00e0fc3f8531ca8abec11fcfaecda023eef0dd,7a0ae0079f29989d36eb2509330490ceef15cf87,liqunfu,liqun_fu@hotmail.com,Fri Jul 26 06:58:47 2019 -0700,1564124327.0,"export sort to onnx

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/21913

Differential Revision: D15982801

Pulled By: houseroad

fbshipit-source-id: 96dbd738c557478fffd48000db7263ae1f9754f5",46.0,9.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.578647265,1.0,2493.0,2.0,366898.3333333333,10208.0,29364.83333,0.0,,0.0,1
pytorch,631baecbcd821124296cfe40e5c297cf2def410c,7a2930b357a4e62bb0bab53bb0d23c607b6ede38,kshitij12345,kshitijkalambarkar@gmail.com,Sat Nov 19 04:09:29 2022 +0000,1668830969.0,"add jvp test with non-contig inputs (#89131)

Ref: https://github.com/pytorch/functorch/issues/1029

We update `test_jvp` to do contiguous and non-contiguous testing in a single test.

Prev time for `test_jvp` : ~28s
New time for `test_jvp`: ~45s

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89131
Approved by: https://github.com/zou3519",43.0,10.0,test/functorch/test_ops.py,1.0,2,1,0,1.0,1771.0,1.0,7382.0,9703.0,22499.5,0.0,Feature Addition,0.0,1
pytorch,da8e037e03c37569258a0fb2fba8a02a45bd497d,7a36c132cef1c3256a26048251ef022f60e22f3e,avmgithub,mendoza1@us.ibm.com,Fri Feb 23 21:35:51 2018 -0600,1519421751.0,"Skip denormal test for now. See issue  #5331 (#5387)

* Skip denormal test for now. See issue #5331

* Skip denormal test for now. See issue #5331",2.0,2.0,test/test_torch.py,1.0,1,1,0,38.0,5652.0,1.0,169072.0,2403.0,24690.85823,0.0,,0.0,1
pytorch,c91af1202a5568ddfa950dd0dac1701dc1def8ce,7a377b9a538e813148f5b700f6e482ce42be0b9a,Owen Anderson,owen.anderson@oculus.com,Fri Aug 03 18:26:57 2018 -0700,1533320817.0,"Add torch.argsort mirroring similar functionality in numpy. (#9600)

Summary:
Per issue #9542
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9600

Differential Revision: D8952338

Pulled By: resistor

fbshipit-source-id: c3f69d62858ad9458ec5ae563e3ff24b1c9283a7",42.0,0.0,"test/test_torch.py,torch/functional.py,torch/tensor.py",3.0,2,2,0.892942742,40.0,9332.0,3.0,188275.66666666663,3308.0,8801.333333,0.0,Feature Addition,0.0,1
pytorch,2bae888f659f991d29e73c91e703c652f4197615,7a697c4683375737cde91da6b061ab0c219b4fe8,atalman,atalman@fb.com,Thu Nov 23 02:14:22 2023 +0000,1700705662.0,"[RelEng] Tag docker images for release, pin unstable and disabled jobs, apply release only changes (#114355)

1. This tags docker images using docker pull/tag/push for current release
2. Sets RELEASE_VERSION_TAG var and regenerates the workflows using the new docker tag
3. Remove conda token setting and Binary tests release changes these are already automated
4. Pin unstable and disabled jobs, autumate: https://github.com/pytorch/pytorch/pull/111675

Test:
```
RELEASE_VERSION=2.2 ./scripts/release/apply-release-changes.sh
Tagging pytorch/manylinux-builder:cuda11.8-main to pytorch/manylinux-builder:cuda11.8-2.2 , dry_run: enabled
Tagging pytorch/manylinux-builder:cuda12.1-main to pytorch/manylinux-builder:cuda12.1-2.2 , dry_run: enabled
Tagging pytorch/libtorch-cxx11-builder:cuda11.8-main to pytorch/libtorch-cxx11-builder:cuda11.8-2.2 , dry_run: enabled
Tagging pytorch/libtorch-cxx11-builder:cuda12.1-main to pytorch/libtorch-cxx11-builder:cuda12.1-2.2 , dry_run: enabled
Tagging pytorch/manylinux-builder:rocm5.6-main to pytorch/manylinux-builder:rocm5.6-2.2 , dry_run: enabled
Tagging pytorch/manylinux-builder:rocm5.7-main to pytorch/manylinux-builder:rocm5.7-2.2 , dry_run: enabled
Tagging pytorch/libtorch-cxx11-builder:rocm5.6-main to pytorch/libtorch-cxx11-builder:rocm5.6-2.2 , dry_run: enabled
Tagging pytorch/libtorch-cxx11-builder:rocm5.7-main to pytorch/libtorch-cxx11-builder:rocm5.7-2.2 , dry_run: enabled
Tagging pytorch/manylinux-builder:cpu-main to pytorch/manylinux-builder:cpu-2.2 , dry_run: enabled
Tagging pytorch/libtorch-cxx11-builder:cpu-main to pytorch/libtorch-cxx11-builder:cpu-2.2 , dry_run: enabled
Tagging pytorch/manylinuxcxx11-abi-builder:cpu-cxx11-abi-main to pytorch/manylinuxcxx11-abi-builder:cpu-cxx11-abi-2.2 , dry_run: enabled
Tagging pytorch/manylinuxaarch64-builder:cpu-aarch64-main to pytorch/manylinuxaarch64-builder:cpu-aarch64-2.2 , dry_run: enabled
Tagging pytorch/conda-builder:cuda11.8-main to pytorch/conda-builder:cuda11.8-2.2 , dry_run: enabled
Tagging pytorch/conda-builder:cuda12.1-main to pytorch/conda-builder:cuda12.1-2.2 , dry_run: enabled
Tagging pytorch/conda-builder:cpu-main to pytorch/conda-builder:cpu-2.2 , dry_run: enabled
/data/users/atalman/pytorch/.github/workflows/generated-linux-binary-manywheel-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-linux-binary-conda-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-linux-binary-manywheel-main.yml
/data/users/atalman/pytorch/.github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml
/data/users/atalman/pytorch/.github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml
/data/users/atalman/pytorch/.github/workflows/generated-windows-binary-wheel-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-windows-binary-conda-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-windows-binary-libtorch-release-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-windows-binary-libtorch-debug-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-windows-binary-libtorch-release-main.yml
/data/users/atalman/pytorch/.github/workflows/generated-windows-binary-libtorch-debug-main.yml
/data/users/atalman/pytorch/.github/workflows/generated-macos-binary-wheel-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-macos-binary-conda-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-macos-binary-libtorch-cxx11-abi-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-macos-arm64-binary-wheel-nightly.yml
/data/users/atalman/pytorch/.github/workflows/generated-macos-arm64-binary-conda-nightly.yml
````

Result of pinning unstable and disabled jobs:
```
# The link to the published list of disabled jobs
DISABLED_JOBS_URL = ""https://ossci-metrics.s3.amazonaws.com/disabled-jobs.json?versionid=kKJlAXdrUbk3CilXbKu.6OwNTGQB8a.B""
# and unstable jobs
UNSTABLE_JOBS_URL = ""https://ossci-metrics.s3.amazonaws.com/unstable-jobs.json?versionid=vzaicOxSsh55iXBXwgGrW6dFeVtPfrhr""
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114355
Approved by: https://github.com/malfet",78.0,11.0,".github/scripts/generate_binary_build_matrix.py,.github/scripts/tag_docker_images_for_release.py,scripts/release/apply-release-changes.sh",3.0,4,2,1.034849545,2.0,394.0,2.0,706535.5,22365.0,51025.5,0.0,Corrective,1.0,1
pytorch,4eb02e863718abf5ff75fa4b296cd2331f938701,7a8152530d490b30a56bb090e9a67397d20e16b1,kshitij12345,kshitijkalambarkar@gmail.com,Wed Aug 24 16:17:50 2022 +0000,1661357870.0,"move pooling test from test_nn to test/nn/test_pooling (#83915)

Ref #63085

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83915
Approved by: https://github.com/albanD",1499.0,1466.0,"test/nn/test_pooling.py,test/test_nn.py,torch/testing/_internal/common_nn.py",3.0,5,2,1.080295916,45.0,28142.0,2.0,1831866.0,6753.0,15742.5,0.0,,0.0,1
pytorch,99cfaf9eeea0a6f20d0b11d211db379473db748e,7a9ab5c232f54430704456d18a22f99838489817,Edward Z. Yang,ezyang@fb.com,Tue Sep 13 05:02:31 2022 -0700,1663045351.0,"Move Python argument related functions to cpp file (#84919)

No changes to contents, just moving things out of header.
I only moved the stuff I suspected I'd be editing; maybe more
things from this header could migrate out.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84919
Approved by: https://github.com/suo",292.0,272.0,"torch/csrc/jit/python/pybind_utils.cpp,torch/csrc/jit/python/pybind_utils.h",2.0,4,1,0.999918361,3.0,1746.0,1.0,885111.0,7284.0,17003.5,0.0,,0.0,1
pytorch,431c80a1289c9fb045dd4e1d619c046c543512f9,7abdc303c69de74b5362f69e30037b186e3c3db0,gchanan,gregchanan@gmail.com,Fri May 18 17:45:10 2018 +0200,1526665510.0,"Don't allow requires_grad to be set on integer Tensor constructors inâ¦ (#7185)

* Don't allow requires_grad to be set on integer Tensor constructors in tensor_new.

* Fix autograd test.

* Fix test_distributions.

* Fix test_jit.

* Fix NN tests.",34.0,28.0,"test/test_autograd.py,test/test_distributions.py,test/test_jit.py,test/test_nn.py,test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp,torch/csrc/autograd/utils/python_variables.h,torch/csrc/utils/tensor_new.cpp",8.0,9,3,2.554778854,42.0,25194.0,6.0,301622.28571428574,1168.0,3135.805292,0.0,Corrective,1.0,1
pytorch,3277d83648433317a502ea4fdc54d3ea02d40ea4,7ad948ffa95af4b19394a182dab477f3de853205,soumith,soumith@fb.com,Wed Mar 01 22:01:10 2017 -0800,1488405670.0,"fix tests to not sys.exit(), also fix fatal error on THC initialization",48.0,42.0,"test/test_cuda.py,test/test_nccl.py,torch/csrc/cuda/Module.cpp,torch/cuda/__init__.py",4.0,5,2,0.963978905,24.0,1552.0,1.0,18433.0,501.0,3355.471519,0.0,Corrective,1.0,1
pytorch,006f1a32f85d2225558b1f8ce1b46bd51073f355,7aec364bdf9ed7297b77e8445a6a6d4116265dde,Taylor Robie,taylorrobie@fb.com,Thu Apr 23 18:45:03 2020 -0700,1587667503.0,"extend gather shape check to handle incorrectly sized outputs (#37102)

Summary:
Fixes a safety issue (Nonsense values and segfaults) introduced by https://github.com/pytorch/pytorch/pull/36875 when in-place gather tries to use incorrect shapes.

Consider the following block of code:
```
k0 = 8
k1 = 8
m = 100

x = torch.rand((k0, k1))
ind = torch.randint(0, k0, (m, k1))
output = torch.empty((m, k1))

print(torch.gather(x, 0, ind, out=output))
print(torch.gather(x, 1, ind, out=output))
```

The first gather is legal, the second is not. (`ind` and `output` need to be transposed) Previously this was caught when the kernel tried to restride inputs for TensorIterator, but we can no longer rely on those checks and must test explicitly. If `m` is small the second gather returns gibberish; if it is large enough to push the read out of memory block the program segfaults.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37102

Differential Revision: D21190580

Pulled By: robieta

fbshipit-source-id: 80175620d24ad3380d78995f7ec7dbf2627d2998",23.0,6.0,"aten/src/ATen/native/cpu/ScatterGatherKernel.cpp,test/test_torch.py",2.0,6,2,0.479832024,41.0,17797.0,2.0,50052.0,1343.0,3614.0,0.0,Corrective,1.0,1
pytorch,3cd825d25eb315f386ded80a065534c52cbc3563,7af433deebc188ceb55ccc0d475a1cf196f8cb6a,gchanan,gregchanan@gmail.com,Tue Feb 06 23:40:37 2018 -0500,1517960437.0,"Add scalar criterion tests (#5087)

* Add criterion scalar tests.

This exposed an issue in MarginRankingLoss with scalars, but the cleanest way to fix is to wait
until forward runs on Variables (so we don't have to wait for the backward to check if something
is a scalar).

* Fix flake8.

* Add error message for margin_ranking_loss with scalars.",73.0,5.0,"test/common_nn.py,test/test_nn.py,torch/nn/functional.py",3.0,3,2,0.744194214,37.0,8627.0,2.0,22713.0,508.0,1516.405869,0.0,Corrective,1.0,1
pytorch,011f6159453e942e6d5b08582428d48bc130d9dc,7af6f9515f50a73126bf1fd59c1561600879dacb,Edward Yang,ezyang@fb.com,Sun Sep 02 04:38:49 2018 -0700,1535863129.0,"Move TensorAccessor to ATen/core

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/11014

Reviewed By: cpuhrsch

Differential Revision: D9561802

fbshipit-source-id: d3dbe6d7e76e2419ead81fb448711f101daee19f",51.0,49.0,"aten/src/ATen/TensorAccessor.h,aten/src/ATen/core/TensorAccessor.h,aten/src/ATen/templates/Tensor.h",3.0,5,1,1.121440543,7.0,353.0,2.0,846963.0,3811.0,10513.33333,0.0,,0.0,1
pytorch,fd3cfb683a0ed1b8cddd52b8fac821ebbe814e33,7afe4afd86d333df4bb64d396720f18df9b64420,Thiago Crepaldi,thiago.crepaldi@microsoft.com,Thu Apr 28 21:40:06 2022 +0000,1651182006.0,"Export aten::to(""cpu"") and aten::to(device=""cpu"")

Fixes https://github.com/facebookresearch/detectron2/pull/4120 after https://github.com/facebookresearch/detectron2/pull/4132 was merged

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76498
Approved by: https://github.com/garymm",77.0,26.0,"test/onnx/test_onnx_export.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.870346055,4.0,4009.0,2.0,382825.0,2717.0,6478.0,0.0,Corrective,1.0,1
pytorch,db1e9b1d6c8b00896cf123319eb59c64914a0600,7b081e5d1e4670f8b26c8f611c2fff7ac6aada4d,Will Feng,willfeng@fb.com,Tue Jul 30 05:21:40 2019 -0700,1564464100.0,"Improve error message for changing tensor metadata after .data or .detach() (#23504)

Summary:
When a user tries to change metadata of a tensor created from `.data` or `.detach()`, we currently shows an error message ""<function_name> is not allowed on Tensor created from .data or .detach()"". However, this error message doesn't suggest what the right fix should look like. This PR improves the error message.

Closes https://github.com/pytorch/pytorch/issues/23393.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23504

Differential Revision: D16547415

Pulled By: yf225

fbshipit-source-id: 37f4a0385442e2b0966386fb14d3d938ecf4230c",39.0,23.0,"aten/src/ATen/SparseTensorImpl.cpp,aten/src/ATen/SparseTensorImpl.h,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,test/test_sparse.py,test/test_torch.py",6.0,6,3,2.346295218,41.0,16898.0,6.0,2934135.333333333,10286.0,29479.33333,0.0,Corrective,1.0,1
pytorch,a769fae91d8184dd1cadce527b9ffa1559bd8c12,7b25cbbef9ffd894516c7a5f92eb62878a17445f,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Fri Jul 06 03:58:48 2018 -0700,1530849528.0,"Test nn.Module on non-contiguous inputs (#9114)

Summary:
1. Let `ModuleTest` raise when they fail on non-contiguous inputs. Fix legacy modules.
2. Fix BN (both THNN and cuDNN) not working on non-contiguous inputs.
3. Fix CUDA EmbeddingBag not working on non-contiguous inputs. To prevent calling `.contiguous()` on in both `forward` and `backward`,
  a. prefix all current `embedding_bag*` functions with `_`, indicating that they require input to be contiguous (there is a check in each function).
  b. create `embedding_bag`, which makes input arguments `.contiguous()`, and calls `_embedding_bag`
3. Make many ATen `embedding*` functions to work on non-contiguous inputs so we don't need to call `input = input.contiguous()` in Python `nn.functional.embedding`.
4. Fix dense-sparse addition when the sparse input is not coalesced and indices or values tensor is not contiguous. This came up in the test cases of Embedding modules with `sparse=True`. Added tests.
5. Update `TensorUtils.cpp` to use `AT_*` macros.

Request:
review from cpuhrsch on the `Embedding*` changes.
review from ezyang on ATen sparse & BN changes.
Closes https://github.com/pytorch/pytorch/pull/9114

Differential Revision: D8717299

Pulled By: SsnL

fbshipit-source-id: 0acc6f1c9522b5b605361e75112c16bbe1e98527",294.0,239.0,"aten/src/ATen/TensorUtils.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,test/common_nn.py,test/test_cuda.py,test/test_nn.py,test/test_sparse.py,tools/autograd/derivatives.yaml,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/SpatialConvolutionLocal.py,torch/legacy/nn/SpatialFullConvolution.py,torch/legacy/nn/VolumetricConvolution.py,torch/legacy/nn/VolumetricFullConvolution.py,torch/nn/functional.py",19.0,14,4,3.081242513,45.0,20599.0,11.0,3023117.263157895,2823.0,6430.333333,0.0,Corrective,1.0,1
pytorch,111da779125577e254eec7f45d272a8f14a8d550,7b2e8c323c97f12cff402b347e59541ad67d75cb,Vitaly Fedyunin,vitalyf@fb.com,Thu Oct 03 19:04:42 2019 -0700,1570129482.0,"Add memory format argument to the `clone` operator (#27106)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27106

Adds memory_format option to the `clone` operator.

Introduce new `clone` behavior if used with `input_t.clone(memory_format=torch.preserve_format)`:
1) If tensor is non-overlapping and dense - output tensor will have the same strides as input tensor.
2) If not (1) and tensor is stored in the channels last format, output tensor going to have channels last format.
3) Output tensor is going to be contiguous in all other cases.

 ---
Dense tensor is the tensor that store values in a contiguous block of memory.
Non-overlapping tensor is the tensor in which elements occupy individual non-repetitive memory.

Test Plan: Imported from OSS

Differential Revision: D17699357

Pulled By: VitalyFedyunin

fbshipit-source-id: 5ae1537c2aca1abf0bf1eec4416846129c156f66",147.0,26.0,"aten/src/ATen/core/OpsAlreadyMovedToC10.cpp,aten/src/ATen/core/TensorBody.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/mkldnn/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/QTensor.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/templates/TensorBody.h,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,test/test_torch.py,tools/autograd/derivatives.yaml,torch/csrc/jit/passes/shape_analysis.cpp,torch/onnx/symbolic_opset9.py",15.0,19,5,3.467943647,43.0,38720.0,15.0,1287248.0,11995.0,33700.33333,0.0,Feature Addition,0.0,1
pytorch,4afd62db099f8e967a30aea66c1f345b5cbc99e2,7b33ef4cffed0dcd5c2506c4db1b2624736a22a3,Piotr Mitros,piotr@mitros.org,Thu Mar 01 13:53:11 2018 -0500,1519912391.0,Documentation cleanup for activation functions (#5457),212.0,36.0,"docs/Makefile,docs/source/nn.rst,docs/source/scripts/build_activation_images.py,torch/nn/modules/activation.py",4.0,6,2,1.685159659,37.0,1942.0,3.0,6491228.666666667,975.0,6840.672317,0.0,Non Functional,0.0,1
pytorch,630af4d7d8812efa43a01d938f0fc53c47802b0b,7b578dd68ef576b4b9bb254f11669e1dd69d7931,Adam Paszke,adam.paszke@gmail.com,Tue May 23 19:42:29 2017 -0400,1495568549.0,Add scatterAdd,45.0,44.0,"test/test_autograd.py,test/test_cuda.py,test/test_torch.py,torch/autograd/_functions/tensor.py,torch/csrc/generic/methods/Tensor.cwrap",5.0,7,2,1.522552075,29.0,7565.0,1.0,4161.0,792.0,11097.20434,0.0,Feature Addition,0.0,1
pytorch,59d1d17775843dda5af09eeb8a20bd5c4dfd747f,7b61b458b1ff46a62e7b2687ba4a694be674778c,li-roy,8813817+li-roy@users.noreply.github.com,Sat Mar 10 04:43:55 2018 -0800,1520657035.0,"Make torch.arange consistent with numpy.arange (#5600)

* Fix arange floating point error

* fix test

* add type cast when calculating arange size

* fix nit

* update test

* use doubles instead of floats to calculate size

* requested changes",40.0,28.0,"aten/src/TH/generic/THTensorMath.c,aten/src/THC/generic/THCTensorMath.cu,test/test_torch.py",3.0,7,2,1.492790774,38.0,10674.0,3.0,2398620.333333333,2437.0,24757.35823,0.0,Corrective,1.0,1
pytorch,4c5b1cc02601ae5bcc00cefa592874139f4e22c2,7b87ecae37450894f46a8df6d0d71adfdc0ecc41,Will Feng,willfeng@fb.com,Thu Dec 27 00:31:47 2018 -0800,1545870707.0,"Move autograd metadata from VariableImpl to TensorImpl (#13827)

Summary:
Changes originally in this PR:
1. Move Variable::Impl data members into TensorImpl as `AutogradMeta` struct
2. Change Variable::Impl functions to use data members in `AutogradMeta` struct
3. Add `shallow_copy_and_detach()` function to each subclass of TensorImpl
4. Do shallow copy when the user calls `make_variable(tensor)` / `make_variable_view(tensor)` / `variable.set_data(tensor)` / `variable.detach()`

Changes moved from https://github.com/pytorch/pytorch/pull/13645:
1. Add a flag to Variable to disallow size/stride/storage_ptr changes from in-place operations such as `resize_` / `resize_as_` / `set_` / `transpose_`, and set this flag to true when people call `tensor.data` in Python.
2. Write text in the docs to actively discourage changing the shape or storage of `tensor_detached` and expecting `tensor` to also be updated.

This is the 1st+2nd PR mentioned in https://github.com/pytorch/pytorch/issues/13638.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13827

Differential Revision: D13507173

Pulled By: yf225

fbshipit-source-id: b177b08438d534a8197e34e1ad4a837e2db0ed6a",418.0,136.0,"aten/src/ATen/SparseTensorImpl.cpp,aten/src/ATen/SparseTensorImpl.h,aten/src/TH/THTensor.cpp,aten/src/TH/THTensor.hpp,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,test/common_methods_invocations.py,test/common_utils.py,test/test_jit.py,test/test_nn.py,test/test_sparse.py,test/test_torch.py,torch/csrc/autograd/functions/accumulate_grad.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/jit/python_ir.cpp,torch/tensor.py",18.0,12,4,2.683697448,45.0,37458.0,14.0,876767.7777777778,6228.0,19373.33333,0.0,Feature Addition,0.0,1
pytorch,7926313235248fd26f1c235b3c67e1533e8a9dc2,7be457c2a4a87fe70c743ee62a42eabc9c00039c,gchanan,gregchanan@gmail.com,Sat Jun 02 15:26:02 2018 -0400,1527953162.0,"Reduce usages of TensorUtils<T>::DataType in THC. (#8056)

TensorUtils<T> is basically ATen-dispatch-lite in that it allows one to do multi-type THC function dispatch with a single call.
However, it is templatized on the Tensor type, and since we are moving to a single Tensor type, this doesn't work.

Most of the functions in TensorUtils (e.g. getDims) can be pulled up a level, to just call THCTensor_nDimension (or directly accessing the member),
but the DataType specific functions are more problematic.

So, this PR does two things:
1) Replaces calls of 'TensorUtils<THCTensor>::DataType' with 'real' since these are identical
2) Templatizes the THC_pointwiseApplyX functions to take scalar types.  To ensure this is done correctly, we static_assert that the scalar type template parameter matches the scalar type of
   the corresponding template parameter.  We will need to get rid of these static_asserts in the future, but this is useful for now.",268.0,251.0,"aten/src/THC/THCApply.cuh,aten/src/THC/THCTensorCopy.cu,aten/src/THC/THCTensorIndex.cu,aten/src/THC/THCTensorMathCompare.cuh,aten/src/THC/THCTensorMathCompareT.cuh,aten/src/THC/generic/THCTensorCopy.cu,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMasked.cu,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMathCompare.cu,aten/src/THC/generic/THCTensorMathCompareT.cu,aten/src/THC/generic/THCTensorMathPairwise.cu,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THCUNN/generic/Abs.cu,aten/src/THCUNN/generic/AbsCriterion.cu,aten/src/THCUNN/generic/BCECriterion.cu,aten/src/THCUNN/generic/DistKLDivCriterion.cu,aten/src/THCUNN/generic/ELU.cu,aten/src/THCUNN/generic/FeatureLPPooling.cu,aten/src/THCUNN/generic/GatedLinearUnit.cu,aten/src/THCUNN/generic/HardTanh.cu,aten/src/THCUNN/generic/LeakyReLU.cu,aten/src/THCUNN/generic/LogSigmoid.cu,aten/src/THCUNN/generic/MSECriterion.cu,aten/src/THCUNN/generic/PReLU.cu,aten/src/THCUNN/generic/RReLU.cu,aten/src/THCUNN/generic/Sigmoid.cu,aten/src/THCUNN/generic/SmoothL1Criterion.cu,aten/src/THCUNN/generic/SoftMarginCriterion.cu,aten/src/THCUNN/generic/SoftPlus.cu,aten/src/THCUNN/generic/SoftShrink.cu,aten/src/THCUNN/generic/Sqrt.cu,aten/src/THCUNN/generic/Square.cu,aten/src/THCUNN/generic/Tanh.cu,aten/src/THCUNN/generic/Threshold.cu",36.0,6,1,3.889645494,5.0,6549.0,7.0,5026863.972222222,678.0,3688.0,0.0,Corrective,1.0,1
pytorch,d905a90f0b528688b9a97caf6bac1b786d9b3f5d,7c0b16c1407b57685e684815cc6677546e27b601,Sam Gross,colesbury@gmail.com,Wed Nov 01 10:04:44 2017 -0400,1509530684.0,"Add torch.take and Tensor.put_ (#3263)

* Add torch.take and Tensor.put_

These are similar to numpy.take and numpy.put. The take function allows
you to linearly index into a tensor without viewing it as a 1D tensor
first. The output has the same shape as the indices. The put function
copies value into a tensor also using linear indices.",414.0,9.0,"docs/source/tensors.rst,docs/source/torch.rst,test/test_cuda.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/Tensor.cwrap,torch/lib/ATen/Declarations.cwrap,torch/lib/TH/generic/THTensorMath.c,torch/lib/TH/generic/THTensorMath.h,torch/lib/THC/THCTensorIndex.cu,torch/lib/THC/THCTensorTypeUtils.cu,torch/lib/THC/THCTensorTypeUtils.cuh,torch/lib/THC/generic/THCTensorIndex.cu,torch/lib/THC/generic/THCTensorIndex.h",17.0,15,4,3.179496699,39.0,24819.0,6.0,324580.4117647059,2042.0,23911.35823,0.0,Feature Addition,0.0,1
pytorch,b6d494d6dab8ec616050c65a134615015d540594,7c13a072861e2703d0f2e8bab50a0fb6bb032c45,Ailing Zhang,ailzhang@fb.com,Tue May 12 20:32:26 2020 -0700,1589315546.0,"[Reland] Remove uses of type() part 2 (#38288)

Summary:
Reland of https://github.com/pytorch/pytorch/issues/38140. It got reverted since it broke slow tests which were only run on master branch(thanks mruberry !). Enabling all CI tests in this PR to make sure they pass.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38288

Reviewed By: mruberry

Differential Revision: D21524923

Pulled By: ailzhang

fbshipit-source-id: 3a9ecc7461781066499c677249112434b08d2783",73.0,74.0,"test/test_autograd.py,test/test_torch.py,torch/autograd/gradcheck.py,torch/backends/cudnn/__init__.py",4.0,5,2,0.637783269,43.0,25031.0,3.0,155936.0,1972.0,5064.5,0.0,,0.0,1
pytorch,6912f7c564b536ad2f7ce75368a6d44f540fa04d,7c1c239db1778de001285758dafd9f54157684af,Jason Ansel,jansel@fb.com,Wed Jan 11 00:08:03 2023 +0000,1673395683.0,"[inductor] Rewrite Triton templates + epilogue fusion (retry) (#91575)

This reverts commit 94262efc7d381ace82aa74ed2f5f5ec76f8fca95 to reland #91105 / #90738.

Fixes https://github.com/pytorch/torchdynamo/issues/2015

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91575
Approved by: https://github.com/ngimel",1584.0,1956.0,"benchmarks/dynamo/microbenchmarks/microbench.py,benchmarks/dynamo/torchbench.py,setup.py,test/inductor/test_select_algorithm.py,test/inductor/test_torchinductor.py,torch/_dynamo/testing.py,torch/_inductor/codecache.py,torch/_inductor/codegen/autotuner.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/triton_conv_delta_x.j2,torch/_inductor/codegen/triton_conv_delta_x_hwc.j2,torch/_inductor/codegen/triton_mm.j2,torch/_inductor/codegen/triton_template.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/config.py,torch/_inductor/debug.py,torch/_inductor/decomposition.py,torch/_inductor/graph.py,torch/_inductor/ir.py,torch/_inductor/kernel/__init__.py,torch/_inductor/kernel/bmm.py,torch/_inductor/kernel/mm.py,torch/_inductor/kernel/mm_common.py,torch/_inductor/lowering.py,torch/_inductor/scheduler.py,torch/_inductor/select_algorithm.py,torch/_inductor/sizevars.py,torch/_inductor/triton_ops/__init__.py,torch/_inductor/triton_ops/autotune.py,torch/_inductor/triton_ops/batched_matmul.py,torch/_inductor/triton_ops/matmul.py,torch/_inductor/triton_ops/mm_perf_model.py,torch/_inductor/utils.py",34.0,11,3,4.180008362,45.0,26240.0,12.0,1415983.9705882352,11158.0,25442.0,0.0,Corrective,1.0,1
pytorch,6133be31bde86543e9c173fb18b675bfb2db75c4,7c2944899be536ab2673a9fdfb531c7b7df2550d,Xiang Gao,qasdfgtyuiop@gmail.com,Tue May 05 14:16:52 2020 -0700,1588688212.0,"Add vec256 for c10::complex (#37690)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37690

Test Plan: Imported from OSS

Differential Revision: D21394694

Pulled By: anjali411

fbshipit-source-id: 4f0e68280e9c9faf398cfb2d213ecdc4f5cde9fb",1162.0,178.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_std_complex_double.h,aten/src/ATen/cpu/vec256/vec256_std_complex_float.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp",7.0,7,1,2.001550643,7.0,2417.0,3.0,529751.6,1680.0,4381.5,0.0,Feature Addition,0.0,1
pytorch,d70f8dd9646909f4d465dfc6a5094a63e83c7c3f,7c472ec5974cc638cb697b4523d88324aee38fa9,Dylan Bespalko,dylan.bespalko@gmail.com,Wed Oct 09 19:46:40 2019 -0700,1570650400.0,"Vectorized complex unary and binary op support. (#26500)

Summary:
Added Complex support with AVX to unary ops and binary ops.

I need to add nan propagation to minimum() and maximum() in the future.
In-tree changes to pytorch to support complex numbers are being submitted here.
Out-of-tree support for complex numbers is here: pytorch-cpu-strided-complex extension

Preliminary Benchmarks are here.

I tried rrii and riri and found that riri is better in most situations.
Divide is very slow because you can't reduce 1/(x+y)
Sqrt is also very slow.
Reciprocal could be sped up after I add conj()
Everything else is typically within 20% of the real number performance.
Questions:

Why does macOS not support mil? #if AT_MKL_ENABLED() && !defined(__APPLE__) in vml.h. MKL does support some complex operations like Abs, so I was curious about trying it.
Is MKL just calling AVX?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26500

Differential Revision: D17835431

Pulled By: ezyang

fbshipit-source-id: 6746209168fbeb567af340c22bf34af28286bd54",1462.0,82.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/NumericUtils.h,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cpu/Loops.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/zmath.h,aten/src/ATen/native/native_functions.yaml,benchmarks/operator_benchmark/benchmark_core.py,c10/core/ScalarType.h,c10/util/Complex.h,docs/source/tensors.rst,docs/source/torch.rst,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/TypeInfo.cpp",25.0,16,5,3.254407345,35.0,23100.0,20.0,1378947.0476190476,12136.0,33984.33333,0.0,Feature Addition,0.0,1
pytorch,7749804099b8a64aea4bf91e298a20976f9b10ad,7c4aef9dfc71c0c79eb0c442a111127793da4611,Edward Yang,ezyang@fb.com,Wed Nov 28 19:05:36 2018 -0800,1543431936.0,"Add support for HIP to DispatchStub. (#14413)

Summary:
I feel a bit bad writing this patch, because there isn't really
any reason not to use the normal dispatch mechanism for CUDA
and HIP here (so we have *yet another dispatcher*), but I don't
really want to sign up to rewrite DispatchStub to deduplicate the
dispatcher right now.

Need to natively add support for HIP here, as I don't want to
have to HIPify files which are not in a CUDA directory.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14413

Differential Revision: D13220358

Pulled By: ezyang

fbshipit-source-id: cc61218322589a1dc2ab8eb9d5ddd3c616f6b712",24.0,3.0,aten/src/ATen/native/DispatchStub.h,1.0,4,1,0,1.0,160.0,1.0,79687.0,5667.0,17207.83333,0.0,Feature Addition,1.0,1
pytorch,12f92f453bd059fd5e58a4e49f10e3de638f0019,7c66ad7455ae7acf215e07a6d4dcd70736c77517,Sebastian Messmer,messmer@fb.com,Wed Jan 30 02:02:21 2019 -0800,1548813741.0,"Add test case for calling c10 ops from pytorch

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/16062

Reviewed By: ezyang

Differential Revision: D13628955

fbshipit-source-id: f6ed3f07db2675bd9ae9251da990ca7b8c963717",10.0,0.0,test/test_torch.py,1.0,1,1,0,40.0,10099.0,1.0,18983.0,6705.0,20744.33333,0.0,Feature Addition,0.0,1
pytorch,dc76db349e9022b8b4e23de19780a5edf63cc5a1,7c729e6321f795e61bf2393aedde3345d6a90f69,ptrblck,ptrblck@users.noreply.github.com,Thu Jan 04 14:52:47 2018 +0100,1515077567.0,- added size_splits to functional (#3837),53.0,13.0,"test/test_torch.py,torch/functional.py",2.0,2,2,0.918295834,38.0,5855.0,2.0,542078.0,2248.0,24330.35823,0.0,Feature Addition,0.0,1
pytorch,d5eea7f1a785fbef56c4e82957f53cca8ffd91a4,7c993d7f0358ab8eff3c0e0f9d9d2374678b6e70,Richard Zou,zou3519@gmail.com,Fri Aug 05 18:01:19 2022 -0700,1659722479.0,"[functorch] refactor get_fallback_and_vmap_exhaustive (#82897)

This PR refactors get_fallback_and_vmap_exhaustive into two helper
functions, `generate_vmap_inputs` and
`compute_quantities_for_vmap_test`. The former is responsible for
generating in_dims and the batched inputs; the latter is responsible for
computing vmap(blah) and the fallback.

The rationale for the refactor is that I want to use these pieces
separately in in-place vmap testing, which is coming soon.

Test Plan:
- run tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82897
Approved by: https://github.com/Chillee",65.0,50.0,"functorch/test/common_utils.py,functorch/test/test_vmap.py",2.0,2,1,0.853070287,2.0,4572.0,2.0,924437.5,6208.0,14403.5,0.0,Perfective,0.0,1
pytorch,df3d1d9378d805dccc518b4d474e3a2c392b7d09,7ca977687432a86faf8ad75f2ec826bb8c8c5d8f,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Feb 19 12:01:22 2021 -0800,1613736082.0,"Fixed _out variants of linear algebra functions (#51560)

Summary:
This PR modifies the behavior of `_out` variants to match the description here https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch
With this PR result and input tensors must be on the same device and have the same ""type kind"".

I skipped `qr` and `eig` in this process as they require a bit more work.

Functions that can use the provided storage directly do so. If `result` is not empty and not in the batched column-major format or does not have the same type as input then we have to allocate a temporary tensor and copy it.

TODO:

- [x] Add more tests for same device and valid safe dtype
- [x] Move inv and solve changes to separate PRs https://github.com/pytorch/pytorch/pull/51968, https://github.com/pytorch/pytorch/pull/51977

Ref. https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51560

Reviewed By: albanD

Differential Revision: D26400734

Pulled By: heitorschueroff

fbshipit-source-id: a6201ed7e919c1670c6ff3ef60217d1dbfb72e67",510.0,126.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,test/test_linalg.py",4.0,5,2,1.442300942,8.0,11007.0,4.0,1269400.0,9038.0,20249.0,0.0,Corrective,1.0,1
pytorch,85777b92b2252dc7e001122732392b14fe4aee21,7cb1aa67b0f3506d6eadf0889cbe5836d0ed6b91,Iurii Zdebskyi,iuriiz@fb.com,Wed May 29 20:19:23 2019 -0700,1559161163.0,"Enabled min, max, minall, maxall, cmin, cmax, cmaxValue, cminValue for bool tensors (#21031)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21031
ghimport-source-id: 379b3e9d20872eb5ad14403ed6751cdb0e730bc5

Reviewed By: ezyang

Differential Revision: D15530499

Pulled By: izdeby

fbshipit-source-id: f113d6974ee18ac3dfb5c0bcff66865345d137d2",314.0,275.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/cuda/NumericLimits.cuh,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/THCNumerics.cuh,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathPointwise.h,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorMathReduce.h,test/test_torch.py",11.0,9,2,2.751792006,41.0,18868.0,7.0,1569752.5454545454,8978.0,26411.83333,0.0,,0.0,1
pytorch,1a4eea57be447970445f207aaf649f665748fc88,7cb7cd580247bce5ad477800e14f981ea26668fc,lezcano,lezcano-93@hotmail.com,Wed May 04 14:27:46 2022 +0000,1651674466.0,"Add linalg.lu

This PR modifies `lu_unpack` by:
- Using less memory when unpacking `L` and `U`
- Fuse the subtraction by `-1` with `unpack_pivots_stub`
- Define tensors of the correct types to avoid copies
- Port `lu_unpack` to be a strucutred kernel so that its `_out` version
does not incur on extra copies

Then we implement `linalg.lu` as a structured kernel, as we want to
compute its derivative manually. We do so because composing the
derivatives of `torch.lu_factor` and `torch.lu_unpack` would be less efficient.

This new function and `lu_unpack` comes with all the things it can come:
forward and backward ad, decent docs, correctness tests, OpInfo, complex support,
support for metatensors and support for vmap and vmap over the gradients.

I really hope we don't continue adding more features.

This PR also avoids saving some of the tensors that were previously
saved unnecessarily for the backward in `lu_factor_ex_backward` and
`lu_backward` and does some other general improvements here and there
to the forward and backward AD formulae of other related functions.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67833

Approved by: https://github.com/IvanYashchuk, https://github.com/nikitaved, https://github.com/mruberry",665.0,470.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.h,aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/allowlist_for_publicAPI.json,test/test_linalg.py,test/test_namedtuple_return_api.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/api/include/torch/linalg.h,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",21.0,20,5,3.2296019,37.0,79145.0,11.0,1975636.523809524,2888.0,6945.5,0.0,Corrective,0.0,1
pytorch,645ad7ad0c89ecef61e89666745324deba31c8b7,7cbef703722e086a867d77e82967af1206e78ec6,Lu Fang,30275821+houseroad@users.noreply.github.com,Thu Apr 26 02:20:45 2018 -0700,1524709245.0,Fix the onnx symbolic for selu and maxpool3d (#6816),17.0,1.0,torch/onnx/symbolic.py,1.0,2,1,0,9.0,979.0,1.0,8180.0,609.0,3520.0,0.0,Corrective,1.0,1
pytorch,6cc7670bed5bf9cf61d08d85ad97c130f15e7c1d,7ccecbbb4e3217e69f33a151f46253762952ae07,Peter Goldsborough,peter@goldsborough.me,Tue Jun 19 18:09:01 2018 -0700,1529431741.0,Create Tensor::options (#8630),30.0,28.0,"aten/src/ATen/TensorOptions.h,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/templates/Tensor.h,torch/csrc/api/src/nn/modules/dropout.cpp,torch/csrc/api/src/nn/modules/rnn.cpp,torch/csrc/autograd/python_variable_indexing.cpp",9.0,14,2,2.639277698,8.0,3065.0,2.0,243235.3333333333,1364.0,3988.305292,0.0,,0.0,1
pytorch,e0a9f16a10135868eeee65fa80d3b6dd6b96fcbc,7cf666f0fc2d8443fc08e612ecc13f61f85faf65,Allan Jabri,ajabri@fb.com,Thu Oct 15 19:51:42 2015 -0700,1444938702.0,add indexAccum,96.0,56.0,"generic/THTensorMath.c,generic/THTensorMath.h",2.0,1,1,0.057143858,24.0,2362.0,1.0,-240610.0,1.0,1.5,0.0,Feature Addition,0.0,1
pytorch,d3612a5914e828db3a82b36c457e1988095be00d,7d25a4125180732e8e6882c898bcadd9c9c637a3,Edward Z. Yang,ezyang@mit.edu,Mon Jan 08 17:21:09 2018 -0500,1515432069.0,"Fix #4492, make it impossible to forget to reset cudnn flags (#4503)

Three stage plan to no more stupidly weird ""why isn't cuDNN enabled""
bugs:

- Add torch.backends.cudnn.disable_global_flags(), which as its name suggests,
  disables global flag setting in cuDNN, so that you are not allowed to
  make changes to this state.  However, the flags() context
  manager continues to work (since they are non-global changes).

- Call disable_global_flags() in test/common.py

- Switch all of the manual flag setting/unsetting in test/test_nn.py
  to use the context manager.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",62.0,41.0,"test/common.py,test/test_nn.py,torch/backends/cudnn/__init__.py",3.0,4,2,1.081734918,38.0,6206.0,3.0,715814.6666666666,419.0,1313.405869,0.0,Corrective,1.0,1
pytorch,dcd686f4780b43c7cbe69d576fbd48bebd9eb19d,7d2a18da0b3427fcbe44b461a0aa508194535885,Justin Chu,justinchu@microsoft.com,Mon Apr 24 15:37:13 2023 -0700,1682350633.0,"Enable ruff in lintrunner (#99785)

### This change

- Implements the ruff linter in pytorch lintrunner. It is adapted from https://github.com/justinchuby/lintrunner-adapters/blob/main/lintrunner_adapters/adapters/ruff_linter.py. It does **both linting and fixing**. ð§
- Migrated all flake8 configs to the ruff config and enabled it for the repo. â
- **`ruff` lints the whole repo in under 2s** ð¤¯

Fixes https://github.com/pytorch/pytorch/issues/94737 Replaces #99280

@huydhn @Skylion007

<!--
copilot:all
-->
### <samp>ð¤ Generated by Copilot at 6b982dd</samp>

### Summary
ð§¹ð ï¸ð¨

<!--
1.  ð§¹ This emoji represents cleaning or tidying up, which is what `ruff` does by formatting and linting the code. It also suggests improving the code quality and removing unnecessary or redundant code.
2.  ð ï¸ This emoji represents tools or fixing, which is what `ruff` is as a code formatter and linter. It also suggests enhancing the code functionality and performance, and resolving potential issues or bugs.
3.  ð¨ This emoji represents art or creativity, which is what `ruff` allows by providing a consistent and configurable style for the code. It also suggests adding some flair or personality to the code, and making it more readable and enjoyable.
-->
Add `[tool.ruff]` section to `pyproject.toml` to configure `ruff` code formatter and linter. This change aims to improve code quality and consistency with a single tool.

> _`ruff` cleans the code_
> _like a spring breeze in the fields_
> _`pyproject.toml`_

### Walkthrough
*  Configure `ruff` code formatter and linter for the whole project ([link](https://github.com/pytorch/pytorch/pull/99785/files?diff=unified&w=0#diff-50c86b7ed8ac2cf95bd48334961bf0530cdc77b5a56f852c5c61b89d735fd711R22-R79))

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99785
Approved by: https://github.com/malfet, https://github.com/Skylion007",552.0,2.0,".flake8,.lintrunner.toml,pyproject.toml,tools/linter/adapters/ruff_linter.py,tools/test/test_selective_build.py,torch/distributed/pipeline/sync/microbatch.py",6.0,8,2,0.860794647,4.0,1582.0,5.0,2046920.2,14994.0,33971.0,0.0,Corrective,1.0,1
pytorch,348867c10b14509518291fc7a03d17a087f9c7bf,7d5f7ed270079ea326545b94e03034f0172cc53c,Yangqing Jia,jiayq@fb.com,Wed Oct 17 19:55:01 2018 -0700,1539806101.0,"Using c10 namespace across caffe2. (#12714)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12714

This is a short change to enable c10 namespace in caffe2. We did not enable
it before due to gflags global variable confusion, but it should have been
mostly cleaned now. Right now, the plan on record is that namespace caffe2 and
namespace aten will fully be supersets of namespace c10.

Most of the diff is codemod, and only two places of non-codemod is in caffe2/core/common.h, where

```
using namespace c10;
```

is added, and in Flags.h, where instead of creating aliasing variables in c10 namespace, we directly put it in the global namespace to match gflags (and same behavior if gflags is not being built with).

Reviewed By: dzhulgakov

Differential Revision: D10390486

fbshipit-source-id: 5e2df730e28e29a052f513bddc558d9f78a23b9b",414.0,433.0,"aten/src/ATen/core/TensorImpl.h,binaries/caffe2_benchmark.cc,binaries/convert_caffe_image_db.cc,binaries/convert_db.cc,binaries/convert_encoded_to_raw_leveldb.cc,binaries/convert_image_to_tensor.cc,binaries/db_throughput.cc,binaries/make_cifar_db.cc,binaries/make_image_db.cc,binaries/make_mnist_db.cc,binaries/predictor_verifier.cc,binaries/print_registered_core_operators.cc,binaries/run_plan.cc,binaries/run_plan_mpi.cc,binaries/speed_benchmark.cc,binaries/split_db.cc,binaries/tsv_2_proto.cc,binaries/zmq_feeder.cc,c10/macros/Macros.h,c10/test/flags_test.cpp,c10/util/Flags.h,caffe2/contrib/nnpack/nnpack_ops.cc,caffe2/contrib/prof/htrace_conf.cc,caffe2/core/allocator.h,caffe2/core/blob_serialization.cc,caffe2/core/blob_test.cc,caffe2/core/common.h,caffe2/core/common_gpu.cc,caffe2/core/context_gpu.cu,caffe2/core/context_gpu_test.cc,caffe2/core/hip/common_hip.cc,caffe2/core/hip/context_hip.cc,caffe2/core/hip/net_async_dag_hip.cc,caffe2/core/hip/net_async_hip_thread_pool_hip.cc,caffe2/core/init.cc,caffe2/core/init_intrinsics_check.cc,caffe2/core/init_omp.cc,caffe2/core/logging.cc,caffe2/core/logging_test.cc,caffe2/core/net.cc,caffe2/core/net_async_base.cc,caffe2/core/net_async_base.h,caffe2/core/net_async_dag_gpu.cc,caffe2/core/net_async_gpu_thread_pool_gpu.cc,caffe2/core/net_async_polling.cc,caffe2/core/net_async_scheduling.cc,caffe2/core/net_async_tracing.cc,caffe2/core/net_dag.cc,caffe2/core/net_gpu_test.cc,caffe2/core/net_simple.cc,caffe2/core/net_test.cc,caffe2/core/numa.cc,caffe2/core/operator.cc,caffe2/core/plan_executor.cc,caffe2/core/workspace.h,caffe2/db/leveldb.cc,caffe2/mobile/contrib/arm-compute/test/gl_model_test.h,caffe2/mobile/contrib/ios/ios_caffe_predictor.cc,caffe2/operators/conv_op.h,caffe2/operators/conv_op_impl.h,caffe2/operators/conv_transpose_op_impl.h,caffe2/operators/conv_transpose_op_mobile_impl.h,caffe2/operators/conv_transpose_unpool_op_base.h,caffe2/operators/create_scope_op.h,caffe2/operators/deform_conv_op.h,caffe2/operators/deform_conv_op_impl.h,caffe2/operators/rnn/recurrent_network_op.h,caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,caffe2/share/contrib/nnpack/conv_op.cc,caffe2/utils/signal_handler.cc,caffe2/utils/threadpool/ThreadPool.cc,modules/observers/perf_observer.cc,modules/rocksdb/rocksdb.cc",73.0,32,5,5.504280364,20.0,19721.0,17.0,845484.1643835617,4691.0,13848.33333,0.0,Feature Addition,0.0,1
pytorch,21d9b0c9dd5ffb31a2badbb3d292ec1b4f3f7e15,7da46097fe6145cb211dbe977405d2646eabb270,Gregory Chanan,gchanan@fb.com,Wed May 31 22:24:52 2017 -0700,1496269492.0,Fix lint errors.,67.0,51.0,"test/test_torch.py,tools/cwrap/plugins/Broadcast.py,torch/_torch_docs.py,torch/legacy/nn/SpatialDivisiveNormalization.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/tensor.py,torch/utils/backcompat/broadcast/warning/__init__.py,torch/utils/backcompat/keepdim/warning/__init__.py",8.0,13,3,1.975412068,32.0,9242.0,8.0,0.0,903.0,11482.44394,0.0,Corrective,1.0,1
pytorch,d46978cc5542ea425f31306dd17e6017d12f9b60,7df176b1f983d5e12fea0fe799d3b9a28d32ff11,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Sun Mar 14 09:07:50 2021 -0800,1615712870.0,"Added OpInfo-based testing of some linalg functions (#51107)

Summary:
Added OpInfo-based testing of the following linear algebra functions:
* cholesky, linalg.cholesky
* linalg.eigh
* inverse, linalg.inv
* qr, linalg.qr
* solve

The output of `torch.linalg.pinv` for empty inputs was not differentiable, now it's fixed.

In some cases, batched grad checks are disabled because it doesn't work well with 0x0 matrices (see https://github.com/pytorch/pytorch/issues/50743#issuecomment-767376085).

Ref. https://github.com/pytorch/pytorch/issues/50006

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51107

Reviewed By: albanD

Differential Revision: D27006115

Pulled By: mruberry

fbshipit-source-id: 3c1d00e3d506948da25d612fb114e6d4a478c5b1",228.0,150.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,0.771115761,8.0,18789.0,4.0,269916.2,9770.0,21599.5,0.0,Corrective,1.0,1
pytorch,0d4eefcd82b7c921c9e1edb35539d083815c10e3,7e16dd299ada5cb273bfdc55d8bc8eb933799279,Jeff Daily,jeff.daily@amd.com,Thu May 28 14:02:42 2020 -0700,1590674562.0,"[ROCm] enable mem leak check for rocm (#35953)

Summary:
CC iotamudelta
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35953

Differential Revision: D21742926

Pulled By: zou3519

fbshipit-source-id: f18534dbb88a84fe98b8d85ce8fde652916a72d5",3.0,11.0,"test/test_cuda.py,torch/testing/_internal/common_utils.py",2.0,4,2,0.591672779,41.0,4562.0,2.0,39360.0,2388.0,6008.0,0.0,,0.0,1
pytorch,f008e8d32d758115c978352e002d86a1faf2f18a,7e1f01d4c0455e88f90e7b4b6906472f4c47e0fb,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Fri Jul 16 07:05:06 2021 -0700,1626419106.0,"Alias for `polygamma` (#59691)

Summary:
See https://github.com/pytorch/pytorch/issues/50345

cc: mruberry kshitij12345

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59691

Reviewed By: gchanan

Differential Revision: D29707514

Pulled By: mruberry

fbshipit-source-id: 40c15e1fda3d9f7013977b0f36a77b228dda6aa5",95.0,28.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_fx.py,test/test_fx_experimental.py,torch/_torch_docs.py,torch/csrc/api/include/torch/special.h,torch/csrc/jit/passes/normalize_ops.cpp,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",13.0,18,4,2.691592879,34.0,39682.0,5.0,136286.6923076923,13871.0,31256.0,0.0,,0.0,1
pytorch,24eb5ad0c5388bd98f3f0ee3296ab4ad2c13bdd4,7e2136c2b5944914540d6c2b5da9735178cb82f6,Tongzhou Wang,ssnl@users.noreply.github.com,Mon Sep 03 16:28:22 2018 -0700,1535992102.0,"remove allclose from test_doc skipped list

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/11187

Differential Revision: D9628349

Pulled By: SsnL

fbshipit-source-id: 0ff94666542ca049a6d82091bd9fc79ec1699ac6",11.0,5.0,test/test_torch.py,1.0,1,1,0,40.0,8815.0,1.0,89114.0,3823.0,10673.33333,0.0,Non Functional,0.0,1
pytorch,7f8e852dff54d073a079f94e2b7d271631c639ee,7e34edf12dbac535c0158e9c9bf02ecf69b10d7c,Nikolay Korovaiko,korovaikon@gmail.com,Wed Jun 29 00:53:45 2022 +0000,1656464025.0,"adding sym_size override (#80357)

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80357
Approved by: https://github.com/ezyang",126.0,24.0,"c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,c10/core/impl/PyInterpreter.cpp,c10/core/impl/PyInterpreter.h,test/test_dynamic_shapes.py,torch/csrc/autograd/python_variable.cpp,torch/csrc/jit/mobile/promoted_prim_ops.cpp,torch/csrc/jit/mobile/promoted_prim_ops.h,torch/csrc/jit/runtime/register_prim_ops.cpp,torch/overrides.py",10.0,10,3,2.278697971,30.0,12142.0,8.0,3143762.3,4834.0,11423.0,0.0,Corrective,1.0,1
pytorch,8b20dde93240642b3fce14b304e2d5e6d09d9891,7e49f4638c80867692d57791f038f4307fdd9762,Philip Meier,github.pmeier@posteo.de,Wed Dec 08 16:46:52 2021 -0800,1638982012.0,"add `OpInfo` for `torch.nn.functional.kl_div` (#65469)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65469

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D31111698

Pulled By: mruberry

fbshipit-source-id: 0af41a2ef2b199db3d8c63050277e72213f04565",47.0,1.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,15047.0,1.0,46686.0,17607.0,41441.5,0.0,Feature Addition,0.0,1
pytorch,1c9347c6666d0bb8b9793b504e9cb597b75f1401,7e4c95695539c0fe0675ab0c91de761247143d8a,BowenBao,bowbao@microsoft.com,Thu Jan 28 01:41:50 2021 -0800,1611798110.0,"[ONNX] Support opset13 Squeeze and Unsqueeze (#50150) (#50906)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50906

In opset 13, squeeze/unsqueeze is updated to take axes as input, instead of attribute.

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D26050883

Pulled By: SplitInfinity

fbshipit-source-id: 7b5faf0e016d476bc75cbf2bfee6918d77e8aecd",205.0,266.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_utility_funs.py,torch/csrc/jit/passes/onnx/helper.cpp,torch/csrc/jit/passes/onnx/helper.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset9.py",11.0,8,2,2.798887533,9.0,13825.0,7.0,2451523.090909091,8410.0,18987.0,0.0,,0.0,1
pytorch,0106fe3934a3aa427392c8645a22373d951b0494,7e4e648c2a37949b71aac0478d995c136ccfcae4,Bin Bao,binbao@fb.com,Thu May 27 17:50:35 2021 -0700,1622137835.0,"Enable NNC fusion for relu6 (#58773)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58773

Test Plan:
```
python test/test_ops.py -k relu6
python test/test_jit_fuser_te.py
```

Reviewed By: bertmaher

Differential Revision: D28721791

Pulled By: desertfire

fbshipit-source-id: a94f711977afd080faae052f66eb8dded3cdc79e",39.0,1.0,"test/test_jit_fuser_te.py,torch/csrc/jit/passes/tensorexpr_fuser.cpp,torch/csrc/jit/runtime/symbolic_script.cpp,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_metaprogramming_utils.py",6.0,9,2,2.062914102,2.0,15878.0,5.0,293212.0,12525.0,28406.0,0.0,,0.0,1
pytorch,79d47c1c5ff5306bdd275196b7171c04bebbdcca,7e55494502478d7f78138bc974e681df52b0635c,Mike Ruberry,mruberry@devfair044.maas,Mon Mar 09 03:00:36 2020 -0700,1583722836.0,"Warns on read-only Numpy array->tensor conversion (#33615)

Summary:
Addresses https://github.com/pytorch/pytorch/issues/5442.

Per title (and see issue). A test is added to test_torch.py to verify the behavior.

Update (with new behavior):

NumPy arrays can be non-writeable (read-only). When converting a NumPy array to a Torch tensor the storage is shared, but the tensor is always writable (PyTorch doesn't have a read-only tensor). Thus, when a non-writeable NumPy array is converted to a PyTorch tensor it can be written to.

In the past, PyTorch would silently copy non-writeable NumPy arrays and then convert those copies into tensors. This behavior violates the from_numpy contract, however, which promises that the tensor and the array share memory.

This PR adds a warning message when a non-writeable NumPy array is converted into a Torch tensor. This will not break any networks, but will make end users aware of the behavior. They can work-around the warning message by marking their NumPy arrays as writeable.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33615

Differential Revision: D20289894

Pulled By: mruberry

fbshipit-source-id: b76df0077399eb91038b12a6bf1917ef38c2cafd",23.0,1.0,"test/test_torch.py,torch/csrc/utils/tensor_numpy.cpp",2.0,4,2,0.994984828,40.0,16107.0,2.0,376777.0,15293.0,40989.33333,0.0,Feature Addition,0.0,1
pytorch,52b8a581970830ad1b9a0c7ec66d16f2e9eae5b8,7e7afcabe70712e8d6bad0bba0adcd93e69cfd6b,Richard Zou,zou3519@gmail.com,Tue Aug 16 16:02:50 2022 -0700,1660665770.0,"[functorch] classify some more test failures (#83520)

Classifies test failures for test_vmapvjp and test_vmapjvpall

Test Plan:
- tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83520
Approved by: https://github.com/samdow",35.0,36.0,functorch/test/test_ops.py,1.0,2,1,0,2.0,1414.0,1.0,0.0,6467.0,14982.5,0.0,,0.0,1
pytorch,fcb0c77c7aac97b6c936c700c67307c6e09f557c,7eeeda5acdd118e7ad5d7884d365c61e95bb595c,Andreas KÃ¶pf,andreas.koepf@xamla.com,Thu Dec 17 09:25:30 2015 +0100,1450344330.0,Remove special handing for OSX search path,7.0,2.0,CMakeLists.txt,1.0,0,0,0,29.0,19.0,1.0,0.0,49.0,324.9047619,0.0,,0.0,1
pytorch,e8ecbcdf010d1e65384ba2d1f8760cc557c02883,7f0dd2487d70015218657e61c68750b8169bf9e8,Sam Gross,sgross@fb.com,Tue Sep 18 01:11:19 2018 -0700,1537233079.0,"Move AT_HOST_DEVICE macro to Macros.h (#10945)

Summary:
```
I'm using AT_HOST_DEVICE outside of Half.h in an upcoming PR. Since this
changes code without making any semantic changes, I wanted to make this
change in a separate PR.
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10945

Differential Revision: D9539821

Pulled By: colesbury

fbshipit-source-id: 0daae40ea78b077a543f7bfeec06b225634540de",71.0,85.0,"aten/src/ATen/core/Half-inl.h,aten/src/ATen/core/Half.h,aten/src/ATen/core/Macros.h,aten/src/ATen/core/TensorAccessor.h",4.0,4,1,1.662709014,1.0,694.0,4.0,436743.25,4175.0,11650.33333,0.0,,0.0,1
pytorch,16ece1c9daefe58ba56db63d9c930a21411998d9,7f183a978f31a87a7350c526386cec92bb6b2740,Mike Ruberry,mruberry@devfair044.maas,Tue Oct 08 16:50:28 2019 -0700,1570553428.0,"Stops common_utils.py from setting the default tensor type (to torch.DoubleTensor) (#27444)

Summary:
This PR stop common_utils.py from setting the default tensor type when it's imported. See issue https://github.com/pytorch/pytorch/issues/27355. This is a frequent source of confusion for test writers.

Many tests relied on this setting (whether they knew it or not), and this PR also updates the test suite to pass without common_utils.py setting the default tensor type. Some larger test files now set the default floating dtype themselves, however. These test files are:

- test_autograd.py
- test_distributions.py
- test_jit.py
- test_nn.py

This is still a significant improvement from today, however. First, these files set the default floating dtype much more clearly than importing it from common_utils. Second, the rest of the test suite no longer sets this globally. Third, this PR is a springboard to updating those tests, too. In particular, as tests are made generic they can be moved aways from relying on this global setting.

Notable technical changes in this PR are:

- Significant updates to test_torch.py to make it pass without setting the default floating dtype globally.
- The default_floating_dtype decorator is now defined in common_utils, a couple versions of this operator were defined in test files previously.
- test_torch-specific parts of common_utils were refactored into test_torch.
- tensor creation methods in common_utils were updated to accept an optional dtype and device.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27444

Differential Revision: D17795235

Pulled By: mruberry

fbshipit-source-id: 7f77271c0c836e69f183ad9057a2c4b29f09d2e1",477.0,415.0,"test/common_device_type.py,test/common_utils.py,test/test_autograd.py,test/test_c10d.py,test/test_cpp_extensions.py,test/test_cuda.py,test/test_dataloader.py,test/test_dist_autograd_fork.py,test/test_distributions.py,test/test_jit.py,test/test_namedtensor.py,test/test_nn.py,test/test_optim.py,test/test_rpc_fork.py,test/test_sparse.py,test/test_torch.py",16.0,1,1,1.554016636,47.0,67597.0,14.0,827672.1875,12090.0,33890.83333,0.0,Feature Addition,0.0,1
pytorch,1cd0dd00cead9d1a0b76d8dae8e685e582bf1977,7f344c5a0bfd9102e9ee0d9fc46fb62cf46177ce,Richard Zou,zou3519@gmail.com,Mon May 17 21:08:22 2021 -0700,1621285702.0,[functorch] Add batch rule for nll_loss_forward for most common cases,115.0,0.0,"functorch/functorch/csrc/BatchRulesLoss.cpp,functorch/test/test_vmap.py",2.0,4,1,0.625923132,1.0,2835.0,1.0,1.0,91.0,192.0,0.0,Feature Addition,0.0,1
pytorch,3eac7164f4cc1ca0512b9450eec23f37be6c201d,7f4ff0e615e0fbeb96acbe82946b2afcc29d310f,Adam Paszke,adam.paszke@gmail.com,Tue Sep 27 18:54:15 2016 -0700,1475002455.0,Fix type conversions in nn,138.0,64.0,"test/common_nn.py,test/test_nn.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/modules/pooling.py,torch/nn/parallel/__init__.py,torch/nn/parallel/functions.py",10.0,5,2,2.719987753,6.0,2523.0,4.0,16070.8,203.0,3370.992183,0.0,Corrective,1.0,1
pytorch,9324181d0ac7b4f7949a574dbc3e8be30abe7041,7f6580a868026ea31c7320d8d0e60e90bbbaca94,kshitij12345,kshitijkalambarkar@gmail.com,Tue Sep 21 15:49:28 2021 -0700,1632239368.0,"OpInfo: nn.functional.conv2d (#65233)

Summary:
Reland : https://github.com/pytorch/pytorch/issues/63517
Reference: https://github.com/pytorch/pytorch/issues/54261

Reference: facebookresearch/functorch#78

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65233

Reviewed By: malfet

Differential Revision: D31025538

Pulled By: zou3519

fbshipit-source-id: b1cd38c22f4cb8eedd3f958e02dd7410dcbb8d8d",57.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.127418512,2.0,11726.0,2.0,211939.0,15614.0,35918.0,0.0,,0.0,1
pytorch,7dcc723d35a8795c7f386cf0a439299a89432a75,7f88934a8fb9b376b32c722ac2f05959da34c147,soulitzer,soulitzer@gmail.com,Thu Sep 15 22:46:16 2022 +0000,1663281976.0,"[reland 2] Call jit decomp in VariableType to improve forward AD coverage (#84976)

Reland of https://github.com/pytorch/pytorch/pull/84675
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84976
Approved by: https://github.com/zou3519",449.0,244.0,"build_variables.bzl,functorch/csrc/BatchRulesHelper.cpp,functorch/csrc/BatchRulesHelper.h,functorch/csrc/BatchRulesViews.cpp,functorch/csrc/DynamicLayer.cpp,functorch/test/test_ops.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/__init__.py,torch/_decomp/decompositions_for_jvp.py,torch/_refs/__init__.py,torch/_refs/special/__init__.py,torch/autograd/gradcheck.py,torch/csrc/Module.cpp,torch/csrc/autograd/VariableTypeUtils.h,torch/csrc/autograd/functions/utils.h,torch/csrc/autograd/jit_decomp_interface.cpp,torch/csrc/autograd/jit_decomp_interface.h,torch/csrc/jit/runtime/decomposition_registry.cpp,torch/csrc/jit/runtime/decomposition_registry.h,torch/fx/experimental/symbolic_shapes.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/_masked.py",23.0,21,3,3.458838519,49.0,37781.0,8.0,139800.14285714287,7379.0,17347.5,0.0,Perfective,0.0,1
pytorch,c78893f912aea3bd5ccf977478d52ffe488ff75f,7fa60b2e4452a1a289fe87ce90559196e03aed32,Soumith Chintala,soumith@fb.com,Wed Jan 04 23:13:05 2017 -0500,1483571585.0,"fixing docs of activations, pixelshuffle, sparse for rst",266.0,141.0,"torch/nn/modules/activation.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py",5.0,3,1,1.074447189,21.0,1634.0,4.0,89266.4,309.0,6392.724559,0.0,Corrective,1.0,1
pytorch,429d90f6487b276b02bd13b26c5291678412b389,7fa897eac045a36e85857e83535aeebff4dfa890,Jianyu Huang,jianyuhuang@fb.com,Tue May 05 07:03:04 2020 -0700,1588662184.0,"[caffe2] L2 regularization for (RowWise)SparseAdagrad fusion on GPUs (#37805)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37805

Resolve the unit test failures after  https://github.com/pytorch/pytorch/pull/37653

Test Plan:
```
buck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_sum_gradient \(caffe2\.caffe2\.fb\.net_transforms\.tests\.fuse_sparse_ops_test\.TestFuseSparseOps\)'
```

```
buck test mode/dev-nosan //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test -- 'test_fuse_sparse_adagrad_with_sparse_lengths_weighted_sum_gradient \(caffe2\.caffe2\.fb\.net_transforms\.tests\.fuse_sparse_ops_test\.TestFuseSparseOps\)'
```

Reviewed By: jspark1105

Differential Revision: D21395764

fbshipit-source-id: e8224a1ecbff5dce42ab732c0977de352fe98914",92.0,33.0,caffe2/sgd/adagrad_fused_op_gpu.cu,1.0,2,1,0,1.0,1111.0,1.0,1082112.0,1674.0,4369.5,0.0,,0.0,1
pytorch,d2f26a450ed16e9a8742c09022c4d515deb32a9b,7faca2a21740de7695000ab8c7c8926fb8e2774d,Pieter Noordhuis,pietern@fb.com,Mon Nov 05 21:49:21 2018 -0800,1541454561.0,"Add new style broadcast support in c10d/gloo (#13497)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13497

This replaces the existing broadcast implementation with the new style collective call in the gloo backend. The CUDA path copies CUDA tensors to CPU tensors and then runs the CPU broadcast implementation.

Reviewed By: teng-li

Differential Revision: D12890013

fbshipit-source-id: 43f346fb2814f421bedc7babf89169703a46bb9c",246.0,43.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.94256824,3.0,2659.0,1.0,2.0,5154.0,15401.33333,0.0,Feature Addition,0.0,1
pytorch,1e34493825ac50994561e389e81b649bbe84dd08,7fcaf3b49e0538713bb8f8b55baaf9c687dcfd4d,Tony Beltramelli,tbeltramelli@gmail.com,Tue Apr 17 15:32:32 2018 +0200,1523979152.0,"Update torch.nn.init and torch.nn.utils.clip_grad (#6173)

Introducing two updates.

1. Add param to He initialization scheme in torch.nn.init
Problem solved:
The function calculate_gain can take an argument to specify the type of non-linearity used. However, it wasn't possible to pass this argument directly to the He / Kaiming weight initialization function.

2. Add util to clip gradient value in torch.nn.utils.clip_grad
Problem solved:
DL libraries typically provide users with easy access to functions for clipping the gradients both using the norm and a fixed value. However, the utils clip_grad.py only had a function to clip the gradient norm.

* add param to He initialization scheme in torch.nn.init

* add util to clip gradient value in torch/nn/utils/clip_grad.py

* update doc in torch.nn.utils.clip_grad

* update and add test for torch.nn.utils.clip_grad

* update function signature in torch.nn.utils.clip_grad to match suffix_ convention

* ensure backward compatibility in torch.nn.utils.clip_grad

* remove DeprecationWarning in torch.nn.utils.clip_grad

* extend test and implementation of torch.nn.utils.clip_grad

* update test and implementation torch.nn.utils.clip_grad",67.0,14.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/init.py,torch/nn/utils/__init__.py,torch/nn/utils/clip_grad.py",5.0,6,3,1.97020033,38.0,8865.0,4.0,925654.4,591.0,3491.0,0.0,Corrective,1.0,1
pytorch,6c8270ea21bb8fefd3f3d56f228930fc99268b46,7fd3c030ef3e4449b6e3744e2285b69c5f732f22,lezcano,lezcano-93@hotmail.com,Mon Apr 05 17:08:52 2021 -0700,1617642532.0,"Write OpInfo for dist (#55092)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/53516
cc anjali411

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55092

Reviewed By: nikithamalgifb

Differential Revision: D27493577

Pulled By: anjali411

fbshipit-source-id: c7e8400a20bbc7138249b249e322b3b23e112336",22.0,14.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,4888.0,1.0,5069.0,10422.0,23117.0,0.0,Corrective,1.0,1
pytorch,268cc117a899ca2d42d63697107b47cf645e620d,7fe6e8e5a20bf1d279a296184b1ee6c28d86763d,Jeffrey Wan,jw3468@fb.com,Tue Apr 27 14:51:54 2021 -0700,1619535114.0,"Refactor C->C to C->R twice (#55692)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55692

### Release notes
get_numerical_jacobian and get_analytical_jacobian only support `grad_out=1` and `fn` no longer accepts functions that return complex output

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D28004614

Pulled By: soulitzer

fbshipit-source-id: 9592c9c69584b4035b39be62252f138dce39d3b5",115.0,130.0,"test/test_autograd.py,torch/autograd/gradcheck.py",2.0,3,2,0.315997133,42.0,9711.0,2.0,194970.5,11323.0,24966.5,0.0,Perfective,0.0,1
pytorch,29c389078bc8f8877df9150f0a5bd53def314561,7ffcb2029501a6759e4a2c60aa174230fba54700,samuela,skainsworth@gmail.com,Thu Mar 29 20:50:08 2018 -0700,1522356608.0,small math cleanups in the docs (#6057),2.0,2.0,torch/_torch_docs.py,1.0,1,1,0,26.0,5910.0,1.0,268857.0,1018.0,6920.672317,0.0,Non Functional,0.0,1
pytorch,05dcf00644fb5d9701ac68b2b0ef66149f228cf4,8013dac43d2acb592cab75317f17d4f9c5b9eb6a,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Thu Aug 16 03:43:20 2018 -0700,1534391000.0,"Fix bincount for empty input (#9757)

Summary:
Added tests too. Fixes #9756 .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9757

Reviewed By: Yangqing

Differential Revision: D9348485

Pulled By: soumith

fbshipit-source-id: e13afadf8dbea20ee6ee595383c522dcbaf8796a",21.0,6.0,"aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/cuda/SummaryOps.cu,test/test_torch.py,torch/_torch_docs.py",4.0,7,3,1.911079956,41.0,14862.0,3.0,1819077.5,3500.0,9503.833333,0.0,Corrective,1.0,1
pytorch,d00c79f2b5db9c5094e69d4bc04fea54f6f4089b,802929608c9340d7634aa2f8e40370f16db81e87,James Reed,jamesreed@fb.com,Wed Jun 20 21:51:53 2018 -0700,1529531513.0,"[JIT] Improve test coverage for ErrorReport instances (#8668)

* [JIT] Coverage for ErrorReport

* Fixes

* lint

* More coverage",259.0,4.0,"test/test_jit.py,torch/csrc/jit/script/compiler.cpp",2.0,5,2,0.156959734,11.0,5491.0,1.0,9749.0,1386.0,4077.805292,0.0,Corrective,1.0,1
pytorch,1f5b392da0bcbafc9dd7ea2307bb91ebbc182a77,8031da5479530077a061c65448d7e114807cdb8b,gchanan,gregchanan@gmail.com,Tue May 01 16:54:43 2018 -0400,1525193683.0,"Implement torch.as_tensor, similar to numpy.asarray. (#7109)

* Implement torch.as_tensor, similar to numpy.asarray.
torch.as_tensor behaves like torch.tensor except it avoids copies if possible; so also somewhat like tensor.new but without the size overloads.
I didn't add a requires_grad field, because we haven't decided on the semantics such as as_param.

* Remove requires_grad for doc.",112.0,12.0,"docs/source/tensors.rst,docs/source/torch.rst,test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp,torch/_torch_docs.py,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h",7.0,9,4,1.869543543,39.0,13897.0,6.0,305278.14285714284,1024.0,2658.805292,0.0,Feature Addition,0.0,1
pytorch,a0831219cf8e06ab8438d50dbf4617db78d931c1,803afd58a08bccfe93cdb7c5b600200108e1c5a6,Richard Zou,zou3519@gmail.com,Mon Oct 09 15:42:23 2017 -0700,1507563743.0,Make MultiLabelMarginCriterion respect the cuda current stream,10.0,4.0,torch/lib/THCUNN/generic/MultiLabelMarginCriterion.cu,1.0,4,1,0,30.0,131.0,1.0,253788.0,1945.0,23763.85823,0.0,,0.0,1
pytorch,34a1d414a56c82a716601baa5092ccb262974801,805ad169243724db88aadbba5b31db1fa6612738,gchanan,gregchanan@gmail.com,Fri Sep 22 15:58:03 2017 -0400,1506095883.0,"Support ""expanding"" an empty tensor to an empty tensor. (#2824)

This doesn't currently support expanding the sizes to (0,), but
we can handle that eventually at the ATen level.",7.0,2.0,"test/test_torch.py,torch/lib/TH/generic/THTensor.c,torch/lib/THC/generic/THCTensor.c",3.0,7,2,1.584962501,38.0,6351.0,2.0,361498.0,1796.0,24841.55562,0.0,,0.0,1
pytorch,9e5d62582c38ce22f6e705abad29147d7cc9a7a5,8066fba2260fe46949680294b78b372067233746,Michael Carilli,mcarilli@nvidia.com,Tue Jun 23 00:11:03 2020 -0700,1592871063.0,"[RELAND2] Change AccumulateGrad to yield `.grad`s that match weights' memory layout (#40358)

Summary:
https://github.com/pytorch/pytorch/pull/40129 fixed the error responsible for the first revert, but exposed another error in the same test.

This PR is intended as the ""master copy"" for merge, and it runs on full CI.
Two other PRs (restricted to run on a small subset of CI) supporting debugging DDP failures/hangs with multiple devices per process (`test_c10d.py:DistributedDataParallelTest.test_grad_layout_1devicemodule_2replicaperprocess`).
- https://github.com/pytorch/pytorch/pull/40290 tries the test with purely rowmajor contiguous params on an untouched master.  In other words https://github.com/pytorch/pytorch/pull/40290 contains none of this PR's diffs aside from the test itself.
- https://github.com/pytorch/pytorch/pull/40178, for comparison, tries the test with this PR's diffs.

Both fail the same way, indicating failure is unrelated to this PR's other diffs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/40358

Differential Revision: D22165785

Pulled By: albanD

fbshipit-source-id: ac7cdd79af5c080ab74341671392dca8e717554e",759.0,118.0,"docs/source/autograd.rst,test/distributed/test_c10d.py,test/distributed/test_data_parallel.py,test/expect/TestAutograd.test_function-x_grad_desc.expect,test/expect/TestAutograd.test_function-y_grad_desc.expect,test/test_autograd.py,test/test_cuda.py,torch/autograd/__init__.py,torch/csrc/autograd/functions/accumulate_grad.h,torch/csrc/autograd/utils/grad_layout_contract.h,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/Utils.hpp,torch/nn/parallel/distributed.py,torch/tensor.py",16.0,17,3,2.970885871,43.0,17883.0,6.0,376539.3125,3091.0,7481.5,0.0,Corrective,1.0,1
pytorch,c07105a7960b824517ec380c14135b7fa838c95d,80a827d3daf5d4376bca893e95af96d5dd9a0b3e,Adam Paszke,adam.paszke@gmail.com,Mon Nov 21 22:03:44 2016 -0500,1479765824.0,Fix data_parallel bugs,81.0,14.0,"test/test_nn.py,torch/nn/parallel/__init__.py,torch/nn/parallel/parallel_apply.py,torch/nn/parallel/replicate.py,torch/nn/parallel/utils.py",5.0,4,2,1.771688212,15.0,1232.0,1.0,73352.0,297.0,2136.091213,0.0,Corrective,1.0,1
pytorch,b2a3d6ba0d3f915bbe9d308710e8ddbfb1c298d7,80cf0ce153bade37d173cd83e5b03d2b07ea478b,Guilherme Leobas,guilhermeleobas@gmail.com,Fri Jan 19 15:36:51 2024 -0300,1705678611.0,"Enhance torch.vmap support from inside torch.compile (#116050)

This work rewrites vmap support in torch.compile by inlining most of
the frames into the existing FX graph. It also unlocks to PyTorch to
support features that were previously missing, such as keyword args.

Fixes: https://github.com/pytorch/pytorch/issues/114306

Pull Request resolved: https://github.com/pytorch/pytorch/pull/116050
Approved by: https://github.com/zou3519",642.0,412.0,".flake8,test/dynamo/test_higher_order_ops.py,torch/_C/_functorch.pyi,torch/_dynamo/guards.py,torch/_dynamo/skipfiles.py,torch/_dynamo/symbolic_convert.py,torch/_dynamo/trace_rules.py,torch/_dynamo/variables/__init__.py,torch/_dynamo/variables/builder.py,torch/_dynamo/variables/builtin.py,torch/_dynamo/variables/ctx_manager.py,torch/_dynamo/variables/functions.py,torch/_dynamo/variables/higher_order_ops.py,torch/_dynamo/variables/torch.py,torch/_functorch/vmap.py,torch/csrc/functorch/init.cpp,torch/testing/_internal/dynamo_test_failures.py,torch/utils/_pytree.py",18.0,12,2,2.180049164,4.0,30052.0,15.0,1594332.388888889,24195.0,54798.0,0.0,Corrective,1.0,1
pytorch,"24701fc5a7691ace5c3097ef27df3f3ccee20e73,dc9a5b7d2fbcf21268b524b9da5ae38a74214a59",80e56cfda9f0816a8b2901aeea755735195f8b23,Sam Gross,sgross@fb.com,Tue Jan 31 01:58:05 2017 -0800,1485827885.0,Merge commit 'dc9a5b7d2fbcf21268b524b9da5ae38a74214a59',13.0,1.0,"torch/lib/THCUNN/CMakeLists.txt,torch/lib/THCUNN/generic/SpatialMaxUnpooling.cu",2.0,4,1,0.749595257,22.0,174.0,1.0,106566.0,97.0,6855.151435,0.0,,0.0,1
pytorch,1f80b972a669abe3ed13d57e42ab5926bdf4ca1b,80f72648049d806a5817c7ac245336ffdf5276d1,Michael Lazos,mlazos@fb.com,Thu May 25 21:48:35 2023 +0000,1685051315.0,"Foreach kernel codegen in inductor (#99975)

[design doc](https://docs.google.com/document/d/1JLr5yMAR8TuKW78ixKeqzfDHhcazwxKo_JXQnP_-wyY/edit?kh_source=GDOCS#heading=h.8x4z4mmet3im)

Add foreach kernel codegen for a single overload of foreach add in Inductor. Coverage will expand to more ops in subsequent PRs.

[example](https://gist.github.com/mlazos/9606fe64100ea2a5ec8265df1739fbe2)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99975
Approved by: https://github.com/jansel",863.0,150.0,"test/inductor/test_cpp_wrapper.py,test/inductor/test_foreach.py,torch/_dynamo/output_graph.py,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/triton_foreach.py,torch/_inductor/codegen/triton_utils.py,torch/_inductor/graph.py,torch/_inductor/lowering.py,torch/_inductor/scheduler.py,torch/_inductor/select_algorithm.py,torch/_inductor/triton_heuristics.py,torch/_inductor/utils.py",12.0,6,2,2.793955314,3.0,12678.0,8.0,648554.5555555555,16234.0,36712.0,0.0,Feature Addition,0.0,1
pytorch,16f5e119b62dc4fc1ca561924f6d364add2297ee,810884411d5ec40f83b02f27d5fc0082e31caef4,Richard Zou,zou3519@gmail.com,Mon Aug 08 15:32:35 2022 -0700,1659972755.0,"[functorch] `transpose_`, `t_` and aliases thereof batching rules (#82903)

Test Plan:
- run tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82903
Approved by: https://github.com/Chillee",38.0,6.0,"functorch/functorch/csrc/BatchRulesDecompositions.cpp,functorch/functorch/csrc/LegacyBatchingRegistrations.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,1.20443404,2.0,6700.0,3.0,98882.25,6225.0,14442.0,0.0,,0.0,1
pytorch,de7bf7efe628826243423b53e2d947a946f06729,811d947da386aa98632d6b8977fbc1dbcf145a9b,Trevor Killeen,killeentm@gmail.com,Wed Sep 28 16:19:21 2016 -0700,1475079561.0,[cutorch refactor] move renorm function into generic,38.0,35.0,"THCNumerics.cuh,THCTensorMath.h,THCTensorMath2.cu,generic/THCTensorMathReduce.cu,generic/THCTensorMathReduce.h",5.0,1,1,1.729201438,27.0,1352.0,2.0,0.0,7.0,14.0,0.0,Perfective,0.0,1
pytorch,956f1c981e70dd6e2470ba00f85f4c4f94879c37,8152433de2420fc64063366bebd7b1feadf38d1f,tktrungna,tktrungna@gmail.com,Sat Jul 24 12:15:04 2021 -0700,1627128904.0,"[1/n] Update testing lib*.so path (#61960)

Summary:
### Issue

Build PyTorch wheel packages during build stage for pull requests and install during test stage.

### Fix
Update all tests which call lib*.so (under `./build folder`), change to call lib*.so in `{ent}/pytorch/lib/python3.8/site-packages/torch`

### Diff
This diff starts to update test_fx, test_backend and test_torchbind first to check if current ci pass

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61960

Test Plan: check of all ci workflows pass

Reviewed By: malfet, saketh-are

Differential Revision: D29823235

Pulled By: tktrungna

fbshipit-source-id: e7f652def698e303d4843fbaedf4859f5eca2fd9",36.0,23.0,"test/jit/test_backends.py,test/jit/test_torchbind.py,test/test_fx.py,torch/testing/_internal/common_utils.py",4.0,5,2,1.984600204,2.0,6778.0,4.0,1121144.0,14116.0,32285.0,0.0,Corrective,1.0,1
pytorch,d024f1134d008395d5bcaa136c6bcd3d77048352,8173d4df69813c65c987ab7ca680b3a28b12d156,Yanli Zhao,yanlizhao@fb.com,Mon Oct 18 21:02:12 2021 -0700,1634590932.0,"move get_cycles_per_ms() to common_utils (#66798)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66798

get_cycles_per_ms is copied and used in a few places, move it to common_utils so that it can be used as a shared util function
ghstack-source-id: 140790599

Test Plan: unit tests

Reviewed By: pritamdamania87

Differential Revision: D31706870

fbshipit-source-id: e8dccecb13862646a19aaadd7bad7c8f414fd4ab",41.0,32.0,"test/test_cuda.py,torch/testing/_internal/common_utils.py,torch/testing/_internal/distributed/rpc/rpc_test.py",3.0,6,2,1.532375254,41.0,13703.0,3.0,378527.0,16344.0,38286.0,0.0,Feature Addition,0.0,1
pytorch,6834dcab1c87e1f2aac3e189ec7ccb20bab8028e,817e83fc018d7a705291f303664ee76592682a31,Wei Yang,weiyang@fb.com,Fri Sep 21 17:26:50 2018 -0700,1537550810.0,"fix PR #11061 (#11815)

Summary:
- fix PR https://github.com/pytorch/pytorch/pull/11061 by moving `detach_()` and `set_requires_grad()` to `torch.tensor_ctor()` and `tensor.new_tensor`, and also removed warnings and `args_requires_grad` from `internal_new_from_data `
- with this patch, the returned tensor from `tensor_ctor()` and `new_tensor` will be detached from source tensor, and set requires_grad based on the input args
- `torch.as_tensor` retains its behavior as documented

gchanan apaszke
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11815

Differential Revision: D9932713

Pulled By: weiyangfb

fbshipit-source-id: 4290cbc57bd449954faadc597c24169a7b2d8259",73.0,54.0,"test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/utils/tensor_new.cpp",4.0,4,2,1.530050943,41.0,18420.0,3.0,149938.5,4262.0,12039.33333,0.0,Corrective,1.0,1
pytorch,108936169c9f972ef944c7c6642854494d85f35d,817f6cc59dae9fc5df540c75aaa28957c3079f9c,Soumith Chintala,soumith@fb.com,Tue Jan 03 03:44:42 2017 -0500,1483415082.0,"adding linspace, logspace, neg and range",142.0,8.0,"docs/source/torch.rst,torch/csrc/generic/methods/Tensor.cwrap,torch/docs.py",3.0,6,2,0.23490657,17.0,3168.0,2.0,35801.666666666664,297.0,6380.724559,0.0,Feature Addition,0.0,1
pytorch,8a254a027118f397051a5982b7193db603bc3833,8199edf5c1fb7073694e18954caa87602fbcce64,Luca Antiga,luca.antiga@orobix.com,Wed Dec 13 22:25:09 2017 +0100,1513203909.0,"Refactor generation of NN derivatives (#4096)

Derivatives for NN functions now have to be specified in tools/autograd/derivatives.yaml. Leaving a function out will result in that function not being available in autograd.

Note that _backward declarations used in derivatives.yaml are auto-generated by aten/src/ATen/nn_parse.py so the content of tools/autograd/derivatives.yaml has to reflect the generated declarations.
This is an inconvenience, although it's smaller than it looks: future kernels will be implemented directly as ATen native functions.

As a help to the user, we could eventually save declarations generated in nn_parse.py to a file.

* Avoid automatic generation of NN derivatives

* Add inplace functions

* Refactor nn preprocessing function

* Use output instead of self in inplace derivatives

* Include grid_sampler in derivatives

* Finish fixing grid_sampler and affine_grid_generator

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

* Factor out setting up derivatives, use the same logic for NN and non-NN codepaths",241.0,127.0,"aten/src/ATen/cudnn/cuDNN.yaml,aten/src/ATen/function_wrapper.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py",4.0,6,2,1.19948442,10.0,3012.0,3.0,387477.25,383.0,2202.5,0.0,Corrective,1.0,1
pytorch,b87c113cf4f9bebb69b04e926556743e8f2b5f94,819d4b2b83fa632bf65d14f6af80a09e7476e87e,Sergey Zagoruyko,zagoruyko2@gmail.com,Sun Feb 26 13:35:24 2017 +0100,1488116124.0,Add finite differences gradcheck (#851),157.0,120.0,"test/common.py,test/common_nn.py,test/test_autograd.py,torch/autograd/__init__.py,torch/autograd/gradcheck.py",5.0,3,2,1.628213066,24.0,2159.0,4.0,433723.75,437.0,3912.075745,0.0,Feature Addition,0.0,1
pytorch,3c00c0169df850b97d098c0cbd17b63ec47162b5,81b995514ea908b635d725e11d1b91ac7ad03eb0,Richard Zou,zou3519@users.noreply.github.com,Tue Oct 31 22:36:26 2017 -0400,1509489386.0,Make THTensor_(var) and THTensor_(std) more numerically stable (#3410),38.0,38.0,"test/test_torch.py,torch/lib/TH/generic/THTensorMath.c",2.0,5,2,0.297472249,39.0,8086.0,2.0,235487.5,164.0,12851.72461,0.0,,0.0,1
pytorch,dfb68151700bfd07408d7029842036be167ef5b2,81cebca3d26b144ec0d4a98dcc11b0dd89af9b63,leslie-fang-intel,leslie.fang@intel.com,Sun Dec 24 01:23:31 2023 +0800,1703381011.0,"[Inductor] [Quant] Fix QConv Binary Inplace Layout Issue (#115613)

This pull request primarily addresses two issues to resolve the `QConvPointWiseBinaryPT2E` layout problem:

- As the changes made in https://github.com/pytorch/pytorch/commit/611a7457cad52d1aa81291f772a49dd280c17ba1, for `QConvPointWiseBinaryPT2E` with post-op `sum`, we should also utilize `NoneLayout` and return `accum` instead of `QConvPointWiseBinaryPT2E`.

- Additionally, this pull request fixes an issue in the `_quantized_convolution_onednn` implementation. Given that we expect `accum` to be inplace changed, we should avoid copying `accum` by changing the memory format or data type inside the kernel implementation. Instead, we have moved the necessary changes of memory format or data type to the lowering of `QConvPointWiseBinaryPT2E`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/115613
Approved by: https://github.com/jgong5, https://github.com/oulgen
ghstack dependencies: #116172",55.0,30.0,"aten/src/ATen/native/quantized/cpu/qconv.cpp,test/quantization/core/test_quantized_op.py,torch/_inductor/ir.py,torch/_inductor/lowering.py",4.0,11,3,1.872678249,5.0,22716.0,3.0,79559.75,23395.0,53109.5,0.0,Corrective,1.0,1
pytorch,13a07f163e48181f7abdd240c791ec0406b3fd7b,81d76753015a2fa9d3b9fdadab1c8b97e7e138c7,Hong Xu,hong@topbug.net,Tue Sep 17 06:14:18 2019 -0700,1568700858.0,"Ensure that n is non-negative in polygamma.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/26294

Differential Revision: D17416847

Pulled By: soumith

fbshipit-source-id: 17d5576e019e31e85c0308fb956524484e526cf6",4.0,0.0,"aten/src/ATen/native/UnaryOps.cpp,test/test_torch.py",2.0,5,2,0.811278124,40.0,13339.0,2.0,29173.5,11450.0,32185.83333,0.0,,0.0,1
pytorch,7b00adf5d31c4fbf5556151514eadea4c1e2aa4b,820ac0df2bcf57282b728c41196d093a03b8540c,Hugh Perkins,hughperkins@gmail.com,Sat Oct 28 12:40:35 2017 +0100,1509194435.0,fix mathjax notation on softmax/softmin (#3338),2.0,2.0,torch/nn/modules/activation.py,1.0,3,1,0,37.0,754.0,1.0,60393.0,2017.0,23880.85823,0.0,Corrective,1.0,1
pytorch,f1624b82b58867a4f2beefd08938dd6d50e6f87e,820c4b05a9e3a7d19645990c2e7b4ec92e118c53,Ksenija Stanojevic,ksenija.stanojevic@gmail.com,Tue Sep 01 08:43:17 2020 -0700,1598949797.0,"[ONNX] Update slice symbolic function (#42935)

Summary:
During scripting, combination of shape (or size()) and slice (e.g x.shape[2:]) produces following error:
 slice() missing 1 required positional argument: 'step'
This happens because aten::slice has 2 signatures:

- aten::slice(Tensor self, int dim, int start, int end, int step) -> Tensor
- aten::slice(t[] l, int start, int end, int step) -> t[]

and when a list is passed instead of tensor the 2nd of the two slice signatures is called, and since it has 4 instead of 5 arguments it produces the above exception.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42935

Reviewed By: houseroad

Differential Revision: D23398435

Pulled By: bzinodev

fbshipit-source-id: 4151a8f878c520cea199b265973fb476b17801fe",57.0,20.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.384322535,3.0,7083.0,2.0,1333707.0,4722.0,10998.0,0.0,,0.0,1
pytorch,acb0ce8885ca92f2d8652a99a80ec81a5319891b,825e919eb8de13ece7277431998cd15ee3c572f4,Adam Paszke,adam.paszke@gmail.com,Tue Jan 31 20:51:26 2017 +0100,1485895886.0,Add torch.unbind,49.0,2.0,"docs/source/torch.rst,test/test_torch.py,torch/functional.py",3.0,4,3,0.761105111,22.0,3046.0,2.0,102804.0,442.0,4346.616645,0.0,Feature Addition,0.0,1
pytorch,7001a2f1e4d8382b4c255342b4b53eae9389f8b1,8277d74e42beac26054fcd51264d92d0a40ec85f,Richard Zou,zou3519@gmail.com,Tue Apr 27 18:52:08 2021 -0700,1619549528.0,"[functorch] Update readme, fix unsqueeze batch rule",71.0,29.0,"functorch/README.md,functorch/functorch/csrc/BatchRulesHelper.cpp,functorch/functorch/csrc/BatchRulesViews.cpp,functorch/test/test_vmap.py",4.0,4,1,1.556860481,1.0,2820.0,3.0,0.0,23.0,98.0,0.0,Corrective,1.0,1
pytorch,87df043f63b997768f00833c90b492bae4a582a5,82a216c45b2f482abd5210f709a950a0be170f79,lezcano,lezcano-93@hotmail.com,Wed Oct 13 14:43:30 2021 -0700,1634136210.0,"Add tensor.{adjoint(),H,mT,mH} methods and properties (#64179)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64179

This PR follows the discussion in https://github.com/pytorch/pytorch/issues/45063#issuecomment-904431478

Fixes https://github.com/pytorch/pytorch/issues/45063

cc ezyang anjali411 dylanbespalko mruberry Lezcano nikitaved rgommers pmeier asmeurer leofang AnirudhDagar asi1024 emcastillo kmaehashi heitorschueroff

Test Plan: Imported from OSS

Reviewed By: bertmaher

Differential Revision: D30730483

Pulled By: anjali411

fbshipit-source-id: 821d25083f5f682450f6812bf852dc96a1cdf9f2",395.0,21.0,"aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensor_view.rst,docs/source/tensors.rst,test/test_fx.py,test/test_fx_experimental.py,test/test_jit.py,test/test_torch.py,test/test_view_ops.py,tools/autograd/gen_inplace_or_view_type.py,tools/autograd/gen_python_functions.py,torch/_C/__init__.pyi.in,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/autograd/python_variable.cpp,torch/csrc/jit/frontend/builtin_functions.cpp,torch/csrc/jit/frontend/sugared_value.cpp,torch/csrc/jit/passes/normalize_ops.cpp,torch/csrc/jit/runtime/register_prim_ops.cpp,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",23.0,20,5,3.800459056,47.0,85037.0,22.0,1710287.0869565215,16202.0,37503.0,0.0,Corrective,1.0,1
pytorch,9ebc433bda3668092a4f9e1c5ecd6fcbfca5f922,82aa51114677b4d2187e1cc2d36f32158b7dddc0,eellison,elias_ellison@brown.edu,Tue Feb 19 19:34:46 2019 -0800,1550604886.0,"move prim::None to prim::Constant (again) (#17186)

Summary:
Trying to land again, make prim::None into a case of prim::Constant. Reverted the previous landing because it broke an important onnx export test.

https://github.com/pytorch/pytorch/pull/16160
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17186

Differential Revision: D14115304

Pulled By: eellison

fbshipit-source-id: 161435fc30460b4e116cdd62c7b2e5b94581dcb7",315.0,180.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/core/jit_type.h,test/cpp/jit/gtest.cpp,test/cpp/jit/no-gtest.cpp,test/cpp/jit/test_misc.h,test/expect/TestJit.test_conv.expect,test/expect/TestScript.test_if_is_none_dispatch.expect,test/expect/TestScript.test_mutable_dce_graph_input.expect,test/test_jit.py,torch/csrc/jit/autodiff.cpp,torch/csrc/jit/constants.cpp,torch/csrc/jit/constants.h,torch/csrc/jit/export.cpp,torch/csrc/jit/import_method.cpp,torch/csrc/jit/ir.cpp,torch/csrc/jit/ir.h,torch/csrc/jit/passes/alias_analysis.cpp,torch/csrc/jit/passes/constant_pooling.cpp,torch/csrc/jit/passes/constant_propagation.cpp,torch/csrc/jit/passes/erase_number_types.cpp,torch/csrc/jit/passes/graph_fuser.cpp,torch/csrc/jit/passes/python_print.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/passes/to_batch.cpp,torch/csrc/jit/passes/utils/subgraph_utils.cpp,torch/csrc/jit/python_ir.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/script/compiler.cpp,torch/csrc/jit/script/file_check.cpp,torch/csrc/jit/script/init.cpp,torch/csrc/jit/symbolic_script.cpp,torch/jit/__init__.py,torch/onnx/symbolic.py,torch/onnx/utils.py",34.0,16,3,3.984777536,16.0,38387.0,5.0,292397.8823529412,7099.0,21790.33333,0.0,,0.0,1
pytorch,9060b7f4e2171d521d756f257ebdf2d933a0e3fb,82b981e4db717d90684a80ca5cefea97e545e7fd,Bram Wasti,bwasti@fb.com,Fri Jun 01 21:41:09 2018 -0700,1527889269.0,"Update from facebook 1ee4edd286a3 (#8040)

* Adding instance weight to batch distill loss

as title

* add bfloat 16-31

added bfloat 16-31 and their respective unit tests

* [CUDA9] Upgrade - fbcode

CUDA9 upgrade diff D5654023 has been out for a while thanks to Pieter. But with time growing it's becoming quite hard to rebase, because of the symlinks and auto-generated build/config files in tp2. Break D5654023 into two diffs, one touching tp2 config files, and another one touching fbcode TARGETS file (adding nvcc flag). These two should be a bit easier to rebase (for detailed procedure see ""Test Plan"").

This diff can only be committed if:
1. CUDA 9 rpm is rolled out fleet-wide (TBD)
2. NVidia driver 390.40 is rolled out fleet-wide (done)
3. Upgrade CUDA 9.1, cudnn 7.1, nccl 2.1 (done)
4. Make sure all dependents are built (done)
5. Test all C2 operators, PyTorch (see test plan)

* Share intermediate int32 buffer across Conv ops

Adding a known type

* [C2 fix] infer function for ensure_cpu_output_op

this is adding the missing device funtion for ensure_cpu_output_op

* [int8] Add blob serializer/deserializer for Int8TensorCPU

To export to logfiledb

* [nomnigraph] Add try catch block to optimization passes in predictor

This will catch failures that happen in the optimization pass.

* Caffe2: avoid static initialization order fiasco for CAFFE_ENFORCE

CAFFE_ENFORCE uses strack trace fetcher. Which is currently a
global static variable. If at static initialization time CAFFE_ENFORCE
is used, this is a SIOF. Recently CAFFE_ENFORCE was added into init
functions registration, so we started to see this.

Meyers singleton is going to provide safety here. If stacktrace
fetcher was not registered yet, it will just use a dummy one.

* NUMA support in SparseNN CPU benchmark

Adding support for NUMA in SparseNN CPU benchmark

* [mobile-roofline] Add logging needed for roofline model

This should be all that's needed

* Let the operators using the same input if the operators are not chained

or else, we have to change the input data dims

* fix null-pointer-use UBSAN errors in in reshape_op.h

* revert previous fix on input blob name

as title

* Adding flag to let MineHardNegative automatically extract single value from dict

Model exporter requires the output of the model to be a struct. This makes it convenient to use those models directly in MineHardNegative by allow automatic extraction of the single element of dict, which is a common use case.

* Reverting change that broke internal tests back to OSS compatible state",245.0,61.0,"binaries/bench_gen/bench_gen.py,caffe2/core/int8_serialization.cc,caffe2/core/logging.cc,caffe2/core/operator.cc,caffe2/core/operator.h,caffe2/core/plan_executor.cc,caffe2/core/predictor.cc,caffe2/core/types.cc,caffe2/operators/prepend_dim_op.h,caffe2/operators/reshape_op.h,caffe2/proto/caffe2.proto,caffe2/python/core.py,caffe2/python/core_test.py,caffe2/python/layers/batch_distill_lr_loss.py,caffe2/python/layers_test.py,caffe2/python/operator_test/adagrad_test.py,caffe2/python/operator_test/adagrad_test_helper.py,caffe2/python/operator_test/conv_test.py,modules/observers/net_observer_reporter.h,modules/observers/net_observer_reporter_print.cc,modules/observers/net_observer_reporter_print.h,modules/observers/perf_observer.cc,modules/observers/perf_observer.h",23.0,11,3,3.419739149,18.0,10375.0,9.0,2378616.636363636,2688.0,25092.35823,0.0,Corrective,1.0,1
pytorch,e83546b6864ed633974c0e2e51dd0cbcee9764e8,82fed06535fbd67b507898d3ec3670ee3f02ded7,Will Feng,yf225@cornell.edu,Wed Jan 24 02:29:32 2018 -0500,1516760972.0,disable qr_big cuda test on Windows (#4747),3.0,1.0,test/test_cuda.py,1.0,1,1,0,37.0,1393.0,1.0,1005.0,423.0,2250.5,0.0,,0.0,1
pytorch,e393a4f03cdf13da6164990c2f1afe174dbce239,8307f21bf6a42818e2f7b44371beb85899c182ec,Adam Paszke,adam.paszke@gmail.com,Fri Dec 15 22:50:20 2017 -0500,1513378220.0,Allow map_location in torch.load to be a string,15.0,2.0,"test/test_torch.py,torch/serialization.py",2.0,2,2,0.787126586,38.0,5531.0,2.0,515693.5,846.0,6599.172317,0.0,,0.0,1
pytorch,758d7dea9c79681b214b4e74828c9bb4d441b51c,833dcaf2d6e8d85996eaa17956152ef0494a245c,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Fri Nov 19 05:56:36 2021 -0800,1637301396.0,"Sparse CSR: Add `torch.sin` (#68123)

Summary:
This PR attempts to add support for `torch.sin` for sparse CSR tensors.

This aims to be a revised implementation (in some form) of https://github.com/pytorch/pytorch/pull/68083, and the implementation aims to be similar to that in [`SparseTensorMath.cpp` file](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/sparse/SparseTensorMath.cpp)

The tests and `empty_like` support for sparse CSR tensors (with a minor correction) are borrowed from https://github.com/pytorch/pytorch/pull/68083 temporarily to assist CI with testing this PR. :)

cc nikitaved pearu cpuhrsch IvanYashchuk krshrimali

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68123

Reviewed By: jbschlosser

Differential Revision: D32533379

Pulled By: cpuhrsch

fbshipit-source-id: eb834d64d16ee12734c77e74fffa4a47614e3dfb",108.0,11.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseCsrTensor.cpp,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp,test/test_sparse_csr.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,1.850020619,12.0,26181.0,4.0,547799.2,17207.0,40527.5,0.0,Corrective,0.0,1
pytorch,cd5f142af46e62d289f5c3339e03910e7e5d7663,83450aa11d244e426c9a9d2d6d780efe056d326c,BowenBao,bowbao@microsoft.com,Tue Jun 15 19:21:31 2021 -0700,1623784891.0,"[ONNX] Add support for torch.bernoulli() export (#57003) (#59536)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59536

Support export HuggingFace - Training DeBERTa model.

Test Plan: Imported from OSS

Reviewed By: nikithamalgifb, ansley

Differential Revision: D29046609

Pulled By: SplitInfinity

fbshipit-source-id: df87e0c6ed0f13463297bdeba73967fcf2aa37ca

Co-authored-by: hwangdeyu <deyhuang@qq.com>",26.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.975119065,3.0,12386.0,1.0,2.0,13023.0,29499.0,0.0,Feature Addition,0.0,1
pytorch,053dff11119d09f2ff8a0c83bb30e8ef08115e5d,836798e0f39bd5a97e6fe6af25973debdcd388c4,Bin Bao,binbao@fb.com,Tue May 23 13:58:23 2023 +0000,1684850303.0,"[inductor] Support precomputed_sizes in CppWrapperCodeGen (#102083)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/102083
Approved by: https://github.com/jansel, https://github.com/ngimel",15.0,17.0,"test/inductor/test_cpp_wrapper.py,torch/_inductor/codegen/wrapper.py",2.0,5,2,0.543564443,1.0,1432.0,2.0,63314.0,16240.0,36717.5,0.0,,0.0,1
pytorch,db86c8c6f52af5ee2b5e12e558be08238da91a60,83df3beacad3e22bdfa95c1d3514af59dceaacdc,Xiang Gao,qasdfgtyuiop@gmail.com,Sat May 16 02:47:36 2020 -0700,1589597256.0,"Add complex support for torch.sum (#38382)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/38382

Test Plan: Imported from OSS

Differential Revision: D21600127

Pulled By: anjali411

fbshipit-source-id: c5338ab10bdcebe4a281b03f78e6f2063186bc32",161.0,98.0,"aten/src/ATen/cuda/DeviceUtils.cuh,aten/src/ATen/native/cuda/Reduce.cuh,aten/src/ATen/native/cuda/ReduceSumProdKernel.cu,aten/src/THC/THCDeviceUtils.cuh,c10/util/complex_type.h,test/test_autograd.py,test/test_torch.py",7.0,10,3,2.18810377,43.0,26513.0,5.0,1680360.1666666667,2137.0,5331.5,0.0,Feature Addition,0.0,1
pytorch,593c5e12e19af6f7286937d9669e56e8c2f423b5,841173c530c82f770e382a94402dc92649299385,Sam Gross,colesbury@gmail.com,Wed Jul 12 21:14:42 2017 -0400,1499894082.0,Use NamedTemporaryFile to avoid filename collisions (#2069),3.0,8.0,test/test_torch.py,1.0,1,1,0,33.0,4168.0,1.0,8361.0,253.0,1103.375885,0.0,Preventative,0.0,1
pytorch,3d6d4f4322e42886349b822449b9e439fac89ae2,84190dafa80b52295edaf81c21c9ddb5bcd3ee0d,BowenBao,bowbao@microsoft.com,Fri Oct 01 04:05:46 2021 -0700,1633061146.0,"[ONNX] Update instance_norm implementation and support training (#60538) (#64375)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64375

* Update the instance_norm track_running_stats=True implementation and support the training mode
* Reference: https://github.com/pytorch/pytorch/blob/9baf75c86edc7f6cd1c04bf9f42d18bc0d05f504/aten/src/ATen/native/Normalization.cpp#L532
* Fix https://github.com/pytorch/pytorch/issues/53887

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D30919605

Pulled By: malfet

fbshipit-source-id: 306eb2a1122bb5d90dcb7c18260a3a2057a21c34

Co-authored-by: hwangdeyu <dejack953@outlook.com>",113.0,18.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.977648278,3.0,13016.0,2.0,702692.5,15854.0,36637.0,0.0,Corrective,1.0,1
pytorch,731b4bf0f119315495e3847e065afca282778ee6,841995d53b7ea51e8dae64e0d3d4f4d888406d8b,Nikita Karetnikov,nikita@karetnikov.org,Mon Oct 17 19:43:28 2022 +0200,1666035808.0,"[primTorch] Add refs for data conversion ops (#86561)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86561
Approved by: https://github.com/lezcano, https://github.com/mruberry, https://github.com/zou3519",246.0,1.0,"test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/test_ops.py,torch/_prims/context.py,torch/_refs/__init__.py,torch/_refs/_conversions.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/refs.py",9.0,8,2,1.726148597,7.0,33065.0,7.0,727812.125,8469.0,20262.5,0.0,Feature Addition,0.0,1
pytorch,9b908ab0d0a947d89ac3137f8c4a05a87c35f568,8423ab4f99fb499d540316e047f0d4a0c9ad630c,Philip Meier,github.pmeier@posteo.de,Fri Jul 09 14:52:22 2021 -0700,1625842342.0,"Fix `CosineAnnealingWarmRestart` annotation (#61106)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/44770.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61106

Reviewed By: 1ntEgr8

Differential Revision: D29635764

Pulled By: walterddr

fbshipit-source-id: ddc45a7f04532a76d033ae7774706da1fa8608f7",1.0,1.0,torch/optim/lr_scheduler.pyi,1.0,2,1,0,2.0,40.0,1.0,30299488.0,13696.0,30949.5,0.0,Corrective,1.0,1
pytorch,4441582f809ee94045e77ec595470bb6e68ba5f6,8473173c36a171c9ad896804ab36049f80a99073,Nikita Shulga,nshulga@fb.com,Tue May 03 20:21:55 2022 +0000,1651609315.0,"Remove breakpad dependency

This functionality does not seem to be used
and there are some requests to update dependency.

Add `third_party` to torch_cpu include directories if compiling with
Caffe2 support, as `caffe2/quantization/server/conv_dnnlowp_op.cc` depends on `third_party/fbgemm/src/RefImplementations.h`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75394
Approved by: https://github.com/janeyx99, https://github.com/seemethere",5.0,373.0,".gitmodules,CMakeLists.txt,caffe2/CMakeLists.txt,cmake/Dependencies.cmake,cmake/Summary.cmake,test/test_cpp_extensions_jit.py,test/test_utils.py,third_party/breakpad,tools/build_variables.bzl,torch/csrc/Module.cpp,torch/csrc/api/include/torch/utils.h,torch/csrc/utils/crash_handler.cpp,torch/csrc/utils/crash_handler.h,torch/csrc/utils/init.cpp,torch/csrc/utils/init.h,torch/testing/_internal/common_utils.py",16.0,13,6,2.56485907,76.0,13453.0,7.0,1001638.9375,2820.0,6761.5,0.0,Feature Addition,0.0,1
pytorch,c88d3a5e765a335b2f617098313f11c5f6927a64,8493b0d5d68233beaed2e7c27b3f4bb3cd2c4e27,Luca Wehrstedt,lcw@fb.com,Thu Aug 13 14:04:09 2020 -0700,1597327449.0,"Enroll TensorPipe agent in C++-only E2E test (#42680)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42680

ghstack-source-id: 109544678

Test Plan: CI

Reviewed By: mrshenli

Differential Revision: D22978714

fbshipit-source-id: 04d6d190c240c6ead9bd9f3b7f3a5f964d7451e8",90.0,6.0,"caffe2/CMakeLists.txt,test/cpp/rpc/CMakeLists.txt,test/cpp/rpc/e2e_test_base.cpp,test/cpp/rpc/e2e_test_base.h,test/cpp/rpc/test_e2e_tensorpipe.cpp",5.0,4,2,1.710083585,14.0,1687.0,3.0,1069981.0,4274.0,9998.0,0.0,,0.0,1
pytorch,a188dbdf3f438e6a5c50e30ec05580dfddc1b8d6,84949672bf1e0e56bf187c0232afcde4708954da,Akihiro Nitta,20610905+akihironitta@users.noreply.github.com,Mon Sep 14 21:15:37 2020 -0700,1600118137.0,"Fix exception chaining in `test/` (#44193)

Summary:
## Motivation
This PR fixes https://github.com/pytorch/pytorch/issues/43770 and is the continuation of https://github.com/pytorch/pytorch/issues/43836.

## Description of the change
This PR fixes exception chaining only in files under `test/` where appropriate.
To fix exception chaining, I used either:
1. `raise new_exception from old_exception` where `new_exception` itself seems not descriptive enough to debug or `old_exception` delivers valuable information.
2. `raise new_exception from None` where raising both of `new_exception` and `old_exception` seems a bit noisy and redundant.

## List of lines containing `raise` in `except` clause:
I wrote [this simple script](https://gist.github.com/akihironitta/4223c1b32404b36c1b349d70c4c93b4d) using [ast](https://docs.python.org/3.8/library/ast.html#module-ast) to list lines where `raise`ing in `except` clause.

- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_cpp_extensions_aot.py#L16
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit.py#L2503
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/model_defs/word_language_model.py#L22
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/verify.py#L73
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/verify.py#L110
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/onnx/test_verify.py#L31
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L255
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L2992
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L3025
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_c10d.py#L3712
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_distributed.py#L3180
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_distributed.py#L3198
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_data_parallel.py#L752
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/distributed/test_data_parallel.py#L776
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_type_hints.py#L151
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser.py#L771
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser.py#L773
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dispatch.py#L105
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_distributions.py#L4738
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_nn.py#L9824
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_namedtensor.py#L843
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser_te.py#L875
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_jit_fuser_te.py#L877
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L31
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L43
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L365
- [x] https://github.com/pytorch/pytorch/blob/f8f35fddd4b2bd788953d6d6ccfadf690b73a3e8/test/test_dataloader.py#L391

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44193

Reviewed By: albanD

Differential Revision: D23681529

Pulled By: malfet

fbshipit-source-id: 7c2256ff17334625081137b35baeb816c1e53e0b",12.0,12.0,"test/onnx/model_defs/word_language_model.py,test/test_cpp_extensions_aot.py,test/test_dataloader.py,test/test_distributions.py,test/test_jit.py,test/test_jit_fuser.py,test/test_jit_fuser_te.py,test/test_namedtensor.py,test/test_type_hints.py",9.0,3,1,3.084962501,39.0,27823.0,9.0,6725816.555555556,5104.0,11656.5,0.0,Corrective,1.0,1
pytorch,bf72a723ef3faf8f72823d4ebebbb69a78b38586,84975339bd3fa6819469f38a696791edc2da54d6,Scott Wolchok,swolchok@fb.com,Thu Oct 12 23:50:25 2023 -0700,1697154625.0,"[PyTorch] AOTI: generate reused thread_locals when tensors provably have static shape (#110892)

If a Tensor can be reused and has static shape, we can just cache it across iterations.

This is meant as a quickly shippable overhead reduction for CPU overhead-bound use cases that we can ship without relying on memory planning.

Differential Revision: [D50023678](https://our.internmc.facebook.com/intern/diff/D50023678/)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110892
Approved by: https://github.com/bertmaher
ghstack dependencies: #110876, #110877, #110909",105.0,22.0,"torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py",2.0,3,1,0.116849761,2.0,8563.0,2.0,366402.5,20737.0,47395.5,0.0,,0.0,1
pytorch,2f27c1b56bc954629fa5fb59c50b41b9e729b078,84a73775d52a08a4ca2879e3f5831b13b4687b28,Ailing,ailzhang@users.noreply.github.com,Tue Mar 20 17:01:05 2018 -0400,1521565265.0,adding fp16 tests in test_nn (#5020),223.0,146.0,"test/common.py,test/test_nn.py",2.0,1,1,0.027014134,38.0,7131.0,2.0,72931.0,1002.0,6885.672317,0.0,Feature Addition,0.0,1
pytorch,da7a27b84750a3aa82a5837c361f8ae8ee8f7817,84d18727bd0d91873d1a981f9bbfc7b4f78d3260,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Tue Apr 06 20:52:12 2021 -0700,1617742332.0,"Added linalg.eig, linalg.eigvals (#52491)

Summary:
This PR adds `torch.linalg.eig`, and `torch.linalg.eigvals` for NumPy compatibility.

MAGMA uses a hybrid CPU-GPU algorithm and doesn't have a GPU interface for the non-symmetric eigendecomposition. It means that it forces us to transfer inputs living in GPU memory to CPU first before calling MAGMA, and then transfer results from MAGMA to CPU. That is rather slow for smaller matrices and MAGMA is faster than CPU path only for matrices larger than 3000x3000.
Unfortunately, there is no cuSOLVER function for this operation.

Autograd support for `torch.linalg.eig` will be added in a follow-up PR.

Ref https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52491

Reviewed By: anjali411

Differential Revision: D27563616

Pulled By: mruberry

fbshipit-source-id: b42bb98afcd2ed7625d30bdd71cfc74a7ea57bb5",993.0,7.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,c10/core/ScalarType.h,docs/source/linalg.rst,test/test_linalg.py,test/test_namedtuple_return_api.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/linalg.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",14.0,20,6,2.528377349,16.0,32861.0,11.0,599320.5714285715,10489.0,23257.0,0.0,Feature Addition,0.0,1
pytorch,1cb3507ed38330af43bf23e255dc64e9215384c4,8535418a06d75025541370cc656a8b6a0330ca0d,Heitor Schueroff,heitorschueroff@fb.com,Mon Sep 13 03:04:19 2021 -0700,1631502259.0,"[Reland] Added reference tests to ReductionOpInfo (#64273)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64273

Reintroduced sample_inputs_prod and constrained the range of values for large reference tests.

This reverts commit e4fd2ab59ce8645f5ae9477c7724b6af82124b3b.

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D30672097

Pulled By: heitorschueroff

fbshipit-source-id: b44ed8dfd5eb0c74c194164dafc3242f6728a78f",240.0,28.0,"aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,test/test_reductions.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.381308012,8.0,13158.0,3.0,611019.0,15379.0,35168.0,0.0,Feature Addition,0.0,1
pytorch,1dffbe759bea65dc5cbf4022259657e726af4d95,854c92078ae07e5f46f14b926957dc02b72fb4d7,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Wed Mar 31 02:26:42 2021 -0700,1617157602.0,"Fixed the default size of the workspace array for MAGMA's SVD (#54875)

Summary:
The problem was that MAGMA might not set the value for the optimal size of the workspace array leaving it uninitialized. This is fixed by setting the default value for `wkopt` variable.

Fixes https://github.com/pytorch/pytorch/issues/54381 and https://github.com/pytorch/pytorch/issues/53976.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54875

Reviewed By: H-Huang

Differential Revision: D27437702

Pulled By: mruberry

fbshipit-source-id: bf61555abc4c50e8ef2dae933df24ce4d4fe4527",4.0,14.0,"aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,test/test_linalg.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.224394445,2.0,14680.0,3.0,52285.0,10237.0,22665.5,0.0,Corrective,1.0,1
pytorch,b875fb281c92dbe88da279a0ff7ad342e7b28b9e,857e3f4a5efcf108f95aa6c5bd735c52e016a202,Jon Walsh,jrwalsh1@gmail.com,Fri May 11 09:00:43 2018 -0700,1526029243.0,Throw error in tensor constructor when numpy strides mismatch (#7440),10.0,0.0,"test/test_torch.py,torch/csrc/utils/tensor_numpy.cpp",2.0,4,2,1,39.0,7428.0,2.0,1822848.0,1067.0,6999.172317,0.0,,0.0,1
pytorch,816d5d8ff72104052f54eaf7f5c94095fa2808b3,8593c6f4f74b9a87b4700deef480f1905b81577f,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Sat Jan 20 20:47:11 2018 +0500,1516481231.0,Adding better KL-Tests (#4739),130.0,73.0,"test/common.py,test/test_distributions.py,torch/distributions/kl.py",3.0,3,2,0.772410131,37.0,2872.0,2.0,78449.33333333333,928.0,6750.172317,0.0,Feature Addition,0.0,1
pytorch,6a40acb4f0545735f34c18fac0d0b18f0397ba0c,85d838a0282f9754b38c32bf378795f9a6b01125,Gregory Chanan,gchanan@fb.com,Thu Apr 27 14:56:12 2017 -0700,1493304972.0,Testing over the following: 1) CPU tensor out-of-place functions 2) CPU tensor in-place functions 3) GPU tensor out-of-place functions 4) GPU tensor in-place functions 5) torch. functions 6) Fallback semantics (use pointwise nElem matching rather than broadcasting),156.0,0.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.235193382,31.0,4234.0,2.0,4.5,875.0,11421.94394,0.0,,0.0,1
pytorch,4f479a98d45074386291e93fdea9d973640e6a50,85dda09f95ee8d9ead5b7b74fa05112cfc188ba9,Soumith Chintala,soumith@fb.com,Tue Jan 03 00:23:20 2017 -0500,1483403000.0,fixed names and other cosmetics,532.0,165.0,torch/docs.py,1.0,1,1,0,8.0,1500.0,1.0,0.0,294.0,6377.724559,0.0,Corrective,1.0,1
pytorch,bafec1637ee4562875c2c81a1e85c7f1c9e66050,85ee94b7be86867a8afde51cae4ce0baff42d93b,Tongzhou Wang,SsnL@users.noreply.github.com,Thu May 31 19:09:54 2018 -0400,1527793794.0,"Add memory leak check in CUDA tests (#7270)

* Add memory leak check in CUDA tests

* Tracking multi-GPU too

* fix run_test.py not running __name__ == '__main__' content; add test for make_cuda_memory_checked_test

* add a comment

* skip if cuda

* 1. Change the wrapper to a method in common.py:TestCase
2. Refactor common constants/method that initialize CUDA context into common_cuda.py
3. Update some test files to use TEST_CUDA and TEST_MULTIGPU

* Fix MaxUnpool3d forward memory leak

* Fix MultiLabelMarginCriterion forward memory leak

* Fix MultiMarginLoss backward memory leak

* default doCUDAMemoryCheck to False

* make the wrapper skip-able

* use TEST_MULTIGPU

* add align_corners=True/False tests for Upsample; fix TEST_CUDNN

* finalize interface

* VolumetricMaxUnpooling_updateOutput

* fix test_nccl

* rename THC caching allocator methods to be clearer

* make the wrapped function a method

* address comments; revert changes to aten/src/THC/THCCachingAllocator.cpp

* fix renamed var",239.0,93.0,"aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/MultiMarginCriterion.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,test/common.py,test/common_cuda.py,test/common_nn.py,test/run_test.py,test/test_cuda.py,test/test_distributed.py,test/test_distributions.py,test/test_legacy_nn.py,test/test_nccl.py,test/test_nn.py,test/test_sparse.py",14.0,5,2,2.631775754,42.0,18842.0,9.0,2355514.846153846,1221.0,3382.305292,0.0,Corrective,1.0,1
pytorch,424c093fc7adbd359a0f2df59d15da363ac8ff7e,86196bf116a67701adb0e259cfaa35a843e128ce,Guilherme Leobas,guilhermeleobas@gmail.com,Mon Oct 30 17:24:08 2023 -0300,1698686648.0,"add batch impl. for inplace `index_add` operation (#112276)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/112276
Approved by: https://github.com/zou3519, https://github.com/kshitij12345, https://github.com/malfet",52.0,9.0,"aten/src/ATen/functorch/BatchRulesScatterOps.cpp,test/functorch/test_ops.py,test/functorch/test_vmap.py",3.0,6,2,0.468247573,1.0,8820.0,1.0,31026.0,21346.0,48667.5,0.0,Feature Addition,0.0,1
pytorch,a559d94a448403586cccb31b00a622743f8bc2fb,86288265add5225be6de7870a88941937d9475de,Adam Lerer,alerer@fb.com,Mon Oct 17 02:01:26 2016 -0700,1476669686.0,Adding rnn cell library,537.0,269.0,"test/test_nn.py,torch/backends/cudnn/__init__.py,torch/csrc/utils.cpp,torch/nn/backends/thnn.py,torch/nn/functions/rnn.py,torch/nn/modules/__init__.py,torch/nn/modules/rnn.py,torch/nn/modules/rnn/__init__.py,torch/nn/modules/rnn/cell.py,torch/nn/modules/rnn/rnn.py",10.0,10,2,2.063608786,12.0,2185.0,5.0,79149.57142857143,24.0,95.66666667,0.0,Feature Addition,0.0,1
pytorch,54abfda12434692ea5902ce5f94062fdac7fde61,8635078d9ed47266e6df89ad5ef887b598a76f25,Sam Pepose,sampepose@fb.com,Thu Mar 28 02:47:43 2019 -0700,1553741263.0,"Adds Cyclical Learning Rate and Momentum (#18001)

Summary:
This implements a cyclical learning rate (CLR) schedule with an optional inverse cyclical momentum. More info about CLR: https://github.com/bckenstler/CLR

This is finishing what #2016 started. Resolves #1909.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18001

Differential Revision: D14451845

Pulled By: sampepose

fbshipit-source-id: 8f682e0c3dee3a73bd2b14cc93fcf5f0e836b8c9",393.0,1.0,"docs/source/optim.rst,test/test_optim.py,torch/optim/lr_scheduler.py",3.0,5,3,1.034857693,34.0,1488.0,3.0,3429649.0,7726.0,23401.33333,0.0,Feature Addition,0.0,1
pytorch,1858773c0c1395bc7fdccc16c7d5ad08b6d37682,86532c921d6aa7039f556fec4ff4e2a9a91aa196,David Riazati,davidriazati@fb.com,Wed Apr 10 18:20:44 2019 -0700,1554920444.0,"Refactor pickler (#19035)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19035
ghimport-source-id: 553977b9963d4877e5066a61702f887e81706598

Differential Revision: D14839341

Pulled By: driazati

fbshipit-source-id: d6e4f21b2df28e2a0a21b26bf08d9905599119ad",15.0,11.0,"torch/csrc/jit/pickler.cpp,torch/csrc/jit/pickler.h",2.0,3,1,0.619382195,1.0,701.0,1.0,755491.0,8023.0,24194.33333,0.0,Perfective,0.0,1
pytorch,388dc4f2a62d1a359d519ba170e0470d1592f54d,865c7eea4854fcfb50584e7f19450b4271c1b4f1,Iurii Zdebskyi,iuriiz@fb.com,Thu Aug 01 14:47:44 2019 -0700,1564670864.0,"Changed tensor comparison return type from uint8 to bool (#21113)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21113
ghimport-source-id: 9c4ba63457a72bfc41894387e0b01be3fd9a9baf

Test Plan: Imported from OSS

Differential Revision: D15552204

Pulled By: izdeby

fbshipit-source-id: a608213668649d058e22b510d7755cb99e7d0037",737.0,249.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/Itertools.cpp,aten/src/ATen/native/LegacyDefinitions.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cuda/LegacyDefinitions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/atest.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/generic/THCTensorMathCompare.cu,aten/src/THC/generic/THCTensorMathCompare.h,aten/src/THC/generic/THCTensorMathCompareT.cu,aten/src/THC/generic/THCTensorMathCompareT.h,test/expect/TestScript.test_listconstruct_erasure.expect,test/onnx/expect/TestOperators.test_equal.expect,test/onnx/expect/TestOperators.test_ge.expect,test/onnx/expect/TestOperators.test_gt.expect,test/onnx/expect/TestOperators.test_isnan.expect,test/onnx/expect/TestOperators.test_le.expect,test/onnx/expect/TestOperators.test_lt.expect,test/onnx/expect/TestOperators.test_ne.expect,test/onnx/test_onnx_opset.py,test/test_distributed.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/symbolic_script.cpp,torch/functional.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",30.0,23,4,3.951237141,43.0,33128.0,18.0,4406602.266666667,10340.0,29601.33333,0.0,,0.0,1
pytorch,c69e33bb11dd275c7a8ea18cbb25e41e1912fc70,867ccc9987b8bce3e12af5ad607ae4db404f6157,Pearu Peterson,pearu.peterson@gmail.com,Wed Oct 20 14:44:32 2021 -0700,1634741072.0,"Strided masked reduction: amin. (#66385)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66385

cc nikitaved pearu cpuhrsch IvanYashchuk

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D31779530

Pulled By: cpuhrsch

fbshipit-source-id: de753c2d191f7980a48831b892d3a1e8a7a547cd",47.0,0.0,"torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,1,0.925225328,2.0,11711.0,2.0,82406.5,16414.0,38524.5,0.0,,0.0,1
pytorch,5a5bca8ef075027d9c49d42a8f6ca1324551d7eb,869081961824dfb10ee944145c3a8d435571c53b,Peter Bell,peterbell10@live.co.uk,Mon Jan 25 12:48:08 2021 -0800,1611578888.0,"OpInfo: Add DecorateInfo class similar to SkipInfo for decorators (#50501)

Summary:
Follow up to https://github.com/pytorch/pytorch/issues/50435

I have confirmed this works by running
```
pytest test_ops.py -k test_fn_gradgrad_fft`
```
with normally and with `PYTORCH_TEST_WITH_SLOW=1 PYTORCH_TEST_SKIP_FAST=1`. In the first case all tests are skipped, in the second they all run as they should.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50501

Reviewed By: ezyang

Differential Revision: D25956416

Pulled By: mruberry

fbshipit-source-id: c896a8cec5f19b8ffb9b168835f3743b6986dad7",73.0,46.0,"torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",2.0,3,1,0.708277217,2.0,3969.0,2.0,636765.5,8322.0,18801.0,0.0,Feature Addition,0.0,1
pytorch,a00efe55c3790789b967facf10c3f426faa98155,8695f0cced016d43298b43a4baf30315061fdacd,Jane Xu,janeyx@meta.com,Wed Nov 23 23:23:17 2022 +0000,1669245797.0,"Rectify `native_batch_norm` schema by splitting it into two legit schemas (#88697)

Using the same repro from the issue (but with BatchNorm2D)

Rectifies native_batch_norm schema by splitting the schema into 2:
1. one will have NON-optional alias-able running_mean and running_var inputs
2. the other will just not have those parameters at all (no_stats variation)

**Calling for name suggestions!**

## test plan
I've added tests in test_functionalization.py as well as an entry in common_method_invocations.py for `native_batch_norm_legit`
CI should pass.

## next steps
Because of bc/fc reasons, we reroute native_batch_norm to call our new schemas ONLY through the python dispatcher, but in 2 weeks or so, we should make `native_batch_norm_legit` the official batch_norm.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88697
Approved by: https://github.com/albanD",598.0,12.0,".gitignore,aten/src/ATen/functorch/BatchRulesNorm.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/cuda/Normalization.cu,aten/src/ATen/native/mkldnn/Normalization.cpp,aten/src/ATen/native/mps/operations/Normalization.mm,aten/src/ATen/native/native_functions.yaml,functorch/_src/partitioners.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,test/lazy/test_reuse_ir.py,test/test_decomp.py,test/test_functionalization.py,test/test_jit_cuda_fuser.py,test/test_meta.py,test/test_ops.py,tools/autograd/derivatives.yaml,torch/_decomp/decompositions.py,torch/jit/_shape_functions.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",22.0,22,5,3.080174436,53.0,63674.0,18.0,1824769.1363636365,9835.0,22725.5,0.0,Feature Addition,1.0,1
pytorch,ca20b569be556ff56ed02712547ee0c5b9123f28,86c64440c9169d94bffb58b523da1db00c896703,vishwakftw,vishwaks@cs.cmu.edu,Thu Nov 07 17:18:38 2019 -0800,1573147118.0,"Make PyTorch Python 3.8 compatible (#29302)

Summary:
PEP 590 modifies the `tp_print` offset to `tp_vectorcall_offset` - which requires a Py_ssize_t object.
Passing a nullptr caused compatibility issues for Python 3.8.

Changelog:
- Modify all occurrences of `nullptr  /* tp_print */` to 0  /* tp_vectorcall_offset */
- Minor formatting changes
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29302

Test Plan:
- Local fresh build with Python 3.8 completed successfully.

Fixes https://github.com/pytorch/pytorch/issues/28060.
Fixes https://github.com/pytorch/pytorch/issues/29162.

Supersedes https://github.com/pytorch/pytorch/pull/28364

Differential Revision: D18372022

Pulled By: ezyang

fbshipit-source-id: 8e9a15b0d0f72101ccc69bd489f5efa216b880bb",177.0,177.0,"tools/autograd/templates/python_torch_functions.cpp,torch/csrc/Device.cpp,torch/csrc/Dtype.cpp,torch/csrc/Generator.cpp,torch/csrc/Layout.cpp,torch/csrc/MemoryFormat.cpp,torch/csrc/PtrWrapper.cpp,torch/csrc/QScheme.cpp,torch/csrc/Size.cpp,torch/csrc/TypeInfo.cpp,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_legacy_variable.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/cuda/Event.cpp,torch/csrc/cuda/Stream.cpp,torch/csrc/generic/Storage.cpp",17.0,8,2,3.430444512,42.0,4983.0,10.0,6912660.588235294,12914.0,35719.33333,0.0,Corrective,1.0,1
pytorch,fac076a82c0e9094563aabfd5bc49c82b6a356c9,86f354c5300471a35b56edee999328e10983d0ed,Xingying Cheng,xcheng16@fb.com,Fri Apr 17 23:16:50 2020 -0700,1587165410.0,"Python binding api to optimize for mobile model on script module. (#36357)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36357
ghstack-source-id: 101907180

Creating a python api entry to optimize mobile model which takes a scripted module as argument and returns an optimized scripted module. The initial optimization features includes inserting and folding prepack ops.

Test Plan: python test/test_optimizer.py

Differential Revision: D20946076

fbshipit-source-id: 93cb4a5bb2371128f802d738eb26d0a4f3b2fe10",105.0,0.0,"test/run_test.py,test/test_mobile_optimizer.py,torch/utils/mobile_optimizer.py",3.0,3,2,0.796465149,8.0,699.0,1.0,65895.0,1173.0,3067.0,0.0,Feature Addition,0.0,1
pytorch,9707431d842a66f78b80cb1cea83697a3e0a914c,87180080f3fee2fccaed01b38d7220ae869e49c5,Richard Zou,zou3519@gmail.com,Wed Aug 25 22:58:13 2021 -0700,1629932293.0,"[functorch] Batch rule coverage for vmap({grad, vjp}) testing",202.0,16.0,"functorch/functorch/csrc/BatchedFallback.cpp,functorch/functorch/csrc/BatchedFallback.h,functorch/functorch/csrc/init.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",6.0,4,1,1.331403588,1.0,4608.0,5.0,2.6666666666666665,311.0,478.0,0.0,,0.0,1
pytorch,fe4f19e16434b4e39ca1bec364800e26a0df94a0,872237c1f285f6e7164d93f2dab144fc5bd87665,Pritam Damania,pritam.damania@fb.com,Thu Jul 30 02:21:31 2020 -0700,1596075691.0,"Output to stderr in distributed tests. (#42139)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42139

A bunch of tests were failing with buck since we would output to
stdout and buck would fail parsing stdout in some cases.

Moving these print statements to stderr fixes this issue.
ghstack-source-id: 108606579

Test Plan: Run the offending unit tests.

Reviewed By: mrshenli

Differential Revision: D22779135

fbshipit-source-id: 789af3b16a03b68a6cb12377ed852e5b5091bbad",10.0,8.0,"test/distributed/test_c10d.py,test/distributed/test_c10d_spawn.py,test/distributed/test_distributed.py,test/distributed/test_nccl.py,test/test_cuda.py,test/test_cuda_primary_ctx.py,torch/testing/_internal/dist_utils.py",7.0,5,2,2.752715279,41.0,9969.0,7.0,2959568.4285714286,3932.0,9249.5,0.0,Corrective,1.0,1
pytorch,15b318de840de61e2e789c013e34d23819715090,8732a1b42ea46fc251369bab4f29332d5e35959e,Gregory Chanan,gchanan@fb.com,Thu Apr 04 18:12:13 2019 -0700,1554401533.0,"Disallow changing the device of a tensor via set_. (#18832)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18832
ghimport-source-id: fde4ad90541ba52dfa02bdd83466f17e6541e535

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18833 [STACK] Cache device on TensorImpl; clean up TensorImpl constructors.
* **#18832 [STACK] Disallow changing the device of a tensor via set_.**
* #18831 [STACK] Stop swapping in Storages of the wrong device for Tensors.

This is necessary to cache the device on a TensorImpl.

Differential Revision: D14766231

fbshipit-source-id: bba61634b2d6252ac0697b96033c9eea680956e8",50.0,3.0,"aten/src/TH/THTensor.cpp,test/test_torch.py,torch/csrc/jit/import.cpp",3.0,7,3,1.072813681,40.0,11781.0,3.0,1643380.0,7895.0,23894.83333,0.0,,0.0,1
pytorch,b89a3b50fb305c4f2be0f5f325f660dc20cc2fb6,8734b174cadc8513bef177c57f1f8267898f34d0,Ailing Zhang,ailzhang@fb.com,Thu Oct 11 03:32:51 2018 -0700,1539228771.0,"Multinomial raise error (#12490)

Summary:
Fixes #12260 #2896

```
torch.multinomial(torch.FloatTensor([0, 1, 0, 0]), 3, replacement=False)
```
The old behavior is that we return `0` after we run out of postive categories. Now we raise an error based on discussion in the issue thread.

- Add testcase for cpu & cuda case, in cuda case `n_samples=1` is a simple special case, so we test against `n_sample=2` instead.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12490

Differential Revision: D10278794

Pulled By: ailzhang

fbshipit-source-id: d04de7a60f60d0c0d648b975db3f3961fcf42db1",24.0,22.0,"aten/src/TH/generic/THTensorRandom.cpp,aten/src/THC/THCTensorRandom.cuh,test/test_cuda.py,test/test_torch.py",4.0,6,2,1.75993053,41.0,12076.0,4.0,931486.25,4547.0,13412.33333,0.0,Corrective,1.0,1
pytorch,aeb3e933515672a61cb21498fdbdef80a46ae0ee,8737c2a1a2c55eb6c9b33f304b104cd32974e57e,Hui Guo,huiguo@fb.com,Fri Mar 12 07:50:14 2021 -0800,1615535414.0,"[TensorExpr] Reland: ""Simplify index expressions constructed in loop flattening. Fixes #51173"" (#53861)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53861

Replaced the iterators in the for-loops with integer index variables due to
overflow when handling empty vectors.

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D26998894

Pulled By: huiguoo

fbshipit-source-id: a1f6475c8ba123968ef7247b4f6f38edbf24b9ef",527.0,43.0,"test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",2.0,7,2,0.999111732,1.0,6535.0,1.0,59698.0,9703.0,21475.0,0.0,Corrective,1.0,1
pytorch,6e0d0f08a9daf099eccd57f2d85bad4bd219f5f2,873f1163806c14ae236538f76c44d04b63bef331,Tongzhou Wang,SsnL@users.noreply.github.com,Thu Feb 08 20:44:18 2018 -0500,1518122658.0,adjust stft result comparison precision to 7e-6 (#5143),1.0,1.0,test/test_torch.py,1.0,1,1,0,38.0,5526.0,1.0,149555.0,2355.0,24568.35823,0.0,,0.0,1
pytorch,b37578b3c0ddd717435d70ec69a92327b09615d7,87465a6e68b62fe9a10482b98f42b48c71aacb68,Kevin Tse,ktse@fb.com,Tue Aug 03 15:01:52 2021 -0700,1628002912.0,"adding operator cumulative_trapezoid (#61615)

Summary:
Stack from [ghstack](https://github.com/ezyang/ghstack):
* https://github.com/pytorch/pytorch/issues/61616
* **https://github.com/pytorch/pytorch/issues/61615**
* https://github.com/pytorch/pytorch/issues/61475

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61615

Reviewed By: malfet, mruberry

Differential Revision: D29975064

Pulled By: NivekT

fbshipit-source-id: 4d4e98f3efb720fdc44eb238ecbf0fa157ac13d7",241.0,3.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/Integration.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_binary_ufuncs.py,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",9.0,13,5,2.167978475,36.0,37031.0,8.0,436834.5555555556,14360.0,32840.0,0.0,Feature Addition,0.0,1
pytorch,0d0f197682f6be142a8634450f6473cef9bdfb2c,8768e64e972b23b4448d43d8513d02dfdcb767ad,Adam Paszke,adam.paszke@gmail.com,Fri Dec 02 14:28:09 2016 +0100,1480688889.0,Allow returning changed gradients from the hooks,622.0,345.0,"test/test_autograd.py,test/test_nn.py,torch/autograd/function.py,torch/autograd/variable.py,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/variable.cpp,torch/nn/modules/module.py",8.0,7,2,1.043053882,17.0,4031.0,1.0,80639.0,324.0,2429.512243,0.0,,0.0,1
pytorch,c506ff97c823b3f1e8c51193170276501f73c60f,8769fec03fcf9318f19da538b5d488d2f97ecbaa,Christian Puhrsch,cpuhrsch@fb.com,Wed Jul 18 20:36:22 2018 -0700,1531946182.0,"Move clamp into ATen (#9506)

Summary:
Glue component of https://github.com/pytorch/pytorch/pull/9319

Important to unblock wanchaol
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9506

Reviewed By: wanchaol

Differential Revision: D8879437

Pulled By: cpuhrsch

fbshipit-source-id: 16ea8a93f3f5df2695180b3a30a583834b7004f1",223.0,115.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py",5.0,6,2,1.986957795,42.0,14127.0,3.0,224468.0,2999.0,7050.333333,0.0,,0.0,1
pytorch,98afce3c56bb5047926fcd10eeeabbce65ab5b99,879a90b322f67181a10792a4bbdd6b51f92cb83c,Rohith Menon,rohithm@fb.com,Fri Mar 06 19:52:56 2020 -0800,1583524376.0,"[ModelLoading] Use byte encoding for uint8, fp16 etc. instead of int32 (#34343)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34343

Use byte encoding for uint8, fp16 etc. instead of int32 in TensorProto serialization/deserialization

tl;dr
- fp16 tensor deserialization 12x faster, serialized size 25% lower
- uint8 tensor deserialization 36x faster, serialized size 25% lower

Test Plan:
```
============================================================================
caffe2/caffe2/fb/predictor/ModelLoaderBenchmark.cpprelative  time/iter  iters/s
============================================================================
BlobProtoInt32DeserializationFloat16                        12.37ms    80.82
BlobProtoByteDeserializationFloat16             1125.46%     1.10ms   909.64
----------------------------------------------------------------------------
BlobProtoInt32DeserializationUInt8                          17.57ms    56.92
BlobProtoByteDeserializationUInt8               3629.45%   484.02us    2.07K
============================================================================
```

Reviewed By: yinghai

Differential Revision: D20137451

fbshipit-source-id: 8ed4be2286a6d4c7e134fcb0832f22bc645039a1",233.0,114.0,"caffe2/core/blob_serialization.cc,caffe2/core/blob_test.cc",2.0,2,1,0.907364666,9.0,1845.0,2.0,20407429.5,15270.0,40936.83333,0.0,,0.0,1
pytorch,2c99ea86540da8dad0d2123e01d17148052f0fef,879cf0b15a54c7848ae710e3d0ec62c4a9d7d3dd,Jeong Ukjae,ukjae@scatterlab.co.kr,Tue Feb 18 17:08:10 2020 -0800,1582045690.0,"fix typing bug of LambdaLR.__init__ (#33271)

Summary:
## problem

```python
class LambdaLR(_LRScheduler):
    """"""Sets the learning rate of each parameter group to the initial lr
    times a given function. When last_epoch=-1, sets initial lr as lr.

    Args:
        optimizer (Optimizer): Wrapped optimizer.
        lr_lambda (function or list): A function which computes a multiplicative
            factor given an integer parameter epoch, or a list of such
            functions, one for each group in optimizer.param_groups.
        last_epoch (int): The index of last epoch. Default: -1.

    Example:
        >>> # Assuming optimizer has two groups.
        >>> lambda1 = lambda epoch: epoch // 30
        >>> lambda2 = lambda epoch: 0.95 ** epoch
        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])
        >>> for epoch in range(100):
        >>>     train(...)
        >>>     validate(...)
        >>>     scheduler.step()
    """"""
```

`LambdaLR` takes a lambda that returns a float and takes a int, or a list of such lambdas.

## related issue

Resolve https://github.com/pytorch/pytorch/issues/32645
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33271

Differential Revision: D19878665

Pulled By: vincentqb

fbshipit-source-id: 50b16caea13de5a3cbd187e688369f33500499d0",2.0,2.0,torch/optim/lr_scheduler.pyi,1.0,2,1,0,1.0,40.0,1.0,563727.0,14803.0,39737.83333,0.0,Corrective,1.0,1
pytorch,8cc57593b90f0196a54e5c08c7344d725ed5cd4e,87a2af6d4a97ce6418fe0597360fdf3a4c1d894f,Aleksei Nikiforov,aleksei.nikiforov@linux.ibm.com,Tue Apr 25 21:05:15 2023 +0000,1682456715.0,"Fix loading data on different encoding (#94503)

Add endianness marker when saving,
and if it doesn't match host endianness when loading data, do a byteswap.

Older data will load correctly only on systems
with same endianness it was saved on.
New data should load correctly on systems
with any endianness.

Fixes #65300
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94503
Approved by: https://github.com/kurtamohler, https://github.com/ezyang",2427.0,3.0,"test/test_serialization.py,torch/serialization.py,torch/storage.py",3.0,2,2,0.132935198,40.0,3559.0,3.0,744682.6666666666,15065.0,34108.0,0.0,Corrective,1.0,1
pytorch,bed3b40523d49dc783c909c4c41c6ed7e12cd62a,87a4baf616565aecd21884754772a0f03babfe2d,Daya Khudia,dskhudia@fb.com,Mon Oct 12 22:59:03 2020 -0700,1602543543.0,"[pt][quant] Support either min or max in qclamp (#45937)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/45937

torch.clamp can now be used with quantized tensors with either min argument or max argument only

Fixes https://github.com/pytorch/pytorch/issues/45928
ghstack-source-id: 114085914

Test Plan:
buck test mode/dev caffe2/test:quantization -- 'test_qclamp'  --print-passing-details
```
Started reporting to test run: https://our.intern.facebook.com/intern/testinfra/testrun/4222124686876909
    â ListingSuccess: caffe2/test:quantization - main (7.602)
    â Pass: caffe2/test:quantization - test_qclamp (quantization.test_quantized_op.TestQuantizedOps) (7.233)
Summary
  Pass: 1
  ListingSuccess: 1
Finished test run: https://our.intern.facebook.com/intern/testinfra/testrun/4222124686876909
```

Reviewed By: jerryzh168

Differential Revision: D24153431

fbshipit-source-id: 9735635a48bcdd88d1dd6dc2f18b59311d45ad90",99.0,12.0,"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qclamp.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,aten/src/ATen/native/quantized/library.cpp,test/quantization/test_quantized_op.py",5.0,9,2,1.803863723,2.0,7750.0,4.0,495419.2,5910.0,13683.5,0.0,Corrective,1.0,1
pytorch,99bc541b5be82be6588ee8eea518298b1723aa1b,87d3d209a6f9255fd4341cfc95399d9ec0e718c2,Zachary DeVito,zdevito@fb.com,Fri Oct 19 01:16:43 2018 -0700,1539911803.0,"Enable JIT tests in fbcode (#12777)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12777

Enables JIT tests in FBCode. Changes pybind11 code to avoid mixing py::args with positinally matched arguments because old versions of PyBind11 leak memory in this case.

Reviewed By: jamesr66a

Differential Revision: D10419708

fbshipit-source-id: 74bc466001b5d363132d1af32e96841b38601827",957.0,872.0,"test/common_methods_invocations.py,test/test_autograd.py,test/test_jit.py,torch/csrc/jit/pybind_utils.h,torch/csrc/jit/script/init.cpp",5.0,5,2,1.314336185,42.0,13443.0,3.0,97718.5,4731.0,13989.33333,0.0,Preventative,0.0,1
pytorch,fc7aa5c3be8c7a6581e649a50f11c9cb038d7b0b,87e369111a5defce56b4f0b3fac53970fa0d5462,gchanan,gregchanan@gmail.com,Fri Apr 06 19:12:05 2018 -0400,1523041925.0,"Add string-style devices to all tensors. (#6283)

* Add string-style devices to all tensors.

Previously, tensors only had a 'get_device' method which would throw an exception on a CPU tensor.   This made it necessary to if/else code that
was meant to be device agnostic.

This PR implements the following:
1) Adds a 'device' property to all tensors that returns a string representation of the device for all tensors.
For cpu tensors this is 'cpu'.  For cuda tensors this is 'cuda:X', where X is the cuda device ordinal.

2) Adds a DeviceSpec class.  This is just a helper class for separating device_type and device_index specification and to allow partial specification.
For example, you can call DeviceSpec('cuda'), DeviceSpec('cuda:0'), DeviceSpec('cuda', 1).
Also has backwards compatibility support for specifying integers, which are treated as cuda devices.

DeviceSpecs have the following properties:
a) device_type: string representation of the device type (i.e. 'cpu' or 'cuda')
b) device_index: integer for the device index (None if not specified)
c) cuda_device_index: for backwards compatibility; behaves roughly like `get_device` did previously.  I.e. if a function previously took integers for cuda devices,
it can now take DeviceSpecs (or strings), and can maintain the old functionality by calling `old_index = DeviceSpec(old).cuda_device_index`.

3) tensor methods and torch. functions that took integer devices can now take integers, strings, or DeviceSpecs.  For example:
torch.randn((2,3), dtype=torch.cuda.float32, device='cuda:1')

TODO in future PRs:
A) Split out cuda from dtype so you don't need to overspecify cuda-ness
B) We currently only support strings/DeviceSpecs in tensor methods and torch. functions.  We should have equivalents torch.cuda.device(...), torch.cuda.device_of, etc.
at the torch. level that work on strings/DeviceSpecs

* Add deviceInt64 to python arg parser.

* device_str.

* Remove device_str.

* remove device prefix from attributes.

* Use const char * instead of string.

* Move autogpu index out of Device.

* comment on is_default.

* Rename torch.DeviceSpec to torch.device.

* comment.

* Fix tests.

* Fix flake8.

* Fix sparse_coo_tensor parameter name.

* Improve error message.

* Remove device_ prefix from C++ device object.

* Allocate static strings.

* Return not implemented from rich compare.

* Move torch::Device to THPDevice.

* Remove cuda index.

* Py_RETURN_NOTIMPLEMENTED doesn't exist in python2.",484.0,61.0,"setup.py,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/Device.cpp,torch/csrc/Device.h,torch/csrc/Module.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/utils/device.cpp,torch/csrc/utils/device.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_new.cpp",13.0,8,3,2.847556515,42.0,11053.0,5.0,236386.3333333333,834.0,1953.305292,0.0,Corrective,1.0,1
pytorch,da2f8c9f1fa6dead0260080395a8ff4eae863574,8811e4d00d64a23b437d7daed8868721fae69941,Nikita Shulga,nshulga@fb.com,Thu Jun 04 20:37:44 2020 -0700,1591303064.0,"Add/fix typing annotations to some functions (#39075)

Summary:
Add missing typing imports to some jit tests
Add typing annotations to `torch.testing._compare_scalars_internal` and `torch.testing._internal.assertTrue`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39075

Differential Revision: D21882468

Pulled By: malfet

fbshipit-source-id: dd9858eb8e11a38411544cc64daf36fced807d76",120.0,77.0,"mypy.ini,test/jit/test_async.py,test/jit/test_list_dict.py,test/test_torch.py,tools/pyi/gen_pyi.py,torch/_C/__init__.pyi.in,torch/testing/__init__.py,torch/testing/_internal/common_utils.py",8.0,8,3,2.176020728,42.0,24492.0,7.0,261022.125,2588.0,6401.5,0.0,Corrective,1.0,1
pytorch,c5dae335e4bf63ba69ba967bc22f6b8228f94814,8819bad86cd15c74dd076012c59e7b1f6081fda4,mfkasim91,firman.kasim@gmail.com,Thu Nov 19 07:42:52 2020 -0800,1605771772.0,"Implement igammac (3rd PR) (#48171)

Summary:
Related: https://github.com/pytorch/pytorch/issues/46183 (torch.igamma)
This is the regularized upper incomplete gamma function.

This is supposed to be exactly the same as https://github.com/pytorch/pytorch/issues/47463, but after rebasing the `viable/strict` branch.

cc: mruberry

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48171

Reviewed By: zhangguanheng66

Differential Revision: D25060107

Pulled By: mruberry

fbshipit-source-id: 89780dea21dbb2141cbc4f7f18192cb78a769b17",271.0,6.0,"aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/IGammaKernel.cu,aten/src/ATen/native/native_functions.yaml,c10/util/math_compat.h,docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py",24.0,17,6,3.845550377,46.0,66445.0,14.0,769779.875,6916.0,15693.0,0.0,,0.0,1
pytorch,01edb8a559ac53ce6c2f986d4c67ab188a2e5115,88429a8084d4dce8dc201c5c6ff7597568206261,Peter Bell,peterbell10@live.co.uk,Thu Feb 08 01:00:52 2024 +0000,1707354052.0,"[inductor] Add split scan kernel (#117992)

This PR adds a new type of triton kernel in which data is persistent but the
reduction dimension is split over multiple blocks (up to the entire kernel).
though this is called a reduction dimension, in actuality we only support scans.
because of this limitation, i have to be able to block fusions of split scan
operations with reductions so chose to add a new `ir.SplitScan` node which
is identical but allows for differentiation in the scheduler.

The split scan kernel is also the first to require an additional workspace buffer
which is used to communicate between cuda blocks. this is slightly tricky as we
the exact scratch space requirement isn't known until the grid size is calculated.
here i workaround the issue by setting a minimum rblock size and always allocating
to the maximum possible grid size for a given input tensor.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/117992
Approved by: https://github.com/jansel
ghstack dependencies: #117991",694.0,99.0,"test/inductor/test_torchinductor.py,test/inductor/test_torchinductor_codegen_dynamic_shapes.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/triton_split_scan.py,torch/_inductor/codegen/triton_utils.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/config.py,torch/_inductor/ir.py,torch/_inductor/scheduler.py,torch/_inductor/triton_helpers.py,torch/_inductor/triton_heuristics.py,torch/_inductor/wrapper_benchmark.py",13.0,5,2,2.938640845,4.0,31082.0,10.0,1429188.3333333333,24960.0,56371.0,0.0,Feature Addition,0.0,1
pytorch,d43f9f7707f07593f015704bf00b685bcfaa8333,8856c1628e8ddb3fe3bd51ed8cfef5c02ad0a609,Bin Bao,binbao@fb.com,Fri Sep 22 02:31:52 2023 +0000,1695349912.0,"[inductor] Change AOTInductor to return output tensors (#109790)

Summary:
Change AOTInductor to directly return output tensors instead of taking pre-allocated output tensors to return the results. This gives several benefits:

* It makes sure AOTInductor has the same behavior when managing the output tensors as the default Inductor, which is widely tested and thus more reliable.
* As we have debugged before, there are cases we still have to codegen extra copy_ ops to fill the pre-allocated output tensors which doesn't make sense for performance.
* With the coming enhanced memory planning, this again will make sure the memory planning logic is the between AOTInductor and Inductor, which will greatly simplify the problem and improve the reliability.

This change also combines D49494954 from Yang and https://github.com/pytorch/pytorch/pull/109560 from Angela.

Differential Revision: D49502318

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109790
Approved by: https://github.com/chenyang78",136.0,226.0,"benchmarks/dynamo/common.py,test/cpp/aot_inductor/test.cpp,test/inductor/test_aot_inductor.py,torch/_inductor/codegen/aot_runtime/interface.cpp,torch/_inductor/codegen/wrapper.py,torch/_inductor/utils.py,torch/csrc/inductor/aot_runtime/interface.h,torch/csrc/inductor/aot_runtime/model.h,torch/csrc/inductor/aot_runtime/model_container.h,torch/csrc/inductor/aoti_torch/tensor_converter.cpp,torch/csrc/inductor/aoti_torch/tensor_converter.h",11.0,14,3,2.955893965,1.0,8789.0,2.0,80632.18181818182,19975.0,45587.5,0.0,Corrective,1.0,1
pytorch,dfdd797723c7d80634d83f3260e9dfd4b2b431f9,888ae1b3d8bfc294ccace3c3c7b4b55046fb7d9e,Nikita Vedeneev,nik@quansight.com,Tue Aug 18 21:12:49 2020 -0700,1597785169.0,"Introducing Matrix exponential (#40161)

Summary:
Implements (batched) matrix exponential. Fixes [https://github.com/pytorch/pytorch/issues/9983](https://github.com/pytorch/pytorch/issues/9983).

The algorithm follows:
```
 Bader, P.; Blanes, S.; Casas, F.
 Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.
 Mathematics 2019, 7, 1174.
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/40161

Reviewed By: zhangguanheng66

Differential Revision: D22951372

Pulled By: ezyang

fbshipit-source-id: aa068cb76d5cf71696b333d3e72cee287b3089e3",1191.0,0.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/FunctionOfAMatrixUtils.cpp,aten/src/ATen/native/FunctionOfAMatrixUtils.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp,aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",15.0,15,5,2.317040337,44.0,48331.0,6.0,190707.7272727273,4377.0,10248.5,0.0,Corrective,1.0,1
pytorch,ec260fe8e9a6adbcb84dd8e933212c7d335db05a,88b42324e73a603fa585583555e2a7a270ac1e89,Martin Raison,raison@fb.com,Wed Mar 15 15:10:48 2017 -0700,1489590648.0,"spcadd, sparseMask, cadd, csub, cmul + tests",1121.0,169.0,"test/common.py,test/test_sparse.py,tools/cwrap/plugins/THPPlugin.py,torch/_utils.py,torch/csrc/generic/SparseTensor.cpp,torch/csrc/generic/methods/SparseTensor.cwrap,torch/lib/THC/THCGeneral.c,torch/lib/THC/THCGeneral.h.in,torch/lib/THC/THCTensorMathPointwise.cuh,torch/lib/THCS/CMakeLists.txt,torch/lib/THCS/THCSTensor.cu,torch/lib/THCS/THCSTensor.h,torch/lib/THCS/THCSparse.cu,torch/lib/THCS/THCSparse.h,torch/lib/THCS/generic/THCSTensor.c,torch/lib/THCS/generic/THCSTensor.cu,torch/lib/THCS/generic/THCSTensor.h,torch/lib/THCS/generic/THCSTensorMath.cu,torch/lib/THPP/tensors/generic/THCSTensor.cpp,torch/lib/THS/generic/THSTensor.c,torch/lib/THS/generic/THSTensorMath.c",21.0,17,3,3.228729327,28.0,6238.0,3.0,992.1052631578948,329.0,15010.107,0.0,Feature Addition,0.0,1
pytorch,4c78fd7f97f71f4c4eeeb8861403d9331372ec65,88c722d84c2adb51a8452f24821c62ba59349d18,Kshiteej K,kshitijkalambarkar@gmail.com,Mon Jan 31 01:24:20 2022 +0500,1643592260.0,"[functorch] fix vmap : index_put_ (handle broadcasting of value) (pytorch/functorch#401)

* fix vmap support for setitem

* update batch rule for _index_put_impl_ as well

* update code as per review

* add test for index_put_

* fix rule to use indexed_shape

* rename function

* address review",171.0,20.0,"functorch/functorch/csrc/BatchRulesScatterOps.cpp,functorch/test/test_vmap.py",2.0,4,1,0.920031157,1.0,3967.0,1.0,0.0,746.0,1018.0,0.0,Corrective,1.0,1
pytorch,506a40ce44f883d6393cceb81827e00fb397069d,88d9fdec2e2db89c9fbf374913444e0c5314dc97,Adam Paszke,adam.paszke@gmail.com,Wed Nov 30 20:01:39 2016 +0100,1480536099.0,Add torch.cuda.set_device,19.0,1.0,"test/test_cuda.py,torch/cuda/__init__.py",2.0,3,2,0.811278124,17.0,862.0,1.0,1641.0,312.0,2365.512243,0.0,Feature Addition,0.0,1
pytorch,9d3c35d1e19af963661366466fad63276d3665c2,88e1c5c1d8b654ec4d8b4e9985eb20fd8f6f653c,Huy Do,huydhn@gmail.com,Fri Jul 15 18:51:38 2022 +0000,1657911098.0,"Apply ufmt linter to all py files under test/onnx (#81335)

Same as https://github.com/pytorch/pytorch/pull/81285 but for `test/onnx`. The merge conflicts in `linrunner.toml` is expected. I will resolve them depending on the merge order of the PR

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81335
Approved by: https://github.com/BowenBao, https://github.com/kit1980",61.0,62.0,".lintrunner.toml,test/onnx/debug_embed_params.py,test/onnx/export_onnx_tests_generator.py,test/onnx/test_caffe2_common.py,test/onnx/test_custom_ops.py,test/onnx/test_models.py,test/onnx/test_models_onnxruntime.py,test/onnx/test_operators.py,test/onnx/test_pytorch_helper.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_caffe2_quantized.py,test/onnx/test_pytorch_onnx_no_runtime.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_pytorch_onnx_onnxruntime_cuda.py,test/onnx/test_pytorch_onnx_shape_inference.py,test/onnx/test_utility_funs.py,test/onnx/test_verify.py",17.0,2,1,3.565249954,7.0,22143.0,5.0,565120.8823529412,5400.0,12752.5,0.0,,0.0,1
pytorch,f5435634b4a1de4bf1c7cdf4d4f9e44418960572,88f70a16708369ad5d179fbe515f43739c0f2591,Ailing Zhang,ailzhang@fb.com,Thu Apr 18 19:07:17 2019 -0700,1555614437.0,"Fix pickling torch.float32 (#18045)

Summary:
Attempt fix for #14057 . This PR fixes the example script in the issue.
The old behavior is a bit confusing here. What happened to pickling is python2 failed to recognize `torch.float32` is in module `torch`, thus it's looking for `torch.float32` in module `__main__`. Python3 is smart enough to handle it.
According to the doc [here](https://docs.python.org/2/library/pickle.html#object.__reduce__), it seems `__reduce__` should return `float32` instead of the old name `torch.float32`. In this way python2 is able to find `float32` in `torch` module.
> If a string is returned, it names a global variable whose contents are pickled as normal. The string returned by __reduce__() should be the objectâs local name relative to its module
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18045

Differential Revision: D14990638

Pulled By: ailzhang

fbshipit-source-id: 816b97d63a934a5dda1a910312ad69f120b0b4de",10.0,4.0,"test/test_torch.py,torch/csrc/Dtype.cpp,torch/csrc/utils/tensor_dtypes.cpp",3.0,4,2,1.492614068,40.0,11567.0,3.0,4218223.0,8153.0,24563.83333,0.0,Corrective,1.0,1
pytorch,39228e7d1da61b054e983f6e4f35485b09e59393,8926f51aa7dbdb0fc004560e10acd9fbb43a9b07,Samantha Andow,samdow@fb.com,Mon Feb 14 20:49:50 2022 -0500,1644871790.0,"[functorch] Add support for randn and rand (pytorch/functorch#409)

[ghstack-poisoned]",144.0,46.0,"functorch/functorch/_src/vmap.py,functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/functorch/csrc/DynamicLayer.cpp,functorch/functorch/csrc/DynamicLayer.h,functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/functorch/csrc/init.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",8.0,5,1,2.392403491,1.0,6190.0,5.0,5.285714285714286,786.0,1079.5,0.0,Feature Addition,0.0,1
pytorch,78a77667ddc6fcdb6ee9ec0cf52d5c61c7052e9e,895cb8fcea85d1879ab04599487a678f3b8b61d7,Tyler Moncur,mtmoncur@gmail.com,Thu Dec 13 03:51:34 2018 -0800,1544673094.0,"Fix resize for edge case tensors (#14874)

Summary:
Certain tensor shapes failed when being resized. This pull request addresses the bug found in #13404.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14874

Differential Revision: D13429788

Pulled By: soumith

fbshipit-source-id: 8aa6451dbadce46d6d1c47a01cb26e6559bcfc8c",10.0,0.0,"aten/src/ATen/native/Resize.h,aten/src/ATen/native/cuda/Resize.cuh,test/test_torch.py",3.0,6,2,1.521928095,40.0,9837.0,3.0,130919.66666666669,6032.0,18735.33333,0.0,Corrective,1.0,1
pytorch,784c46ba1d8067949ee9384bc6e7c7d23dff24f9,89ea6acde2101ce259bc8c52c38a40bae0ec41bc,Will Feng,yf225@cornell.edu,Thu Jun 07 02:49:12 2018 -0400,1528339752.0,"[NEEDS REVIEW] Add nan and inf probability check to multinomial (#7647)

* Add nan and inf probs check to multinomial

* fix bug

* Spawn CUDA test in subprocess

* Make sure invalid input won't pass the test case

* Try to fix error

* Test failure cases in Python 3 only

* Try to fix Windows error

* Move CUDA test to test_cuda.py

* fix issues

* fix module name error

* no need to check for CUDA existence in test_cuda

* Use PY3",85.0,0.0,"aten/src/TH/generic/THTensorRandom.cpp,aten/src/THC/THCNumerics.cuh,aten/src/THC/THCTensorRandom.cuh,test/test_cuda.py,test/test_torch.py",5.0,6,2,1.944249556,40.0,10976.0,4.0,880391.2,689.0,3701.0,0.0,Corrective,1.0,1
pytorch,c3bc927920dfb4f348091b4b1847c872e2b71732,8a016693c0808ec8353370fd4c48f4049a372b74,gchanan,gregchanan@gmail.com,Fri Apr 20 03:41:44 2018 -0400,1524195704.0,"Fix performance regression of simple indexing cases (#6793)

* Fix performance regression on simple cases of indexing

Dispatches to the old kernels

* Adapt JIT test

The test was expected to fail, but due to the change in the previous diff, it would now dispatch to index_select, which succeeds. I modified the function to go through the advanced indexing codepath

* Only do checks once, properly AutoNoGil, AutoGPU.",161.0,6.0,"test/test_cuda.py,test/test_indexing.py,test/test_jit.py,torch/csrc/autograd/python_variable_indexing.cpp",4.0,4,2,1.283092064,38.0,5170.0,3.0,55810.25,929.0,2240.805292,0.0,Corrective,1.0,1
pytorch,7c5014d803373b4b788b38718c46360a35df2872,8a20e22239e7ef6153dfe8aea34909b2df76fde3,Adam Paszke,adam.paszke@gmail.com,Sat Dec 31 17:46:10 2016 +0100,1483206370.0,Add torch.stack,19.0,0.0,"test/test_torch.py,torch/functional.py",2.0,2,2,0.949452015,19.0,2773.0,1.0,0.0,285.0,6368.224559,0.0,Feature Addition,0.0,1
pytorch,b5e1df046e6c1a025c1339d61bccd2cd8aa127c4,8a4eb50ed1ae875bcca6458b232fff289942911e,Sam Gross,colesbury@gmail.com,Wed Jun 28 21:43:21 2017 -0400,1498686201.0,"Speed up torch.matmul for 3D+ x 2D/1D tensors (#1931)

If the left tensor is 3D+ and the right tensor is at most 2D, we can
fold the batch into the matrix dimension and use torch.mm instead of
torch.bmm. In practice, this is faster especially if the right tensor is
column major.",61.0,15.0,"test/test_torch.py,torch/functional.py",2.0,2,2,0.789749254,31.0,4364.0,2.0,273657.5,247.0,1095.875885,0.0,,0.0,1
pytorch,76953beee3357024f6132b0133d2e987ce57c1e4,8a6b076196f4169910b2c98d37e1faeaec316050,Elias Ellison,elias.ellison@gmail.com,Wed Aug 10 18:56:38 2022 +0000,1660157798.0,"lift numpy tensor, add randperm support (#83191)

Couple changes needed to trace huggingface w fake tensors.

Similar to https://github.com/pytorch/pytorch/pull/81927, need to call liftfresh for tensors created from numpy tensors. Also adds randperm for meta.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83191
Approved by: https://github.com/bdhirsh",24.0,2.0,"test/test_fake_tensor.py,torch/_meta_registrations.py,torch/csrc/utils/tensor_numpy.cpp",3.0,4,2,1.334679141,8.0,1732.0,3.0,504277.3333333333,6321.0,14683.5,0.0,Feature Addition,0.0,1
pytorch,3a2e2044cd36c1c300039aa915863b3e74df396b,8a90249bc2079f5ae7398ff87fc7c362852836e3,Bin Bao,binbao@meta.com,Sat Dec 02 01:05:25 2023 -0800,1701479125.0,"[inductor] Update triton pin (#114772)

Differential Revision: [D51761353](https://our.internmc.facebook.com/intern/diff/D51761353)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/114772
Approved by: https://github.com/shunting314, https://github.com/atalman",16.0,29.0,".ci/docker/ci_commit_pins/triton.txt,.github/scripts/build_triton_wheel.py,benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv,benchmarks/dynamo/timm_models.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/utils.py",6.0,11,4,1.844397435,1.0,4728.0,6.0,2742583.1666666665,22643.0,51504.5,0.0,,0.0,1
pytorch,749d51414af11da48bdb195375f702822450aded,8aa0ae3836587d3e103d8e0ef2e5fbb563a6b5d2,Tongzhou Wang,SsnL@users.noreply.github.com,Thu Apr 12 19:03:22 2018 -0400,1523559802.0,Support arbitrary number of batch dimensions in *FFT (#6528),70.0,48.0,"aten/src/ATen/native/SpectralOps.cpp,test/test_autograd.py,test/test_torch.py,torch/_torch_docs.py",4.0,6,3,1.729996176,40.0,16424.0,2.0,75342.0,2558.0,24915.85823,0.0,,0.0,1
pytorch,f34c848f525ca346637cb57e8b7a607285769e50,8aa51741066bd8f21ec9c14132d88290ded08eb8,Jerry Zhang,jerryzh@fb.com,Mon Nov 19 23:25:43 2018 -0800,1542669943.0,"Tensor construction: combine Resize+mutable_data - 3/4 (#13944)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13944

Pull Request resolved: https://github.com/pytorch/pytorch/pull/13854

Codemod generated with clangr shard mode, 25 files per diff,
motivation: https://github.com/pytorch/pytorch/pull/12407

Reviewed By: ezyang

Differential Revision: D13054836

fbshipit-source-id: 5de07a156687f1ee607d0450410881d9176a87a7",110.0,97.0,"caffe2/operators/locally_connected_op_impl.h,caffe2/operators/lpnorm_op.cc,caffe2/operators/map_ops.h,caffe2/operators/merge_id_lists_op.h,caffe2/operators/moments_op.h,caffe2/operators/multi_class_accuracy_op.cc,caffe2/operators/ngram_ops.h,caffe2/operators/one_hot_ops.cc,caffe2/operators/order_switch_ops.h,caffe2/operators/pack_rnn_sequence_op.h,caffe2/operators/pack_segments.cc,caffe2/operators/pad_op.cc,caffe2/operators/perplexity_op.cc,caffe2/operators/rank_loss_op.cc,caffe2/operators/reduce_front_back_max_ops.h,caffe2/operators/reduce_front_back_sum_mean_ops.h,caffe2/operators/reduce_ops.h,caffe2/operators/reduction_ops.h,caffe2/operators/reshape_op.h,caffe2/operators/resize_op.cc,caffe2/operators/reverse_packed_segs_op.h,caffe2/operators/rmac_regions_op.cc,caffe2/operators/rnn/recurrent_network_blob_fetcher_op.h,caffe2/operators/roi_align_op.cc",24.0,3,1,4.251038178,11.0,5012.0,2.0,503843.5416666667,5510.0,16711.33333,0.0,,0.0,1
pytorch,6464e69e21ac364b32ab99920215b281c5361a31,8aa8f791fc0ce646519d13abef68745baa0a7b6a,Soumith Chintala,soumith@gmail.com,Wed Jan 18 13:39:33 2017 -0500,1484746773.0,add more torch.* and Tensor docs (#476),316.0,15.0,"torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/Module.cpp",3.0,2,1,0.915505839,22.0,6123.0,2.0,40808.0,136.0,867.1202836,0.0,Feature Addition,0.0,1
pytorch,b40dbdc49f8edc7e3ff597455187768cebb89aba,8b3f58d311a281732a9fca7a8c063d351caf7574,Mike Ruberry,mruberry@devfair044.h1.fair,Fri Jan 21 19:37:39 2022 -0800,1642793859.0,"Labels more elementwise binary operators correctly as BinaryUfuncInfos (#71622)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/66322.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/71622

Reviewed By: ngimel

Differential Revision: D33702106

Pulled By: mruberry

fbshipit-source-id: bd0b7b9172cb161daebc5852b9546e734be8ac17
(cherry picked from commit 02f2ff1646414c0978135b4d69e3d0d82b0c9ac1)",486.0,368.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,3.0,15533.0,1.0,37576.0,99.0,251.0,0.0,Corrective,1.0,1
pytorch,a49b7b0f58ee4d483774977f8bd33ded6fc15b26,8b492bbc476e66709da51646aaefaa0963975854,Adam Paszke,adam.paszke@gmail.com,Thu Nov 24 22:28:32 2016 +0100,1480026512.0,Return accreal as correct python types,12.0,4.0,"test/test_torch.py,tools/cwrap/plugins/THPPlugin.py",2.0,4,2,0.954434003,15.0,2944.0,1.0,180867.0,306.0,2144.591213,0.0,Corrective,0.0,1
pytorch,4335aac6e62c16e13770fe65d1bd365948a849fe,8b4dea3f56a2c5791c4f4b332c6c6a2311984365,Ivan Ogasawara,ivan.ogasawara@gmail.com,Mon Feb 11 12:52:06 2019 -0800,1549889526.0,"Added scientific notation on set_printoptions (#16876)

Summary:
This PR fixes #15683
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16876

Differential Revision: D14021703

Pulled By: soumith

fbshipit-source-id: 1f603a7d24e331831d8d389f4a704c6a5b070b0c",18.0,0.0,"test/test_torch.py,torch/_tensor_str.py",2.0,2,2,0.99107606,40.0,10504.0,2.0,2421018.0,6945.0,21403.83333,0.0,Corrective,1.0,1
pytorch,5249c43d9396bc19c8778b5c1fe0c2213af76314,8b53515b8aa3657112b9cfa5ed8e811767e2262d,Lara,lahaidar@microsoft.com,Tue Nov 12 02:25:11 2019 -0800,1573525511.0,"Add ONNX Export Support for torch.scalar_tensor (#28713)

Summary:
Support exporting torch.scalar_tensor() to ONNX.
This will allow making operations on dynamic scalars (like x.size(dim) where x is a tensor of dynamic shape) and exporting them to ONNX.

This is a dummy example of operations that could not be exported dynamically before this PR:

```
size_x = x.size(0)
size_y = y.size(0)
size_x_y_static = torch.tensor([size_x , size_y])  # size_x_y_static is traced as constant

size_x = torch.scalar_tensor(size_x).unsqueeze(0)
size_y = torch.scalar_tensor(size_y).unsqueeze(0)
size_x_y_dynamic = torch.cat((size_x , size_y))  # size_x_y_dynamic is dynamic and depends on x and y's size
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28713

Reviewed By: hl475

Differential Revision: D18438880

Pulled By: houseroad

fbshipit-source-id: c1651e480a41602c7c7452ffc4acba40a2b3827c",21.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.958711883,2.0,3935.0,1.0,283459.0,13051.0,35969.33333,0.0,Feature Addition,0.0,1
pytorch,790b69e0964f1779c1f071e8f5e7a9a35967985c,8b8c4096eed7fdeab5955837a487745e85a5124b,Heitor Schueroff,heitorschueroff@fb.com,Thu Apr 01 03:21:49 2021 -0700,1617247309.0,"Added OpInfo gradcheck_wrapper to replace output_func (#54914)

Summary:
Added a field to `OpInfo` to provide a wrapper function for gradcheck. This is useful for functions that need to perform some extra input/output processing to work with gradcheck.

fixes https://github.com/pytorch/pytorch/issues/50837

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54914

Reviewed By: H-Huang

Differential Revision: D27435234

Pulled By: heitorschueroff

fbshipit-source-id: fa3e9b61f3d3df221243fd142ddb8b7861dbf669",133.0,196.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.376831139,2.0,5475.0,1.0,56499.0,10287.0,22760.0,0.0,Corrective,1.0,1
pytorch,ec484981c67dcb304ab1f010c536aa44dff9e236,8b9e3e6fd4477443d1aa7f738611776462146e07,kshitij12345,kshitijkalambarkar@gmail.com,Thu Mar 11 19:25:46 2021 -0800,1615490746.0,"[complex] enable complex autograd cumsum (#53240)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/53182

Turns out that there is no need to update formula :)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53240

Reviewed By: VitalyFedyunin

Differential Revision: D26948582

Pulled By: anjali411

fbshipit-source-id: 450aab0d585f15385dd1748c2a3ddf787df0764b",48.0,4.0,"tools/autograd/gen_variable_type.py,torch/testing/_internal/common_methods_invocations.py",2.0,5,2,0.235193382,13.0,4994.0,2.0,82325.0,9675.0,21436.0,0.0,Corrective,1.0,1
pytorch,7f44c0d01167ad8e85b16d2f248daa5fa262a9ef,8ba8713f5d31beed7af6b9a003812aae52a0445a,Richard Zou,zou3519@users.noreply.github.com,Fri Mar 09 03:18:55 2018 -0500,1520565535.0,"torch.load() / torch.save() support arbitrary file-like object (#5466)

* Test serialization file-like object API guarantees and update docs.

* Implement torch.load() / torch.save() for arbitrary file-like objects

* Add tests for torch.load/save for file-like objects

* Fix compiler errors

* Throw error if user tries torch.save(tensor, StringIO.StringIO)

* Skip test_serialization_container_filelike. Investigation pending.

* Address comments

* Fix _test_serialization_container

* Address comments

* fix comment

* Use PyBuffer_FromReadWriteMemory

* Fix build by removing inlining

* Fix clang builds?

* Address comments

* Don't use memoryview in python 2

* Ensure doRead/doWrite templates are instantiated before they're used in generic/serialization.cpp",380.0,51.0,"test/test_torch.py,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/serialization.cpp,torch/csrc/generic/serialization.h,torch/csrc/serialization.cpp,torch/csrc/serialization.h,torch/serialization.py",7.0,4,2,2.16650965,39.0,6526.0,5.0,7100659.285714285,2431.0,24741.85823,0.0,Corrective,1.0,1
pytorch,cd813f16bf5c9c9b9c54e9b8d243919be62934ce,8bab4689438755a7b3fc1a737395760402d20a52,albanD,desmaison.alban@gmail.com,Tue Sep 21 19:55:47 2021 -0700,1632254147.0,"Reduce test size for max_pool (#65336)

Summary:
Fixe OOM in slow gradcheck tests.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65336

Reviewed By: malfet

Differential Revision: D31059007

Pulled By: albanD

fbshipit-source-id: 2dd6967d88663558e37f8c0836ad33333c92dfb5",4.0,4.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,10258.0,1.0,4888.0,15626.0,35936.0,0.0,Corrective,1.0,1
pytorch,2e841e68b2fe20707e89d85274fe7dd95d5073eb,8bb720304973f9a22562d9b314edfee4558076fc,Ivan Yashchuk,IvanYashchuk@users.noreply.github.com,Thu Apr 28 19:23:37 2022 +0000,1651173817.0,"Add torch.linalg.ldl_factor_ex and torch.linalg.ldl_solve

This PR adds a function for computing the LDL decomposition and a function that can solve systems of linear equations using this decomposition. The result of `torch.linalg.ldl_factor_ex` is in a compact form and it's required to use it only through `torch.linalg.ldl_solve`. In the future, we could provide `ldl_unpack` function that transforms the compact representation into explicit matrices.

Fixes https://github.com/pytorch/pytorch/issues/54847.

cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69828
Approved by: https://github.com/Lezcano, https://github.com/mruberry, https://github.com/albanD",1727.0,3.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h,aten/src/ATen/native/cuda/linalg/CUDASolver.cpp,aten/src/ATen/native/cuda/linalg/CUDASolver.h,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/allowlist_for_publicAPI.json,test/test_linalg.py,test/test_namedtuple_return_api.py,torch/csrc/api/include/torch/linalg.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_cuda.py,torch/testing/_internal/common_methods_invocations.py",19.0,17,4,3.226209256,16.0,58672.0,16.0,3315022.5263157897,2712.0,6470.0,0.0,Corrective,1.0,1
pytorch,69db88ab38557a2e6221521e3bbcc2753d97d2b6,8bb907b8b4e34281be65b5b6e54356e800dbd667,vfdev,vfdev.5@gmail.com,Thu Sep 30 03:56:45 2021 +0200,1632974205.0,"[functorch] Added roll batching rule (pytorch/functorch#145)

* Added roll batching rule
Description:
- Added roll batching rule
- Updated tests

Related to pytorch/functorch#112

* Fixes issue with no dims",30.0,3.0,"functorch/functorch/csrc/BatchRulesViews.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,4,1,1.06479483,1.0,4129.0,2.0,1.0,396.0,572.0,0.0,Corrective,1.0,1
pytorch,dfdc3069e73caa0ae9344bd1130dd3bdc42353ee,8bcf01631a14385df95230da4424b09f0375c52b,Jagadish Krishnamoorthy,jagdish.krishna@gmail.com,Wed Aug 04 17:18:47 2021 -0700,1628097527.0,"[ROCm] update magma (#62502)

Summary:
Update magma to point to magma_ctrl_launch_bounds branch.
When upstream magma branch is used,  cholesky tests in test_ops.py and test_linalg.py
fails due to ""Intel MKL ERROR: Parameter 4 was incorrect on entry to DPOTRF.""
Suspect commit: [35325212b15c5baadd7493d61b19b2db2635cb68](https://bitbucket.org/icl/magma/commits/35325212b15c5baadd7493d61b19b2db2635cb68) in magma master.

Signed-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62502

Reviewed By: malfet

Differential Revision: D30089171

Pulled By: seemethere

fbshipit-source-id: b07234ce66d48e3af113640995f923ee586b3cd9",7.0,3.0,.circleci/docker/common/install_rocm.sh,1.0,3,1,0,1.0,124.0,1.0,3066057.0,14396.0,32944.5,0.0,Corrective,0.0,1
pytorch,83f360887f66700ae3fd64ee5b112211aacb5726,8bd0522c20f89f12f6a22020889c0e3c96624140,Luke Yeager,lukeyeager@users.noreply.github.com,Thu Apr 13 17:49:49 2017 -0700,1492105789.0,"Add tests and GPU impls for sparse optimizers

Summary:
These GPU paths are probably even buggier than the CPU paths for sparse gradients with duplicate indices. Both paths cause multiple momentum updates in a single iteration, but only the GPU path is non-deterministic. Depending on how we decide to address the issues on the CPU path, pooyadavoodi has a good idea for how to match dense behavior with the sparse GPU ops.
Closes https://github.com/caffe2/caffe2/pull/254

Reviewed By: bwasti

Differential Revision: D4871680

Pulled By: dzhulgakov

fbshipit-source-id: 220be57a0f699a22ea85ed4f7022d92d362d06b3",409.0,52.0,"caffe2/python/operator_test/adagrad_test.py,caffe2/python/operator_test/adam_test.py,caffe2/python/operator_test/momentum_sgd_test.py,caffe2/sgd/adagrad_op.cc,caffe2/sgd/adagrad_op.h,caffe2/sgd/adagrad_op_gpu.cu,caffe2/sgd/adam_op.cc,caffe2/sgd/adam_op.h,caffe2/sgd/adam_op_gpu.cu,caffe2/sgd/momentum_sgd_op.cc,caffe2/sgd/momentum_sgd_op.h,caffe2/sgd/momentum_sgd_op_gpu.cu",12.0,4,1,2.999149093,4.0,1072.0,7.0,8256612.4,785.0,2266.833333,0.0,Corrective,1.0,1
pytorch,5ce94991eb7a9a9d258c1f2be49e63aa3fd2ec68,8be205ae1364ff3c48b41200282bafd4805cb63c,Ivan Yashchuk,IvanYashchuk@users.noreply.github.com,Mon Dec 21 18:09:28 2020 -0800,1608574168.0,"Added linalg.solve (#48456)

Summary:
This PR adds `torch.linalg.solve`.

`linalg_solve_out` uses in-place operations on the provided result tensor.

I modified `apply_solve` to accept tensor of Int instead of std::vector, that way we can write a function similar to `linalg_solve_out` but removing the error checks and device memory synchronization.

In comparison to `torch.solve` this routine accepts 1-dimensional tensors and batches of 1-dim tensors for the right-hand-side term. `torch.solve` requires it to be at least 2-dimensional.

Ref. https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48456

Reviewed By: izdeby

Differential Revision: D25562222

Pulled By: mruberry

fbshipit-source-id: a9355c029e2442c2e448b6309511919631f9e43b",378.0,31.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/api/include/torch/linalg.h,torch/csrc/autograd/FunctionsManual.cpp,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",12.0,19,5,2.701002856,16.0,29899.0,7.0,545770.8333333334,7677.0,17268.5,0.0,Feature Addition,0.0,1
pytorch,cc32de8ef90011de47ea6b0c7b1109b31ce81a70,8c14630e35675d1c55d893bf5e49234061235adb,Sam Gross,colesbury@gmail.com,Fri Jan 13 05:51:18 2017 -0500,1484286678.0,"Fix Tensor.apply_() (#444)

Fixes #411",7.0,1.0,"test/test_torch.py,torch/csrc/generic/methods/TensorApply.cwrap",2.0,5,2,0.811278124,21.0,2881.0,2.0,810575.0,318.0,6404.724559,0.0,Corrective,1.0,1
pytorch,611c771fc8b9940005c0e2431e6a27495dd0ac5c,8c18220a59f4a1ee1aca905cce32865499333aca,Tongzhou Wang,SsnL@users.noreply.github.com,Tue Feb 27 00:32:08 2018 -0500,1519691528.0,"Fix layer_norm initialization and nn.Module docs (#5422)

* Fix LN initialization; Support single int normalized_shape

* disable docstring inheritance

* fix sphinx warnings",16.0,7.0,"docs/source/conf.py,docs/source/nn.rst,docs/source/notes/faq.rst,torch/nn/modules/normalization.py",4.0,6,2,1.772560706,35.0,1675.0,3.0,2994532.75,2406.0,24693.85823,0.0,Corrective,1.0,1
pytorch,05ebd152070bf161b7abb78e88588dd78b1e6f89,8c2d35c75442859421d0c130979aaf05d3a2d2d4,Alican Bozkurt,alicanb@gmail.com,Wed Jan 17 10:58:08 2018 -0500,1516186688.0,Refactor distributions (#4688),252.0,248.0,"test/test_distributions.py,torch/distributions/beta.py,torch/distributions/dirichlet.py,torch/distributions/gamma.py,torch/distributions/kl.py,torch/distributions/normal.py",6.0,3,2,2.223164282,8.0,2616.0,5.0,407677.8333333333,920.0,6728.672317,0.0,Perfective,0.0,1
pytorch,36b476ccdd0ebaadf12d889801f8646a871fbaa7,8c74e1b840003832cb0a3fb84ee065a508620d70,Peter Bell,peterbell10@live.co.uk,Sun Apr 18 09:13:03 2021 -0700,1618737183.0,"Vectorize copysign on CPU (#51792)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51792

Test Plan: Imported from OSS

Reviewed By: agolynski

Differential Revision: D27769007

Pulled By: mruberry

fbshipit-source-id: 65fceb9f59ed6afee4452278992340da104ed5fe",93.0,36.0,"aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,c10/util/copysign.h",11.0,9,2,2.955175408,8.0,7702.0,6.0,3146286.1,10962.0,24182.5,0.0,,0.0,1
pytorch,381b3d8f4b25487499f17fc99b2b8413a82b3ccf,8c8f8829f0a1e491af46d45abfb770566fd19b79,Jeffrey Wan,jw3468@fb.com,Tue Apr 13 17:02:07 2021 -0700,1618333327.0,"Factor out numerical logic (#54479)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54479

This change is similar to #54049 in that it helps us factor out some code that can be used in both fast and slow versions of gradcheck.
 - `compute_gradient` and `compute_numerical_jacobian_cols` have  fewer responsibilities:
   - compute_numerical_jacobian_cols essentially only handles the complexity of complex derivatives
   - compute_gradient handles only finite differencing (and doesn't worry about different layouts and indexing into the input tensor)
  - we have two stages again where we first compute the columns separately, then combine them

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D27728727

Pulled By: soulitzer

fbshipit-source-id: fad3d5c1a91882621039beae3d0ecf633c19c28c",113.0,83.0,torch/autograd/gradcheck.py,1.0,2,1,0,28.0,868.0,1.0,109.0,10711.0,23688.5,0.0,,0.0,1
pytorch,69eeef072781196080aafeb002ed55e628023820,8c9c169b4839e2ab224a2810ae26cc65300f774f,Michael Lazos,mlazos@meta.com,Wed May 08 19:15:22 2024 -0700,1715195722.0,"LRScheduler composability kernel tests (#125383)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/125383
Approved by: https://github.com/eellison
ghstack dependencies: #123751, #123752, #123753",106.0,8.0,test/inductor/test_compiled_optimizers.py,1.0,2,1,0,1.0,662.0,1.0,677154.0,28426.0,67468.5,0.0,,0.0,1
pytorch,e6bbbb017e1ad829316e7d7ea0f9ca9e78421da2,8c9caf185be66b996df5caf4b37a0e902ae3ca41,"Gao, Xiang",qasdfgtyuiop@gmail.com,Sat Apr 06 01:13:39 2019 -0700,1554513219.0,"Add numpy like repeat as torch.repeat_interleave (#18395)

Summary:
Fixes: https://github.com/pytorch/pytorch/issues/14093
cc: SsnL
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18395

Differential Revision: D14599509

Pulled By: umanwizard

fbshipit-source-id: 2391a1cc135fe5bab38475f1c8ed87c4a96222f3",222.0,0.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/Repeat.cpp,aten/src/ATen/native/Repeat.h,aten/src/ATen/native/cuda/Repeat.cu,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py",12.0,10,4,2.908827788,42.0,29697.0,5.0,160997.0,7947.0,23998.83333,0.0,Corrective,1.0,1
pytorch,094df38e2f358d2a87d6083e850b083c90a34d3a,8cb32ba630238f8de15695c9b0492ca02d1a9518,mr.Shu,mr@shu.io,Thu Nov 30 16:46:04 2017 +0100,1512060364.0,"rnn.py: Note zero defaults for hidden state/cell

* Add a note on zero defaults for hidden states/cells of
  RNNs/LSTMs/GRUs.

* Should fix the note in #434

Signed-off-by: mr.Shu <mr@shu.io>",4.0,0.0,torch/nn/modules/rnn.py,1.0,3,1,0,33.0,695.0,1.0,527256.0,828.0,6556.672317,0.0,Corrective,1.0,1
pytorch,55e3b23abe0a4ff8f87903beb1d831e8839fb6b7,8d025bbc2dfd038eb0748b780fb91dbb9fc9a2a6,Eli Uriegas,eliuriegas@fb.com,Tue Nov 09 23:44:31 2021 -0800,1636501471.0,".github: Migrate macOS workflows to GHA (#67717)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67717

Signed-off-by: Eli Uriegas <eliuriegas@fb.com>

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D32287733

Pulled By: seemethere

fbshipit-source-id: 8df6b20aada818ad39895ef87dc280098e09707b",556.0,125.0,".circleci/config.yml,.circleci/generate_config_yml.py,.github/generated-ciflow-ruleset.json,.github/scripts/generate_ci_workflows.py,.github/templates/common.yml.j2,.github/templates/ios_ci_workflow.yml.j2,.github/templates/macos_ci_workflow.yml.j2,.github/workflows/generated-ios-12-5-1-arm64-coreml.yml,.github/workflows/generated-ios-12-5-1-arm64-custom-ops.yml,.github/workflows/generated-ios-12-5-1-arm64-full-jit.yml,.github/workflows/generated-ios-12-5-1-arm64-metal.yml,.github/workflows/generated-ios-12-5-1-arm64.yml,.github/workflows/generated-ios-12-5-1-x86-64-coreml.yml,.github/workflows/generated-ios-12-5-1-x86-64-full-jit.yml,.github/workflows/generated-ios-12-5-1-x86-64.yml,.github/workflows/generated-macos-10-15-py3-arm64.yml,.github/workflows/generated-macos-10-15-py3-lite-interpreter-x86-64.yml,.github/workflows/generated-macos-10-15-py3-x86-64.yml,.jenkins/pytorch/macos-build.sh,.jenkins/pytorch/macos-common.sh,.jenkins/pytorch/macos-lite-interpreter-build-test.sh,.jenkins/pytorch/macos-test.sh",22.0,7,3,3.409258253,5.0,10952.0,9.0,2103895.222222222,16978.0,39941.0,0.0,,0.0,1
pytorch,c414bf0aaf216f3fa84e2edc3fbdf017256e4e72,8d1a6975d277a2fc3614c74d7967fd0ea835784f,Adam Paszke,adam.paszke@gmail.com,Wed Jan 18 23:53:13 2017 +0100,1484783593.0,Fix for non-contiguous from_numpy (#489),29.0,5.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.977417818,22.0,3617.0,2.0,301797.5,88.0,5120.735245,0.0,Corrective,1.0,1
pytorch,fb186c00799a318a61666458ae03100c0826f1ec,8d2b9a08f42188cce5c4c44c616e7c456eb01030,Edward Z. Yang,ezyang@fb.com,Tue Nov 07 13:16:09 2017 +0800,1510060569.0,"Some documentation for derivatives.yaml

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",45.0,0.0,tools/autograd/derivatives.yaml,1.0,2,1,0,10.0,653.0,1.0,0.0,347.0,2164.5,0.0,Non Functional,0.0,1
pytorch,cd6167ff63bdf7c77ebe280953b59716b02df29d,8d35b6cec7146f0cffded101b806eb4dc3d47daf,Ansha Yu,ansha@fb.com,Sat Dec 07 00:08:46 2019 -0800,1575677326.0,"embedding_bag make_bag_size optimization (#30701)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30701

From James' PR https://github.com/pytorch/pytorch/pull/19715

embedding_bag microbenchmarks:
Baseline: P123020983
Refactor make_bag_size, no changing at::zeros to at::empty (this diff): P123021393
Inference benchmark on T6_SKL - _embedding_bag self time only:
bs=40, baseline: .302 ms/iter
bs=40, with diff: .244 ms/iter
bs=1 baseline: .148 ms/iter
bs=1 with diff: .124 ms/iter
The bigger gap comes from fb::embedding_bag_byte_rowwise_offsets, I'm looking into that one too.

Test Plan:
MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./inference_benchmark_nolr_emb.par --pt-scripted-model=traced_model.pt --pt-inputs=""batch_size_40/pt_inputs.pth"" --iters=3000 --warmup-iters=100
buck run mode/opt //caffe2/benchmarks/operator_benchmark:benchmark_all_other_test -- --tag_filter all --iterations 3000 --operators embeddingbag

Reviewed By: yinghai, qizzzh

Differential Revision: D18800166

fbshipit-source-id: 820e6ece0b6ade72ee42409661f92c548f43a4cb",9.0,4.0,aten/src/ATen/native/EmbeddingBag.cpp,1.0,4,1,0,6.0,708.0,1.0,1468676.0,13712.0,37419.83333,0.0,Perfective,0.0,1
pytorch,75155df8b4a1e87e22b6778a51619d9faf7c64f5,8d570bc708a63009f3be3f892767bdf4c2ac13a4,chengjun,chengjun.lu@intel.com,Tue Jul 07 19:43:19 2020 -0700,1594150999.0,"Decouple DataParallel/DistributedDataParallel from CUDA (#38454)

Summary:
Decouple DataParallel/DistributedDataParallel from CUDA to support more device types.
- Move torch/cuda/comm.py to torch/nn/parallel/comm.py with minor changes for common devices support. Torch.cuda.comm is kept as is for backward compatibility
- Provide common APIs to arbitrary device types without changing existing CUDA APIs in torch.cuda space.
- Replace the torch.cuda calls in DataParellel/DistributedDataParallel with the new APIs.

Related RFC: [https://github.com/pytorch/pytorch/issues/36160](https://github.com/pytorch/pytorch/issues/36160)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/38454

Differential Revision: D22051557

Pulled By: mrshenli

fbshipit-source-id: 7842dad0e5d3ca0f6fb760bda49182dcf6653af8",360.0,287.0,"mypy.ini,test/distributed/test_c10d.py,torch/_utils.py,torch/cuda/_utils.py,torch/cuda/comm.py,torch/nn/parallel/_functions.py,torch/nn/parallel/comm.py,torch/nn/parallel/data_parallel.py,torch/nn/parallel/distributed.py,torch/nn/parallel/replicate.py",10.0,6,2,2.258110521,40.0,5804.0,7.0,8674053.0,3430.0,8152.0,0.0,,0.0,1
pytorch,d434ac35e49a886542a8487e46ec8a49b569ad97,8d7338e820b1296e01b0d3ba1166907a98c26542,peter,peterghost86@gmail.com,Tue Dec 29 17:56:01 2020 -0800,1609264561.0,"Enable tests using named temp files on Windows (#49640)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49640

Reviewed By: ngimel

Differential Revision: D25681548

Pulled By: malfet

fbshipit-source-id: 0e2b25817c98d749920cb2b4079033a2ee8c1456",55.0,58.0,"test/jit/test_tracer.py,test/test_autograd.py,test/test_jit.py,test/test_profiler.py,test/test_serialization.py,test/test_throughput_benchmark.py,torch/testing/_internal/common_utils.py",7.0,5,2,2.627980481,44.0,29172.0,7.0,2573223.714285714,7769.0,17574.0,0.0,,0.0,1
pytorch,fea4a56af3d565a06372266f871f8ca615f9a2c8,8d7a025703c03b7c0c8438b6f7488d64308f9050,Lara,lahaidar@microsoft.com,Wed May 22 20:25:45 2019 -0700,1558556745.0,"ONNX Export Scatter

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18543

Differential Revision: D14658639

Pulled By: houseroad

fbshipit-source-id: 5d7821b54d2fc93f71120155adf328897d13aff6",163.0,0.0,"caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic_opset9.py",4.0,6,3,1.517613103,11.0,5734.0,4.0,2723508.25,8824.0,26063.83333,0.0,,0.0,1
pytorch,4aeb98dee9756119f6a6414338e92f2b52c83346,8dbb0990bccb7b12f986f5cbc182c384041334ff,Peter Bell,peterbell10@live.co.uk,Fri Aug 19 02:32:18 2022 +0100,1660876338.0,"Move `torch.linalg` opinfos to opinfo.definitions (1/2) (#83547)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83547
Approved by: https://github.com/albanD",1184.0,813.0,"torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/linalg.py",2.0,5,1,0.976032722,6.0,19938.0,2.0,18230.0,6606.0,15259.5,0.0,,0.0,1
pytorch,84047ff3421b62ae277c082ce2ad97708154f54b,8e343ba5dbaff36f98090a491c5eee7ad67ee005,Omkar Salpekar,osalpekar@fb.com,Tue Nov 23 21:36:02 2021 -0800,1637703362.0,"Revert D32611368: [pytorch][PR] Initial version of general convolution_backward

Test Plan: revert-hammer

Differential Revision:
D32611368 (https://github.com/pytorch/pytorch/commit/445b31abff8bfdbd6e74acc420585089e95ef585)

Original commit changeset: 26d759b7c908

fbshipit-source-id: e91f45f0f31150e60d657a3964b7e42027beff58",59.0,586.0,"aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,tools/autograd/derivatives.yaml",5.0,7,3,1.526857335,45.0,35455.0,1.0,19191.0,17292.0,40657.5,0.0,,0.0,1
pytorch,637dcdc279bf83a7ffbe692d402480696b9fe3f3,8e4fe5dcf4e2890ceaaf96686614c76e5106e5f1,li-roy,8813817+li-roy@users.noreply.github.com,Wed Jun 20 02:11:13 2018 -0700,1529460673.0,"Fix serialization for Parameters (#8633)

* Fix serialization for Parameters

* address comments

* addres comments",27.0,0.0,"test/test_torch.py,torch/nn/parameter.py",2.0,3,2,0.503258335,39.0,7697.0,2.0,2884648.5,2741.0,25234.85823,0.0,Corrective,1.0,1
pytorch,679e8e9d480b57241f70051e1b9281b09d378344,8ef057255d8a80b0e5415e6045b28d8175d89f3b,leslie-fang-intel,leslie.fang@intel.com,Fri Aug 25 12:54:13 2023 +0800,1692968053.0,"[Quant][PT2E] Enable qconv for quantization 2.0 export (#104580)

**Summary**
Enable `qconv1d/2d/3d`, `qconv2d_relu`, `qconv2d_add`, and `qconv2d_add_relu` operator for quantization 2.0 export with oneDNN library.

**Test Plan**
```
python -u -m pytest -s -v test_quantized_op.py -k test_qconv1d_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv2d_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv3d_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv2d_relu_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv2d_add_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv2d_add_relu_pt2e
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/104580
Approved by: https://github.com/jgong5, https://github.com/jerryzh168",1093.0,0.0,"aten/src/ATen/native/quantized/cpu/OnednnUtils.h,aten/src/ATen/native/quantized/cpu/qconv.cpp,aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp,aten/src/ATen/native/quantized/library.cpp,test/quantization/core/test_quantized_op.py",5.0,9,2,1.656731067,4.0,9863.0,3.0,3630676.6,19033.0,43187.0,0.0,Feature Addition,0.0,1
pytorch,75c2b34c8603b402af0d04f78426393b2905f3f0,8f0f97749c4e184da08e9b7d40e9ebd50247fc25,Pieter Noordhuis,pietern@fb.com,Mon Nov 05 21:49:20 2018 -0800,1541454560.0,"Add allgather support in c10d/gloo (#13424)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13424

This adds support for the allgather collective call in the gloo backend. The gloo implementation does not support multiple inputs per rank (nor one or more outputs per rank), so we use a temporary flattened buffer and unflatten once the collective finishes.

Reviewed By: teng-li

Differential Revision: D12832009

fbshipit-source-id: 2f5c1934a338589cef1d3192bd92ada135fecd7a",200.0,3.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/Utils.hpp",3.0,4,2,1.522281335,3.0,2472.0,2.0,2.6666666666666665,5151.0,15398.33333,0.0,Feature Addition,0.0,1
pytorch,c07a123b2699423f8ab6ca1d3db3f11c3961be89,8f7ae770403e9a84551ae2504e6646e2322bae14,Hui Guo,huiguo@fb.com,Tue Aug 03 01:32:21 2021 -0700,1627954341.0,"[nnc] Add context-sensitive simplification for div/mod (#60688)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/60688

Test Plan: Imported from OSS

Reviewed By: navahgar, ZolotukhinM

Differential Revision: D29373313

Pulled By: huiguoo

fbshipit-source-id: 90d7f2fbfce583b0ea3b0f1c7899e22b0210bd62",620.0,0.0,"test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",3.0,7,2,1.03085893,1.0,7979.0,3.0,732096.3333333334,14344.0,32807.5,0.0,Feature Addition,0.0,1
pytorch,aa017db59c712778089cdda05ee54c1adc7a3777,8f9b11cf334bf15c35909094edd868892bddbc6f,Shen Li,shenli@fb.com,Tue Apr 09 19:06:04 2019 -0700,1554836764.0,"Propagate ProcessGroup timeout to Store (#16571)

Summary:
closes #16520

Hi pietern, I am not sure if this is the expected way to pass timeout to `Store`, could you please help take a look? Thanks!

Questions:
1. How do I write tests for this? I wanted to do something like `test_barrier_timeout_global`, but it seems I need to set the pg's timeout larger than the `Store`'s default timeout (3 min) to see a difference, which is too long for a unit test. And I do not want to change the `Store`'s default timeout either. Any suggestion?
2. Should I also propagate timeout configuration down to `PrefixStore` in `_new_process_group_helper`?
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16571

Differential Revision: D13954527

Pulled By: mrshenli

fbshipit-source-id: 77f2653903f24255207233eb298f7c0321119a87",63.0,0.0,"test/test_c10d.py,torch/distributed/distributed_c10d.py,torch/lib/c10d/Utils.hpp",3.0,5,2,0.955555401,2.0,3667.0,3.0,413712.6666666667,8003.0,24147.83333,0.0,Corrective,1.0,1
pytorch,40f7f6e095ce450cbd013e67be267627cd204f14,8fbe003d4ed946804d67a6d3bcd84eb6c3df9a4a,Edward Z. Yang,ezyang@fb.com,Fri Oct 27 20:35:27 2017 -0700,1509136527.0,"Miscellaneous ONNX fixes and behavior changes.

- Deleted Addmm/Concat Function class, as this is now native ATen operator

- Resurrected ONNX operator for Concat (now called 'cat')

- Add a ""fake"" Expand ONNX operator, which we now do the optimization on;
  this helps prevent us from emitting a warning that 'expand' is not supported.
  We still fail if any of these Expand operators make it to the final model,
  until we actually formalize Expand in ONNX.  This also simplifies the
  fuseBroadcast code, because single-return ONNX nodes don't get select nodes.

- New error reporting strategy.  If we fail to export an operator because of
  something, we emit a warning, but otherwise keep going.  At the very end,
  in export.cpp, we now check if there are any ATen operators left over.  If
  there are, we bug out.  This assumes that ATen is lower case and ONNX is upper
  case.  You're now supposed to 'return _unimplemented(msg)' in these cases.

- New toString() method on Graph, for getting the string graph (useful for
  slapping it into error messages.)

- Some of the legacy symbolics (still in Python symbolic method of Function
  subclass) have been cleaned up for clarity.)

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",112.0,148.0,"torch/autograd/_functions/blas.py,torch/autograd/_functions/tensor.py,torch/csrc/jit/export.cpp,torch/csrc/jit/interned_strings.h,torch/csrc/jit/ir.h,torch/csrc/jit/passes/onnx.cpp,torch/nn/_functions/dropout.py,torch/nn/_functions/padding.py,torch/nn/_functions/thnn/pooling.py,torch/onnx/__init__.py,torch/onnx/symbolic.py",11.0,10,1,3.013270242,29.0,3972.0,4.0,159408.54545454544,54.0,899.5,0.0,Corrective,1.0,1
pytorch,0bb0ee883e8586a2f17ad185f298fdbcfe59eec4,8fd171a6fd4e7b3a9bd9399d228e526f0fe6cf5a,SsnL,tongzhou.wang.1994@gmail.com,Fri Nov 03 22:13:08 2017 -0400,1509747188.0,add test_index to test_cuda,27.0,14.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.377646321,38.0,5800.0,2.0,232703.5,2073.0,23997.35823,0.0,Feature Addition,0.0,1
pytorch,84f168539725292d0b824c30d1d872fb30811803,8ff1a8fdcac79356e4abdb5e0b1bea3a0c5f6237,lezcano,lezcano-93@hotmail.com,Thu Jan 27 18:33:44 2022 -0800,1643308424.0,"Implement forward AD for linalg.svd and improve svd_backward (#70253)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/70253

I included a derivation of the formula in the complex case, as it is
particularly tricky. As far as I know, this is the first time this formula
is derived in the literature.

I also implemented a more efficient and more accurate version of svd_backward.
More importantly, I also added a lax check in the complex case making sure the loss
function just depends on the subspaces spanned by the pairs of singular
vectors, and not their joint phase.

cc jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano

Test Plan: Imported from OSS

Reviewed By: mikaylagawarecki

Differential Revision: D33751982

Pulled By: mruberry

fbshipit-source-id: c2a4a92a921a732357e99c01ccb563813b1af512
(cherry picked from commit 391319ed8f2e0ecc1e034d8eaecfb38f5ea4615f)",501.0,206.0,"aten/src/ATen/native/LinearAlgebra.cpp,test/test_linalg.py,test/test_ops.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/linalg/__init__.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",9.0,13,4,1.680539361,15.0,42008.0,4.0,351736.5555555556,250.0,512.5,0.0,Feature Addition,0.0,1
pytorch,f137c0c05aece496302972dc29614d39ef97daef,9000f40e6140319cab87c088a16e9fc1dce552e6,Adam Paszke,adam.paszke@gmail.com,Mon Oct 24 08:31:03 2016 +0200,1477297863.0,Add torch.from_numpy,46.0,1.0,"test/test_torch.py,torch/csrc/Module.cpp",2.0,3,2,0.973385435,11.0,3351.0,2.0,215.5,258.0,2322.14599,0.0,Feature Addition,0.0,1
pytorch,e8dc34eaebd7fbb6433c6e019b50d099ea505a8b,904d549ca48bce0ccf5ff8b94163e6c2862466f7,Edward Z. Yang,ezyang@meta.com,Wed Feb 15 19:37:00 2023 -0800,1676489820.0,"Add some simple sanity tests to ValueRanges (#94905)

To start, I simply test that unary/binary ops agree with reference when
the ranges are singleton.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/94905
Approved by: https://github.com/lezcano, https://github.com/eellison",162.0,0.0,".lintrunner.toml,test/test_value_ranges.py",2.0,1,1,0.054185698,3.0,896.0,1.0,27295.0,12533.0,29510.0,0.0,Feature Addition,0.0,1
pytorch,ae2573745509330d7aff78f44386b730ba4c4767,90532d5f5773a80395852b3143677a01479d1508,cpuhrsch,cpuhrsch@googlemail.com,Mon Jun 18 20:07:01 2018 -0400,1529352421.0,Don't use MKL VML for log2 if below MKL build 20180406 (#8614),7.0,1.0,aten/src/ATen/cpu/vml.h,1.0,4,1,0,1.0,122.0,1.0,365200.0,1351.0,3947.805292,0.0,,0.0,1
pytorch,6daaeb2bdaf5244ad2904a4471844c7288c5fbcb,906c50eb69daf763e48573c8201cfdbd6a60654e,Rohan Varma,rvarm1@fb.com,Tue May 12 19:39:19 2020 -0700,1589312359.0,"Remove dead code in ddp.{h, cpp} (#37990)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37990

The code in `ddp.{h, cpp}` and the corresponding pybind implementations are no longer used. The pybinded calls were all private APIs and only ran in unittests, so we should remove these unused APIs.

https://github.com/pytorch/pytorch/pull/20234 from a year ago also mentioned that we should delete `_dist_broadcast_coalesced`

Verified that all tests pass with cuda by running `test_c10d` on a gpu-enabled machine.
ghstack-source-id: 103885383

Test Plan: CI

Differential Revision: D21443879

fbshipit-source-id: 764d8681ca629056bfe2c260ffab47fa5bdf07ff",6.0,542.0,"test/distributed/test_c10d.py,tools/build_variables.bzl,torch/CMakeLists.txt,torch/csrc/distributed/c10d/ddp.cpp,torch/csrc/distributed/c10d/ddp.h,torch/csrc/distributed/c10d/init.cpp,torch/testing/_internal/common_distributed.py",7.0,9,3,1.854203575,6.0,5705.0,7.0,8981613.857142856,1969.0,5061.5,0.0,,0.0,1
pytorch,e772a440cb66673d130fb8c516bffac7769176b6,90876246342a151221465fd5bca040b435260769,Gregory Chanan,gchanan@fb.com,Mon May 15 18:48:25 2017 -0700,1494874105.0,"Revert ""Restore examples with keepdim=True default.""

This reverts commit 6fab62173e842bbf550de1c68cfae507ca35b800.",14.0,14.0,torch/_torch_docs.py,1.0,1,1,0,19.0,4528.0,1.0,0.0,873.0,11418.44394,0.0,,0.0,1
pytorch,94ebf52ea326dec22300a2d7bacd8a15d8f0c12f,908924204841ab43896627a85174efaf14fa9a6f,Kazuaki Ishizaki,ishizaki@gmail.com,Fri Nov 03 07:53:33 2023 +0000,1698998013.0,"Fix typo under test directory (#112346)

This PR fixes typo in comments and messages under `test` directory. This PR also fixes related typo in messages under `torch` directory.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/112346
Approved by: https://github.com/kit1980, https://github.com/ezyang",78.0,78.0,"test/distributed/_shard/sharded_optim/test_sharded_optim.py,test/distributed/_shard/sharded_tensor/test_sharded_tensor.py,test/distributed/_spmd/test_tracing.py,test/distributed/_tensor/test_dtensor_ops.py,test/distributed/checkpoint/e2e/test_fine_tuning.py,test/distributed/checkpoint/test_dtensor_resharding.py,test/distributed/elastic/agent/server/test/local_elastic_agent_test.py,test/distributed/elastic/timer/local_timer_example.py,test/distributed/fsdp/test_fsdp_core.py,test/distributed/fsdp/test_fsdp_fine_tune.py,test/distributed/fsdp/test_fsdp_memory.py,test/distributed/fsdp/test_fsdp_meta.py,test/distributed/fsdp/test_fsdp_misc.py,test/distributed/fsdp/test_fsdp_state_dict.py,test/distributed/pipeline/sync/test_bugs.py,test/distributed/tensor/parallel/test_tp_random_state.py,test/distributed/test_c10d_common.py,test/distributed/test_c10d_nccl.py,test/distributed/test_device_mesh.py,test/distributed/test_dynamo_distributed.py,test/dynamo/test_aot_autograd.py,test/dynamo/test_exc.py,test/dynamo/test_misc.py,test/dynamo/test_modules.py,test/dynamo/test_python_autograd.py,test/dynamo/test_recompile_ux.py,test/dynamo/test_replay_record.py,test/dynamo/test_repros.py,test/dynamo/test_subclasses.py,test/dynamo/test_unspec.py,test/export/test_serialize.py,test/functorch/attn_ft.py,test/functorch/test_aotdispatch.py,test/functorch/test_control_flow.py,test/functorch/test_eager_transforms.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/fx/test_gradual_type.py,test/fx/test_subgraph_rewriter.py,test/fx/test_z3_gradual_types.py,torch/_dynamo/variables/torch_function.py,torch/distributed/_device_mesh.py,torch/fx/experimental/validator.py",43.0,29,2,5.210090642,2.0,59779.0,39.0,6433726.093023256,21501.0,49136.5,0.0,Corrective,1.0,1
pytorch,39508501a4816af546b603d6030f857808ac5669,909b8eba0daf19650e5b2a7f8896bb85b1c11d6e,Jie,jiej@nvidia.com,Fri Dec 27 01:14:33 2019 -0800,1577409273.0,"cudnn grouped convolution nhwc patch (#31444)

Summary:
Earlier cudnn version doesn't support grouped convolution in NHWC well. Legit
configuration in later cudnn version might return CUDNN_STATUS_NOT_SUPPORTED.
We are falling back to NCHW when runtime check of cudnn version is < 7.6.0 to
keep the logic simple.

Note:
We might update the heuristics, 7.6.0 is very conservative.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31444

Differential Revision: D19232414

Pulled By: VitalyFedyunin

fbshipit-source-id: 4c2d79ed347c49cd388bbe5b2684dbfa233eb2a3",50.0,10.0,"aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/cudnn/Conv.cpp,test/test_nn.py",3.0,6,2,1.497837163,42.0,12761.0,2.0,387189.6666666667,13998.0,38102.83333,0.0,,1.0,1
pytorch,ea5819045e26521bc3ac3be30a13d2eefd557497,909f31764faf65c80f4d8eaeae8a12c67eb24c17,Aron Barreira Bordin,aron.bordin@gmail.com,Thu Jun 15 11:41:38 2017 -0300,1497526898.0,"Add nn.padding to docs fixes #1127 (#1808)

* exposed nn.padding modules

* using functional",152.0,6.0,"docs/source/nn.rst,torch/nn/modules/padding.py",2.0,5,2,0.751286577,29.0,993.0,2.0,94902.5,954.0,10933.92866,0.0,Corrective,1.0,1
pytorch,939060925f28c9498da42225f216d838e1f7f4ca,90d31cb311e8de0b91da7891077755911b6479bb,Thiago Crepaldi,thiago.crepaldi@microsoft.com,Sat Apr 23 21:24:25 2022 +0000,1650749065.0,"Emit ATen ops when symbolics raise + minor fixes

Currently `torch.onnx.export(.., operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK)` only issues ATen ops through explicit requests (e.g. `g.at()`) calls inside each op symbolic function. This is done based on specific conditions such as `operator_export_type==OperatorExportTypes.ONNX_ATEN_FALLBACK)` or `is_caffe2_aten_fallback()`

This PR extends the ATen fallback mechanism for scenarios when the symbolic function raises `RuntimeError` during export. The idea is that partial implementation of existing ONNX ops can fallback to ATen as a last resort. That is valuable because each operator can have many input combinations and not all are always implemented.

A minor fix was done to make sure the `overload_name` attribute is added to explicit ATen op fallback requests when a symbolic is not registered to a particular op.

ps: The behavior for builds with BUILD_CAFFE2=1 is not changed to ensure BC.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74759
Approved by: https://github.com/garymm, https://github.com/msaroufim",107.0,4.0,".github/merge_rules.json,scripts/onnx/test.sh,test/onnx/test_onnx_export.py,torch/_C/__init__.pyi.in,torch/onnx/utils.py,torch/testing/_internal/common_utils.py",6.0,10,4,1.65277545,8.0,5973.0,5.0,456301.2,2556.0,6045.5,0.0,Corrective,1.0,1
pytorch,a31fd7f45373be3f01e5b822be513a164927c64b,90e63cc41f06f6f5920916492921704e660a853d,BowenBao,bowbao@microsoft.com,Wed Apr 21 05:58:06 2021 -0700,1618984686.0,"[ONNX] Add support for prim::min (#55259) (#56168)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56168

Add support for prim::min operator and update full_like

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D27866144

Pulled By: SplitInfinity

fbshipit-source-id: f4af4b8171ed8bd7980fa3141f5fc9811e2bc367",36.0,3.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.918295834,3.0,11207.0,1.0,3.0,11064.0,24402.5,0.0,Feature Addition,0.0,1
pytorch,342e62f1e3849118034958cc012d68b1b5e25418,90f9e8103c9b1108ae923f5ab67bcc566e942699,Shen Li,shenli@fb.com,Wed Dec 12 23:18:57 2018 -0800,1544656737.0,"Implement torch.tril_indices and torch.triu_indices (#12653) (#14904)

Summary:
This is an optimized implementation that does the following:

1. created an empty Tensor of correct size.
2. fill the Tensor with correct values.

The following three designs to fill in the Tensor result in roughly the same performance. Hence, the 2nd option is taken for simpler code, and to return contiguous tensors.

1. Sequential: fill row coordinates first, then columns. This results in two for-loop and more arithmetic operations.
2. Interleaved: fill in index coordinates one by one, which jumps between the two output Tensor rows in every iteration.
3. Transpose: create a n X 2 Tensor, fill the Tensor sequentially, and then transpose it.

<img width=""352"" alt=""screen shot 2018-12-10 at 3 54 39 pm"" src=""https://user-images.githubusercontent.com/16999635/49769172-07bd3580-fc94-11e8-8164-41839185e9f9.png"">

NOTE:

This implementation returns a 2D tensor, instead of a tuple of two tensors. It means that users will not be able to do the following:

```python
x = torch.ones(3, 3)
i = torch.tril_indices(3, 3)
x[i]  # need to first convert the 2D tensor into a tuple of two 1D tensors.
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14904

Reviewed By: zou3519

Differential Revision: D13433027

Pulled By: mrshenli

fbshipit-source-id: 41c876aafcf584832d7069f7c5929ffb59e0ae6a",350.0,1.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,test/test_cuda.py,test/test_torch.py,tools/autograd/gen_python_functions.py,torch/_torch_docs.py",6.0,8,4,1.840075997,42.0,23151.0,6.0,173761.5,6024.0,18725.83333,0.0,Corrective,0.0,1
pytorch,9c35a680943c208752356b24d16c23f3c7cf2b34,90faf43151352ea5d4f0fa6f37cd2017bcf473b2,Lillian Johnson,lillianjohnson@fb.com,Thu Dec 03 03:52:39 2020 -0800,1606967559.0,"Support for OpInfo-based testing for operators in JIT (#47696)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/47696

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D25212436

Pulled By: Lilyjjo

fbshipit-source-id: 1fd2884d86b2afd6321ae1599d755b4beae4670a",272.0,11.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.954107888,2.0,2081.0,1.0,73840.0,7154.0,16183.0,0.0,,0.0,1
pytorch,2c6c53f5ceb48503ac5d1a5e7f81bba69e32f31f,90fd4df695a0a4d95e9a429f4fbe52d9be245c00,Will Feng,willfeng@fb.com,Sat Jun 30 21:24:05 2018 -0700,1530393845.0,"Add flag for disabling tests with multiprocessing spawn start method (#9061)

Summary:
This will resolve some of the timeout issues in CPU and GPU tests internally.
Closes https://github.com/pytorch/pytorch/pull/9061

Reviewed By: ezyang

Differential Revision: D8707471

Pulled By: yf225

fbshipit-source-id: 9dc82a2c9da0c540ae015442f74b9b2b1a67a246",22.0,4.0,"test/common.py,test/test_cuda.py,test/test_dataloader.py,test/test_multiprocessing.py,test/test_torch.py",5.0,1,1,2.210860643,41.0,11462.0,5.0,1863855.8,2787.0,6305.333333,0.0,Feature Addition,0.0,1
pytorch,9e45c898912720621328efe84cae79b33c517b5f,910c01020e78c76c3adc0dc603e9814cc6c830ca,mingfeima,mingfei.ma@intel.com,Fri Jan 14 03:59:21 2022 -0800,1642132761.0,"add BFloat16 support for AdaptiveMaxPool2d on CPU (#66929)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66929

Test Plan: Imported from OSS

Reviewed By: mikaylagawarecki

Differential Revision: D33353199

Pulled By: VitalyFedyunin

fbshipit-source-id: d402d5deb7ca766259ca42118ddc16625e134c4c",165.0,18.0,"aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp,test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,0.599048685,43.0,36136.0,3.0,2510939.6666666665,18361.0,43468.0,0.0,Feature Addition,0.0,1
pytorch,461e887d92b37ee6ecae43f66b015a50c5ac2478,913f1f75b33be186474c65cafd5daad91a6e5855,BowenBao,bowbao@microsoft.com,Fri Apr 23 05:21:06 2021 -0700,1619155266.0,"Revert ""Revert [ONNX] Redesign inplace conversion"" (#56675)

Summary:
Adjust how MutationRemover is used to avoid creating aliasDb multiple times for the same graph.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/56675

Reviewed By: pbelevich

Differential Revision: D27945692

Pulled By: SplitInfinity

fbshipit-source-id: a6c548438e88ddee18ef03a6f0461ab9eaaaa829",926.0,637.0,".jenkins/caffe2/test.sh,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp,torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp,torch/csrc/jit/passes/remove_mutation.cpp,torch/csrc/jit/passes/remove_mutation.h,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",8.0,10,3,1.129135879,8.0,14166.0,2.0,144153.25,11186.0,24728.0,0.0,Preventative,0.0,1
pytorch,632910e2a8e2ee78f0048ccc5f458f0bf8a5934a,914d3ca2ba0d63437d835da10364cccb2bb41a16,Jiong Gong,jiong.gong@intel.com,Fri Jun 21 00:19:58 2024 -0700,1718929198.0,"[inductor][cpp] BF16 AMX micro-gemm support (#127195)

This PR adds the intrinsics based micro-gemm for BF16 using Advanced Matrix eXtension (AMX) instructions available in Intel 4th and 5th Xeon processors. A compilation check is added to `codecache.py` to check the validity of the compiler support. Also, since AMX requires an initialization in the Linux kernel to extra register states, an initialization function is added to do that and triggered via `codecache.py`.

Performance speedups with >=10% on BF16 AMP, max_autotune vs. no autotune, measured on Intel(R) Xeon(R) Platinum 8488C:
Static shapes
Single-threaded
| Model Family | Model Name | Speedup |
|--------------|------------|---------|
| timm_models | mixer_b16_224 | 1.54 |
| timm_models | convit_base | 1.53 |
| huggingface | MobileBertForQuestionAnswering | 1.52 |
| torchbench | fastNLP_Bert | 1.44 |
| torchbench | llama | 1.33 |
| timm_models | swin_base_patch4_window7_224 | 1.31 |
| torchbench | dlrm | 1.28 |
| torchbench | timm_vision_transformer_large | 1.28 |
| huggingface | MobileBertForMaskedLM | 1.27 |
| timm_models | vit_base_patch16_224 | 1.26 |
| timm_models | beit_base_patch16_224 | 1.23 |
| timm_models | jx_nest_base | 1.21 |
| torchbench | pyhpc_equation_of_state | 1.18 |
| huggingface | Speech2Text2ForCausalLM | 1.15 |
| timm_models | pit_b_224 | 1.14 |
| timm_models | twins_pcpvt_base | 1.14 |
| torchbench | maml_omniglot | 1.1 |
| timm_models | botnet26t_256 | 1.1 |

Multi-threaded
| Model Family | Model Name | Speedup |
|--------------|------------|---------|
| torchbench | BERT_pytorch | 1.35 |
| torchbench | lennard_jones | 2.43 |
| torchbench | hf_Albert | 1.35 |
| torchbench | hf_T5 | 1.34 |
| torchbench | soft_actor_critic | 1.34 |
| torchbench | fastNLP_Bert | 1.28 |
| huggingface | LayoutLMForSequenceClassification | 1.26 |
| torchbench | llama | 1.24 |
| huggingface | GPT2ForSequenceClassification | 1.19 |
| torchbench | hf_Bart | 1.17 |
| torchbench | hf_Bert_large | 1.16 |
| torchbench | hf_GPT2 | 1.16 |
| timm_models | gmixer_24_224 | 1.16 |
| torchbench | hf_GPT2_large | 1.15 |
| torchbench | maml_omniglot | 1.14 |
| torchbench | hf_Bert | 1.13 |
| torchbench | hf_DistilBert | 1.13 |
| torchbench | hf_T5_large | 1.12 |
| huggingface | MT5ForConditionalGeneration | 1.11 |

Dynamic shapes
Single-threaded
| Model Family | Model Name | Speedup |
|--------------|------------|-------|
| timm_models | mixer_b16_224 | 1.52 |
| timm_models | convit_base | 1.5 |
| huggingface | MobileBertForQuestionAnswering | 1.49 |
| torchbench | fastNLP_Bert | 1.42 |
| torchbench | timm_vision_transformer_large | 1.28 |
| timm_models | swin_base_patch4_window7_224 | 1.27 |
| torchbench | llama | 1.26 |
| huggingface | MobileBertForMaskedLM | 1.25 |
| timm_models | vit_base_patch16_224 | 1.25 |
| timm_models | beit_base_patch16_224 | 1.24 |
| timm_models | jx_nest_base | 1.2 |
| torchbench | dlrm | 1.19 |
| timm_models | pit_b_224 | 1.13 |
| timm_models | twins_pcpvt_base | 1.13 |
| torchbench | hf_Bert_large | 1.12 |
| torchbench | hf_BigBird | 1.11 |
| huggingface | Speech2Text2ForCausalLM | 1.11 |
| timm_models | eca_botnext26ts_256 | 1.11 |
| timm_models | botnet26t_256 | 1.1 |

Multi-threaded
| Model Family | Model Name | Speedup |
|--------------|------------|-------|
| torchbench | BERT_pytorch | 1.18 |
| torchbench | lennard_jones | 2.18 |
| torchbench | hf_Albert | 1.37 |
| torchbench | soft_actor_critic | 1.31 |
| huggingface | GPT2ForSequenceClassification | 1.29 |
| torchbench | hf_T5 | 1.28 |
| torchbench | fastNLP_Bert | 1.27 |
| torchbench | hf_Bart | 1.21 |
| torchbench | hf_Bert_large | 1.19 |
| torchbench | hf_T5_large | 1.19 |
| torchbench | hf_Bert | 1.16 |
| torchbench | hf_GPT2 | 1.16 |
| huggingface | CamemBert | 1.16 |
| torchbench | hf_GPT2_large | 1.13 |
| torchbench | functorch_maml_omniglot | 1.12 |
| huggingface | BertForMaskedLM | 1.12 |
| huggingface | MT5ForConditionalGeneration | 1.12 |
| torchbench | hf_DistilBert | 1.11 |
| timm_models | mixnet_l | 1.11 |
| timm_models | tf_mixnet_l | 1.11 |

No perf regressions.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/127195
Approved by: https://github.com/jansel",545.0,42.0,"aten/src/ATen/cpu/Utils.cpp,aten/src/ATen/cpu/Utils.h,test/inductor/test_cpu_repro.py,test/inductor/test_cpu_select_algorithm.py,torch/_C/_cpu.pyi,torch/_dynamo/trace_rules.py,torch/_inductor/codecache.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_micro_gemm.py,torch/_inductor/codegen/cpp_prefix.h,torch/cpu/__init__.py,torch/csrc/cpu/Module.cpp",12.0,14,3,2.193413562,4.0,12863.0,7.0,527676.1666666666,30294.0,75455.5,0.0,Feature Addition,1.0,1
pytorch,49eb82a7b25733f09c2ab35609d3b844e0d500f9,915050ed6605396ed0907cb6d4a5297d51e93692,Xu Zhao,xzhao9@fb.com,Tue Nov 17 07:16:02 2020 -0800,1605597362.0,"Fix typing errors in torch.distributed.distributed_c10d.* (#47532)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/47532

Test Plan: Imported from OSS

Reviewed By: walterddr

Differential Revision: D24952501

Pulled By: xuzhao9

fbshipit-source-id: 9b2dd1069eb1729c24be00f46da60d6a0439a8da",96.0,84.0,"mypy.ini,torch/_C/_distributed_c10d.pyi,torch/csrc/distributed/c10d/init.cpp,torch/distributed/distributed_c10d.py",4.0,6,1,0.40765867,7.0,4212.0,4.0,430194.5,6834.0,15532.0,0.0,Corrective,1.0,1
pytorch,5950240bdf4fe0734ea47180dfbd9b69c7091fa9,91611fe1d13ef303b5ddc3f96c230149c75eac4f,soulitzer,soulitzer@gmail.com,Wed Sep 29 23:59:25 2021 -0700,1632959965.0,"Decouple forward AD checks from backward AD in OpInfo tests and gradcheck (#65040)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/64999

- Adds a flag to gradcheck `check_backward_ad` that can be used to disable gradcheck for backward ad
  - This is a bit bc-breaking in terms of positional args, but I prefer this ordering
- In OpInfo tests for forward ad:
  - set `check_backward_ad` False
- In test_ops treat `supports_autograd` as if it is `supports_backward_ad` (it basically already is)
  - the only modification needed is to no longer skip forward ad tests if `supports_autograd` is false
  - test_dtype, test_variant_consistency, etc behave correctly as-is
  - In a follow-up PR, we can rename it to actually be `supports_backward_ad`
- Testing
  - https://github.com/pytorch/pytorch/pull/65060

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65040

Reviewed By: albanD

Differential Revision: D31238177

Pulled By: soulitzer

fbshipit-source-id: f068d4cbe7ffb094930b16cddb210583b9b7b2c4",156.0,40.0,"test/test_autograd.py,test/test_ops.py,torch/autograd/gradcheck.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,2,1.943058489,42.0,22221.0,4.0,373188.25,15817.0,36568.0,0.0,Corrective,1.0,1
pytorch,3f185ac18e095d6470fbba6fc4b914c062fdf455,9191b639ba1e6051abf46fb0b3adf3e70876b728,BowenBao,bowbao@microsoft.com,Thu Feb 04 20:35:27 2021 -0800,1612470927.0,"[ONNX] Enable remaining failed tests in opset13 (#50806) (#51518)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51518

* enable remaining test in opset13

* add comments for error version test info

* fix comments:opset12 unbind problem

* add ignore[no-redef]

* fix format

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D26203122

Pulled By: SplitInfinity

fbshipit-source-id: e7d95bd2ce13f79f11965be82f640379cd55ff0f

Co-authored-by: hwangdeyu <deyhuang@qq.com>",20.0,21.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset13.py",3.0,4,2,1.363179498,3.0,7652.0,2.0,224309.66666666663,8678.0,19522.5,0.0,Corrective,1.0,1
pytorch,01be4d6b205be248b803e97f218f6755c3001d68,91a8d3325ed4a0263c7a84d8c0c4b5e8e950836f,SsnL,tongzhou.wang.1994@gmail.com,Wed Oct 18 21:53:46 2017 -0400,1508363626.0,"test sparse dp, broadcast_coalesced, reduce_add_coalesced",109.0,24.0,"test/test_cuda.py,test/test_nn.py",2.0,1,1,0.727902505,38.0,5666.0,1.0,177673.0,49.0,885.5,0.0,Feature Addition,0.0,1
pytorch,162e1003c910c5a880686250266920e873bc7af2,91ab0d9680615d121f987ea86ec34335827ed89d,Lillian Johnson,lillianjohnson@fb.com,Fri Apr 09 21:23:42 2021 -0700,1618003422.0,"[hackathon] port addmv to OpInfo (#55545)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55545

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D27629053

Pulled By: Lilyjjo

fbshipit-source-id: d7a114e21d3b90c2563a26d7103703988114353d",42.0,6.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,5023.0,1.0,7781.0,10614.0,23480.0,0.0,Feature Addition,0.0,1
pytorch,03f1cab801ee887b4236277c821029bb8d0c04ab,91c4ba79805044a25d93a906fa48440307913b9c,Adam Paszke,adam.paszke@gmail.com,Sun Apr 02 20:58:46 2017 -0700,1491166726.0,Add torch.arange and deprecate torch.range,214.0,47.0,"docs/source/torch.rst,test/common.py,test/test_autograd.py,test/test_cuda.py,test/test_multiprocessing.py,test/test_nn.py,test/test_torch.py,tools/cwrap/plugins/BeforeAfterCall.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/Tensor.cwrap,torch/csrc/utils.h,torch/legacy/nn/MaskedSelect.py,torch/legacy/nn/PartialLinear.py,torch/nn/_functions/thnn/sparse.py",16.0,15,4,3.025679533,28.0,16814.0,2.0,244012.625,590.0,5443.322669,0.0,Feature Addition,0.0,1
pytorch,eb4238fc26653431fe9073b3c9fea01640112c5f,91e4f7788c06934782da75e3f1d2147a0a5233fe,soulitzer,soulitzer@gmail.com,Thu Feb 10 03:25:40 2022 -0800,1644463540.0,"Gradcheck forward AD respects requires grad but run with requires_grad=False (#72309)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72309

Fixes: https://github.com/pytorch/pytorch/issues/72113

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D33991570

Pulled By: soulitzer

fbshipit-source-id: 610de162e9848d2d3b12e0fb039860fd9dee844f
(cherry picked from commit a7ecb13610a4e01d91a2ecff107ac1cf6cd94cba)",109.0,19.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/MaxPooling.cpp,aten/src/ATen/native/mkldnn/Pooling.cpp,test/test_autograd.py,test/test_overrides.py,torch/autograd/gradcheck.py",7.0,8,3,1.627144175,43.0,18397.0,7.0,3105057.0,614.0,1454.0,0.0,Corrective,1.0,1
pytorch,7656ef73f1ae73798ae965da6dedd260b7cb4f01,91eb1b9bb93ddd997691295114a5e34bd61793ad,Peter Bell,peterbell10@live.co.uk,Mon Aug 22 13:23:55 2022 +0100,1661174635.0,"Move _masked opinfos to opinfo/definitions/_masked.py (#83763)

Ref #82518
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83763
Approved by: https://github.com/albanD",1211.0,781.0,"torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/__init__.py,torch/testing/_internal/opinfo/definitions/_masked.py,torch/testing/_internal/opinfo/utils.py",4.0,5,1,1.180015255,6.0,18153.0,1.0,0.0,6677.0,15521.0,0.0,,0.0,1
pytorch,343a9733499dce60b936a969ae5aaa016d9ebcae,9210e8f54030121bf319074a260d7f4f31967741,BowenBao,bowbao@microsoft.com,Wed Mar 09 14:20:30 2022 -0800,1646835630.0,"[ONNX] Adds overload_name to Aten op (#69378) (#73280)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73280

This PR adds a new attribute overload_name to the Aten node so that third party applications can implement calls to libtorch without using PyTorch source code.

This is necessary because torch's torch::jit::findOperatorFor(fullname) requires a full name, including operator and overload names.

ATen op was originally created for Caffe2, which leveraged the availability of the pytorch yaml files to create calls to the aten oeprators directly, not relying on torch::jit::findOperatorFor

The first part of the PR refactors all symbolics that create Aten ops, so that there is a single helper for this operator. Next all symbolics are updated to pass in the relevant overload name, if empty string is not applicable

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D34625645

Pulled By: malfet

fbshipit-source-id: 37d58cfb5231833768172c122efc42edf7d8609a
(cherry picked from commit e92f09117d3645b38bc3235b30aba4b4c7c71dfa)",53.0,39.0,"caffe2/contrib/aten/aten_op_template.h,test/expect/TestPytorchExportModes.test_aten_fallback.expect,test/expect/TestPytorchExportModes.test_onnx_aten.expect,test/expect/TestScript.test_listconstruct_erasure.expect,test/onnx/expect/TestOperators.test_at_op.expect,test/onnx/expect/TestOperators.test_embedding_bags.expect,test/onnx/expect/TestOperators.test_layer_norm_aten.expect,torch/csrc/jit/passes/onnx/shape_type_inference.h,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",12.0,13,3,2.645101628,9.0,6887.0,10.0,25074805.916666668,1270.0,3122.0,0.0,Feature Addition,0.0,1
pytorch,0c936f94d647a2c422d29cafaa923047dd243473,9232356e5fd7eb811c7980a88a93c7db578fce35,Ailing Zhang,ailzhang@fb.com,Fri May 08 15:14:06 2020 -0700,1588950846.0,"remove uses of type() and type_as() part 1. (#38029)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/38029

Differential Revision: D21468523

Pulled By: ailzhang

fbshipit-source-id: 14b7185d43eb03f630cfaa2d70e02d637ff8551b",62.0,59.0,"test/distributed/test_c10d.py,test/distributed/test_distributed.py,test/test_cuda.py,test/test_jit.py,test/test_multiprocessing.py,test/test_nn.py,torch/testing/_internal/common_utils.py",7.0,5,2,2.328590328,46.0,40660.0,7.0,928694.1428571428,1863.0,4810.0,0.0,,0.0,1
pytorch,ef0ef70cf51f056d9a38ce81abdf1f1b78ac7568,9235277dba6ca75926d5c7204d7f25d8f5ce4a3e,Will Feng,yf225@cornell.edu,Thu Mar 01 17:21:17 2018 -0500,1519924877.0,"Re-enable some CUDA tests on Windows (#5446)

This PR enables the following tests on Windows again:

CUDA HalfTensor tests in test_torch.py and test_nn.py
test_Conv2d_deterministic_cudnn in test_nn.py
test_*Tensor_qr_big in test_cuda.py

The issues are no longer reproducible, possibly because of an upgrade to the display driver.

* Reenable CUDA HalfTensor tests on Windows

* Reenable test_Conv2d_deterministic_cudnn on Windows

* Reenable test_*Tensor_qr_big on Windows",7.0,18.0,"test/test_cuda.py,test/test_nn.py,test/test_torch.py",3.0,1,1,1.426960814,38.0,13687.0,2.0,26810.666666666668,458.0,2304.5,0.0,,0.0,1
pytorch,bd7ac755d8b5dd00916609069362d6a263d51158,926e011cdefb44969b12f1a6ed8e15420e651c15,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Wed Mar 03 06:30:56 2021 -0800,1614753056.0,"Fixed out= variant of linalg.solve (#51968)

Summary:
This PR modifies the behavior of the `linalg_solve_out` variant to match the description here https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch
With this PR result and input tensors must be on the same device and have the same ""type kind"".
It's allowed to pass out tensors with complex dtypes for float inputs.

`linalg_solve_out` was broken for batched vector inputs and it's now fixed.

Ref. https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51968

Reviewed By: H-Huang

Differential Revision: D26728825

Pulled By: mruberry

fbshipit-source-id: c06fe937e7f452193b23ba09ca6cfa2703488455",125.0,38.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,test/test_linalg.py",2.0,5,2,0.944300727,2.0,9879.0,1.0,3536.0,9388.0,20790.0,0.0,Corrective,1.0,1
pytorch,62bd2ddec7f8a2008b31571970acc775251fdea4,92885ebe167a077d07c046c5d02e4b11bb2da024,Muthu Arivoli,ma381@duke.edu,Wed Aug 12 20:14:36 2020 -0700,1597263276.0,"Implement hypot (#42291)

Summary:
Related to https://github.com/pytorch/pytorch/issues/38349
Closes https://github.com/pytorch/pytorch/issues/22764

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42291

Reviewed By: malfet

Differential Revision: D22951859

Pulled By: mruberry

fbshipit-source-id: d0118f2b6437e5c3f775f699ec46e946a8da50f0",169.0,0.0,"aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_bfloat16.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu,aten/src/ATen/native/native_functions.yaml,c10/util/math_compat.h,docs/source/tensors.rst,docs/source/torch.rst,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",23.0,19,6,3.956983369,44.0,52922.0,13.0,3067400.086956522,4260.0,9977.5,0.0,,0.0,1
pytorch,0b17f4b87e2333e722ad4c1d0a50a21ce71ecf7e,92a0f7835e7e2f6342fe2fd32a25299ea86aed31,mseitzer,mseitzer@users.noreply.github.com,Mon Apr 02 13:16:44 2018 +0100,1522675004.0,Support returning dictionaries in DataParallel (#6113),28.0,1.0,"test/test_nn.py,torch/nn/parallel/scatter_gather.py",2.0,4,2,0.66319684,38.0,7166.0,1.0,244014.0,1026.0,6944.172317,0.0,,0.0,1
pytorch,00999fd8b1d26feadd621c4495cb6d96c2d76050,92bc444ee32f578ee9ebc2ecb76b283881f806a8,Jiong Gong,jiong.gong@intel.com,Wed May 29 09:07:27 2024 +0000,1716973647.0,"[inductor][cpp] epilogue support for gemm template (#126019)

As part of #125683, this PR adds the epilogue support for c++ gemm template by reusing the c++ vector codegen on sub-slices of tensors. This is implemented by retracing the epilogue IR nodes with new ranges and offsets. The new `codegen_loop_bodies` and `codegen_functions` methods are added to c++ vector codegen for this purpose. This is leveraged by the `store_output` method of the template kernel for epilogue codegen and store to the final result.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/126019
Approved by: https://github.com/jansel
ghstack dependencies: #124021",369.0,73.0,"test/inductor/test_cpu_select_algorithm.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_template.py,torch/_inductor/codegen/cpp_template_kernel.py,torch/_inductor/select_algorithm.py",7.0,5,2,2.0872763,4.0,8621.0,2.0,4072.285714285714,29227.0,71655.5,0.0,Feature Addition,0.0,1
pytorch,b13db682eca321abf33ed9ce4c5235bde1c20fd7,92f552cd565e01509d7a23ff1f349c3e93c7edeb,Kshiteej K,kshitijkalambarkar@gmail.com,Wed Feb 02 14:20:19 2022 +0500,1643811619.0,"[functorch] index_put : batch-rule (pytorch/functorch#426)

* index_put : batch rule

* update and add test

* refactor code

* update incorrect comment

* update comments and function name

* add more test samples",152.0,44.0,"functorch/functorch/csrc/BatchRulesScatterOps.cpp,functorch/test/functorch_additional_op_db.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,1.092054863,1.0,5689.0,3.0,0.5,752.0,1024.5,0.0,Corrective,0.0,1
pytorch,bb097e2a5003048da5e7fd3f331f4029fa87b12a,930f1812555698597bf3e3cd396fa108f127124c,Tongzhou Wang,SsnL@users.noreply.github.com,Tue Apr 10 17:11:05 2018 -0400,1523380265.0,"Fix fft when any of the input dimensions is not aligned (#6118)

* fix fft when any of the input dimensions is not like complex type; add test for ifft+fft

* clarify the comments

* Address comments: add note; add helper function

* use at::nullopt

* add notes on conjugate symmetry; fix complex-to-real cloning condition (should be advanced data layout rather than base_istride)

* add at::sum_intlist and at::prod_intlist

* revert optional<vector> helper due to windows compiler error",237.0,108.0,"aten/src/ATen/Utils.h,aten/src/ATen/native/SpectralOpsUtils.h,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/mkl/SpectralOps.cpp,test/test_torch.py",5.0,7,2,1.248957016,39.0,7318.0,2.0,759468.2,577.0,3462.0,0.0,Corrective,1.0,1
pytorch,3f4090652cd827ebb449789f6e49aff6b08dbe33,931a4913b1c74697711cb0a51bb227b89ba27f24,Bin Bao,binbao@fb.com,Thu Mar 16 13:54:10 2023 +0000,1678974850.0,"[inductor] Refactor memory management code in wrapper codegen (#96768)

Summary: use inheritance to simplify CppWrapperCodeGen and to prepare for AOT codegen

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96768
Approved by: https://github.com/jansel",146.0,235.0,"torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py",2.0,3,1,0.147004487,1.0,5428.0,2.0,33009.5,13438.0,31163.0,0.0,Perfective,0.0,1
pytorch,3d80bd31d87f92557f36c76c6ee4bbcee326725b,937950e06473518e7a4852abc459039fe2686f20,Sam Gross,sgross@fb.com,Tue Oct 03 15:49:54 2017 -0700,1507045794.0,"Move default arguments to function declaration

 * Make alpha, beta in addmm kwarg_only
 * Move kwarg_only arguments to the end
 * _out variants now have output arguments at the beginning",4438.0,4575.0,"aten/CMakeLists.txt,aten/src/ATen/ATen.h,aten/src/ATen/CheckGenerator.h,aten/src/ATen/Context.cpp,aten/src/ATen/Declarations.cwrap,aten/src/ATen/Generator.h,aten/src/ATen/Half.h,aten/src/ATen/Local.cwrap,aten/src/ATen/Scalar.cpp,aten/src/ATen/Scalar.h,aten/src/ATen/ScalarType.h,aten/src/ATen/Storage.h,aten/src/ATen/TensorAccessor.h,aten/src/ATen/TensorImpl.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/gen.py,aten/src/ATen/nn_parse.py,aten/src/ATen/preprocess_declarations.py,aten/src/ATen/templates/Functions.h,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/TensorMethods.h,aten/src/ATen/templates/Type.cpp,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeMethods.h,aten/src/ATen/test/atest.cpp,aten/src/ATen/test/basic.cpp,aten/src/meter/ClassErrorMeter.cc,aten/tools/Declarations.cwrap",28.0,7,1,1.405554199,1.0,7197.0,18.0,0.0,252.0,1417.5,0.0,Feature Addition,0.0,1
pytorch,314351d0ef9aedb4f39936141c62b604eef40533,9384d31af55decd1f13a97fae5f92a281e10d409,Ivan Yashchuk,IvanYashchuk@users.noreply.github.com,Tue Jan 12 14:50:01 2021 -0800,1610463001.0,"Added linalg.pinv (#48399)

Summary:
This PR adds `torch.linalg.pinv`.

Changes compared to the original `torch.pinverse`:
 * New kwarg ""hermitian"": with `hermitian=True` eigendecomposition is used instead of singular value decomposition.
 * `rcond` argument can now be a `Tensor` of appropriate shape to apply matrix-wise clipping of singular values.
 * Added `out=` variant (allocates temporary and makes a copy for now)

Ref. https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48399

Reviewed By: zhangguanheng66

Differential Revision: D25869572

Pulled By: mruberry

fbshipit-source-id: 0f330a91d24ba4e4375f648a448b27594e00dead",380.0,17.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,torch/csrc/api/include/torch/linalg.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",9.0,15,4,2.470391214,12.0,23682.0,6.0,323770.3333333333,8032.0,18219.5,0.0,Feature Addition,0.0,1
pytorch,608a932c1a997a3a83620418c30857877ec76638,93888a3779301feb67e5459e4e438f968ff20666,Richard Zou,zou3519@gmail.com,Tue Apr 20 21:28:02 2021 -0700,1618954082.0,[functorch] a lot of files,12610.0,0.0,"functorch/.gitignore,functorch/examples/.gitignore,functorch/examples/dp_cifar10/.gdbinit,functorch/examples/dp_cifar10/cifar10_expandweights.py,functorch/examples/dp_cifar10/cifar10_opacus.py,functorch/examples/dp_cifar10/cifar10_transforms.py,functorch/examples/dp_cifar10/cifar10_transforms.py.~1~,functorch/examples/dp_cifar10/cifar10_transforms.py.~2~,functorch/examples/dp_cifar10/cifar10_transforms.py.~3~,functorch/examples/dp_cifar10/cifar10_transforms.py.~4~,functorch/examples/dp_cifar10/make_functional.py,functorch/examples/dp_cifar10/make_functional.py.~1~,functorch/examples/ensembling/parallel_train.py,functorch/examples/maml_omniglot/.gdbinit,functorch/examples/maml_omniglot/.gitignore,functorch/examples/maml_omniglot/README.md,functorch/examples/maml_omniglot/maml-omniglot-higher.py,functorch/examples/maml_omniglot/maml-omniglot-ptonly.py,functorch/examples/maml_omniglot/maml-omniglot-transforms.py,functorch/examples/maml_omniglot/support/omniglot_loaders.py,functorch/examples/maml_regression/evjang.py,functorch/examples/maml_regression/evjang_transforms.py,functorch/examples/maml_regression/evjang_transforms_module.py,functorch/functorch/__init__.py,functorch/functorch/_src/__init__.py,functorch/functorch/_src/eager_transforms.py,functorch/functorch/_src/make_functional.py,functorch/functorch/_src/vmap.py,functorch/functorch/csrc/BatchedFallback.cpp,functorch/functorch/csrc/BatchedFallback.h,functorch/functorch/csrc/BatchedTensorImpl.cpp,functorch/functorch/csrc/BatchedTensorImpl.h,functorch/functorch/csrc/BatchingMetaprogramming.h,functorch/functorch/csrc/BatchingRegistrations.cpp,functorch/functorch/csrc/Constants.h,functorch/functorch/csrc/DynamicLayer.cpp,functorch/functorch/csrc/DynamicLayer.h,functorch/functorch/csrc/TensorWrapper.cpp,functorch/functorch/csrc/TensorWrapper.h,functorch/functorch/csrc/VmapMode.cpp,functorch/functorch/csrc/VmapMode.h,functorch/functorch/csrc/VmapTransforms.cpp,functorch/functorch/csrc/VmapTransforms.h,functorch/functorch/csrc/init.cpp,functorch/setup.py,functorch/test/test_eager_transforms.py,functorch/test/test_vmap.py",47.0,11,1,4.495135061,1.0,0.0,0.0,0.0,15.0,75.5,0.0,,0.0,1
pytorch,876bcc06b9fe269edbf71ae67cae60b42c2f8a4e,9390f7d3d61da11e9e5e3fd67ef2514379cfce64,gchanan,gregchanan@gmail.com,Mon Jan 22 23:14:22 2018 -0500,1516662862.0,"Implement a (data-only) Variable factory (#4753)

* Implement a (data-only) Variable factory.

Implements a function, torch.autograd.variable that is modeled after np.array.  The main difference between it and new() and
the tensor constructors is it inteprets a python number as data, i.e. as a 0-dimensional tensor (we currently don't expose
that at the pytorchl level, so it will temporarily end up as a 1-dimensional tensor), rather than a size.

The main difference currently between torch.autograd.variable and np.array is that np.autograd.variable is stricter, e.g.
passing a PyFloat when an integral type is the default tensor type will result in an array; np.array basically lets anything
through (floating-point / integral mismatch, overflow, etc).  This is to keep it consistent with Variable.new when called with
a sequence, although we can loosen the checks later.

This will be renamed to torch.tensor once we merge Variable and tensor.

* Address review comments.",66.0,1.0,"test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp,torch/autograd/__init__.py,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h",5.0,8,3,1.54696838,38.0,5857.0,5.0,1119773.4,458.0,1399.405869,0.0,Feature Addition,0.0,1
pytorch,2c71b679d2902c960fb5512efd4a982f339d2017,9394e65b44ddbb72a6812cadfb800eb9f49ade20,Richard Zou,zou3519@users.noreply.github.com,Mon Dec 18 07:05:58 2017 -0500,1513580758.0,"Add proper shape checking to torch.cat (#4087)

* Fix catArray in THTensor

Asserts that the inputs have the same size except in the
cat dimension or are empty (or a mix of both).

* Fix catArray for THCTensor

* Document torch.cat shape checks

* Fix types",190.0,139.0,"aten/src/TH/generic/THTensorMath.c,aten/src/THC/generic/THCTensorMath.cu,test/test_cuda.py,test/test_torch.py,torch/_torch_docs.py",5.0,8,3,1.589227324,38.0,15352.0,5.0,900767.0,2199.0,24225.35823,0.0,Corrective,1.0,1
pytorch,10faa303bc6fdcbc687f0e5bdbcfa1c6f6197a90,93ed476e7d7d8026260e917bfda656bd9170dfcb,soumith,soumith@fb.com,Fri Dec 23 01:36:47 2016 -0800,1482457007.0,"adding LAPACK double bindings, adding fmod and remainder",18.0,20.0,"test/test_cuda.py,torch/csrc/generic/methods/TensorMath.cwrap",2.0,5,2,0.949452015,17.0,2524.0,1.0,94.0,191.0,7074.289675,0.0,Feature Addition,0.0,1
pytorch,8788e92f0f3f23249161fdb91aafa4ecc7d4f131,93eedc51a5f8a6ba18bf4c87a26e1cc3b34cc177,Richard Zou,zou3519@gmail.com,Thu Aug 18 13:57:23 2022 -0700,1660831043.0,"[functorch] re-classify linalg.eigh in vmap testing (#83614)

Similar to the previous PR, linalg.eigh doesn't give unique output.

Test Plan:
- new tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83614
Approved by: https://github.com/samdow",23.0,1.0,functorch/test/test_vmap.py,1.0,2,1,0,2.0,4486.0,1.0,57301.0,6608.0,15261.5,0.0,,0.0,1
pytorch,f5aa8d55adb511b3b138237e0a1249a9fe02c713,940a0ab67bc933b0d6c24540a9e3100d213eec49,Tongzhou Wang,SsnL@users.noreply.github.com,Fri Mar 16 13:23:00 2018 -0400,1521206580.0,"Add logdet and slogdet (#5393)

* 1. Add logdet and slogdet in ATen side
2. Previously, det can return result with incorrect sign upon seeing symmetric
   matrices. This is caused by the wrong assumption I had on SVD (when input is
   symmetric U=V^T). This fixes it.
3. Moreover, after fixing 2 now QR is always needed for det forward. So I moved
   SVD to backward call. Since this is a specific variant of SVD, it is named as
   _svd_with_positive_UV_det, with derivative.yaml entry being svd_backward.
4. Updated/added backward functions for det, logdet and slogdet, which uses
   _svd_with_positive_UV_det and svd_backward inside.
5. Optimized svd_backward:
   a. Avoid unnecessary kernels when only sigma has gradient (this is the usual
      case, and also true with *det backward functions).
   b. Fix SVD double backward by avoiding a nan.

* 1. Add/update grad checks for det, logdet, and slogdet.
2. Fix an incorrect check for dim_args_idx in test_autograd.py
3. Add option to only test a subset of output values, specified by
   test_output_indices, for cases like slogdet where only the
   second output is differentiable.
4. Add better doc for the test generating list.

* Add/improve output tests for det, logdet and slogdet
Add a scaling to random matrices so closeness checks are more robust

* Remove unnecessaery Variable wrappers in some test files

* Add logdet slogdet docs

* Improve an err msg in THTensorLapack.c

* add inverse-based backward for invertible matrices
use svd only for non-invertible case, so don't need the special variant anymore

* use LU rather than QR",467.0,164.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorLapack.c,test/test_autograd.py,test/test_cuda.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_tensor_docs.py,torch/_torch_docs.py",10.0,11,4,2.638931569,39.0,22653.0,9.0,573180.0,2471.0,24802.85823,0.0,Corrective,1.0,1
pytorch,b0e33fb473cf736dbf4efd747f2edbe4ade6d5ce,942ca477a695bb976cc1f1a23e7af32f7ca3b591,Adam Lerer,alerer@fb.com,Thu Oct 13 20:35:51 2016 -0700,1476390951.0,Copying weights for CUDNN,361.0,118.0,"test/common_nn.py,test/test_nn.py,torch/autograd/__init__.py,torch/autograd/function.py,torch/backends/cudnn/rnn.py,torch/nn/functions/rnn.py,torch/nn/modules/__init__.py,torch/nn/modules/rnn.py",8.0,8,2,2.335783986,10.0,2243.0,4.0,255488.25,20.0,85.16666667,0.0,,0.0,1
pytorch,293a139d79217f7fa3f5ff51c90ee59d80b5a530,945ce71b18357454bf9ac353ae2556ea7719cb57,Brian Vaughan,bvaughan@fb.com,Wed Dec 11 23:34:02 2019 -0800,1576107242.0,"Correctly handle scalar types, fix parse of numpy ints (#30486)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30486

Fixes: https://github.com/pytorch/pytorch/issues/29252

There is some incorrect code in the handling of parsing python numbers that led to issue #29252:

When we allow interpretation of a zero-dim numpy integer value
as a scalar in pytorch, we incorrectly parse the int as a float.

This PR also fixes the issue described in the ""FIXME"" here:
https://github.com/pytorch/pytorch/pull/27628/files#diff-f539198dd366265fb8dc2d661bc5d5bcR1487

Test Plan: Added a unit test based on the example given in the issue.

Differential Revision: D18932520

Pulled By: nairbv

fbshipit-source-id: f6416f28dfd73ac72c1042042851d76beb5fcf65",66.0,27.0,"test/test_dataloader.py,test/test_torch.py,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_numbers.h,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_numpy.cpp,torch/csrc/utils/tensor_numpy.h",7.0,4,2,2.275500691,40.0,18701.0,6.0,2818309.1428571427,13800.0,37605.33333,0.0,Corrective,1.0,1
pytorch,1b147a52f573643b2775c9bd3efca123e1fcb41a,945d40dca6e6fc98ccbc209d7d19e99ac8d4badd,albanD,desmaison.alban@gmail.com,Thu Jul 29 13:40:50 2021 -0700,1627566050.0,"Also disable inplace fw AD for acos on windows (#62360)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/62304

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62360

Reviewed By: malfet, bdhirsh

Differential Revision: D29973310

Pulled By: albanD

fbshipit-source-id: 3b033e779f557724602c5a87f497698f2262a12e",2.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8266.0,1.0,116895.0,14245.0,32585.5,0.0,Corrective,1.0,1
pytorch,49c2da0ee06c2e3d9d300b76b655d31a6b7756ee,948df6c7a9935011e3142c60f96a821c23595be0,kshitij12345,kshitijkalambarkar@gmail.com,Wed May 26 05:00:55 2021 -0700,1622005255.0,"[numpy] torch.i0: promote integer inputs to float (#52735)

Summary:
Reference : https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52735

Reviewed By: zou3519

Differential Revision: D28630505

Pulled By: mruberry

fbshipit-source-id: e81a35dfc1a322daf0c44718901470fac677bc94",25.0,5.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,torch/testing/_internal/common_methods_invocations.py",3.0,8,2,0.69984284,9.0,7867.0,3.0,540776.0,12466.0,28228.0,0.0,,0.0,1
pytorch,2de0bb3df494ca213965e5384a69cab91e0d87ea,94938be367cf72cb5758abdd2deab2d5786bae3f,gchanan,gregchanan@gmail.com,Wed Feb 28 17:52:11 2018 -0500,1519840331.0,"Support dtypes in legacy new constructors. (#5343)

* Support dtypes in legacy new constructors.

* Add comment about why we don't have dtype for sparse (indices, values).

* separate legacy tensor ctor vs new (new includes dtypes).

* Use TypeError.",141.0,1.0,"test/test_cuda.py,test/test_sparse.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h",6.0,7,3,1.151415387,38.0,8875.0,5.0,421442.0,564.0,1722.905869,0.0,Feature Addition,0.0,1
pytorch,c470055319611f5f8047f5c562dff3a0834b580b,949559552004db317bc5ca53d67f2c62a54383f5,Sam Gross,colesbury@gmail.com,Tue Dec 19 23:57:14 2017 -0500,1513727834.0,Move reflection/replication padding to ATen (#4258),89.0,64.0,"aten/src/ATen/nn.yaml,aten/src/ATen/nn_parse.py,aten/src/THNN/generic/THNN.h,tools/autograd/derivatives.yaml,torch/nn/_functions/thnn/auto_double_backwards.py,torch/nn/functional.py",6.0,11,3,2.420131442,28.0,5246.0,4.0,1310674.0,380.0,1184.405869,0.0,Feature Addition,0.0,1
pytorch,2e2200d76c611eed8d0aed2ff93e0adc344407d2,949cbf1d654a616fd9a18e8e64429509ddb07552,David Berard,dberard@fb.com,Mon May 09 22:59:17 2022 -0700,1652137157.0,"[NVFuser] Opinfos for extremal values in binary ufuncs

Added slow tests for comparing the eager & fused outputs for given extremal inputs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75917

Approved by: https://github.com/jjsjann123, https://github.com/eellison",110.0,13.0,"test/test_jit_cuda_fuser.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.940777348,5.0,23364.0,2.0,21054.5,3008.0,7248.5,0.0,Feature Addition,0.0,1
pytorch,33a163d886d1a7b236bc34e69d5a7415a133bc23,94d621584a8d2780252546aa787aab23203221b2,mingfeima,mingfei.ma@intel.com,Tue Aug 24 15:54:36 2021 -0700,1629820476.0,"optimize BFloat16 elemwise operators CPU: sigmoid, sigmoid_backward, tanh_backward, addcmul, addcdiv (#55221)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55221

Test Plan: Imported from OSS

Reviewed By: bdhirsh

Differential Revision: D28836797

Pulled By: VitalyFedyunin

fbshipit-source-id: 6b79098c902ffe65d228668118ef36fb49bab800",143.0,46.0,"aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,torch/testing/_internal/common_methods_invocations.py",4.0,8,2,1.622952099,8.0,11114.0,4.0,969614.25,14877.0,34109.5,0.0,Feature Addition,0.0,1
pytorch,0be334a1baba4a96a1341e8951e25a03bcf71301,94ef2b9b48d74d0671f5720eefd355f674870a61,Jacob Szwejbka,jakeszwe@fb.com,Fri May 14 18:10:00 2021 -0700,1621015800.0,"[Pytorch] Better doc strings for bundled inputs (#56591)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56591

title
ghstack-source-id: 128926699

Test Plan: na

Reviewed By: dreiss

Differential Revision: D27912185

fbshipit-source-id: 1a8f267af21afb7b4393b9ec0792dd17c48e57cb",47.0,6.0,torch/utils/bundled_inputs.py,1.0,2,1,0,1.0,264.0,1.0,1998360.0,12112.0,27460.0,0.0,Non Functional,0.0,1
pytorch,d6050582122c09e52e51b518f5af9be04e12e021,94ff31f54def530b38874c3fb36eaa97f5e2aa60,Alican Bozkurt,alicanb@gmail.com,Mon Dec 18 21:44:35 2017 -0500,1513633475.0,"Implement Exponential distribution (#4234)

* add exponential distribution

* add exponential tests

* fix default val of sample_shape

* lambd->rate

* updates per review

* remove notes, keep failure_rate same in exponential test",97.0,7.0,"test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/distribution.py,torch/distributions/exponential.py",4.0,3,2,1.450239268,8.0,683.0,2.0,11577.333333333334,2215.0,24265.35823,0.0,Corrective,1.0,1
pytorch,d2e429864cb27318476537fea5e55e2beaaffc68,9500e8a0815e4488492f7867d59fe0600217121b,Peter Bell,peterbell10@live.co.uk,Tue Dec 01 04:44:28 2020 -0800,1606797868.0,"Testing: Improve interaction between dtypes and ops decorators (#48426)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48426

Tests are run on the intersection of the dtypes requested and the types that are
supported by the operator (or are _not_ if `unsupported_dtypes_only` is used).

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D25205835

Pulled By: mruberry

fbshipit-source-id: 2c6318a1a3dc9836af7361f32caf9df28d8a792b",69.0,32.0,"test/test_ops.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,1.470086328,2.0,4830.0,4.0,1348262.25,7057.0,15965.5,0.0,Perfective,0.0,1
pytorch,10da1fc3f869075d698fbcda6e0b3ece739973d2,950f7c023706db6db64a37b7b3c8b760679f7d3f,Heitor Schueroff,heitorschueroff@fb.com,Thu Aug 26 14:17:24 2021 -0700,1629987444.0,"Added API tests to ReductionOpInfo and ported amax/amin/nansum tests (#62899)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/62899

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D30408816

Pulled By: heitorschueroff

fbshipit-source-id: 6cb0aa7fa7edba93549ef873baa2fb8a003bd91d",397.0,86.0,"test/test_reductions.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.999996908,2.0,11988.0,1.0,4325.0,14957.0,34265.0,0.0,Feature Addition,0.0,1
pytorch,9d6521c3a07392a291ef18a920a2bb2acf26dfc9,952592511942bd43158f43f2f2227b27545f4e80,fehiepsi,fehiepsi@gmail.com,Mon Jul 23 17:03:06 2018 -0700,1532365386.0,"Low rank multivariate normal (#8635)

Summary:
This pull request implements low rank multivariate normal distribution where the covariance matrix has the from `W @ W.T + D`. Here D is a diagonal matrix, W has shape n x m where m << n. It used ""matrix determinant lemma"" and ""Woodbury matrix identity"" to save computational cost.

During the way, I also revise MultivariateNormal distribution a bit. Here are other changes:
+ `torch.trtrs` works with cuda tensor. So I tried to use it instead of `torch.inverse`.
+ Use `torch.matmul` instead of `torch.bmm` in `_batch_mv`. The former is faster and simpler.
+ Use `torch.diagonal` for `_batch_diag`
+ Reimplement `_batch_mahalanobis` based on `_batch_trtrs_lower`.
+ Use trtrs to compute term2 of KL.
+ `variance` relies on `scale_tril` instead of `covariance_matrix`

TODO:
- [x] Resolve the fail at `_gradcheck_log_prob`
- [x] Add test for KL

cc fritzo stepelu apaszke
Pull Request resolved: https://github.com/pytorch/pytorch/pull/8635

Differential Revision: D8951893

Pulled By: ezyang

fbshipit-source-id: 488ee3db6071150c33a1fb6624f3cfd9b52760c3",547.0,53.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/kl.py,torch/distributions/lowrank_multivariate_normal.py,torch/distributions/multivariate_normal.py",6.0,5,3,2.003860124,10.0,5042.0,3.0,2012750.4,3048.0,7361.833333,0.0,Feature Addition,0.0,1
pytorch,5acc27c00a131c3e8ce4dad6e06222bb40f30dc7,9552cc65d450bc43851df7f45f06190b3b419968,Himangshu,hlahkar@gmail.com,Wed Dec 23 23:40:00 2020 -0800,1608766800.0,"Creation of test framework for Sparse Operators (#48488)

Summary:
Fixes #{issue number}

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48488

Reviewed By: ngimel

Differential Revision: D25696487

Pulled By: mruberry

fbshipit-source-id: dc4f57c6628f62b74dd321f3f6b0fff86f25b040",38.0,1.0,"test/test_sparse.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.73206669,29.0,5691.0,2.0,93692.5,7753.0,17482.0,0.0,Corrective,1.0,1
pytorch,8464654ae43149a77bda2cb54f3f7425e4e308ea,957f37686a3974df12612c448dbdc700ac861f41,bhack,bhack@users.noreply.github.com,Tue Feb 20 20:26:35 2024 +0000,1708460795.0,"Refactor instance_descriptor for new triton version (#119636)

Check https://github.com/pytorch/pytorch/pull/119457#issuecomment-1936764161

Pull Request resolved: https://github.com/pytorch/pytorch/pull/119636
Approved by: https://github.com/shunting314",38.0,5.0,"torch/_inductor/codegen/triton_utils.py,torch/_inductor/utils.py",2.0,3,1,0.446481347,1.0,1403.0,2.0,424347.0,25355.0,57224.0,0.0,Perfective,0.0,1
pytorch,37688148aec4b2a4db3ecb80987054d535af93bc,958d517643cc9d9a129d7bd4847d0c20e3fcd38f,Gary Miguel,garymiguel@microsoft.com,Mon Nov 08 22:29:12 2021 -0800,1636410552.0,"[ONNX] Fix new_full and full_like for Python 3.9 (#67124) (#67806)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67806

Previously new_full would fail with errors like:
`TypeError: only integer tensors of a single element can be converted to an index`

And full_like would trigger warnings like:
`DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.`

Test Plan: Imported from OSS

Reviewed By: msaroufim

Differential Revision: D32181301

Pulled By: malfet

fbshipit-source-id: 2cf262cfef36c18e7b2423efe1e1d4fa3438f0ba

Co-authored-by: Bowen Bao <bowbao@microsoft.com>",3.0,3.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.918295834,3.0,13659.0,2.0,488872.5,16929.0,39785.5,0.0,Corrective,1.0,1
pytorch,a3e1bd1fb9ad1dbc8506068d206969ca08872404,95a1725a4a524d215817224237f3bb4a5ff9b84f,Abdelrauf,quickwritereader@gmail.com,Thu Dec 10 21:35:02 2020 -0800,1607636102.0,"Vsx initial support issue27678 (#41541)

Summary:
### Pytorch Vec256 ppc64le support
implemented types:

- double
- float
- int16
- int32
- int64
- qint32
- qint8
- quint8
- complex_float
- complex_double

Notes:
All basic vector operations are implemented:
There are a few problems:
- minimum maximum nan propagation for ppc64le is missing and was not checked
- complex multiplication, division, sqrt, abs are implemented as PyTorch x86. they can overflow and have precision problems than std ones.  That's why they were either excluded or tested in smaller domain range
- precisions of the implemented float math functions

~~Besides, I added CPU_CAPABILITY for power. but as because of  quantization errors for DEFAULT I had to undef and  use vsx for DEFAULT too~~

#### Details
##### Supported math functions

+ plus sign means vectorized, -  minus sign means missing,   (implementation notes are added inside braces)
(notes). Example: -(both ) means it was also missing on x86 side
g( func_name)  means vectorization is using func_name
sleef - redirected to the Sleef
unsupported

function_name | float | double | complex float | complex double
|-- | -- | -- | -- | --|
acos | sleef | sleef | f(asin) | f(asin)
asin | sleef | sleef | +(pytorch impl) | +(pytorch impl)
atan | sleef | sleef | f(log) | f(log)
atan2 | sleef | sleef | unsupported | unsupported
cos | +((ppc64le:avx_mathfun) ) | sleef | -(both) | -(both)
cosh | f(exp)   | -(both) | -(both) |
erf | sleef | sleef | unsupported | unsupported
erfc | sleef | sleef | unsupported | unsupported
erfinv | - (both) | - (both) | unsupported | unsupported
exp | + | sleef | - (x86:f()) | - (x86:f())
expm1 | f(exp)  | sleef | unsupported | unsupported
lgamma | sleef | sleef |   |
log | +  | sleef | -(both) | -(both)
log10 | f(log)  | sleef | f(log) | f(log)
log1p | f(log)  | sleef | unsupported | unsupported
log2 | f(log)  | sleef | f(log) | f(log)
pow | + f(exp)  | sleef | -(both) | -(both)
sin | +((ppc64le:avx_mathfun) ) | sleef | -(both) | -(both)
sinh | f(exp)  | sleef | -(both) | -(both)
tan | sleef | sleef | -(both) | -(both)
tanh | f(exp)  | sleef | -(both) | -(both)
hypot | sleef | sleef | -(both) | -(both)
nextafter | sleef  | sleef | -(both) | -(both)
fmod | sleef | sleef | -(both) | -(both)

[Vec256 Test cases Pr https://github.com/pytorch/pytorch/issues/42685](https://github.com/pytorch/pytorch/pull/42685)
Current list:

- [x] Blends
- [x] Memory: UnAlignedLoadStore
- [x] Arithmetics: Plus,Minu,Multiplication,Division
- [x] Bitwise: BitAnd, BitOr, BitXor
- [x] Comparison: Equal, NotEqual, Greater, Less, GreaterEqual, LessEqual
- [x] MinMax: Minimum, Maximum, ClampMin, ClampMax, Clamp
- [x] SignManipulation: Absolute, Negate
- [x] Interleave: Interleave, DeInterleave
- [x] Rounding: Round, Ceil, Floor, Trunc
- [x] Mask: ZeroMask
- [x] SqrtAndReciprocal: Sqrt, RSqrt, Reciprocal
- [x] Trigonometric: Sin, Cos, Tan
- [x] Hyperbolic: Tanh, Sinh, Cosh
- [x] InverseTrigonometric: Asin, ACos, ATan, ATan2
- [x] Logarithm: Log, Log2, Log10, Log1p
- [x] Exponents: Exp, Expm1
- [x] ErrorFunctions: Erf, Erfc, Erfinv
- [x] Pow: Pow
- [x] LGamma: LGamma
- [x] Quantization: quantize, dequantize, requantize_from_int
- [x] Quantization: widening_subtract, relu, relu6
Missing:
- [ ] Constructors, initializations
- [ ] Conversion , Cast
- [ ] Additional: imag, conj, angle (note: imag and conj only checked for float complex)

#### Notes on tests and testing framework
- some math functions are tested within domain range
- mostly testing framework randomly tests against std implementation within the domain or within the implementation domain for some math functions.
- some functions are tested against the local version. ~~For example, std::round and vector version of round differs. so it was tested against the local version~~
- round was tested against pytorch at::native::round_impl. ~~for double type on **Vsx  vec_round failed  for  (even)+0 .5 values**~~ . it was solved by using vec_rint
- ~~**complex types are not tested**~~  **After enabling complex testing due to precision and domain some of the complex functions failed for vsx and x86 avx as well. I will either test it against local implementation or check within the accepted domain**
- ~~quantizations are not tested~~  Added tests for quantizing, dequantize, requantize_from_int, relu, relu6, widening_subtract functions
- the testing framework should be improved further
- ~~For now `-DBUILD_MOBILE_TEST=ON `will be used for Vec256Test too~~
Vec256 Test cases will be built for each CPU_CAPABILITY

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41541

Reviewed By: zhangguanheng66

Differential Revision: D23922049

Pulled By: VitalyFedyunin

fbshipit-source-id: bca25110afccecbb362cea57c705f3ce02f26098",4893.0,1.0,"aten/src/ATen/Version.cpp,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vsx/vec256_common_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_complex_double_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_complex_float_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_double_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_float_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_int16_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_int32_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_int64_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_qint32_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_qint8_vsx.h,aten/src/ATen/cpu/vec256/vsx/vec256_quint8_vsx.h,aten/src/ATen/cpu/vec256/vsx/vsx_helpers.h,aten/src/ATen/native/DispatchStub.cpp,aten/src/ATen/native/DispatchStub.h,cmake/Codegen.cmake,cmake/Dependencies.cmake,cmake/Modules/FindVSX.cmake",19.0,9,2,3.586001502,17.0,2795.0,6.0,6504950.833333333,7396.0,16624.0,0.0,Feature Addition,0.0,1
pytorch,39e8311a29b5713c8858cab73a8f713a7f3d531c,95d17dc93da3d5df258de332083597a8e47ce5e3,Jason Ansel,jansel@meta.com,Mon Mar 06 19:25:50 2023 -0800,1678130750.0,"[inductor] Reland #95567 part 1 (#96023)

This is the non-problematic part of #95567.  The errors were coming from
IR printing changes which will be next in the stack.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96023
Approved by: https://github.com/ngimel, https://github.com/mlazos",48.0,39.0,"benchmarks/dynamo/common.py,test/inductor/test_torchinductor.py,torch/_dynamo/config.py,torch/_dynamo/exc.py,torch/_dynamo/output_graph.py,torch/_inductor/codecache.py,torch/_inductor/graph.py,torch/_inductor/utils.py,torch/fx/interpreter.py",9.0,8,3,2.396265615,2.0,13758.0,6.0,125423.33333333331,13072.0,30440.5,0.0,,0.0,1
pytorch,1c6ff53b60610b03a515918819fd3f1f3f5c7142,95f0fa8a929c59d56862465a52fee721c1288080,Adam Paszke,adam.paszke@gmail.com,Sun Jan 15 20:20:05 2017 +0100,1484511605.0,Change .grad attribute of Variables to be a Variable,173.0,106.0,"test/common_nn.py,test/test_autograd.py,test/test_multiprocessing.py,test/test_nn.py,test/test_optim.py,test/test_utils.py,torch/autograd/variable.py,torch/csrc/autograd/function.cpp,torch/nn/modules/module.py,torch/optim/adadelta.py,torch/optim/adagrad.py,torch/optim/adam.py,torch/optim/adamax.py,torch/optim/asgd.py,torch/optim/optimizer.py,torch/optim/rmsprop.py,torch/optim/rprop.py,torch/optim/sgd.py",18.0,8,2,2.598865385,21.0,6806.0,4.0,92420.61111111112,335.0,4825.976424,0.0,,0.0,1
pytorch,b675f07bb662563bc463e9c3ab280b0f4679f3c7,960513006f536871779a0ce67c9c1e650b1a4ac8,BowenBao,semisqg@gmail.com,Wed Apr 24 19:39:52 2019 -0700,1556134792.0,"Support exporting squeeze & unsqueeze with negative dim attribute

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/19297

Reviewed By: zrphercule

Differential Revision: D14953525

Pulled By: houseroad

fbshipit-source-id: 8d7eecd2804b8e27d3ee4ad6e763352818d02d0c",44.0,1.0,"test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic.py",2.0,4,2,0.970950594,12.0,3345.0,2.0,327654.5,8296.0,24826.33333,0.0,,0.0,1
pytorch,c73f0e457e7126ad25c2769baa96d6461d38ce4a,963ae25e41b60720410b4f10be595d8350cc6948,Peter Bell,peterbell10@live.co.uk,Fri Sep 24 02:42:07 2021 -0700,1632451327.0,"Migrate THCAtomics to ATen (#65470)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65470

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D31148184

Pulled By: ngimel

fbshipit-source-id: aaac3dfb5f2c6f88e9bd922b3a56d0a16a861e17",379.0,378.0,"aten/src/ATen/cuda/Atomic.cuh,aten/src/ATen/cuda/CUDAApplyUtils.cuh,aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu,aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu,aten/src/ATen/native/cuda/AveragePool3d.cu,aten/src/ATen/native/cuda/DilatedMaxPool3d.cu,aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu,aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cuh,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/FractionalMaxPool2d.cu,aten/src/ATen/native/cuda/FractionalMaxPool3d.cu,aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu,aten/src/ATen/native/cuda/Indexing.cu,aten/src/ATen/native/cuda/KernelUtils.cuh,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/NLLLoss2d.cu,aten/src/ATen/native/cuda/RNN.cu,aten/src/ATen/native/cuda/ReflectionPad.cu,aten/src/ATen/native/cuda/ReplicationPadding.cu,aten/src/ATen/native/cuda/ScatterGatherKernel.cu,aten/src/ATen/native/cuda/SortingRadixSelect.cuh,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/UpSample.cuh,aten/src/ATen/native/cuda/UpSampleLinear1d.cu,aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu,aten/src/ATen/native/cuda/im2col.cuh,aten/src/ATen/test/cuda_atomic_ops_test.cu,aten/src/THC/THCAtomics.cuh",30.0,8,1,1.680040992,7.0,12924.0,22.0,6214675.034482759,15699.0,36186.5,0.0,,0.0,1
pytorch,5fda3b094c9247e7f0d9e0a76a78fa7b8a476774,9699c703c2b4bad90ec2fadad25b13150d78d1ba,Nikita Vedeneev,nik@quansight.com,Fri Feb 19 17:26:26 2021 -0800,1613755586.0,"Stable sort for the CPU take 2. (#51790)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/38681.
A duplicate of https://github.com/pytorch/pytorch/pull/50052 created to become importable to the fb internal tests.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51790

Reviewed By: agolynski

Differential Revision: D26279045

Pulled By: glaringlee

fbshipit-source-id: 348e171dee9c370a76002b65d0c82c329f57a421",249.0,29.0,"aten/src/ATen/LegacyTHFunctionsCUDA.h,aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp,aten/src/ATen/native/CompositeRandomAccessorCommon.h,aten/src/ATen/native/NamedTensor.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/Sorting.h,aten/src/ATen/native/cpu/SortingKernel.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/TensorCompare.cpp,test/test_sort_and_select.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",15.0,13,4,3.115247421,45.0,39047.0,9.0,1468498.8,9044.0,20260.5,0.0,Corrective,1.0,1
pytorch,6de619e4a4fd7ce55c3a2f86e666fa9f29026724,96bc7faa50a5ab91dd8514a180038fc22bdaa8cf,Negin Raoof,neginmr@utexas.edu,Wed Oct 21 18:20:52 2020 -0700,1603304452.0,"[ONNX] Export var, var_mean and std_mean ops (#45678)

Summary:
Adding export for var, var_mean and std_mean ops

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45678

Reviewed By: houseroad

Differential Revision: D24398811

Pulled By: bzinodev

fbshipit-source-id: bf51422a9e035d521156c0fa6e77898aac83a380",316.0,103.0,"test/onnx/expect/TestOperators.test_std.expect,test/onnx/test_onnx_opset.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",4.0,5,2,1.802589103,3.0,8050.0,4.0,2934318.5,6123.0,14130.5,0.0,Feature Addition,0.0,1
pytorch,e5e451a9dbcd3c94849fa898b27a8dc7649ec8b7,96ee23e198101fe4913e26503ac6d5d7b260ef76,Edward Z. Yang,ezyang@meta.com,Tue May 16 15:33:17 2023 -0700,1684251197.0,"Print restarting analysis at INFO level with a exception breadcrumb (#101573)

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/101573
Approved by: https://github.com/albanD",39.0,11.0,"test/test_utils.py,torch/_dynamo/convert_frame.py,torch/fx/experimental/symbolic_shapes.py,torch/utils/_traceback.py",4.0,6,2,1.821901189,40.0,4566.0,3.0,367561.75,16034.0,36156.5,0.0,,0.0,1
pytorch,f101949390801d586de77e833380c787a1a28450,96eec95ecedf512db8fe1ec277b69c2d2783d77f,anjali411,chourdiaanjali123@gmail.com,Fri Mar 27 21:38:26 2020 -0700,1585345106.0,"torch.from_numpy for complex dtypes (#35531)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/35531

Differential Revision: D20693581

Pulled By: anjali411

fbshipit-source-id: d53e26b4175452fa00b287efbfceea18104c1364",28.0,14.0,"caffe2/python/pybind_state.cc,test/test_torch.py,torch/_torch_docs.py,torch/csrc/utils/tensor_numpy.cpp",4.0,6,3,1.61681147,48.0,26589.0,4.0,442282.75,555.0,1626.5,0.0,,0.0,1
pytorch,73b06a0268bb89c09a86f16fa0f72818baa4b250,96f548a1ac6e54791d81ed17d5bf6660ceba701d,Bin Bao,binbao@fb.com,Mon Apr 03 17:21:01 2023 +0000,1680542461.0,"[inductor] Add an AOT mode for the Triton backend (#98214)

Summary:
This is a copy of https://github.com/pytorch/pytorch/pull/97152 to make
the landing easier.

This PR implements a two-pass wrapper codegen for the Triton
backend to achieve ahead-of-time compilation. In the first pass, the
regular python wrapper code will be generated, and then the generated
code will be executed to perform Triton compilation and autotuning.
After that, the second pass wrapper codegen will generate C++ wrapper
with proper CUDA API to load and launch Triton-generated CUDA kernels.

Like the AOT mode for the cpp backend, the next step would be to provide
a more complete API for AOT.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98214
Approved by: https://github.com/eellison",542.0,147.0,"test/inductor/aot/cpp/CMakeLists.txt,test/inductor/aot/cpp/test.cpp,test/inductor/aot/cpp/test.py,test/inductor/aot/cpp/test.sh,test/inductor/aot/cuda/CMakeLists.txt,test/inductor/aot/cuda/test.cpp,test/inductor/aot/cuda/test.py,test/inductor/aot/cuda/test.sh,torch/_inductor/__init__.py,torch/_inductor/codecache.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/compile_fx.py,torch/_inductor/config.py,torch/_inductor/graph.py,torch/_inductor/triton_heuristics.py",17.0,8,2,3.207633466,2.0,9039.0,9.0,869441.9230769231,14074.0,32270.0,0.0,Feature Addition,0.0,1
pytorch,f2cf673d3a52098e307493028ae77f7ebe715364,96f61bff301e00bd4169072ffc18608337218dcc,Adam Paszke,adam.paszke@gmail.com,Sun Oct 09 00:46:59 2016 -0700,1475974019.0,Add LAPACK functions,515.0,117.0,"setup.py,test/test_torch.py,tools/cwrap/cwrap.py,tools/cwrap/plugins/BeforeAfterCall.py,tools/cwrap/plugins/BeforeCall.py,tools/cwrap/plugins/BoolOption.py,tools/cwrap/plugins/THPPlugin.py,tools/cwrap/plugins/__init__.py,torch/csrc/Module.cpp,torch/csrc/generic/TensorMethods.cwrap",10.0,7,3,1.941227683,10.0,7511.0,2.0,506577.375,238.0,2285.14599,0.0,Feature Addition,0.0,1
pytorch,af2680e9ce2eda49e4f5b2776cb6f4740d19806e,97052c5fa8bc97afa7288bd438225b7ae5558241,Mengchi Zhang,mengchi@fb.com,Wed Jul 08 21:05:57 2020 -0700,1594242357.0,"Extend SparseAdagrad fusion with stochastic rounding FP16 (#41107)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/41107

Extend row wise sparse Adagrad fusion op to FP16 (stochastic rounding) for PyTorch.

Differential Revision: D22195408

fbshipit-source-id: e9903ca7ca3b542fb56f36580e69bb2a39b554f6",166.0,51.0,"caffe2/sgd/adagrad_fused_op_gpu.cu,caffe2/sgd/adagrad_fused_op_gpu.cuh,caffe2/sgd/rowwise_adagrad_fused.cc",3.0,2,1,1.042087305,1.0,1362.0,3.0,3203425.0,3468.0,8224.5,0.0,,0.0,1
pytorch,a98f43e9f054049126b9ccd3e28060775b4de365,9707431d842a66f78b80cb1cea83697a3e0a914c,Richard Zou,zou3519@gmail.com,Wed Aug 25 22:00:43 2021 -0700,1629928843.0,[functorch] Test for batch rule coverage,120.0,4.0,"functorch/test/common_utils.py,functorch/test/test_vmap.py",2.0,2,1,0.458685816,1.0,3314.0,2.0,1.0,310.0,475.0,0.0,,0.0,1
pytorch,0bc57f47f0d9dd93180e8f3d2fb57ece0eb149b5,973e306c84228d0900c672065cfbc889b5686fa2,Hui Guo,huiguo@fb.com,Mon Feb 22 23:04:56 2021 -0800,1614035096.0,"changed TE 'Allocate' API to take one argument 'Buf' instead of three arguments 'Var', 'dtype', 'dims'. (#50167)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50167

Test Plan:
Imported from OSS

`python test/test_jit_fuser_te.py`
`python test/test_jit_fuser_legacy.py`
`python test/test_jit_fuser.py`
`build/bin/test_tensorexpr`

Reviewed By: ZolotukhinM

Differential Revision: D25814342

Pulled By: huiguoo

fbshipit-source-id: 44cba7f92365b826c9cb1d385a94858934570dee",157.0,152.0,"test/cpp/tensorexpr/test_cpp_codegen.cpp,test/cpp/tensorexpr/test_cuda.cpp,test/cpp/tensorexpr/test_loopnest.cpp,test/cpp/tensorexpr/test_reductions.cpp,test/cpp/tensorexpr/test_registerizer.cpp,test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_printer.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/loopnest.cpp,torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp,torch/csrc/jit/tensorexpr/stmt.h",14.0,7,2,3.377714925,2.0,25220.0,12.0,2470242.0,9113.0,20377.5,0.0,,0.0,1
pytorch,e2e1189f416b235a17e1e053c13f8644606a43a9,977d3bcc46b81306093ee77b8a7fac2526c7f307,Oguz Ulgen,oulgen@meta.com,Sun Oct 22 05:39:24 2023 -0700,1697953164.0,"[Inductor] Support user defined triton kernels in inductor (#111434)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/111434
Approved by: https://github.com/jansel",203.0,18.0,"test/dynamo/test_functions.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/graph.py,torch/_inductor/ir.py,torch/_inductor/lowering.py,torch/_inductor/triton_heuristics.py,torch/_inductor/utils.py",7.0,5,2,2.004274742,2.0,19515.0,4.0,298221.14285714284,20988.0,47946.5,0.0,,0.0,1
pytorch,10c4b98ade8349d841518d22f19a653a939e260c,97a604ef57c15bfcba318d9db4af3b7b17b76a6d,Brian Vaughan,nairbv@users.noreply.github.com,Thu Jul 04 02:29:08 2019 -0700,1562207348.0,"Rereapply optional ScalarType interface changes that were reverted in D16079809 (#22456)

Summary:
re-apply changes reverted in:
https://github.com/pytorch/pytorch/pull/22412

Also change log_softmax to take positional arguments. Long-term we do want the kwarg-only interface, but seems to currently be incompatible with jit serialization.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22456

Differential Revision: D16097159

Pulled By: nairbv

fbshipit-source-id: 8cb73e9ca18fc66b35b873cf4a574b167a578b3d",561.0,555.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/native_functions.yaml,test/common_methods_invocations.py,test/cpp_extensions/msnpu_extension.cpp,test/onnx/expect/TestOperators.test_norm.expect,test/onnx/expect/TestOperators.test_norm_p1.expect,test/onnx/expect/TestOperators.test_norm_p2.expect,test/onnx/test_operators.py,test/test_jit.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/load_derivatives.py,tools/autograd/templates/Functions.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/symbolic_script.cpp,torch/csrc/jit/symbolic_variable.h,torch/csrc/jit/tracer.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",22.0,17,4,3.762499424,16.0,39691.0,7.0,139130.27272727274,9776.0,28417.33333,0.0,,0.0,1
pytorch,b4144940357e016c8fe0daa6c281f80428676b8f,97a82a3018cd51b11f907b97aacda03bd40eda67,ngimel,ngimelshein@nvidia.com,Wed Mar 22 22:06:31 2017 -0700,1490220391.0,fix formatting in upsampling docs (#1067),4.0,4.0,torch/nn/modules/upsampling.py,1.0,3,1,0,23.0,115.0,1.0,8222.0,553.0,4173.055248,0.0,Corrective,1.0,1
pytorch,95b1232752082dd9e41d2f3c2c5815e77dd869f0,97ae431e3e4f029b30477485acc53e48e7146402,BowenBao,bowbao@microsoft.com,Wed Mar 09 14:20:30 2022 -0800,1646835630.0,"[ONNX] Add symbolic support for torch.nn.cosinesimilarity (#72128) (#73283)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73283

* Add support for torch.nn.cosine_similarity

* Remove fallback logic

* Fix onnx test failures

* Fix opset version

* Modify rtol

* Add aten fallback mode

* fix mypy

* gate with caffe2 fallback

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D34625650

Pulled By: malfet

fbshipit-source-id: bf15d32b1d7055d0ca166d9941ba90b5c8e81cc2
(cherry picked from commit 7086031c52e1bea9bead6966d44e2635060194db)",20.0,5.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_utility_funs.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.426960814,4.0,15912.0,3.0,0.0,1273.0,3126.5,0.0,Corrective,1.0,1
pytorch,79c27ba4ef609c56907c504e15737fbaf8cc8f87,97c1e90f46230ee16605765d57202b60622511e5,Lara,lahaidar@microsoft.com,Thu Dec 12 04:05:21 2019 -0800,1576123521.0,"ONNX Interpolate Add Scales Params (#28324)

Summary:
Fix for : https://github.com/pytorch/pytorch/issues/27176
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28324

Reviewed By: hl475

Differential Revision: D18309133

Pulled By: houseroad

fbshipit-source-id: 348bb41393442c6b107d88fc2cd3224e0afa3ccf",1661.0,896.0,".jenkins/caffe2/test.sh,aten/src/ATen/native/UpSample.h,aten/src/ATen/native/UpSampleBicubic2d.cpp,aten/src/ATen/native/UpSampleBilinear2d.cpp,aten/src/ATen/native/UpSampleLinear1d.cpp,aten/src/ATen/native/UpSampleNearest1d.cpp,aten/src/ATen/native/UpSampleNearest2d.cpp,aten/src/ATen/native/UpSampleNearest3d.cpp,aten/src/ATen/native/UpSampleTrilinear3d.cpp,aten/src/ATen/native/cuda/UpSample.cuh,aten/src/ATen/native/cuda/UpSampleBicubic2d.cu,aten/src/ATen/native/cuda/UpSampleBilinear2d.cu,aten/src/ATen/native/cuda/UpSampleLinear1d.cu,aten/src/ATen/native/cuda/UpSampleNearest1d.cu,aten/src/ATen/native/cuda/UpSampleNearest2d.cu,aten/src/ATen/native/cuda/UpSampleNearest3d.cu,aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp,aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp,test/onnx/expect/TestOperators.test_upsample_nearest.expect,test/onnx/expect/TestOperators.test_upsample_nearest_scale.expect,test/onnx/expect/TestOperators.test_upsample_nearest_scale_default_scale_factor.expect,test/onnx/expect/TestOperators.test_upsample_nearest_size.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_jit.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/symbolic_script.cpp,torch/nn/functional.py,torch/onnx/symbolic_caffe2.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",41.0,21,5,4.644732423,47.0,60429.0,24.0,3872171.9736842094,13808.0,37633.33333,0.0,Corrective,1.0,1
pytorch,51884c647965d704bd25150784729f8449459264,97dfc7e3006a767bfc8b98c011a8e9b40742ddea,Jane Xu,janeyx@fb.com,Tue Jun 08 22:59:32 2021 -0700,1623193172.0,"[Reland] Adding run specified tests option to run_test.py (#59649)

Summary:
Reland of https://github.com/pytorch/pytorch/issues/59487

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59649

Reviewed By: samestep

Differential Revision: D28970751

Pulled By: janeyx99

fbshipit-source-id: 6e28d4dcfdab8a49da4b6a02c57516b08bacd7b5",97.0,17.0,".gitignore,.jenkins/pytorch/win-test-helpers/build_pytorch.bat,.jenkins/pytorch/win-test-helpers/test_python_first_shard.bat,.jenkins/pytorch/win-test-helpers/test_python_jit_legacy.bat,.jenkins/pytorch/win-test-helpers/test_python_second_shard.bat,test/run_test.py,tools/export_slow_tests.py,torch/testing/_internal/common_utils.py",8.0,8,4,1.105774485,44.0,4044.0,3.0,23255.25,12844.0,29150.0,0.0,Feature Addition,0.0,1
pytorch,d0662f2f76c7e68b44530247141af252e01a9b7c,97f29bda59deab8c063cf01f0a8ff4321b93c55e,Mike Ruberry,mruberry@devfair044.h1.fair,Sun Oct 31 11:24:31 2021 -0700,1635679471.0,"Relaxes tolerance on ROCm test_noncontiguous_samples_matmul (#67593)

Summary:
This test is narrowly failing intermittently. See https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-bionic-rocm4.3.1-py3.6-test1/7736//console for an example. Relevant snippet:

```
12:28:43 ======================================================================
12:28:43 FAIL [0.104s]: test_noncontiguous_samples_matmul_cuda_float32 (__main__.TestCommonCUDA)
12:28:43 ----------------------------------------------------------------------
12:28:43 Traceback (most recent call last):
12:28:43   File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 1422, in wrapper
12:28:43     method(*args, **kwargs)
12:28:43   File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 1422, in wrapper
12:28:43     method(*args, **kwargs)
12:28:43   File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 371, in instantiated_test
12:28:43     result = test(self, **param_kwargs)
12:28:43   File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 737, in test_wrapper
12:28:43     return test(*args, **kwargs)
12:28:43   File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_device_type.py"", line 920, in only_fn
12:28:43     return fn(self, *args, **kwargs)
12:28:43   File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 1041, in wrapper
12:28:43     fn(*args, **kwargs)
12:28:43   File ""test_ops.py"", line 262, in test_noncontiguous_samples
12:28:43     self.assertEqual(actual_grad, expected_grad)
12:28:43   File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_utils.py"", line 1903, in assertEqual
12:28:43     super().assertTrue(result, msg=self._get_assert_msg(msg, debug_msg=debug_msg))
12:28:43 AssertionError: False is not true : Tensors failed to compare as equal!With rtol=1.3e-06 and atol=1e-05, found 1 element(s) (out of 10) whose difference(s) exceeded the margin of error (including 0 nan comparisons). The greatest difference was 1.2278556823730469e-05 (-1.458460807800293 vs. -1.4584730863571167), which occurred at index 7.
```

Setting an absolute tolerance of 1e-4, which is what this PR does, should make the test pass consistently.

cc jeffdaily sunway513 jithunnair-amd ROCmSupport KyleCZH

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67593

Reviewed By: ngimel

Differential Revision: D32050986

Pulled By: mruberry

fbshipit-source-id: f15bc8c4516be0a859afcfa76d52334c0b2c58a5",4.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,12313.0,1.0,25593.0,16737.0,39209.5,0.0,Corrective,1.0,1
pytorch,86a96cd759af3a6510557ee8f04fbf05be87d4a4,97f50edf46a58b68e07160154ff3802ef445b007,Thomas Viehmann,tv.github@beamnet.de,Fri Jun 16 00:10:56 2017 +0200,1497571856.0,Add documentation for Cholesky lapack functions (#1816),196.0,13.0,torch/_torch_docs.py,1.0,1,1,0,19.0,4545.0,1.0,27431.0,660.0,5382.672317,0.0,Feature Addition,0.0,1
pytorch,197f9f08269d68416e38e2e25bbe8bad8963680a,980d6f258912e8d4474c5ccfe1a1e4f88f226af5,Antonio Cuni,anto.cuni@gmail.com,Mon Apr 05 15:44:16 2021 -0700,1617637456.0,"torch.linalg.det (#53119)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/51652.
In particular:
- the main implementation is in `torch.linalg.det` now. `torch.det` is just a deprecated alias to it
- add a new `OpInfo` for `torch.linalg.det`
- remove the old-style tests for `torch.det` (this is similar to what we did for `torch.linalg.slogdet`, see https://github.com/pytorch/pytorch/issues/49194)
- added a `out=` argument to `torch.linalg.det`, but **not** to `torch.det`.

It is worth noting that I had to skip few tests:
- `TestGradientsCuda::test_fn_gradgrad_linalg_det_cuda_float64`. This is not a regression: the functionality is broken also on master, but the test is not executed properly due to https://github.com/pytorch/pytorch/issues/53361.

And the following tests which fails only on ROCm:
- `test_variant_consistency_jit_cuda_{float64,float32}`
- `test_fn_grad_cuda_float64`

I think that the ROCm tests fail because the current linalg.det backward is unstable if the matrix has repeated singular values, see https://github.com/pytorch/pytorch/issues/53364 .

(At the moment of writing some CI jobs are still running but I believe the build will be green, since the only difference wrt the last push is the skip of the ROCm tests)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53119

Reviewed By: H-Huang

Differential Revision: D27441999

Pulled By: mruberry

fbshipit-source-id: 5eab14c4f0a165e0cf9ec626c3f4bb23359f2a9e",98.0,46.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/csrc/jit/passes/normalize_ops.cpp,torch/linalg/__init__.py,torch/onnx/symbolic_opset11.py,torch/testing/_internal/common_methods_invocations.py",11.0,17,4,2.167719395,18.0,32973.0,10.0,1314462.0,10419.0,23115.5,0.0,Corrective,1.0,1
pytorch,5e04bb2c1cc32dcce96c71b3867fc787eb227728,98307a282162750aaf66f52f4b6a15fc020c6679,XiaobingSuper,xiaobing.zhang@intel.com,Sat Aug 22 02:57:54 2020 -0700,1598065074.0,"Fix bfloat16 erfinv get incorrect value problem for cpu path (#43399)

Summary:
Fix https://github.com/pytorch/pytorch/issues/43344

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43399

Reviewed By: albanD

Differential Revision: D23264789

Pulled By: pbelevich

fbshipit-source-id: 8b77c0f6ca44346e44599844fb1e172fdbd9df6c",12.0,5.0,"aten/src/ATen/cpu/vec256/vec256_bfloat16.h,test/test_torch.py",2.0,6,2,0.322756959,42.0,21547.0,2.0,417382.0,4481.0,10451.5,0.0,Corrective,1.0,1
pytorch,f6c0cd8cad1053a65518ada507616a2b21e20c5f,9832ecd9cd0ff4bcdf28cbd56414557962f9bd9b,vfdev,vfdev.5@gmail.com,Tue Mar 01 15:49:10 2022 +0100,1646149750.0,"[functorch] Added temporary odict pytree registration (pytorch/functorch#544)

* Added temporary odict pytree registration

Description:

- Added temporary odict pytree registration
  - combine_state_for_ensemble can work on torchvision segmentation models

Related to https://github.com/pytorch/functorch/issues/445#issuecomment-1050946177

* Added pytree odict test",44.0,2.0,"functorch/functorch/_src/vmap.py,functorch/test/test_vmap.py",2.0,4,1,0.95033767,1.0,3988.0,2.0,1.0,835.0,1170.0,0.0,Feature Addition,0.0,1
pytorch,6d13a334f6d2df0c4ab176e06f4f1fa909b0c0e6,9854df673cca5aa99ab5ad9ee9eb41655cf934d4,Nick Gibson,nickg@fb.com,Tue Apr 21 20:28:13 2020 -0700,1587500893.0,"[TensorExpr] Fix bug in For elimination in the IRSimplifier. (#36965)

Summary:
When doing elimination of For loops which execute once, e.g. `for i = 0; i < 1; ++i { thing; } => thing;` we do var substitution while the temporary simplifier ExprNodes still exist, which could put them in an invalid state and leave unsimplified terms in the expression. The fix is to apply substitution before simplifying the body of the for loop.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36965

Differential Revision: D21145248

Pulled By: nickgg

fbshipit-source-id: d874600c7a098fc05b8ef3109e516e2eaa2c24e0",43.0,5.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",3.0,7,2,1.006463025,2.0,3623.0,2.0,343376.3333333333,1259.0,3317.5,0.0,Corrective,1.0,1
pytorch,6ae99aa5bc616f63649be3831cb10bf0d09c477e,987f1ccf496a1f02b54378a6aeb50d9df0b3ac31,Brennan Vincent,btv@fb.com,Mon May 20 22:58:51 2019 -0700,1558393131.0,"Add ""ndim"" property to tensor (#20565)

Summary:
For compatibility with numpy.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20565

Differential Revision: D15374390

Pulled By: umanwizard

fbshipit-source-id: 4ab209a5fb27d8ba27ee7eb6b67b858ce2480594",22.0,0.0,"docs/source/tensors.rst,test/test_torch.py,torch/_tensor_docs.py,torch/csrc/autograd/python_variable.cpp",4.0,6,3,1.749902506,41.0,15868.0,4.0,927683.5,8777.0,25985.83333,0.0,Feature Addition,0.0,1
pytorch,fca77c9e256b9f67ae7d593ba242cab033bcd89f,989e8ff781d0f0d65e924c51cc2ad6bf1ae1fe97,gchanan,gregchanan@gmail.com,Sun Nov 26 18:19:51 2017 -0500,1511720391.0,"Implement is_sparse, is_distributed as native function, (#3838)

work towards cpu() working on sparse tensors.",15.0,3.0,"aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,tools/autograd/templates/python_variable_methods.cpp",3.0,7,2,1.530493057,10.0,818.0,3.0,433529.6666666667,2156.0,24135.85823,0.0,,0.0,1
pytorch,34dc34e8a02c4620188d60c7c5841a01b36033d6,98a9235dce649e85afe86c061d358246bbc0c4ab,soulitzer,soulitzer@gmail.com,Mon Dec 12 17:42:50 2022 -0500,1670866970.0,"Fix prelu ref when a.ndim < 2 (#89809)

Fixes https://github.com/pytorch/pytorch/issues/89560

Previously the test case for ""input is 1-D or scalar + weight is not scalar"" did not exist; adding it introduced some failures:
- forward AD (fixed in this PR)
- vmap (filed https://github.com/pytorch/pytorch/issues/89895)
- ref/meta (fixed this PR, though this also regresses nvFuser support)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89809
Approved by: https://github.com/ngimel",27.0,10.0,"test/functorch/test_ops.py,torch/_decomp/decompositions.py,torch/_refs/nn/functional/__init__.py,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py",5.0,11,2,2.18433403,7.0,31530.0,5.0,256889.4,10457.0,23915.0,0.0,Corrective,1.0,1
pytorch,0b328874c62fe38ffd1b8bc0b333e5d2f6d4d2c1,98c02c20b161ff8e4181e24336de4936039e9f5f,Tzu-Wei Huang,huang.dexter@gmail.com,Mon Jan 01 22:44:03 2018 +0800,1514846643.0,fixes #4403  (#4407),2.0,1.0,torch/nn/modules/rnn.py,1.0,3,1,0,33.0,760.0,1.0,775081.0,881.0,6657.172317,0.0,Corrective,1.0,1
pytorch,68f6b9873b6bdc0e99208d95b51ecc7f435eb90f,98c293c1efc54a3b01ab215bc9ff6c3974760531,Nikita Shulga,nshulga@fb.com,Tue Apr 21 07:57:14 2020 -0700,1587455834.0,"Do not use VLAs in vec256_qint.h (#36855)

Summary:
Use `std::decay_t<decltype(foo)>::size()`  instead of `foo.size()` to help compiler with static array allocations.
Even if `Vec256::size()` is `constexpr`, `foo.size()` (where `foo` is of type `Vec256`) is not an integral constant expression, therefore compiler have to use VLAs, which are not part of C++ standard.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36855

Test Plan: CI

Differential Revision: D21151194

Pulled By: malfet

fbshipit-source-id: eaf3e467c7f7ee6798ca82fe9f8fae725011ead0",88.0,88.0,aten/src/ATen/cpu/vec256/vec256_qint.h,1.0,5,1,0,2.0,1490.0,1.0,130652.0,1242.0,3213.0,0.0,,0.0,1
pytorch,2cee02cc866c8bcd08a73fa32ae1c2afb48c018e,98e5f2c80873eba03bd2df0bfbe9887f3251124c,Tongzhou Wang,SsnL@users.noreply.github.com,Tue Jan 02 17:39:21 2018 -0500,1514914761.0,nllloss doc (#4438),29.0,23.0,torch/nn/modules/loss.py,1.0,3,1,0,35.0,864.0,1.0,51880.0,2236.0,24316.35823,0.0,Non Functional,0.0,1
pytorch,3a4ca7a2696ac5f8d3a32108648f588bbc2b1eaa,98e67448fa78bd1bc6f05920ad03efceecc10066,Adam Paszke,adam.paszke@gmail.com,Fri Sep 29 15:52:35 2017 -0700,1506700355.0,"Large Softmax and LogSoftmax refactor

- Cleaned up THNN and THCUNN code and kernels
- Improved THCUNN kernel performance 5x, making it match cuDNN performance
- Added support for computing softmax over arbitrary dims
  NOTE: The default dim for 3D inputs is now 1 (used to be 0)
- Both functions now accept inputs with arbitrarily many dimensions
- Autograd functions no longer save the input (it's unnecessary)
- Added cuDNN bindings for softmax, but they are unused as THCUNN
  matches or even exceeds cuDNN performance",774.0,566.0,"setup.py,test/common_nn.py,test/test_nn.py,tools/autograd/gen_variable_type.py,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/softmax.cpp,torch/csrc/autograd/functions/softmax.h,torch/csrc/cudnn/Descriptors.h,torch/csrc/cudnn/Exceptions.h,torch/legacy/nn/LogSoftMax.py,torch/legacy/nn/SoftMax.py,torch/legacy/nn/SoftMin.py,torch/legacy/nn/SpatialSoftMax.py,torch/lib/THCUNN/LogSoftMax.cu,torch/lib/THCUNN/generic/LogSoftMax.cu,torch/lib/THCUNN/generic/SoftMax.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THNN/generic/LogSoftMax.c,torch/lib/THNN/generic/SoftMax.c,torch/lib/THNN/generic/THNN.h,torch/nn/_functions/thnn/activation.py,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/auto_double_backwards.py,torch/nn/functional.py,torch/nn/modules/activation.py",25.0,19,3,3.805077293,38.0,14647.0,2.0,12883.391304347826,767.0,6467.672317,0.0,Feature Addition,0.0,1
pytorch,fee67c2e1ae0ceb3d9c5beab471296627dc41b44,98f67e90d57cb03e367bd6c9b66f08f8d83a237c,Sam Gross,colesbury@gmail.com,Wed Oct 19 17:21:03 2016 -0400,1476897663.0,Fix super call in Container.modules and Container.parameters (#142),20.0,5.0,"test/test_nn.py,torch/nn/modules/container.py",2.0,4,2,0.795040279,9.0,1050.0,1.0,71203.0,48.0,53.15263348,0.0,Corrective,1.0,1
pytorch,1321ed04460145b20f767bb20b10225af79758d0,9962b908570989ddf0607824b07f6010540dbd2a,Richard Zou,zou3519@gmail.com,Fri Nov 12 23:04:53 2021 -0800,1636758293.0,[functorch] Better error on item() and data dependent control flow,48.0,0.0,"functorch/functorch/csrc/BatchRulesDynamic.cpp,functorch/test/test_vmap.py",2.0,4,1,0.918295834,1.0,3315.0,2.0,5.0,523.0,725.5,0.0,,0.0,1
pytorch,ee38a467eab5faf352b34f56b21de215de0cd321,9984f4bb8b1d83e0b257a422a60a24f6c83d399e,Peter Bell,peterbell10@live.co.uk,Tue Oct 12 20:09:55 2021 -0700,1634069395.0,"Remove native_functions.yaml dependency from some reduction operators (#64173)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64173

This one also required restructuring the code a bit to move the kernel
code into seperate files. So, I've mainly focused on CUDA which is
where the real build-time issues are.

Test Plan: Imported from OSS

Reviewed By: jbschlosser, ezyang

Differential Revision: D30728581

Pulled By: dagitses

fbshipit-source-id: a69eea5b4100d16165a02660dde200c8f648683d",248.0,129.0,"aten/src/ATen/core/Tensor.cpp,aten/src/ATen/core/TensorBase.h,aten/src/ATen/native/LinearAlgebra.h,aten/src/ATen/native/ReduceAllOps.h,aten/src/ATen/native/ReduceOps.h,aten/src/ATen/native/TensorCompare.h,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/native/cpu/SumKernel.cpp,aten/src/ATen/native/cuda/Equal.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/cuda/Reduce.cuh,aten/src/ATen/native/cuda/ReduceLogicKernel.cu,aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu,aten/src/ATen/native/cuda/ReduceMomentKernel.cu,aten/src/ATen/native/cuda/ReduceNormKernel.cu,aten/src/ATen/native/cuda/ReduceOps.cpp,aten/src/ATen/native/cuda/ReduceOps.h,aten/src/ATen/native/cuda/ReduceSumProdKernel.cu,aten/src/ATen/native/cuda/TensorCompare.cpp,aten/src/ATen/native/cuda/TensorCompare.cu,caffe2/CMakeLists.txt",21.0,8,2,3.545482078,15.0,6176.0,14.0,6053348.117647059,16186.0,37424.5,0.0,,0.0,1
pytorch,d661e646add3256b85687faab4197d2d1aab3dec,99b154b8bebb8eed79d10f5c83798263ae04153f,BowenBao,bowbao@microsoft.com,Wed Aug 18 20:25:19 2021 -0700,1629318319.0,"[ONNX] Support lstm_cell symbolic (#61476) (#62757)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62757

Support lstm_cell symbolic

Test Plan: Imported from OSS

Reviewed By: SplitInfinity

Differential Revision: D30349061

Pulled By: msaroufim

fbshipit-source-id: f236177e3e5c62a30b7e4d91a623bcaef21b5eb1

Co-authored-by: jiafatom <jiafa@microsoft.com>",27.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.975119065,3.0,12802.0,2.0,905971.0,14742.0,33753.5,0.0,,0.0,1
pytorch,452ebd03a95463900c93cd5563d205a5c87c2e99,9a0d1c5446c02f84b94a312193f7b26704965a47,Mike Ruberry,mruberry@devfair044.h1.fair,Sat Apr 16 01:18:08 2022 +0000,1650071888.0,"Adds binary_cross_entropy opinfo

Per title.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75876
Approved by: https://github.com/ngimel",73.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,17075.0,1.0,220.0,2348.0,5498.0,0.0,Feature Addition,0.0,1
pytorch,147de8243b97f5c6e21a5bd948a4be981dea1b9e,9a2db6f091695a0ddd7955c107df25a10e9ad3ac,Joel Schlosser,jbschlosser@fb.com,Wed Nov 10 15:51:14 2021 -0800,1636559474.0,"Factor backend routing logic out of convolution forward (#67790)

Summary:
This PR introduces a new function `_select_conv_backend` that returns a `ConvBackend` enum representing the selected backend for a given set of convolution inputs and params.

The function and enum are exposed to python for testing purposes through `torch/csrc/Module.cpp` (please let me know if there's a better place to do this).

A new set of tests validates that the correct backend is selected for several sets of inputs + params. Some backends aren't tested yet:
* nnpack (for mobile)
* xnnpack (for mobile)
* winograd 3x3 (for mobile)

Some flowcharts for reference:
![conv_routing_graph md](https://user-images.githubusercontent.com/75754324/140828957-1135b400-38c0-4c9f-87ef-4f33ceebeeae.png)
![conv_nogroup_routing_graph md](https://user-images.githubusercontent.com/75754324/140828977-ed223a4e-aa86-49f1-9925-c0f6b9ab36af.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67790

Reviewed By: zou3519

Differential Revision: D32280878

Pulled By: jbschlosser

fbshipit-source-id: 0ce55174f470f65c9b5345b9980cf12251f3abbb",593.0,196.0,"aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,test/test_nn.py,torch/csrc/Module.cpp,torch/testing/_internal/common_device_type.py",5.0,9,3,1.506796076,46.0,23045.0,3.0,277141.8,16994.0,39990.0,0.0,Corrective,0.0,1
pytorch,658d4c7ea8bbd4b6adc825e08c200b6bde76696b,9a48f8d7c3311360d7e218ad36c8524a665ebd0d,SsnL,tongzhou.wang.1994@gmail.com,Fri Dec 22 19:46:38 2017 -0500,1513971998.0,add tests for btrifact_with_info and doc for btriunpack,32.0,3.0,"docs/source/torch.rst,test/test_torch.py,torch/_tensor_docs.py,torch/functional.py",4.0,4,3,1.708096422,38.0,7888.0,3.0,162190.5,2221.0,24287.35823,0.0,Feature Addition,0.0,1
pytorch,739e6af86940f558f384a4a8922df8dc660d5660,9a7c196040f55d921a4deee42cbb1dd294b8f23b,Gregory Chanan,gchanan@fb.com,Wed Sep 12 19:57:32 2018 -0700,1536782252.0,"Move Type, Tensor, TensorMethods to core.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/11519

Reviewed By: yf225

Differential Revision: D9771684

Pulled By: gchanan

fbshipit-source-id: a57ee2072af99ce856f895c688b09d750a8606e0",2633.0,21.0,"aten/src/ATen/ATen.h,aten/src/ATen/Tensor.h,aten/src/ATen/Type.h,aten/src/ATen/core/ATenCoreTest.cpp,aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/gen.py,aten/src/ATen/templates/NativeFunctions.h,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/TensorMethods.h,aten/src/ATen/templates/Type.h,cmake/Codegen.cmake",13.0,6,2,1.727906401,10.0,1287.0,7.0,693871.625,4052.0,11303.33333,0.0,,0.0,1
pytorch,ad88afcff85ad2a92ea90fdc4b26f71dbf004848,9a8f71f23ee8c45d908e71af1900acd9b41268e4,Edward Z. Yang,ezyang@meta.com,Sun Apr 09 19:54:51 2023 -0400,1681070091.0,"Convert logging f-strings to use % format (#98697)

Codemod done with
https://gist.github.com/ezyang/2e8b0463cdc6be278478495b23ff0530 with
assistance from ChatGPT.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98697
Approved by: https://github.com/voznesenskym",138.0,130.0,"benchmarks/dynamo/common.py,benchmarks/dynamo/microbenchmarks/operator_inp_utils.py,test/jit/fixtures_srcs/generate_models.py,tools/linter/adapters/constexpr_linter.py,tools/linter/adapters/s3_init.py,tools/linter/adapters/update_s3.py,tools/nightly.py,torch/_dynamo/backends/distributed.py,torch/_dynamo/debug_utils.py,torch/_dynamo/eval_frame.py,torch/_dynamo/guards.py,torch/_dynamo/output_graph.py,torch/_dynamo/symbolic_convert.py,torch/_dynamo/utils.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/triton.py,torch/_inductor/decomposition.py,torch/_inductor/graph.py,torch/_inductor/pattern_matcher.py,torch/_inductor/sizevars.py,torch/backends/xeon/run_cpu.py,torch/distributed/_spmd/distribute.py,torch/distributed/_spmd/iter_graph_module.py,torch/distributed/checkpoint/_dedup_tensors.py,torch/distributed/elastic/agent/server/api.py,torch/distributed/elastic/agent/server/local_elastic_agent.py,torch/distributed/elastic/multiprocessing/__init__.py,torch/distributed/elastic/multiprocessing/api.py,torch/distributed/elastic/multiprocessing/errors/error_handler.py,torch/distributed/elastic/rendezvous/etcd_rendezvous.py,torch/distributed/elastic/rendezvous/etcd_server.py,torch/distributed/elastic/timer/api.py,torch/distributed/elastic/timer/file_based_local_timer.py,torch/distributed/elastic/timer/local_timer.py,torch/distributed/elastic/utils/distributed.py,torch/distributed/launcher/api.py,torch/distributed/nn/jit/instantiator.py,torch/distributed/rpc/_utils.py,torch/distributed/run.py,torch/fx/experimental/symbolic_shapes.py,torch/fx/passes/infra/pass_manager.py,torch/fx/passes/utils/matcher_utils.py,torch/nn/parallel/distributed.py,torch/profiler/_memory_profiler.py,torch/testing/_internal/common_distributed.py",45.0,41,4,4.877135072,20.0,35427.0,32.0,4431359.022222222,14350.0,32789.5,0.0,,0.0,1
pytorch,e422b27f179186f4bd73e83f11aaee876ccd9acd,9aae82bc2cf298628fb94b6db9f736b53a0c3b46,Ailing Zhang,ailzhang@fb.com,Fri Feb 22 22:19:04 2019 -0800,1550873944.0,"Improvements for current AD (#17187)

Summary:
This PR removes a few size of `self` that passed from forward pass to backward pass when `self` is already required in backward pass. This could be reason that cause the potential slow down in #16689 . I will attach a few perf numbers (still a bit volatile among runs tho) I got in the comment.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17187

Differential Revision: D14179512

Pulled By: ailzhang

fbshipit-source-id: 5f3b1f6f26a3fef6dec15623b940380cc13656fa",837.0,478.0,"aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/cpp/jit/test_misc.h,test/expect/TestFuser.test_lstm_cuda-backward.expect,test/expect/TestFuser.test_lstm_cuda-forward.expect,test/expect/TestFuser.test_milstm_cuda-backward.expect,test/expect/TestFuser.test_milstm_cuda-forward.expect,test/expect/TestJit.test_cpp_cuda.expect,test/test_jit.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/csrc/jit/autodiff.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/symbolic_script.cpp",16.0,14,4,2.810086343,15.0,29649.0,6.0,436400.25,7181.0,21988.83333,0.0,Feature Addition,0.0,1
pytorch,6f53b4efea8746d4ff296cc8c5ddd35fd4975998,9abc66674559ac214dad31ca42bc0d981d5260db,Roy Li,royboy@fb.com,Thu Sep 13 18:51:17 2018 -0700,1536864677.0,"stop allowing extra positional args in arg parser (#10499)

Summary:
Arg parser allowed additional positional args to be parsed into keyword-only params.

Fixes a couple cases:
- The positional argument happens to be of the right type, and it just works silently. Now, we fail as expected.
- The positional argument fails later down the line. Now, we fail at the appropriate time and get a better error message.

Pre-fix:
```
>>> torch.cuda.LongTensor((6, 0), 1, 1, 0)
tensor([6, 0], device='cuda:1')
```
Post-fix:
```
>>> torch.cuda.LongTensor((6, 0), 1, 1, 0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: new() received an invalid combination of arguments - got (tuple, int, int, int), but expected one of:
 * (torch.device device)
 * (torch.Storage storage)
 * (Tensor other)
 * (tuple of ints size, torch.device device)
 * (object data, torch.device device)
```

Pre-fix:
```
>>> a = torch.tensor(5)
>>> a.new_zeros((5,5), 0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: new_zeros(): argument 'dtype' (position 2) must be torch.dtype, not int
```

Post-fix:
```
>>> a = torch.tensor(5)
>>> a.new_zeros((5,5), 0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: new_zeros() takes 1 positional argument but 2 were given
```

fixes #8351
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10499

Differential Revision: D9811093

Pulled By: li-roy

fbshipit-source-id: ce946270fd11b264ff1b09765db3300879491f76",15.0,0.0,"test/test_torch.py,torch/csrc/utils/python_arg_parser.cpp",2.0,4,2,0.996791632,40.0,9543.0,2.0,35567.0,4090.0,11420.83333,0.0,Corrective,1.0,1
pytorch,6fd48e24f109a08612495df05ad75392f0d2e6b2,9ad14f6b43b34015cde5a8a4d5961be522499a24,Mingfei Ma,mingfei.ma@intel.com,Tue May 19 19:59:08 2020 -0700,1589918348.0,"cover nn.Conv1d in mkldnn model conversion logic (#38528)

Summary:
current `to_mkldnn` model conversion logic under `torch.utils.mkldnn` does not cover `nn.Conv1d`. This patch fills the gap, using similar logic to `nn.Conv2d`. The model conversion will remove unnecessary memory format reorders of input/output tensors and thus speedup the model.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38528

Differential Revision: D21640325

Pulled By: albanD

fbshipit-source-id: c3340153b5c524e020c097eb4b9e2ffcbde8896d",62.0,17.0,"test/test_mkldnn.py,torch/utils/mkldnn.py",2.0,3,2,0.870188335,2.0,648.0,2.0,4834358.0,2199.0,5456.5,0.0,,1.0,1
pytorch,2ab8126e365f21680ac38aea43282fb4c4290c78,9ade03959392e5a90b74261012de1d806cab2253,Rong Rong (AI Infra),rongr@fb.com,Wed Jul 14 00:49:14 2021 -0700,1626223754.0,"fix test file not found issue (#61610)

Summary:
it should not error out if the file is not found.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61610

Reviewed By: samestep

Differential Revision: D29687958

Pulled By: walterddr

fbshipit-source-id: 17cacba8daa131df9bfb37fd58d6e4870ff75198",12.0,6.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2498.0,1.0,109648.0,13820.0,31142.0,0.0,Corrective,1.0,1
pytorch,1a9efbbc92057d494217f3ace0f87c319495d577,9afe9fba29ff9d34294c4feeb5e8e2aced190b63,albanD,desmaison.alban@gmail.com,Mon May 17 19:32:15 2021 -0700,1621279935.0,"Reland OpInfo support for forward AD (#58304)

Summary:
Try 3 to land this.
Trying ci-all label to ensure we test everything.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58304

Reviewed By: heitorschueroff

Differential Revision: D28474343

Pulled By: albanD

fbshipit-source-id: 8230fa3c0a8d3633f09999e7c2f47dbdc5fe57e9",54.0,12.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.932707613,2.0,7485.0,1.0,274044.0,12160.0,27547.5,0.0,,0.0,1
pytorch,a548c6b18ff5a337b6329ed7b2381cfb4d3da2ed,9b0393fcf12dc0dce6f83244d95b56e8dd2e5219,Ksenija Stanojevic,ksenija.stanojevic@gmail.com,Fri Jul 10 20:04:16 2020 -0700,1594411456.0,"[ONNX]Fix export of flatten (#40418)

Summary:
Shape is passed to _reshape_to_tensor as a Constant and cannot infer shape of the input when model is exported with dynamic axes set. Instead of a Constant pass output of a subgraph Shape-Slice-Concat to compute the shape for the Reshape node in _reshape_to_tensor function.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/40418

Reviewed By: hl475

Differential Revision: D22480127

Pulled By: houseroad

fbshipit-source-id: 11853adb6e6914936871db1476916699141de435",90.0,41.0,"test/onnx/expect/TestOperators.test_flatten.expect,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",6.0,5,2,2.199682829,5.0,10086.0,5.0,1666317.6666666667,3524.0,8354.5,0.0,Corrective,1.0,1
pytorch,4a145cd95c312828da66a638191eeb6675705977,9b272c08cfcb0453d1e883871cd5fa0d0ed534a1,Jerry Zhang,jerryzh@fb.com,Tue Dec 11 01:13:51 2018 -0800,1544490831.0,"Remove partially initialized Tensor in Deserialization (#14197)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14197

Pull Request resolved: https://github.com/pytorch/pytorch/pull/13642

Previously we pass in a patially initialized Tensor to Deserialize and it will fill
it with the result of deserialization of a tensor proto. Now we want it to return
a Tensor directly since it's just a shared pointer to TensorImpl.

Reviewed By: dzhulgakov

Differential Revision: D12874357

fbshipit-source-id: 12b80a763375da23cfa64a74d6bc186d8d03b94f",178.0,57.0,"binaries/benchmark_helper.cc,caffe2/core/blob_serialization.cc,caffe2/core/blob_serialization.h,caffe2/core/blob_test.cc,caffe2/operators/map_ops.h,caffe2/operators/tensor_protos_db_input.h",6.0,4,2,1.491812521,11.0,2775.0,6.0,1750417.5,5965.0,18519.83333,0.0,,0.0,1
pytorch,e1b5dbf699a0c1e948c692767288a34482c37f8a,9b2bd284b3da4c2e703ac1d2fe8d1c40dfa01e93,SsnL,tongzhou.wang.1994@gmail.com,Mon Dec 10 15:36:06 2018 -0800,1544456166.0,"Convert int8 numpy array to CharTensor (#14700)

Summary:
When rewriting `default_collate`, I noticed that `from_numpy` and `as_tensor` and `tensor` all do not work on `np.int8` arrays.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14700

Reviewed By: weiyangfb

Differential Revision: D13305297

Pulled By: soumith

fbshipit-source-id: 2937110f65ed714ee830d50098db292238e9b2a9",17.0,12.0,"test/test_torch.py,torch/csrc/utils/tensor_numpy.cpp",2.0,4,2,0.479832024,40.0,9745.0,1.0,218.0,5946.0,18486.83333,0.0,,0.0,1
pytorch,558516fcdb04c45831989f9a588ff82e8edb5561,9b31280ccf60a9db491342f97ac47c20481b8c5c,Luca Antiga,luca.antiga@orobix.com,Mon Nov 27 16:22:34 2017 +0100,1511799754.0,"Have __sizeof__ account for size of stored elements (#3821)

* Have __sizeof__ account for size of stored elements

* Conform to sizeof specification",16.0,0.0,"test/test_torch.py,torch/storage.py",2.0,2,2,0.69621226,38.0,5037.0,2.0,1670269.5,365.0,2179.5,0.0,,0.0,1
pytorch,bd0e9a73c79128153be56a4de1a3086ae1f4e960,9b626a8047c86f0ee19261d7caefa7242d2e8731,Arkadiusz NowaczyÅski,ar.nowaczynski@gmail.com,Tue Mar 07 15:40:18 2017 +0100,1488901218.0,Fix documentation - replace 'matrix' with 'vector' (#951),1.0,1.0,torch/_torch_docs.py,1.0,1,1,0,11.0,4325.0,1.0,254517.0,521.0,3526.555248,0.0,Corrective,1.0,1
pytorch,8eded5aece1b23155eb7cc2265fb658a63aa2e13,9b6441ecbc86dbffc14a52277d7b37018a1a0bb2,Alican Bozkurt,alicanb@gmail.com,Sat Jan 13 10:26:14 2018 -0500,1515839174.0,Implement Multinomial distribution (#4624),200.0,13.0,"test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/categorical.py,torch/distributions/constraints.py,torch/distributions/multinomial.py",5.0,3,2,1.501305871,8.0,1832.0,3.0,312733.25,912.0,6715.672317,0.0,,0.0,1
pytorch,abdf82f3025b818a76f2a860ec25ec3efef67953,9b690516014a4a6e1c197b8b192b52be25c1dc41,samdow,samdow@fb.com,Thu Jun 02 19:47:37 2022 -0400,1654199257.0,[functorch] fix ci,6.0,1.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1450.0,1.0,0.0,1093.0,1484.0,0.0,Corrective,1.0,1
pytorch,24af02154c1a599e2d785ed667c8c2968e230e87,9b7eceddc8b526e494024195472c80e5d7ab8577,Adam Paszke,adam.paszke@gmail.com,Wed Dec 28 16:49:26 2016 -0800,1482943766.0,Accept outputs in out argument,491.0,435.0,"test/test_torch.py,tools/cwrap/cwrap.py,tools/cwrap/plugins/ArgcountChecker.py,tools/cwrap/plugins/KwargsPlugin.py,tools/cwrap/plugins/THPPlugin.py,torch/autograd/functions/blas.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/Tensor.cwrap,torch/csrc/generic/methods/TensorCompare.cwrap,torch/csrc/generic/methods/TensorMath.cwrap,torch/csrc/generic/methods/TensorRandom.cwrap,torch/legacy/nn/BCECriterion.py,torch/legacy/nn/Bilinear.py,torch/legacy/nn/CMul.py,torch/legacy/nn/Cosine.py,torch/legacy/nn/CosineDistance.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/DotProduct.py,torch/legacy/nn/Euclidean.py,torch/legacy/nn/Exp.py,torch/legacy/nn/Index.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/MM.py,torch/legacy/nn/MV.py,torch/legacy/nn/MaskedSelect.py,torch/legacy/nn/Max.py,torch/legacy/nn/Min.py,torch/legacy/nn/MixtureTable.py,torch/legacy/nn/Normalize.py,torch/legacy/nn/PairwiseDistance.py,torch/legacy/nn/PartialLinear.py,torch/legacy/nn/Replicate.py,torch/legacy/nn/SpatialCrossMapLRN.py,torch/legacy/nn/Sum.py,torch/legacy/nn/WeightedEuclidean.py,torch/legacy/optim/adamax.py,torch/legacy/optim/rmsprop.py,torch/legacy/optim/rprop.py,torch/nn/functions/loss.py,torch/nn/functions/thnn/normalization.py,torch/nn/functions/thnn/sparse.py",41.0,16,3,4.090363083,20.0,11215.0,6.0,69918.9268292683,349.0,3014.196975,0.0,,0.0,1
pytorch,cb3232fdb9d742c8a07fb5eecb7d88a8deaf01f1,9b875e1256d679d96615bf4bfdde63d3d813bab4,Alban Desmaison,albandes@fb.com,Thu Nov 07 16:32:51 2019 -0800,1573144371.0,"Buffer python warning to avoid deadlocks

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/26613

Test Plan: Imported from OSS

Differential Revision: D18249633

Pulled By: albanD

fbshipit-source-id: 863f52400e1b97943a67a9e1abb09ae8d045e7f0",203.0,88.0,"c10/util/Exception.cpp,c10/util/Exception.h,test/test_indexing.py,test/test_jit.py,test/test_torch.py,torch/csrc/Exceptions.cpp,torch/csrc/Exceptions.h,torch/csrc/Module.cpp",8.0,5,3,2.447680996,44.0,33483.0,8.0,8069167.125,12908.0,35698.83333,0.0,Preventative,0.0,1
pytorch,a60ff6985dfa3ed108f862ee1443740c0bc7a1da,9ba22661564c7f6d3d25aba1416b7101179483ab,Richard Zou,zou3519@gmail.com,Wed Jul 13 14:12:33 2022 -0700,1657721553.0,[functorch] Reduce flaky tests in test_vmap via skips and tolerances,7.0,3.0,functorch/test/test_vmap.py,1.0,2,1,0,1.0,4239.0,1.0,0.0,1158.0,1556.5,0.0,,0.0,1
pytorch,b142a224c6ee4ed99361837bc8dd3f78cdb8f234,9bbe1d632e6536934550b6973366e2acdd4e1995,Thiago Crepaldi,thiago.crepaldi@microsoft.com,Thu Apr 14 23:18:45 2022 +0000,1649978325.0,"Fix ONNX ATen fallback for non-caffe2 engines

This PR introduces 3 BC changes:

First, this PR propagates `BUILD_CAFFE2` flag to `libtorch` and `libtorch_python`, which is necessary for non-caffe2 ONNX runtimes when using `ONNX_ATEN_FALLBACK` operator export type.

Second, as a complement of https://github.com/pytorch/pytorch/pull/68490, this PR refactors Caffe2's Aten ops symbolics to consider not only the `operator_export_type` (aka `ONNX_ATEN_FALLBACK`) to emit Caffe2 Aten ops, but also whether `BUILD_CAFFE2` (which is called `torch.onnx._CAFFE2_ATEN_FALLBACK` in python binding) is set.

Lastly, it renames `onnx::ATen` to `aten::ATen` for ONNX spec consistency in a BC fashion.
ONNX doesn't have `ATen` op on its spec, but PyTorch ONNX converter emits them. Non-Caffe2 backend engines would be mislead by such operator's name/domain. A non-ideal workaround would be to have Aten ops handled based on its name and ignore the (non-complaint) domain. Moreover, users could incorrectly file bugs to either ONNX or ONNX Runtime when they inspect the model and notice the presence of an unspecified ONNX operator.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/73954
Approved by: https://github.com/BowenBao, https://github.com/malfet, https://github.com/garymm, https://github.com/jiafatom",146.0,112.0,"aten/src/ATen/core/interned_strings.h,binaries/bench_gen/bench_gen.py,caffe2/CMakeLists.txt,caffe2/contrib/aten/README.md,caffe2/contrib/aten/aten_op_template.h,caffe2/contrib/aten/aten_test.py,caffe2/contrib/aten/docs/sample.py,caffe2/python/benchmark_generator.py,test/expect/TestPytorchExportModes.test_aten_fallback.expect,test/expect/TestPytorchExportModes.test_onnx_aten.expect,test/expect/TestScript.test_listconstruct_erasure.expect,test/onnx/expect/TestOperators.test_at_op.expect,test/onnx/expect/TestOperators.test_aten_embedding_2.expect,test/onnx/expect/TestOperators.test_embedding_bags.expect,test/onnx/expect/TestOperators.test_layer_norm_aten.expect,test/onnx/test_operators.py,test/quantization/eager/test_quantize_eager_ptq.py,torch/CMakeLists.txt,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/serialization/export.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py,torch/testing/_internal/common_quantization.py,torch/testing/_internal/common_utils.py",26.0,26,5,3.687909798,16.0,23310.0,20.0,7199021.076923077,2295.0,5425.5,0.0,Corrective,1.0,1
pytorch,94ff31f54def530b38874c3fb36eaa97f5e2aa60,9bf5e40dfa66a78092a1c0dc953aed05406e1bf4,Edward Z. Yang,ezyang@mit.edu,Mon Dec 18 21:47:57 2017 -0500,1513633677.0,"Refactor cudnn code layout / make build more robust. (#4201)

* Refactor cudnn code layout / make build more robust.

When I previously moved cuDNN into ATen, I wasn't too familiar with the
ATen native function directory layout, and so I did a number of
suboptimal things.  This commit fixes those problems.

- If NO_CUDA was set but cuDNN is installed on your system, we'd incorrectly
  assume that CUDNN was enabled, to hilarious effect.

- We now distinguish between cudnn implementation files and cudnn
  native function files.  The native files now live in ATen/native/cudnn,
  and are *unconditionally compiled*, even when we are not building with cuDNN.
  This means that we can unconditionally declare cudnn functions in yaml
  and they are always available, even if they are broken.  The cuDNN specific
  files live in 'cudnn', they are *never* installed, and they are used
  purely for implementation purposes.  I had to add stub implementations of
  all ATen functions to achieve this.

- I had written headers for at::native functions manually, but codegen
  will generate them for me automatically.  So I deleted the headers.
  That lets me get rid of some header install logic as well.

- There's a new note about ATen preprocessor philosophy.",1727.0,1691.0,"aten/CMakeLists.txt,aten/README.md,aten/src/ATen/CMakeLists.txt,aten/src/ATen/Config.h.in,aten/src/ATen/cudnn/AffineGridGenerator.cpp,aten/src/ATen/cudnn/AffineGridGenerator.h,aten/src/ATen/cudnn/BatchNorm.cpp,aten/src/ATen/cudnn/BatchNorm.h,aten/src/ATen/cudnn/Conv.cpp,aten/src/ATen/cudnn/Conv.h,aten/src/ATen/cudnn/GridSampler.cpp,aten/src/ATen/cudnn/GridSampler.h,aten/src/ATen/cudnn/cuDNN.yaml,aten/src/ATen/gen.py,aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,aten/src/ATen/native/cudnn/BatchNorm.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/cudnn/GridSampler.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/templates/NativeFunctions.h,torch/csrc/Module.cpp,torch/csrc/autograd/functions/batch_normalization.cpp,torch/csrc/autograd/functions/convolution.cpp",23.0,11,2,2.73166797,39.0,5242.0,7.0,1173269.3157894737,2216.0,24276.85823,0.0,Corrective,1.0,1
pytorch,517fb2f41055e708a209a66fc0a95b76a4056fe8,9c218b419f8d28de8a6054afa54d7305a5e1bee7,Sergey Zagoruyko,zagoruyko2@gmail.com,Wed Jan 18 00:24:01 2017 +0300,1484699041.0,kl_div and docs (#429),147.0,2.0,torch/nn/functional.py,1.0,2,1,0,9.0,534.0,1.0,109454.0,352.0,4868.976424,0.0,Non Functional,0.0,1
pytorch,4e3b03217d624dde0ec64f0c621b97450174f5bc,9c2715bbb208fc47a2b3bab2d8fdb7e14cd82a1a,Bin Bao,binbao@fb.com,Thu Sep 21 00:25:24 2023 +0000,1695255924.0,"[inductor] Clean up AOTInductor runtime ABI (#109678)

Summary: Change the AOTInductor runtime interface to avoid referring to aten data structures directly, mostly at::Tensor and ProxyExecutor. This a combination of https://github.com/pytorch/pytorch/pull/109436,  https://github.com/pytorch/pytorch/pull/109498, https://github.com/pytorch/pytorch/pull/109450, https://github.com/pytorch/pytorch/pull/109606, plus a few internal build changes.

Reviewed By: frank-wei

Differential Revision: D49374820

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109678
Approved by: https://github.com/frank-wei, https://github.com/chenyang78",614.0,381.0,"build_variables.bzl,setup.py,test/cpp/aot_inductor/test.cpp,test/inductor/test_aot_inductor.py,torch/_inductor/codegen/aot_runtime/interface.cpp,torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py,torch/_inductor/utils.py,torch/csrc/inductor/aot_runtime/cuda_utils.h,torch/csrc/inductor/aot_runtime/interface.h,torch/csrc/inductor/aot_runtime/model.h,torch/csrc/inductor/aot_runtime/model_container.h,torch/csrc/inductor/aot_runtime/proxy_executor.h,torch/csrc/inductor/aoti_torch/c/shim.h,torch/csrc/inductor/aoti_torch/proxy_executor.h,torch/csrc/inductor/aoti_torch/shim_common.cpp,torch/csrc/inductor/aoti_torch/tensor_converter.cpp,torch/csrc/inductor/aoti_torch/tensor_converter.h",18.0,13,2,3.590616396,45.0,14422.0,9.0,304715.4285714286,19934.0,45503.0,0.0,Preventative,0.0,1
pytorch,561fc8d96a8e970218c43915bb7be818f88f0321,9c39e8cecb0e6bbe436614439551ad3f70841513,Trevor Killeen,killeent@users.noreply.github.com,Tue Sep 19 14:38:18 2017 -0400,1505831898.0,Parity with NumPy newaxis placement in indexing (#2779),81.0,22.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.991757024,38.0,6211.0,2.0,206279.5,1758.0,24786.05562,0.0,,0.0,1
pytorch,3fe35300eda292019190cc4ca602304a974ebd71,9c617140f79f82628484e7545b228be990994196,Pieter Noordhuis,pietern@fb.com,Thu Oct 18 19:09:36 2018 -0700,1539889776.0,"Try to reduce c10d test flakiness (#12782)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12782

We have seen the ""Address already in use"" error popup a few times when instantiating the TCPStore. The port that it uses is dynamically generated through common.find_free_port(), which binds a new socket to a random port, closes the socket, and returns the port that the OS had assigned. If some other process grabs that port in the time between closing the socket and the TCPStore binding to it, the bind error shows up. This commit changes most tests to use the FileStore instead and includes a retry when testing the TCPStore.

Differential Revision: D10433401

fbshipit-source-id: 8dd575ac91a3cddd1cc41ddb0ff4311ddc58c813",29.0,16.0,test/test_c10d.py,1.0,1,1,0,2.0,835.0,1.0,47284.0,4712.0,13939.33333,0.0,Feature Addition,0.0,1
pytorch,828a6a3b3943a0a0701ecacacd2bcc34fc03fe03,9c87543124f0b15db0a55af66c5553bc5af014a7,Ailing Zhang,ailzhang@fb.com,Sun Mar 31 15:41:46 2019 -0700,1554046906.0,"Enforce check ad in test_jit (#18509)

Summary:
If a test triggers autodiff, it must have a `DifferentiableGraph` in its differentiated forward graph, and this subgraph must have either the original aten node, or the corresponding nodes used in AD formula.

Typically a forward differentiable graph looks like this:
```
graph(%i0 : Float(),
      %i1 : Float()):
  %3 : Float() = prim::DifferentiableGraph_0(%i0, %i1)
  return (%3)
with prim::DifferentiableGraph_0 = graph(%0 : Float(),
      %1 : Float()):
  %2 : Float() = aten::max(%0, %1)
  return (%2)
```
which tells us `aten::max(Tensor self, Tensor other) -> Tensor` is symbolically differentiable.

Update: there're a lot of cases (fusions/ConstantChunk/python implementations) that breaks it so I decided to make the check optionally take node names if different from function name.
~~[OLD]Theoretically I could also check if `aten::max` is in the differentiable block or not to be more precise, but there're also cases like `chunk` where in a differentiable block it's replaced with a prim node (ConstantChunk) and we will have to special case them. Any suggestions here (to be more precise or no) is very welcome!~~

We used to have a list containing nn tests should be run against AD, I moved it to an field when constructing our test(either torch or nn). I think it's cleaner this way, and it matches the fact that for the same op we support one schema of it but not all, in this way we could just turn on the corresponding test which triggers that supported schema.

cc: apaszke zdevito wanchaol ngimel for a review

[Done] :
- Going through a manual second pass of all tests to check if they should enable AD test or not....
- Add a readme about how to add AD for an op and how to add/enable its test in test_jit.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18509

Differential Revision: D14696811

Pulled By: ailzhang

fbshipit-source-id: c5e693277baac585cd3aed5ab2c0e7faa5e6f29f",559.0,457.0,"test/common_methods_invocations.py,test/test_autograd.py,test/test_jit.py,torch/csrc/jit/README.md,torch/csrc/jit/init.cpp,torch/csrc/jit/symbolic_script.cpp",6.0,4,2,0.959666295,42.0,20383.0,4.0,314389.0,7789.0,23635.33333,0.0,Feature Addition,0.0,1
pytorch,3cba9e8aaa81009a724bcc8c2545c11e070b00ac,9c8f9f0ecb3f350cbd058f9cd30143de11177747,Roy Li,royboy@fb.com,Sun Jun 30 11:07:46 2019 -0700,1561892866.0,"Remove many usages of Type (#21941)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21941
ghimport-source-id: f20cca6229daba9eb8652adb3d959266ae081ef1

Test Plan: Imported from OSS

Differential Revision: D15893331

Pulled By: li-roy

fbshipit-source-id: c988b16008ff0e2725a88c6025afd4aabdaca45a",315.0,286.0,"aten/src/ATen/native/TypeProperties.cpp,aten/src/ATen/test/cuda_tensor_interop_test.cpp,aten/src/ATen/test/scalar_test.cpp,aten/src/ATen/test/tensor_interop_test.cpp,test/test_torch.py,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_torch_functions.cpp,tools/autograd/templates/python_torch_functions_dispatch.h,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/Generator.cpp,torch/csrc/Module.cpp,torch/csrc/autograd/VariableTypeManual.cpp,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_legacy_variable.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/tensor/python_tensor.h,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h,torch/csrc/utils/tensor_types.cpp,torch/csrc/utils/tensor_types.h",24.0,14,4,2.882994642,42.0,19730.0,21.0,2860698.1666666665,9697.0,28172.83333,0.0,,0.0,1
pytorch,7605d196fe21a03e4a1841af01ee1955dc52c114,9cb8b437786648dfcf1e6a60dced70b6ee983311,Sam Gross,colesbury@gmail.com,Tue Nov 14 17:59:06 2017 -0500,1510682346.0,"Split off in-place NN functions (#3683)

For example, this splits threshold into threshold(), which is now
never in-place, and threshold_() which is always in-place.

This simplifies the in-place vs. non-in-place logic in
gen_variable_type.py, which was bug-prone.",238.0,165.0,"aten/src/ATen/nn.yaml,aten/src/ATen/nn_parse.py,aten/src/ATen/preprocess_declarations.py,docs/source/nn.rst,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,torch/nn/functional.py",9.0,10,4,2.501076334,29.0,5912.0,8.0,526220.8888888889,328.0,925.9058694,0.0,Corrective,1.0,1
pytorch,00737efdb288f664018e1c9a8c960ea0bfcc94e6,9ccae891026eb69396ef9e2521d53e01731ef57b,albanD,desmaison.alban@gmail.com,Tue Apr 13 13:17:20 2021 -0700,1618319840.0,"port addcmul to OpInfo (#55517)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55517

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D27649413

Pulled By: albanD

fbshipit-source-id: e1faf25cf7f9c3636f62db1512aee78fd7c4f9b6",30.0,16.0,"test/test_autograd.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.689064156,45.0,20827.0,3.0,54067.66666666666,10698.0,23646.0,0.0,Feature Addition,0.0,1
pytorch,6a23214a47a3ee30e9129bad3b38af4c2081d1d4,9cfc10d52e0d0a8576b0a5a347fa6fa8da86244a,Mike Ruberry,mruberry@devfair044.maas,Fri May 15 23:22:03 2020 -0700,1589584923.0,"Updates assertEqual to use torch.isclose-like logic (#37294)

Summary:
Edit: this has been updated to reflect the PR's current status, which has changed after review.

This PR updates the behavior of the assertEqual, assertNotEqual, and assert_allclose to be consistent with each other and torch.isclose. It corrects several additional bugs in the current implementations and adds extensive testing and comments, too.

These updates follow from changes to assertEqual like https://github.com/pytorch/pytorch/pull/34258 and https://github.com/pytorch/pytorch/pull/37069, and from our discussion of torch.isclose for complex tensors (see https://github.com/pytorch/pytorch/issues/36462), where we decided to implement a NumPy-compatible mathematical notion of ""closeness"" for complex tensors that is not a great fit for our testing framework.

The detailed changelist is:

- New test framework functions for comparing tensors and scalars
  - Tensors are compared using isclose; the real and imaginary parts of complex tensors are compared independently
  - Scalars are compared using the same algorithm
  - assertEqual and assert_allclose now use this common comparison function, instead of each implementing their own with divergent behavior
  - assertEqual-like debug messages are now available for all tensor and scalar comparisons, with additional context when comparing the components of sparse, quantized, and complex tensors
- Extensive testing of the comparison behavior and debug messages
- Small Updates
  - assertEqual now takes an ""exact_device"" argument, analogous to ""exact_dtype"", which should be useful in multidevice tests
  - assertEqual now takes an ""equal_nan"" argument for argument consistency with torch.isclose
  - assertEqual no longer takes the ""allow_inf"" keyword, which misleadingly only applied to scalar comparisons, was only ever set (rarely) to true, and is not supported by torch.isclose
- Bug fixes:
  - the exact_dtype attribute has been removed (no longer needed after https://github.com/pytorch/pytorch/pull/38103)
  - message arguments passed to assertEqual are now handled correctly
  - bool x other dtype comparisons are now supported
  - uint8 and int8 tensor comparisons now function properly
  - rtol for integer comparisons is now supported (default is zero)
  - rtol and atol for scalar comparisons are now supported
  - complex scalar comparisons are now supported, analogous to complex tensor comparisons
  - assertNotEqual is now equivalent to the logical negation of assertEqual
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37294

Differential Revision: D21596830

Pulled By: mruberry

fbshipit-source-id: f2576669f7113a06f82581fc71883e6b772de19b",686.0,256.0,"test/test_autograd.py,test/test_distributions.py,test/test_nn.py,test/test_optim.py,test/test_torch.py,torch/testing/__init__.py,torch/testing/_internal/common_utils.py",7.0,4,2,1.903489716,45.0,44661.0,7.0,365961.8571428572,2125.0,5317.0,0.0,Corrective,1.0,1
pytorch,1c0a01e70965d208a5e6d4b1d6500c6845a12b23,9d17157fae0be67c589550d19034c1444cb442cf,Mike Ruberry,mruberry@devfair044.h1.fair,Fri Apr 15 23:39:50 2022 +0000,1650065990.0,"Adds OpInfos for l1_loss and smooth_l1_loss

Per title
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75877
Approved by: https://github.com/ngimel",50.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,16975.0,1.0,1917.0,2343.0,5494.5,0.0,Feature Addition,0.0,1
pytorch,"c0600e655a0360e3a1598416222b89a6d8be66fe,b5cf1d2fc71604f472a07d0181a05a7f09e276c2",9d2d88431395e40e77527a3b14a0a5ac7c5cc9e3,soumith,soumith@fb.com,Sat Dec 31 00:50:25 2016 -0800,1483145425.0,Merge commit 'b5cf1d2fc71604f472a07d0181a05a7f09e276c2',8.0,0.0,torch/lib/THCUNN/CMakeLists.txt,1.0,3,1,0,19.0,50.0,1.0,2642.0,207.0,11000.93299,0.0,,0.0,1
pytorch,cff4826f28f6267b6e6d1f207e06aac8160a6c77,9d5ac03b9a7ad238e45e5e8a4e00f26708cbb1c8,Pearu Peterson,pearu.peterson@gmail.com,Wed Mar 22 10:14:20 2023 +0200,1679480060.0,"Deprecate gradcheck check_sparse_nnz argument as duplicate of masked argument (#97187)

As in the title.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97187
Approved by: https://github.com/soulitzer",166.0,150.0,"test/test_autograd.py,test/test_sparse.py,torch/autograd/gradcheck.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,2,1.796899161,47.0,37741.0,4.0,461263.5,13598.0,31443.5,0.0,,0.0,1
pytorch,53083b83530441dcc08aad07886a72a04dda845e,9d6521c3a07392a291ef18a920a2bb2acf26dfc9,Gregory Chanan,gchanan@fb.com,Mon Jul 23 15:34:10 2018 -0700,1532360050.0,"Support n-dimensional empty tensors in CUDA non-reduction dimension fâ¦ (#9658)

Summary:
â¦unctions.

This also unifies the error checkign between scatter/scatterAdd on CUDA.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9658

Differential Revision: D8941527

Pulled By: gchanan

fbshipit-source-id: 750bbac568f607985088211887c4167b67be11ea",338.0,284.0,"aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/TensorTransformations.cu,aten/src/THC/THCTensorSort.cu,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THC/generic/THCTensorScatterGather.cu,aten/src/THC/generic/THCTensorSort.cu,aten/src/THC/generic/THCTensorTopK.cu,test/test_torch.py",9.0,8,2,2.537803125,40.0,11171.0,5.0,1178476.3333333333,3047.0,7358.833333,0.0,Feature Addition,0.0,1
pytorch,779b128872e0d887d05e103faf672fa0c0bbe285,9d9bc93bfba9328a419e533cd2c74b831f60bd61,anjali411,chourdiaanjali123@gmail.com,Thu Dec 19 20:58:59 2019 -0800,1576789139.0,"Added error message to indicate that reduction operations are not supported for dim>=64 (#31476)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/23159
Currently we don't support reduction operations for dim>=64 and we should give a descriptive RuntimeError indicating the same
Diff: D19179039
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31476

Differential Revision: D19179039

Pulled By: anjali411

fbshipit-source-id: 58568f64627bf3df6b3e00a1498544c030e74a0e",8.0,1.0,"aten/src/ATen/native/ReduceOps.cpp,test/test_torch.py",2.0,5,2,0.503258335,40.0,15833.0,2.0,252950.5,13944.0,38021.83333,0.0,Feature Addition,0.0,1
pytorch,"15bece50d10012dfe66238e588b4960cbc27beab,ae3a8d5d2eaa1b15d825b86ce706b046e68733b8",9da882e39632b4086c07bc9f0a20959713841384,Soumith Chintala,soumith@gmail.com,Wed Jul 19 16:21:52 2017 -0400,1500481312.0,Merge commit 'ae3a8d5d2eaa1b15d825b86ce706b046e68733b8',5.0,0.0,torch/lib/THCUNN/CMakeLists.txt,1.0,3,1,0,33.0,82.0,1.0,3511.0,1201.0,14419.11365,0.0,,0.0,1
pytorch,049dede3beb9f4839e035f31c04cc021b24e897c,9e016f77a8fc8c628f8a80f594cd071ae48ddf21,anjali411,chourdiaanjali123@gmail.com,Thu Apr 16 15:21:49 2020 -0700,1587050509.0,"Added complex types to get_all_dtypes and turned on masked_fill for complex (#36335)

Summary:
1. Added complex dtypes to get_all_dtypes to unify testing for complex dtypes with other dtypes so that they don't get out of sync with behavior supported for other dtypes.
2. resolves https://github.com/pytorch/pytorch/issues/36322, https://github.com/pytorch/pytorch/issues/36327
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36335

Differential Revision: D21045603

Pulled By: anjali411

fbshipit-source-id: 5089306b66fdc18148e831f56298da5de673be67",53.0,22.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cuda/TriangularOps.cu,test/test_torch.py,torch/testing/__init__.py",5.0,9,3,0.88542019,41.0,18676.0,4.0,3105678.2,1117.0,2923.0,0.0,Feature Addition,0.0,1
pytorch,907fc8822cd8acc691cf48c7d063374acf0335cf,9e6a6f5d274b5e2bfe5c7a8c94aeddfcec39c8f2,Richard Zou,zou3519@users.noreply.github.com,Thu Nov 18 13:35:59 2021 -0500,1637242559.0,[functorch] copy batch rule (pytorch/functorch#278),20.0,0.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/test/test_vmap.py",2.0,4,1,0.721928095,1.0,3653.0,2.0,2.5,545.0,756.0,0.0,,0.0,1
pytorch,1f7309dfe3e4fe056e366930af05af63b371eed2,9e6b7e6e6e10ad0a6f42b6d73e16158db2e95f95,kshitij12345,kshitijkalambarkar@gmail.com,Fri May 07 09:49:06 2021 -0700,1620380946.0,"OpInfo: expand and expand_as (#57606)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57606

Reviewed By: albanD

Differential Revision: D28249191

Pulled By: mruberry

fbshipit-source-id: d985ab4e8a99b116c45953e621092929a9a8028e",58.0,8.0,"test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.390452131,2.0,10580.0,3.0,161758.0,11768.0,26547.0,0.0,,0.0,1
pytorch,a83171378611a900e10b2fb02324fd03a238282f,9e97ccbd7a84a891be2d6ffaa6b14a04e5877d6c,Eli Uriegas,eliuriegas@fb.com,Tue Nov 02 21:37:07 2021 -0700,1635889027.0,".github: Migrate iOS workflows to GHA (#67645)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67645

Signed-off-by: Eli Uriegas <eliuriegas@fb.com>

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D32104367

Pulled By: seemethere

fbshipit-source-id: 08ff043ed5d0b434322f1f3f20dce2a4f5fa88c1",1045.0,61.0,".circleci/config.yml,.circleci/generate_config_yml.py,.github/generated-ciflow-ruleset.json,.github/scripts/generate_ci_workflows.py,.github/templates/ios_ci_workflow.yml.j2,.github/workflows/generated-ios-12-5-1-arm64-coreml.yml,.github/workflows/generated-ios-12-5-1-arm64-custom-ops.yml,.github/workflows/generated-ios-12-5-1-arm64-full-jit.yml,.github/workflows/generated-ios-12-5-1-arm64-metal.yml,.github/workflows/generated-ios-12-5-1-arm64.yml,.github/workflows/generated-ios-12-5-1-x86-64-coreml.yml,.github/workflows/generated-ios-12-5-1-x86-64-full-jit.yml,.github/workflows/generated-ios-12-5-1-x86-64.yml,scripts/build_ios.sh",14.0,6,3,3.647407947,3.0,9630.0,3.0,493360.0,16779.0,39351.5,0.0,,0.0,1
pytorch,33cc1d6a64608e808e72343ef1fcca8c9c5dcdec,9ecaeb0962388f6148052c99e5ea7866410c1375,kshitij12345,kshitijkalambarkar@gmail.com,Wed Nov 25 21:16:27 2020 -0800,1606338987.0,"[numpy] Add unary-ufunc tests for `erf` variants (#47155)

Summary:
Adding Unary Ufunc Test entry for `erf` variants.

We use scipy functions for reference implementation.

We can later update the tests once these functions will update integer input to float.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47155

Reviewed By: ngimel

Differential Revision: D25176654

Pulled By: mruberry

fbshipit-source-id: cb08efed1468b27650cec4f87a9a34e999ebd810",26.0,5.0,"test/test_torch.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,1.081876149,43.0,23545.0,3.0,471622.0,6998.0,15839.0,0.0,Feature Addition,0.0,1
pytorch,a425e1cbf81eb995d4c77038095c398c6f200604,9ef8eb4cbcb3909e66cdc125cf00f7d1a471707b,Spandan Tiwari,sptiwari@microsoft.com,Thu Apr 25 23:24:10 2019 -0700,1556234650.0,"Fix case for `activations` attribute in nn.RNN ONNX export. (#19368)

Summary:
This PR addresses the https://github.com/pytorch/pytorch/issues/19366 issue.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19368

Reviewed By: zrphercule

Differential Revision: D15043949

Pulled By: houseroad

fbshipit-source-id: 9b90410307d31bc5f2fd14aa0cdd33b22572ed7c",5.0,2.0,"caffe2/python/onnx/backend.py,torch/onnx/symbolic.py",2.0,5,2,0.863120569,14.0,2701.0,2.0,3403410.0,8331.0,24905.33333,0.0,Corrective,1.0,1
pytorch,ca55468416ec96d31564304efe7a63bd92892e0b,9f44274373177b9f84e9aaeb466714adf75b2303,Michael Lazos,mlazos@fb.com,Wed Feb 14 07:45:16 2024 +0000,1707896716.0,"Add tests to verify disabled optimizers (#118919)

As title

Pull Request resolved: https://github.com/pytorch/pytorch/pull/118919
Approved by: https://github.com/janeyx99",307.0,76.0,"test/inductor/test_compiled_optimizers.py,torch/testing/_internal/common_optimizers.py",2.0,5,2,0.936263792,1.0,2476.0,2.0,650412.0,25168.0,56813.5,0.0,Feature Addition,0.0,1
pytorch,fc2ead094471dad946f9f44e903b02aea5324021,9f832c8d3ea31666d21c87b5fe4792a68a545036,kshitij12345,kshitijkalambarkar@gmail.com,Fri Jan 08 14:28:05 2021 -0800,1610116085.0,"[numpy] torch.exp: promote integer inputs to float (#50093)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50093

Reviewed By: H-Huang

Differential Revision: D25803549

Pulled By: mruberry

fbshipit-source-id: e6f245b5e728f2dca6072f8c359f03dff63aa14d",20.0,11.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,test/test_torch.py,test/test_unary_ufuncs.py,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py",6.0,12,3,2.039211537,44.0,14846.0,6.0,526515.6666666666,7954.0,17982.0,0.0,,0.0,1
pytorch,c28575a4ebdada363a27cd60d0ea4bd98f520180,9f89692dcdc7ddd5ce7f5394b1832eb9c95e64f7,Soumith Chintala,soumith@gmail.com,Fri Jan 20 21:56:37 2017 -0500,1484949397.0,adding documentation for some lapack functions (#528),322.0,22.0,torch/_torch_docs.py,1.0,1,1,0,3.0,3970.0,1.0,40796.0,144.0,882.6202836,0.0,Feature Addition,0.0,1
pytorch,71e772f8277e2eee2917e338ef0c4aaea6ebfe66,9f8ade04ccf9647bf7fe1eba906cfdb73d23e4b6,Yang Chen,yangche@fb.com,Thu Feb 08 11:16:52 2024 -0800,1707391012.0,"[aot_inductor] replace TORCH_CHECK with AOTI_CHECK in the generate cpp code (#119220)

In some cases where we have TORCH_CHECK in loops, it may cause the host
compiler to spend hours optimizing the run_impl function. This PR
mitigated the issue by replacing TORCH_CHECK with a custom AOTI_CHECK,
where we force the underneath assert function to be noinline.

If forcing noinline caused any serious perf regression, we could
either add an option to turn on/off enable noinline. Or, we could
another an option to just turn AOTI_CHECK into a no-op, similar
to the ```assert``` macro from cassert.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/119220
Approved by: https://github.com/hl475, https://github.com/desertfire",40.0,1.0,"torch/_inductor/codegen/cpp.py,torch/csrc/inductor/aoti_torch/c/shim.h,torch/csrc/inductor/aoti_torch/shim_common.cpp",3.0,7,1,1.314630224,3.0,5163.0,2.0,249035.0,24950.0,56342.0,0.0,Feature Addition,0.0,1
pytorch,e5752f2cb4cf86db370d6d9cb3d6158f04d3251c,9fefab5ac6b1c668157166cc07a8d7713ee4e9f7,Sam Gross,sgross@fb.com,Thu Oct 25 16:40:59 2018 -0700,1540485659.0,"Add support for reductions to TensorIterator (#11908)

Summary:
This adds support for reductions like sum() and mul() to TensorIterator.
Performance is similar to existing optimized code for CPU, and generally
better than existing code for CUDA kernels.

The templatized CUDA kernel requires fewer instantiations than the
existing THCReduce/THCReduceAll code. For example, sum() previously
generated 43 CUDA kernels, while it now requires only one (larger)
CUDA kernel. I suspect this should reduce code-size and
compilation time, but I haven't measured it.

Below are timings for sum() on [CPU](https://ark.intel.com/products/81908/Intel-Xeon-Processor-E5-2680-v3-30M-Cache-2_50-GHz) (12 threads and 1 thread) and CUDA with various tensor sizes.

CPU

| Reduction (dim)      | Master  | PR      | Master (1 thread) | PR (1 thread) |
|----------------------|---------|---------|-------------------|---------------|
| 1024x1024 (all)      | 22 us   | 34 us   | 136 us            | 147 us        |
| 1024x1024 (0)        | 30 us   | 28 us   | 160 us            | 160 us        |
| 1024x1024 (1)        | 25 us   | 25 us   | 171 us            | 146 us        |
| 1024x10x1024 (all)   | 542 us  | 550 us  | 4.14 ms           | 3.11 ms       |
| 1024x10x1024 (0)     | 658 us  | 690 us  | 6.80 ms           | 5.93 ms       |
| 1024x10x1024 (1)     | 761 us  | 757 us  | 3.34 ms           | 3.52 ms       |
| 1024x10x1024 (2)     | 538 us  | 545 us  | 3.73 ms           | 3.04 ms       |
| 1024x1024x1024 (all) | 72 ms   | 71 ms   | 364 ms            | 357 ms        |
| 1024x1024x1024 (0)   | 94 ms   | 90 ms   | 935 ms            | 927 ms        |
| 1024x1024x1024 (1)   | 80 ms   | 86 ms   | 881 ms            | 688 ms        |
| 1024x1024x1024 (2)   | 71 ms   | 71 ms   | 456 ms            | 354 ms        |

CUDA

| Reduction (dim)      | M40 base | M40 PR  | P100 base | P100 PR   |
|----------------------|----------|---------|-----------|-----------|
| 1024x10x1024 (all)   | 238 us   | 182 us  | 136 us    | 97 us     |
| 1024x10x1024 (0)     | 166 us   | 179 us  | 105 us    | 84 us     |
| 1024x10x1024 (1)     | 181 us   | 182 us  | 89 us     | 91 us     |
| 1024x10x1024 (2)     | 180 us   | 168 us  | 88 us     | 79 us     |
| 1024x1024x1024 (all) | 17.5 ms  | 16.4 ms | 8.23 ms   | 7.48 ms   |
| 1024x1024x1024 (0)   | 27.2 ms  | 28.6 ms | 7.63 ms   | 7.38 ms   |
| 1024x1024x1024 (1)   | 16.5 ms  | 16.3 ms | 7.66 ms   | 7.40 ms   |
| 1024x1024x1024 (2)   | 17.8 ms  | 16.4 ms | 8.37 ms   | 7.31 ms   |

Timings were generated with this script:
https://gist.github.com/colesbury/d3238b266d8a9872fe6f68f77619b379
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11908

Differential Revision: D10071760

Pulled By: colesbury

fbshipit-source-id: 40e37a0e6803f1628b94cc5a52a10dfbb601f3d6",1541.0,789.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/Parallel.h,aten/src/ATen/core/Macros.h,aten/src/ATen/core/Range.cpp,aten/src/ATen/core/Range.h,aten/src/ATen/cuda/Array.h,aten/src/ATen/cuda/CUDAContext.cpp,aten/src/ATen/cuda/CUDAContext.h,aten/src/ATen/cuda/detail/OffsetCalculator.cuh,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOps.h,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/TensorIteratorReduce.cpp,aten/src/ATen/native/cpu/Loops.h,aten/src/ATen/native/cpu/Reduce.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.h,aten/src/ATen/native/cuda/CUDAReduceOps.cpp,aten/src/ATen/native/cuda/Loops.cuh,aten/src/ATen/native/cuda/Reduce.cu,aten/src/ATen/native/cuda/Reduce.cuh,aten/src/ATen/native/cuda/ReduceOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,aten/src/THC/THCDeviceUtils.cuh,test/expect/TestJit.test_alexnet.expect,test/test_autograd.py,tools/autograd/derivatives.yaml",29.0,14,3,3.56403757,42.0,12764.0,13.0,1844938.65,4874.0,14360.83333,0.0,Feature Addition,0.0,1
pytorch,30e1c74dc19ae2b622b46ebcdb7972c42775ac80,a00d5878497af2a0bb599fe939369de49256b1ea,Philip Meier,github.pmeier@posteo.de,Wed Aug 18 14:36:22 2021 -0700,1629297382.0,"add `OpInfo` for `torch.linalg.tensorinv` (#62326)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/53739.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62326

Reviewed By: H-Huang

Differential Revision: D30136376

Pulled By: zou3519

fbshipit-source-id: 04ec9450e8866667649af401c7559b96ddc91491",32.0,6.0,"aten/src/ATen/native/LinearAlgebra.cpp,torch/testing/_internal/common_methods_invocations.py",2.0,7,2,0.789749254,8.0,11881.0,1.0,64367.0,14723.0,33735.0,0.0,Corrective,1.0,1
pytorch,822277f30247607064fca234c197cdf89c2aed0a,a015964cf8171395a2fab7ab7cd230c4e3b40e40,Pearu Peterson,pearu.peterson@gmail.com,Wed Oct 20 16:37:46 2021 -0700,1634747866.0,"Strided masked reduction: prod. (#66386)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66386

cc nikitaved pearu cpuhrsch

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D31779598

Pulled By: cpuhrsch

fbshipit-source-id: 304a3d6abc794a49de5b044aade6cfd727758495",47.0,0.0,"torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,1,0.999673426,2.0,11666.0,2.0,5966.0,16423.0,38536.5,0.0,,0.0,1
pytorch,991028cd9ffdf073f2be77438beff2dc0cab3fca,a029ec2c88edf430e682789684f1b27bfe607bf2,David Berard,dberard@fb.com,Mon Nov 28 20:19:25 2022 -0800,1669666765.0,"Move gpu slow tests to sm86 (#87880)

NVFuser tests (which are slow tests) would be better to run on more
modern GPU hardware.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87880
Approved by: https://github.com/malfet",40.0,4.0,".github/workflows/trunk.yml,test/test_torch.py,torch/testing/_internal/opinfo/definitions/_masked.py",3.0,8,3,1.047693201,44.0,10075.0,3.0,781175.6666666666,9982.0,22971.5,0.0,,0.0,1
pytorch,834bd3134eab322151a769589f6fb47c6c888342,a07ffe8d0ef782fda5e93e1da0019d25a0a35938,Saketh Are,saketh@fb.com,Fri Dec 03 05:21:27 2021 -0800,1638508887.0,"Add OpInfos for combinations, cartesian_prod, sum_to_size, ldexp, as_strided (#68853)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68853

Reviewed By: davidberard98

Differential Revision: D32811147

Pulled By: saketh-are

fbshipit-source-id: 941dcf949072f8d10faf4d5a0fa0ef409ac6a7db",163.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.132388383,2.0,16001.0,2.0,73000.5,17495.0,41147.5,0.0,Feature Addition,0.0,1
pytorch,49493948a8fa8f83cd12d0ecf93f2a8b235be449,a08091a42db2f6d1bf59a12f8ccedce0f5fd70ae,gchanan,gregchanan@gmail.com,Thu Apr 26 20:52:58 2018 -0400,1524775978.0,"Implement matmul_out and dot_out. (#6961)

* Implement matmul_out and dot_out.

* Fix autograd by only calling _out variants if we have an out ourselves.

* Disallow mismatched types in dot_out.

* Make sure out variant doesn't have a method.

* Do proper type conversion.",49.0,15.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py",3.0,5,2,1.049886625,39.0,8068.0,2.0,795328.0,983.0,2387.805292,0.0,Corrective,1.0,1
pytorch,f521823b7b4f2413599d71a2350feb8322f09a50,a08119afc27f44ff488cbf18c11659d255f378a8,Edward Yang,ezyang@fb.com,Thu Jul 19 21:06:53 2018 -0700,1532034413.0,"Eliminate direct access to size/strides of THTensor; replace them with std::vector (#9561)

Summary:
* THTensor now stores `sizes_` and `strides_` which is a `std::vector<int64_t>`
* Anywhere a ""public"" API function made use of a int64_t* of sizes, I opted to just finagle it out of the tensor using THTensor_getSizePtr rather than try to rewrite all of these sites to use ArrayRef. They should use ArrayRef eventually, but not yet.
* There are new utility functions for resizing sizes/strides in one go (THTensor_resizeDim), or replacing sizes and strides with completely new values (THTensor_setSizesAndStrides)
* Anywhere you said `t->size[n] = 0`, we now say `THTensor_setSizeAt(t, n, 0)`, ditto for strides
* Anywhere you said `t->size[n]`, we now say `t->size(n)` (coming soon: ditto for strides)

Previous review of just the `std::vector` change in #9518, but I'm planning to merge this all in one go.

Note for gchanan: review from commit ""ci"" and after
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9561

Reviewed By: cpuhrsch

Differential Revision: D8901926

Pulled By: ezyang

fbshipit-source-id: 483cf275060ab0a13845cba1ece39dd127142510",2174.0,2177.0,"CMakeLists.txt,aten/src/ATen/templates/TensorDense.cpp,aten/src/ATen/templates/TensorDerived.cpp,aten/src/README.md,aten/src/TH/THGeneral.cpp,aten/src/TH/THTensor.hpp,aten/src/TH/THTensorApply.h,aten/src/TH/THTensorDimApply.h,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensorConv.cpp,aten/src/TH/generic/THTensorFastGetSet.hpp,aten/src/TH/generic/THTensorLapack.cpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorRandom.cpp,aten/src/THC/THCTensor.cpp,aten/src/THC/THCTensorRandom.cuh,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensorIndex.cu,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMathBlas.cu,aten/src/THC/generic/THCTensorMathMagma.cu,aten/src/THC/generic/THCTensorMathPairwise.cu,aten/src/THC/generic/THCTensorMathReduce.cu,aten/src/THCUNN/generic/BatchNormalization.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/Im2Col.cu,aten/src/THCUNN/generic/IndexLinear.cu,aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/MultiMarginCriterion.cu,aten/src/THCUNN/generic/PReLU.cu,aten/src/THCUNN/generic/SparseLinear.cu,aten/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/SpatialAveragePooling.cu,aten/src/THCUNN/generic/SpatialConvolutionLocal.cu,aten/src/THCUNN/generic/SpatialConvolutionMM.cu,aten/src/THCUNN/generic/SpatialCrossMapLRN.cu,aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu,aten/src/THCUNN/generic/SpatialFullDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialMaxUnpooling.cu,aten/src/THCUNN/generic/SpatialReflectionPadding.cu,aten/src/THCUNN/generic/SpatialReplicationPadding.cu,aten/src/THCUNN/generic/SpatialSubSampling.cu,aten/src/THCUNN/generic/TemporalConvolution.cu,aten/src/THCUNN/generic/TemporalMaxPooling.cu,aten/src/THCUNN/generic/TemporalReflectionPadding.cu,aten/src/THCUNN/generic/TemporalReplicationPadding.cu,aten/src/THCUNN/generic/TemporalRowConvolution.cu,aten/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricFullDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricReplicationPadding.cu,aten/src/THNN/generic/Col2Im.c,aten/src/THNN/generic/FeatureLPPooling.c,aten/src/THNN/generic/Im2Col.c,aten/src/THNN/generic/IndexLinear.c,aten/src/THNN/generic/LookupTable.c,aten/src/THNN/generic/MultiLabelMarginCriterion.c,aten/src/THNN/generic/MultiMarginCriterion.c,aten/src/THNN/generic/PReLU.c,aten/src/THNN/generic/SparseLinear.c,aten/src/THNN/generic/SpatialAdaptiveAveragePooling.c,aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c,aten/src/THNN/generic/SpatialAveragePooling.c,aten/src/THNN/generic/SpatialConvolutionLocal.c,aten/src/THNN/generic/SpatialConvolutionMM.c,aten/src/THNN/generic/SpatialConvolutionMap.c,aten/src/THNN/generic/SpatialDilatedConvolution.c,aten/src/THNN/generic/SpatialDilatedMaxPooling.c,aten/src/THNN/generic/SpatialFullConvolutionMap.c,aten/src/THNN/generic/SpatialFullDilatedConvolution.c,aten/src/THNN/generic/SpatialMaxUnpooling.c,aten/src/THNN/generic/SpatialReflectionPadding.c,aten/src/THNN/generic/SpatialReplicationPadding.c,aten/src/THNN/generic/SpatialSubSampling.c,aten/src/THNN/generic/TemporalConvolution.c,aten/src/THNN/generic/TemporalMaxPooling.c,aten/src/THNN/generic/TemporalReflectionPadding.c,aten/src/THNN/generic/TemporalReplicationPadding.c,aten/src/THNN/generic/TemporalRowConvolution.c,aten/src/THNN/generic/TemporalSubSampling.c,aten/src/THNN/generic/VolumetricAdaptiveAveragePooling.c,aten/src/THNN/generic/VolumetricAdaptiveMaxPooling.c,aten/src/THNN/generic/VolumetricAveragePooling.c,aten/src/THNN/generic/VolumetricConvolution.c,aten/src/THNN/generic/VolumetricConvolutionMM.c,aten/src/THNN/generic/VolumetricDilatedConvolution.c,aten/src/THNN/generic/VolumetricDilatedMaxPooling.c,aten/src/THNN/generic/VolumetricFullDilatedConvolution.c,aten/src/THNN/generic/VolumetricMaxUnpooling.c,aten/src/THNN/generic/VolumetricReplicationPadding.c,cmake/public/utils.cmake,setup.py,torch/CMakeLists.txt",100.0,15,3,5.833968877,72.0,40933.0,23.0,1912380.71,3015.0,7184.833333,0.0,,0.0,1
pytorch,903ad90325507bb8856cfa932dd8d43e35b03fbd,a09c4d3997f2e57eedc22cb38c896f9e8f9a5607,Daya Khudia,dskhudia@fb.com,Tue Mar 10 23:48:48 2020 -0700,1583884128.0,"[pt][quant] Vectorized qmul and more methods on qint data types (#34376)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34376

Vectorized implementation of qmul. qmul is now ~16x faster on my development machine. This implementation works for qint8, quint8 and qint32. Also added some commonly used operations, such as multiply operator, requantize operation etc., to qint vector classes for future use.

```
#!/usr/bin/env python

import time
import torch
import torch.nn as nn
torch.set_num_threads(1)
# print(torch.__config__.parallel_info())

A = torch.rand(1, 54, 54, 256)
B = torch.rand(1, 54, 54, 256)

scale = .05
zero_point = 50

for dtype in [torch.quint8, torch.qint8]:

    qA = torch.quantize_per_tensor(A, scale=scale, zero_point=zero_point,
            dtype=dtype)
    qB = torch.quantize_per_tensor(B, scale=scale, zero_point=zero_point,
            dtype=dtype)

    NITER = 1000
    s = time.time()
    for i in range(NITER):
        out = torch.ops.quantized.mul(qA, qB, scale=scale, zero_point=zero_point)
    time_per_iter = (time.time() - s) / NITER

    print('dtype: {} time per iter ms: {:.3f}'.format(dtype, time_per_iter * 1000))
```
### Before
dtype: torch.quint8 time per iter ms: 6.714
dtype: torch.qint8 time per iter ms: 6.780

### After
dtype: torch.quint8 time per iter ms: 0.431
dtype: torch.qint8 time per iter ms: 0.417

### Test
Modified qmul tests to include qint8 and qint32 data types.

python test/test_quantized.py TestQuantizedOps.test_qmul_relu_same_qparams
python test/test_quantized.py TestQuantizedOps.test_qmul_relu_different_qparams
python test/test_quantized.py TestQuantizedOps.test_qmul_broadcast
ghstack-source-id: 99862681

Differential Revision: D20308515

fbshipit-source-id: 4fa65b2ba433cfd59260fc183a70f53a6fcc36b4",863.0,371.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qmul.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,aten/src/ATen/quantized/Quantizer.cpp,aten/src/ATen/quantized/Quantizer.h,test/test_quantized.py",7.0,11,2,1.236817403,1.0,5948.0,4.0,1646236.4285714286,15354.0,41097.83333,0.0,Feature Addition,0.0,1
pytorch,0557143801acdf9683034ced7d45b9794c318002,a0a2d9885ac44e6d8e48e2ebefefd48922d42014,soumith,soumith@fb.com,Thu Sep 15 17:34:23 2016 -0700,1473960863.0,adding docstrings for activation functions,455.0,163.0,"docs/docutils/doc2md.py,docs/nn.md,test/test_multiprocessing.py,torch/nn/modules/activation.py",26.0,6,3,1.246624802,4.0,1280.0,2.0,21557.5,159.0,810.969697,0.0,Feature Addition,0.0,1
pytorch,875d63ed04c7a26bb60c26fffbbf5c1da6dcecee,a0c9d70fba659be0cce04b6d83d8aa427a57724c,Freey0,freey7955@gmail.com,Tue Jul 20 15:58:51 2021 -0700,1626796731.0,"bitwise_and: Port to structured (#60813)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/60813

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D29449374

Pulled By: ezyang

fbshipit-source-id: d7e236ad841dcb9d5914352d117a34b10894bb91",17.0,27.0,"aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu,aten/src/ATen/native/native_functions.yaml,torch/testing/_internal/common_methods_invocations.py",6.0,9,2,1.745097218,12.0,21026.0,1.0,1.0,13964.0,31437.0,0.0,,0.0,1
pytorch,6d2e39559aa3e9aff9f81d42da5739b165e73c3e,a0ce84e476b8b3f2614085ad151ecb4c764b1aac,John Chiotellis,johny-c@users.noreply.github.com,Sat Oct 28 15:15:58 2017 +0200,1509203758.0,fix triplet margin loss documentation (#3339),2.0,2.0,"torch/nn/functional.py,torch/nn/modules/loss.py",2.0,3,1,1,36.0,2218.0,2.0,74341.0,783.0,6481.172317,0.0,Corrective,1.0,1
pytorch,b08a186153ee39d3db28d40b707dd0c793661653,a0def0b57e56d3476263aebe82e11d73f5677941,Brian Vaughan,bvaughan@fb.com,Wed Nov 28 04:31:18 2018 -0800,1543379478.0,"check for invalid ranges in torch.arange

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/13915

Differential Revision: D13222110

Pulled By: nairbv

fbshipit-source-id: fcff1ad058fbf792d0fdf4aa75d77f22e3b7483b",34.0,2.0,"aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/generic/THCTensorMath.cu,test/test_torch.py",3.0,7,2,1.468815341,40.0,12343.0,3.0,452889.3333333333,5651.0,17174.83333,0.0,,0.0,1
pytorch,60f4d285aff3fe6fa850a263e9a63a2a57d411ab,a0fb1ab86e88d5c98733d7e6e5aa3b5811fe24f4,Adam Paszke,adam.paszke@gmail.com,Tue Sep 13 22:09:46 2016 -0700,1473804586.0,Reduce precision for addmm and rsqrt CUDA tests,12.0,5.0,test/test_cuda.py,1.0,1,1,0,6.0,304.0,1.0,16.0,164.0,2948.032937,0.0,Feature Addition,0.0,1
pytorch,b18c298f24452edee16591ec08343f09bfa15a40,a0fc14c20f2cfb07b85e534b41a6f228f8d4861e,Nikita Shulga,nshulga@fb.com,Fri Oct 22 20:36:41 2021 -0700,1634935001.0,"[ONNX] Add diagonal symbolic (#64454) (#66144)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66144

* Add logic and tests

* minor edits

* Eliminate expand ops

* Fix flake and editing

* Modified errant message

* Add overrun check

* Add overrun descriptions

* Remove emptyline

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D31424095

fbshipit-source-id: 5b8ef6ac21c32d43c3dbc8e51e1ef30bffb19c25",144.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset13.py",2.0,4,2,0.966773137,3.0,10477.0,2.0,4.0,16503.0,38690.5,0.0,Corrective,1.0,1
pytorch,8c8dc791ef3da241bf8c7122372a26abf3cbe60f,a1534cc37dbb5a3bfe077cb138a7507b5a2d8ef5,Adam Paszke,adam.paszke@gmail.com,Sun Feb 12 01:30:02 2017 +0100,1486863002.0,Fix auto-gpu in cat,28.0,17.0,"test/test_cuda.py,torch/csrc/generic/methods/Tensor.cwrap",2.0,5,2,0.764204507,22.0,1393.0,1.0,272359.0,462.0,4379.116645,0.0,Corrective,1.0,1
pytorch,99b842d62a7a717b66115792802b4df5f319898e,a1b35d830abfa35e5ffa706f30dd5c261a590817,Richard Zou,zou3519@users.noreply.github.com,Wed Dec 22 16:01:56 2021 -0500,1640188916.0,"[functorch] Fix getitem (pytorch/functorch#364)

Fixes https://github.com/pytorch/functorch/issues/363

This PR:
- adds a batch rule for _index_put_impl_
- fixes the index_put_ batch rule
- adds a new OpInfo so we can actually test this
- fixes the fallback paths to error out on Tensor?[], otherwise they are
very wrong.",97.0,6.0,"functorch/functorch/csrc/BatchRulesScatterOps.cpp,functorch/functorch/csrc/BatchedFallback.cpp,functorch/test/functorch_additional_op_db.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",5.0,4,1,1.593842329,1.0,5806.0,4.0,1.2,680.0,928.0,0.0,Corrective,1.0,1
pytorch,086ce765a594c259e9519082270d9b22881fea44,a1c46e5f8fe64530af2cc8ee4526855faf14347b,Michael Lazos,mlazos@fb.com,Sat Mar 18 04:17:31 2023 +0000,1679113051.0,"component-level configurable logging for dynamo, inductor, aot (#94858)

Summary:

Adds NNC-like logging that is configured through an env var `TORCH_COMPILE_LOGS`
Examples:
`TORCH_LOGS=""dynamo,guards"" python script.py` - prints dynamo logs at level INFO with guards of all functions that are compiled

`TORCH_LOGS=""+dynamo,guards,graph"" python script.py` - prints dynamo logs at level DEBUG with guards and graphs (in tabular) format of all graphs that are compiled

[More examples with full output](https://gist.github.com/mlazos/b17f474457308ce15e88c91721ac1cce)

Implementation:
The implementation parses the log settings from the environment, finds any components (aot, dynamo, inductor) or other loggable objects (guards, graph, etc.) and generates a log_state object. This object contains all of the enabled artifacts, and a qualified log name -> level mapping. _init_logs then adds handlers to the highest level logs (the registered logs), and sets any artifact loggers to level DEBUG if the artifact is enabled.

Note: set_logs is an alternative for manipulating the log_state, but if the environment contains TORCH_LOGS, the environment settings will be prioritized.

Adding a new log:
To add a new log, a dev should add their log name to torch._logging._registrations (there are examples there already).

Adding a new artifact:
To add a new artifact, a dev should add their artifact name to torch._logging._registrations as well.
Additionally, wherever the artifact is logged, `torch._logging.getArtifactLogger(__name__, <artifact_name>)` should be used instead of the standard logging implementation.

[design doc](https://docs.google.com/document/d/1ZRfTWKa8eaPq1AxaiHrq4ASTPouzzlPiuquSBEJYwS8/edit#)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94858
Approved by: https://github.com/ezyang",909.0,157.0,".lintrunner.toml,test/dynamo/test_logging.py,test/functorch/test_logging.py,torch/__init__.py,torch/_dynamo/config.py,torch/_dynamo/convert_frame.py,torch/_dynamo/exc.py,torch/_dynamo/logging.py,torch/_dynamo/output_graph.py,torch/_dynamo/utils.py,torch/_functorch/aot_autograd.py,torch/_inductor/codegen/triton.py,torch/_inductor/debug.py,torch/_inductor/graph.py,torch/_logging/__init__.py,torch/_logging/_internal.py,torch/_logging/_registrations.py,torch/testing/_internal/logging_utils.py",18.0,11,2,2.888875105,44.0,11848.0,9.0,298196.5,13486.0,31242.5,0.0,Corrective,1.0,1
pytorch,dbfee42a7db8f6ac01b106e28acfe2c0f4b5f56b,a1db5b0f2bcd96f1e884ff63ebcf2b01f543a77a,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Wed Nov 11 18:54:29 2020 -0800,1605120869.0,"Added CUDA support for complex input for torch.inverse #2 (#47595)

Summary:
`torch.inverse` now works for complex inputs on GPU.
Opening a new PR here. The previous PR was merged and reverted due to a bug in tests marked with `slowTest`.
Previous PR https://github.com/pytorch/pytorch/pull/45034

Ref. https://github.com/pytorch/pytorch/issues/33152

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47595

Reviewed By: navahgar

Differential Revision: D24840955

Pulled By: anjali411

fbshipit-source-id: ec49fffdc4b3cb4ae7507270fa24e127be14f59b",499.0,161.0,"aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cuda/CUDABlas.h,aten/src/ATen/cuda/CUDASolver.cpp,aten/src/ATen/cuda/CUDASolver.h,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu,test/test_autograd.py,test/test_linalg.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",13.0,12,4,2.959975192,46.0,47757.0,6.0,281015.53846153844,6662.0,15148.0,0.0,Corrective,1.0,1
pytorch,3c2ecc6b15d3deeabd5905849d242bbee2264eee,a1fa99504430dab8cbc74bf921cd8ad2aab80078,Adam Paszke,adam.paszke@gmail.com,Thu Jan 26 03:21:49 2017 +0100,1485400909.0,"Fixes and improvements (#593)

* Fix error in ELU backward

* Add --seed flag for testst st

* Add test for BatchNorm eval

* Fix autograd.backward docs

* Support cc flags in cuDNN search

* Fix IndexSelect backward formula",116.0,57.0,"test/common.py,test/run_test.sh,test/test_autograd.py,test/test_cuda.py,test/test_dataloader.py,test/test_legacy_nn.py,test/test_multiprocessing.py,test/test_nccl.py,test/test_nn.py,test/test_optim.py,test/test_sparse.py,test/test_torch.py,test/test_utils.py,tools/setup_helpers/cudnn.py,torch/autograd/__init__.py,torch/autograd/_functions/tensor.py,torch/nn/_functions/thnn/auto.py",17.0,9,3,3.337316446,22.0,10186.0,5.0,169613.0,395.0,4931.476424,0.0,Corrective,1.0,1
pytorch,c6f41ae01b2014e4001996b30fd0f429002ecb87,a201027e9327b8682ac841802326a6fede00564a,Johannes M Dieterich,Johannes.Dieterich@amd.com,Fri Jan 10 17:46:52 2020 -0800,1578678412.0,"Abstract atomic add calls (#31992)

Summary:
Instead of a mixture of direct calls to library provided atomicAdd calls, such as float atomicAdd(float*, float) and calls provided internally, such as void atomicAdd(long*, long), abstract to one API void gpuAtomicAdd(T*, T) in THCAtomics.cuh for the PyTorch backend.

The advantage of this approach is that it allows us to more easily distinguish between capabiltiies of different platforms (and their versions). Additionally, the abstraction of void returning atomicAdds allows us to, in the future, support fast HW instructions on some platforms that will not return the previous value.

Call sites that do not satisfy above conditions and are either highly platform specific (__half2 atomicAdd fast path in one operator) or require the return explicitly (some int atomicAdd invocations) are left untouched. The Caffe2 backend also remains untouched.

While here, add a bunch of includes of THCAtomics.cuh that were missing before.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31992

Differential Revision: D19330220

Pulled By: ezyang

fbshipit-source-id: d6ab73ec5168c77e328faeef6c6f48eefba00861",114.0,52.0,"aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu,aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu,aten/src/ATen/native/cuda/AveragePool3d.cu,aten/src/ATen/native/cuda/DilatedMaxPool3d.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/FractionalMaxPool2d.cu,aten/src/ATen/native/cuda/FractionalMaxPool3d.cu,aten/src/ATen/native/cuda/GridSampler.cuh,aten/src/ATen/native/cuda/KernelUtils.cuh,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/ReflectionPad.cu,aten/src/ATen/native/cuda/ReplicationPadding.cu,aten/src/ATen/native/cuda/SortingRadixSelect.cuh,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/UpSample.cuh,aten/src/ATen/native/cuda/UpSampleLinear1d.cu,aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu,aten/src/THC/THCAtomics.cuh,aten/src/THC/THCTensorIndex.cu,aten/src/THC/THCTensorScatterGather.cu,aten/src/THCUNN/SpatialClassNLLCriterion.cu,docs/cpp/source/notes/tensor_basics.rst",24.0,11,2,3.75047094,6.0,10126.0,15.0,5371426.291666667,14163.0,38434.83333,0.0,Feature Addition,0.0,1
pytorch,c9bb811d6a12fa50362d6506eaf186c92bb21132,a20ac05c8b18ff0f07a46bc8f9002baa3d7a17c1,Justus Schwabedal,jusjusjus@users.noreply.github.com,Wed Jan 10 20:42:37 2018 +0100,1515616957.0,Added method cuda to PackedSequence. (#4430),111.0,3.0,"test/test_nn.py,torch/nn/utils/rnn.py",2.0,4,2,0.999111732,37.0,5586.0,1.0,119362.0,906.0,6710.172317,0.0,Feature Addition,0.0,1
pytorch,a63524684d02131aef4f2e9d2cea7bfe210abc96,a229b4526f41eed34a871e488f5c86d43f63d013,Xuehai Pan,XuehaiPan@pku.edu.cn,Thu Feb 09 20:16:46 2023 +0000,1675973806.0,"[BE] Prefer dash over underscore in command-line options (#94505)

Preferring dash over underscore in command-line options. Add `--command-arg-name` to the argument parser. The old arguments with underscores `--command_arg_name` are kept for backward compatibility.

Both dashes and underscores are used in the PyTorch codebase. Some argument parsers only have dashes or only have underscores in arguments. For example, the `torchrun` utility for distributed training only accepts underscore arguments (e.g., `--master_port`). The dashes are more common in other command-line tools. And it looks to be the default choice in the Python standard library:

`argparse.BooleanOptionalAction`: https://github.com/python/cpython/blob/4a9dff0e5adc91cbb1ed68c495dac64ccfe608bd/Lib/argparse.py#L893-L895

```python
class BooleanOptionalAction(Action):
    def __init__(...):
            if option_string.startswith('--'):
                option_string = '--no-' + option_string[2:]
                _option_strings.append(option_string)
```

It adds `--no-argname`, not `--no_argname`. Also typing `_` need to press the shift or the caps-lock key than `-`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/94505
Approved by: https://github.com/ezyang, https://github.com/seemethere",631.0,456.0,"benchmarks/distributed/rpc/parameter_server/launcher.py,benchmarks/distributed/rpc/rl/README.md,benchmarks/distributed/rpc/rl/launcher.py,benchmarks/dynamo/common.py,benchmarks/dynamo/distributed.py,benchmarks/dynamo/runner.py,benchmarks/dynamo/test.py,benchmarks/fastrnns/bench.py,benchmarks/fastrnns/profile.py,benchmarks/fastrnns/test.py,benchmarks/framework_overhead_benchmark/framework_overhead_benchmark.py,benchmarks/instruction_counts/execution/work.py,benchmarks/instruction_counts/worker/main.py,benchmarks/operator_benchmark/README.md,benchmarks/operator_benchmark/benchmark_runner.py,benchmarks/profiler_benchmark/profiler_bench.py,benchmarks/record_function_benchmark/record_function_bench.py,benchmarks/sparse/dlmc/README.md,benchmarks/sparse/dlmc/matmul_bench.py,benchmarks/sparse/dlmc/test.sh,benchmarks/sparse/spmm.py,benchmarks/sparse/spmv.py,benchmarks/sparse/test_csr.sh,benchmarks/tensorexpr/HowToRun.md,benchmarks/tensorexpr/__main__.py,benchmarks/tensorexpr/microbenchmarks.py,benchmarks/transformer/better_transformer_vs_mha_functional.py,benchmarks/transformer/sdp.py,benchmarks/upload_scribe.py,binaries/bench_gen/bench_gen.py,docs/source/elastic/quickstart.rst,docs/source/elastic/train_script.rst,functorch/examples/dp_cifar10/cifar10_opacus.py,functorch/examples/dp_cifar10/cifar10_transforms.py,functorch/examples/maml_omniglot/maml-omniglot-higher.py,functorch/examples/maml_omniglot/maml-omniglot-ptonly.py,functorch/examples/maml_omniglot/maml-omniglot-transforms.py,scripts/release_notes/commitlist.py,test/backends/xeon/test_launch.py,test/distributed/launcher/api_test.py,test/distributed/launcher/bin/test_script.py,test/distributed/launcher/bin/test_script_init_method.py,test/distributed/launcher/bin/test_script_is_torchelastic_launched.py,test/distributed/launcher/bin/test_script_local_rank.py,test/distributed/launcher/launch_test.py,test/distributed/launcher/run_test.py,test/distributed/test_launcher.py,test/edge/CMakeLists.txt,test/test_jit_fuser.py,test/test_jit_fuser_legacy.py,test/test_jit_legacy.py,test/test_jit_profiling.py,test/test_jit_simple.py,tools/code_analyzer/gen_operators_yaml.py,tools/code_analyzer/gen_oplist.py,tools/generate_torch_version.py,tools/jit/gen_unboxing.py,tools/jit/test/test_gen_unboxing.py,tools/linter/adapters/clangtidy_linter.py,tools/linter/clang_tidy/generate_build_files.py,tools/lite_interpreter/gen_selected_mobile_ops_header.py,tools/onnx/update_default_opset_version.py,tools/setup_helpers/generate_code.py,tools/substitute.py,torch/CMakeLists.txt,torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md,torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_disk_savings.py,torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_forward_time.py,torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_model_metrics.py,torch/autograd/profiler.py,torch/backends/xeon/run_cpu.py,torch/csrc/jit/tensorexpr/codegen_external.py,torch/distributed/elastic/agent/server/local_elastic_agent.py,torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py,torch/distributed/launch.py,torch/distributed/launcher/api.py,torch/distributed/run.py,torch/fx/passes/splitter_base.py,torch/testing/_internal/codegen/random_topo_test.py,torch/testing/_internal/common_utils.py,torch/utils/_freeze.py,torch/utils/_zip.py,torch/utils/benchmark/examples/blas_compare.py,torch/utils/benchmark/examples/end_to_end.py,torch/utils/benchmark/examples/spectral_ops_fuzz_test.py,torch/utils/benchmark/utils/valgrind_wrapper/timer_callgrind_template.cpp,torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py,torchgen/gen.py,torchgen/gen_backend_stubs.py,torchgen/gen_executorch.py,torchgen/gen_lazy_tensor.py",91.0,75,9,5.354866021,16.0,32415.0,68.0,25979411.153846152,12275.0,28744.0,0.0,Feature Addition,0.0,1
pytorch,769abddfa3873cfe41514a466c85089838357ce7,a23009f98fb58995aa61808a443566e0f2b68a97,Zafar Takhirov,cc.rafaz@zafar.cc,Wed Feb 12 01:54:12 2020 -0800,1581472452.0,"Quantized leaky relu

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33004

Test Plan: Imported from OSS

Differential Revision: D19740193

Pulled By: z-a-f

fbshipit-source-id: 32542d5465db44190366a2f8b737305a03b5fa76",138.0,0.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qrelu.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py,torch/nn/quantized/functional.py",6.0,11,3,2.252863236,10.0,10422.0,4.0,1756544.1666666667,14681.0,39514.83333,0.0,,0.0,1
pytorch,1250acef90a3387febb2d70ec643038270c6ef84,a24291a5549571d8a53c9f7572984ec18540025c,Negin Raoof,neginmr@utexas.edu,Mon Oct 07 19:49:45 2019 -0700,1570477785.0,"Unfold export (#24970)

Summary:
ONNX export for Unfold in symbolic opset9 + op and ORT tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24970

Reviewed By: hl475

Differential Revision: D17495106

Pulled By: houseroad

fbshipit-source-id: fcd179a1213c0f219628f25c09e66fcfe4c5df50",148.0,1.0,"test/onnx/expect/TestOperators.test_unfold.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",4.0,5,2,0.956199046,4.0,4320.0,2.0,292544.0,12069.0,33841.33333,0.0,,0.0,1
pytorch,1d5af9599dcfd5b1e9688ff0bfc66f0e44589123,a2463cbc384520288f75cc8ef6fe8fb42071a5cd,Daya Khudia,dskhudia@fb.com,Thu Dec 12 23:51:32 2019 -0800,1576194692.0,"Adding quantized clamp kernel (#30541)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30541

ghstack-source-id: 95450749

Adding quantized clamp kernel

Test Plan:
Added test.

buck test mode/dev //caffe2/test:quantized -- 'test_qclamp \(test_quantized\.TestQuantizedOps\)' --print-passing-details

Differential Revision: D18739628

fbshipit-source-id: 38a029ab96c5b0689bb15c67dc4f274883e74975",228.0,0.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qclamp.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py,torch/nn/quantized/functional.py",7.0,13,3,2.335888607,10.0,10785.0,4.0,1506858.6666666667,13831.0,37711.33333,0.0,Feature Addition,0.0,1
pytorch,bf1df4f3af8ffcc91e081d49c619488eb9c3ce2e,a24930ba933cae0acbb9fc266a0035f01ec810de,Richard Zou,zou3519@gmail.com,Thu May 20 15:41:34 2021 -0700,1621525294.0,"[functorch] Fix being unable to call .backward() after vmap

Fixes pytorch/functorch#37",30.0,3.0,"functorch/functorch/__init__.py,functorch/functorch/_src/vmap.py,functorch/functorch/csrc/DynamicLayer.cpp,functorch/functorch/csrc/DynamicLayer.h,functorch/functorch/csrc/init.cpp,functorch/test/test_vmap.py",6.0,5,1,1.970964483,1.0,3837.0,4.0,0.0,100.0,207.0,0.0,Corrective,1.0,1
pytorch,e3d5826b929559092cb266404f9757cbbb23b027,a24db91a384db1d10e463c6c15caa84cb9b25e48,Francisco Massa,fvsmassa@gmail.com,Sun Jun 11 07:07:48 2017 +0100,1497164868.0,"Add SELU activation function (#1769)

* Add SELU activation function

* Remove unnecessary case

* Add Function for SELU + tests and fix RReLU inplace

* Fix extra line in doc

* Fix tests

Remove in-place tests for RReLU. For some reason they fail on legacy nn, but passes on nn

* SELU in new-style Function

It also supports double backprop, verifyed with gradgradcheck

* Fix flake8",111.0,14.0,"test/test_nn.py,torch/nn/_functions/thnn/activation.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py",5.0,6,2,1.825243686,30.0,4938.0,1.0,303181.0,852.0,10462.21462,0.0,Corrective,1.0,1
pytorch,4fcd1c3123f54b0e587436f2eee2bd54d6d73ba1,a25062ab50bf7633a94c56d3fd11c00f4c5fb8f6,Nick Gibson,nickg@fb.com,Wed May 27 01:56:17 2020 -0700,1590544577.0,"[TensorExpr] Fix elimination of For loops with empty bodies (#38883)

Summary:
We do try to eliminate empty For loops, but missed a case where the body Block exists but is empty. In that case we can eliminate the loop as well.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38883

Differential Revision: D21723680

Pulled By: nickgg

fbshipit-source-id: 49610b0524af5b9ec30ef3b4cc0c8461838259c3",24.0,0.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",3.0,7,2,1.043435696,2.0,3941.0,2.0,1219458.0,2338.0,5925.0,0.0,Corrective,1.0,1
pytorch,f6c708f86990f3fbae2558bbd6363f74782dfda4,a2641500bf126082e3466271f0b7c225ad050528,Sam Gross,colesbury@gmail.com,Mon Mar 12 20:20:40 2018 -0400,1520886040.0,"Implement torch.reshape and Tensor.reshape (#5575)

* Implement torch.reshape and Tensor.reshape

This implements reshape which has similar semantics to numpy.reshape. It
will return a view of the source tensor if possible. Otherwise, it
returns a copy.

* Remove in-place reshape_ that was an alias for resize_

* Update documentation",173.0,15.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/lib/THD/master_worker/worker/dispatch/Tensor.cpp",10.0,13,4,2.303309781,39.0,23402.0,9.0,1099015.6,2452.0,24782.35823,0.0,Non Functional,0.0,1
pytorch,5455ba634cfdab822eface6b67c903616da68691,a27fdfd38c98e57da1b52dbac8f95f0f0931601d,James Reed,jamesreed@fb.com,Sat Aug 31 00:20:36 2019 -0700,1567210836.0,"Vectorized quantized relu/relu6 (#25496)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25496

Benchmark Script

```
import torch, time

sizes = [
    (1, 56, 56, 256),
    (1, 28, 28, 512),
    (1, 14, 14, 1024),
    (1, 7, 7, 2048),
]

NITER = 1000

for dtype in [torch.qint8, torch.quint8, torch.qint32]:
    print('*****', str(dtype), '*****')

    print('\t*****relu*****')
    print('\tsize',
          'time (float ms)',
          'time (quant ms)',
          'quant / float',
          sep='\t')
    for size in sizes:
        # NHWC
        x = torch.rand(*size)

        # NCHW
        x = x.permute([0, 2, 3, 1])

        # Test float
        s = time.time()
        for i in range(NITER):
            torch.relu(x)
        time_per_iter_float = (time.time() - s) / NITER

        # Test quantized
        q_x = torch.quantize_linear(x, 0.5, 1, dtype)

        s = time.time()
        for i in range(NITER):
            torch.relu(q_x)
        time_per_iter_quant = (time.time() - s) / NITER

        print('\t',
              size,
              time_per_iter_float * 1000,
              time_per_iter_quant * 1000,
              time_per_iter_quant / time_per_iter_float,
              sep='\t')

    print('\t*****relu6*****')
    print('\tsize',
          'time (float ms)',
          'time (quant ms)',
          'quant / float',
          sep='\t')
    for size in sizes:
        # NHWC
        x = torch.rand(*size)

        # NCHW
        x = x.permute([0, 2, 3, 1])

        # Test float relu6
        s = time.time()
        for i in range(NITER):
            torch._C._nn.hardtanh(x, 0., 6.)
        time_per_iter_float_6 = (time.time() - s) / NITER

        # Test quantized relu6
        q_x = torch.quantize_linear(x, 0.5, 1, dtype)

        s = time.time()
        for i in range(NITER):
            torch.ops.quantized.relu6(q_x)
        time_per_iter_quant_6 = (time.time() - s) / NITER

        print('\t',
              size,
              time_per_iter_float_6 * 1000,
              time_per_iter_quant_6 * 1000,
              time_per_iter_quant_6 / time_per_iter_float_6,
              sep='\t')

```

Before this change (AVX2)

```
$ OMP_NUM_THREADS=1 python relu_bench.py
***** torch.qint8 *****
	*****relu*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	0.28845906257629395	0.32473158836364746	1.1257458353479874
		(1, 28, 28, 512)	0.12658190727233887	0.1621997356414795	1.2813816692816096
		(1, 14, 14, 1024)	0.060466766357421875	0.08151435852050781	1.3480852943031985
		(1, 7, 7, 2048)	0.021933555603027344	0.04172706604003906	1.9024305404582809
	*****relu6*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	1.0264298915863037	0.4686436653137207	0.45657640054641424
		(1, 28, 28, 512)	0.4577608108520508	0.23253798484802246	0.5079901541051298
		(1, 14, 14, 1024)	0.22967290878295898	0.11695981025695801	0.509245129853278
		(1, 7, 7, 2048)	0.12731575965881348	0.060141801834106445	0.4723830105187069
***** torch.quint8 *****
	*****relu*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	0.28515172004699707	0.32268643379211426	1.1316306762551913
		(1, 28, 28, 512)	0.1268613338470459	0.1618938446044922	1.2761480562681475
		(1, 14, 14, 1024)	0.06022787094116211	0.08164644241333008	1.355625578946535
		(1, 7, 7, 2048)	0.018331527709960938	0.04460000991821289	2.432967433149516
	*****relu6*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	1.027123212814331	0.5206699371337891	0.50692062124382
		(1, 28, 28, 512)	0.4589383602142334	0.25958728790283203	0.565625605542444
		(1, 14, 14, 1024)	0.23261427879333496	0.13058066368103027	0.561361341867771
		(1, 7, 7, 2048)	0.13072657585144043	0.06684517860412598	0.5113358027528374
***** torch.qint32 *****
	*****relu*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	0.285900354385376	0.44794583320617676	1.5667900593168678
		(1, 28, 28, 512)	0.12691712379455566	0.21081137657165527	1.6610160258035915
		(1, 14, 14, 1024)	0.05957603454589844	0.10731720924377441	1.8013486473507283
		(1, 7, 7, 2048)	0.01675701141357422	0.05678510665893555	3.388737123669683
	*****relu6*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	1.0314903259277344	0.6447939872741699	0.6251090980366052
		(1, 28, 28, 512)	0.4572310447692871	0.3106963634490967	0.6795172090859886
		(1, 14, 14, 1024)	0.2294166088104248	0.1586904525756836	0.6917130080447454
		(1, 7, 7, 2048)	0.12760710716247559	0.07992196083068848	0.6263127705647926

```

After this change (AVX2)

```
$ OMP_NUM_THREADS=1 python relu_bench.py

***** torch.qint8 *****
	*****relu*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	0.2889232635498047	0.06460881233215332	0.22361928056034167
		(1, 28, 28, 512)	0.13853216171264648	0.013955354690551758	0.10073729102343015
		(1, 14, 14, 1024)	0.0721442699432373	0.007253408432006836	0.10054032617855548
		(1, 7, 7, 2048)	0.015225648880004883	0.004289150238037109	0.28170557930505313
	*****relu6*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	1.042311191558838	0.06422209739685059	0.061615089540392104
		(1, 28, 28, 512)	0.46384429931640625	0.01335287094116211	0.028787399049295198
		(1, 14, 14, 1024)	0.2301616668701172	0.007760286331176758	0.033716675920477994
		(1, 7, 7, 2048)	0.12573981285095215	0.004631757736206055	0.03683604763827976
***** torch.quint8 *****
	*****relu*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	0.2877991199493408	0.0571134090423584	0.1984488661828141
		(1, 28, 28, 512)	0.12664175033569336	0.013076543807983398	0.10325618347283565
		(1, 14, 14, 1024)	0.06389951705932617	0.005294084548950195	0.08285014961904974
		(1, 7, 7, 2048)	0.016280174255371094	0.003660917282104492	0.22486966199988284
	*****relu6*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	1.0244698524475098	0.05978655815124512	0.05835853344870231
		(1, 28, 28, 512)	0.454937219619751	0.013289213180541992	0.02921109244842504
		(1, 14, 14, 1024)	0.22972846031188965	0.0077877044677734375	0.03389960676705229
		(1, 7, 7, 2048)	0.125657320022583	0.0045795440673828125	0.03644470586003093
***** torch.qint32 *****
	*****relu*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	0.28399205207824707	0.2665698528289795	0.9386525111468004
		(1, 28, 28, 512)	0.12665152549743652	0.12166023254394531	0.9605903447756557
		(1, 14, 14, 1024)	0.0598299503326416	0.059305429458618164	0.9912331387355795
		(1, 7, 7, 2048)	0.014290809631347656	0.012906551361083984	0.9031364698031366
	*****relu6*****
	size	time (float ms)	time (quant ms)	quant / float
		(1, 56, 56, 256)	1.020923376083374	0.27229976654052734	0.2667191024513184
		(1, 28, 28, 512)	0.4564201831817627	0.12390279769897462	0.2714665176181136
		(1, 14, 14, 1024)	0.23244047164916992	0.05935955047607422	0.25537527976482316
		(1, 7, 7, 2048)	0.1271505355834961	0.014976024627685547	0.11778184463762029

```

Test Plan: Imported from OSS

Differential Revision: D17141891

Pulled By: jamesr66a

fbshipit-source-id: 14b8c3330017c518a6b385780a449ca51efef0ce",228.0,11.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/qrelu.cpp",2.0,8,1,0.631901806,1.0,566.0,2.0,94429.0,11087.0,31251.83333,0.0,,0.0,1
pytorch,fd608cd3138c8488b07464350c3d3f2bf3bd90ae,a2e94b80fa99715e2a5fa043fb3aad43c3ebfd02,lezcano,lezcano-93@hotmail.com,Tue Oct 19 16:05:26 2021 -0700,1634659526.0,"Create linalg.matrix_exp (#62715)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62715

Fixes https://github.com/pytorch/pytorch/issues/61648

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D31641698

Pulled By: mruberry

fbshipit-source-id: 2e2965d14807b6b4fada4b809d539066dd0ba277",120.0,77.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/api/include/torch/linalg.h,torch/csrc/jit/passes/normalize_ops.cpp,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",13.0,20,5,2.808287476,33.0,53333.0,9.0,422468.9230769231,16370.0,38444.0,0.0,Corrective,1.0,1
pytorch,de24bb4b6639b515825c51bf2bcc16c7f561972f,a32e98b700abb620ba773ec668112ea080bdd788,Allen Ye,allenye0119@gmail.com,Tue Aug 22 07:45:54 2017 +0800,1503387954.0,Add documentation for std/var unbiased argument (#2509),20.0,4.0,torch/_torch_docs.py,1.0,1,1,0,23.0,4845.0,1.0,467552.0,1348.0,15523.06175,0.0,Feature Addition,0.0,1
pytorch,d1d2687d34f70b7e58065ce19f03c8771a85813a,a34301064a3c151650d078b5f8d166f55bacfce2,Mikayla Gawarecki,mikaylagawarecki@gmail.com,Tue Jun 28 20:10:51 2022 +0000,1656447051.0,"Add integer support for gpuAtomicMin/Max/Mul (#80320)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80320
Approved by: https://github.com/cpuhrsch",168.0,82.0,"aten/src/ATen/cuda/Atomic.cuh,aten/src/ATen/test/cuda_atomic_ops_test.cu",2.0,5,1,0.327444919,2.0,693.0,2.0,6524845.5,4850.0,11452.0,0.0,Feature Addition,0.0,1
pytorch,a33aeed1dce5d2ccab0662375a9bbe203cba994d,a3442f62bc2d01b64e554e925e9e0efa89c5e4c3,gchanan,gregchanan@gmail.com,Fri Mar 09 15:52:53 2018 -0500,1520610773.0,"Support native namespace functions with type dispatch. (#5576)

* Support native namespace functions with type dispatch.

Use 'ones' as an example.  Note this is a ""halfway"" solution; i.e. the call chain is:
at::ones(shape, dtype) -> dtype.ones(shape, dtype) -> CPUFloatType.ones(shape, dtype) -> at::native::ones(shape, dtype)

The ""nicer"" solution would probably be something like:
at::ones(shape, dtype) -> dtype.ones(shape) -> CPUFloatType.ones(shape) -> at::native::ones(shape, this)

* Fix type inference.

* Fix test install.

* Fix extensions.

* Put dtype argument at the beginning.

* Fix extension.cpp.

* Fix rnn.

* Move zeros in the same manner.

* Fix cuda.

* Change randn.

* Change rand.

* Change randperm.

* Fix aten contrib.

* Resize in randperm_out.

* Implement eye.

* Fix sparse zeros.

* linspace, logspace.

* arange.

* range.

* Remove type dispatch from gen_python_functions.

* Properly generate maybe_init_cuda for type dispatch functions not named type.

* Don't duplicate dtype, this parameters for native type dispatched functions.

* Call VariableType factory methods from the base type so it gets version number 0.

* Address review comments.",651.0,421.0,"aten/contrib/data/test/basic.cc,aten/contrib/meter/AUCMeter.cc,aten/contrib/meter/test/basic.cc,aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/cuda/Embedding.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/atest.cpp,aten/src/ATen/test/basic.cpp,aten/src/ATen/test/broadcast_test.cpp,aten/src/ATen/test/dlconvertor_test.cpp,aten/src/ATen/test/native_test.cpp,aten/src/ATen/test/scalar_tensor_test.cpp,aten/src/ATen/test/scalar_test.cpp,aten/src/ATen/test/test_install/main.cpp,aten/src/ATen/test/undefined_tensor_test.cpp,aten/src/ATen/test/wrapdim_test.cpp,test/cpp_extensions/doubler.h,test/cpp_extensions/extension.cpp,tools/autograd/derivatives.yaml,tools/autograd/gen_autograd.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/Functions.h,tools/autograd/templates/python_torch_functions.cpp,torch/backends/cudnn/rnn.py,torch/csrc/autograd/python_function.cpp,torch/csrc/jit/autodiff.cpp,torch/csrc/jit/interpreter_autograd_function.cpp,torch/csrc/jit/script/compiler.cpp,torch/csrc/jit/test_jit.cpp",41.0,24,4,4.132971629,34.0,16852.0,23.0,1872757.925,596.0,1835.905869,0.0,Corrective,1.0,1
pytorch,db13049b8844f3e7a15ebd902572639c19b21fc1,a348975e00081334ac96d855932d2753a62f1e77,Jane Xu,janeyx@fb.com,Wed Oct 05 06:33:25 2022 +0000,1664951605.0,"Add opteinsum backend to give users control (#86219)

This achieves the same things as https://github.com/pytorch/pytorch/pull/85908 but using backends instead of kwargs (which breaks torchscript unfortunately). This also does mean we let go of numpy compatibility BUT the wins here are that users can control what opt einsum they wanna do!

The backend allows for..well you should just read the docs:
```
.. attribute::  torch.backends.opteinsum.enabled

    A :class:`bool` that controls whether opt_einsum is enabled (on by default). If so,
    torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)
    to calculate an optimal path of contraction for faster performance.

.. attribute::  torch.backends.opteinsum.strategy

    A :class:`str` that specifies which strategies to try when `torch.backends.opteinsum.enabled` is True.
    By default, torch.einsum will try the ""auto"" strategy, but the ""greedy"" and ""optimal"" strategies are
    also supported. Note that the ""optimal"" strategy is factorial on the number of inputs as it tries all
    possible paths. See more details in opt_einsum's docs
    (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).
```

In trying (and failing) to land 85908, I discovered that jit script does NOT actually pull from python's version of einsum (because it cannot support variadic args nor kwargs). Thus I learned that jitted einsum does not subscribe to the new opt_einsum path calculation. Overall, this is fine since jit script is getting deprecated, but where is the best place to document this?

## Test plan:
- added tests to CI
- locally tested that trying to set the strategy to something invalid will error properly
- locally tested that tests will pass even if you don't have opt-einsum
- locally tested that setting the strategy when opt-einsum is not there will also error properly
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86219
Approved by: https://github.com/soulitzer, https://github.com/malfet",164.0,16.0,"docs/source/backends.rst,test/test_linalg.py,torch/backends/opt_einsum/__init__.py,torch/functional.py,torch/testing/_internal/common_utils.py",5.0,8,3,1.772163784,30.0,12927.0,4.0,346609.75,8030.0,18966.0,0.0,Feature Addition,0.0,1
pytorch,1de44a6f54d599e40340ee0e4ffa9e0038e09f51,a35136dd7307d3cb7ca0f5ec1f7cf234d6ce3ee7,BowenBao,bowbao@microsoft.com,Tue Jul 23 23:57:09 2019 -0700,1563926229.0,"Add support for onnx tensor index export (#21716)

Summary:
Support exporting
* Standard tensor indexing like
```
x = torch.ones(4, 5)
ind = torch.tensor([0, 1])

return x[ind]
```
* [Advanced indexing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing) like
```
x = torch.ones(4,5,6,7,8)
ind1 = torch.tensor([0, 1])
ind2 = torch.tensor([[3], [2]])
ind3 = torch.tensor([[2, 2], [4, 5]])

return x[2:4, ind1, None, ind2, ind3, :]
```
It would be ideal if ONNX can natively support indexing in future opsets, but for opset <= 10 it will always need this kind of workarounds.

There are still various limitations, such as not supporting advanced indexing with negative indices, not supporting mask indices of rank > 1, etc. My feeling is that these are less common cases that requires great effort to support using current opset, and it's better to not make the index export more cumbersome than it already is.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21716

Reviewed By: zrphercule

Differential Revision: D15902199

Pulled By: houseroad

fbshipit-source-id: 5f1cc687fc9f97da18732f6a2c9dfe8f6fdb34a6",209.0,9.0,"caffe2/onnx/backend.cc,test/onnx/test_onnx_opset.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",6.0,6,3,1.923510805,8.0,6828.0,2.0,221827.5,10124.0,29202.83333,0.0,Feature Addition,0.0,1
pytorch,cd35091d9b9ba74631de2cbbaf989471c526645b,a36f95fe2614fd511e87b20c6a8fc4762e39f9bf,Gregory Chanan,gchanan@fb.com,Wed May 17 21:15:50 2017 -0700,1495055750.0,"Add broadcast support for fused-matmul broadcasting. Functions are: addmm, addbmm, addr, addmv, baddbmm.",204.0,66.0,"test/test_cuda.py,test/test_torch.py,tools/cwrap/plugins/Broadcast.py,torch/_torch_docs.py,torch/csrc/generic/methods/TensorMath.cwrap",5.0,8,3,1.576955706,31.0,11247.0,4.0,0.0,888.0,11438.94394,0.0,Feature Addition,0.0,1
pytorch,f0d09572b0ae9b4b6eab91812b0c275ade446921,a3715efd8b8386a1c35dcf7341514b71e88f227a,Driss Guessous,drisspg@fb.com,Wed Jan 25 01:21:12 2023 +0000,1674609672.0,"Remove windows check for cmake to build Fused kernels (#91909)

# Summary
Add support for fused attention kernels (FlashAttention and memory-efficient attention) on Windows. Previously we could not do this because the fixes required c++17 to do this but we have since update the PyTorch standard.

This PR:
- Changes invocations of unsigned long to the fixed width integer type
- Adds in the #define FP16_SWITCH(COND, ...) which has been added to the flash_attention main branch
- Changes the some macros used within mem-efficient attention code in order to work around the VA_ARG discrepancy between clang/gcc and msvc. An alternative would be setting the global flag Zc:preprocessor
- Selectively applies /Zc:lambda to only the mem-efficient sources since applying this globally caused quantization files to not compile

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91909
Approved by: https://github.com/cpuhrsch",45.0,23.0,"CMakeLists.txt,aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/native/transformers/cuda/attention_backward.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/iterators/epilogue_predicated_tile_iterator.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h,caffe2/CMakeLists.txt,test/functorch/test_ops.py,test/test_transformers.py",9.0,11,3,2.581518974,76.0,9579.0,7.0,3956787.6666666665,11652.0,26774.5,0.0,Corrective,1.0,1
pytorch,56dd2836ec9af5fa4993127a027c7c43b947e740,a376dd344c901b4557d753e065e3726d402df839,Brian Vaughan,bvaughan@fb.com,Tue Dec 03 23:17:02 2019 -0800,1575415022.0,"Added check for torch.where on CPU that both arguments have same dtype (#30662)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30662

Cherry picked from: https://github.com/pytorch/pytorch/pull/29081

Test Plan: Imported from OSS

Differential Revision: D18782295

Pulled By: nairbv

fbshipit-source-id: 897ab25ddf8819ca34f5e86c5d3f41debb56cb04

Co-authored-by: ifedan",72.0,0.0,"aten/src/ATen/native/TensorCompare.cpp,test/test_nn.py,test/test_torch.py",3.0,5,2,1.084776577,43.0,25405.0,3.0,430338.6666666667,13597.0,37204.33333,0.0,Feature Addition,0.0,1
pytorch,3a5f48d55f5c715fdf911eae34dd484bffea324a,a37e22de7059d06b75e4602f0568c3154076718a,Xinya Zhang,Xinya.Zhang@amd.com,Tue Mar 12 01:16:51 2024 +0000,1710206211.0,"Add Flash Attention support on ROCM (#121561)

This patch addresses the major limitations in our previous [PR #115981](https://github.com/pytorch/pytorch/pull/115981) through the new dedicated repository [AOTriton](https://github.com/ROCm/aotriton)

- [x] Only supports MI200 series GPU (i.e., `gcnArchName == gfx90a:sramecc+:xnack-`).
    * MI300X is supported. More architectures will be added once Triton support them.
- [x] Only supports power of two sequence lengths.
    * Now it support arbitrary sequence length
- [ ] No support for varlen APIs.
    * varlen API will be supported in the next release of AOTriton
- [x] Only support head dimension 16,32,64,128.
    * Now it support arbitrary head dimension <= 256
- [x] Performance is still being optimized.
    * Kernel is selected according to autotune information from Triton.

Other improvements from AOTriton include
* Allow more flexible Tensor storage layout
* More flexible API

This is a more extensive fix to #112997

Pull Request resolved: https://github.com/pytorch/pytorch/pull/121561
Approved by: https://github.com/malfet, https://github.com/atalman",264.0,326.0,"CMakeLists.txt,aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip,caffe2/CMakeLists.txt,cmake/Dependencies.cmake,cmake/External/aotriton.cmake,cmake/External/oort.cmake,test/test_transformers.py,torch/testing/_internal/common_cuda.py",9.0,15,5,1.431353923,73.0,10541.0,7.0,1384429.0,26107.0,61932.0,0.0,Corrective,1.0,1
pytorch,ae9b66dd94fb30edfa0730572d41e570b7643cd4,a3b33139daad86a382c4288637eec42d710ac7bd,Jacob Szwejbka,jakeszwe@fb.com,Mon May 17 18:35:41 2021 -0700,1621276541.0,"[Pytorch] Add non mutator bundled inputs method (#58408)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58408

Itd be nice to have a version of bundle inputs that didnt mutate the original class/object. So now there is!
ghstack-source-id: 129127316

Test Plan: The new unittests

Reviewed By: dhruvbird

Differential Revision: D28460231

fbshipit-source-id: f6f7a19e264bddfaa177304cbde40336060a237a",145.0,1.0,"test/test_bundled_inputs.py,torch/utils/bundled_inputs.py",2.0,3,2,0.794584376,1.0,589.0,2.0,1259786.0,12150.0,27525.5,0.0,Feature Addition,0.0,1
pytorch,47b4ec8aa7d6834313128af352657394d2c1520b,a3fc076f22aa835d54dce423a4ec237bfdc0d704,Huy Do,huydhn@gmail.com,Sat Jul 23 06:04:19 2022 +0000,1658556259.0,"Copy black config to ufmt and run lintrunner -a (#82043)

The last entry is `torch/onnx/**/*.py` will be covered in a separated PR to onnx code owner

### Description
After ufmt (black + usort) covers the same set of files as black, when we can remove black and keep only one ""true"" linter for pytorch
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82043
Approved by: https://github.com/kit1980",8.0,8.0,".lintrunner.toml,test/test_dynamo_cudagraphs.py,torch/package/package_importer.py",3.0,3,2,1.061278124,3.0,1666.0,2.0,340609.0,5680.0,13260.0,0.0,,0.0,1
pytorch,ea652973f2131d0241eca05f5c77f62230c1d14e,a4123decf79468e1b4a1205d32356f0920d3ba01,Richard Zou,rzou@fb.com,Fri Mar 15 14:41:08 2019 -0700,1552660868.0,"Implement at::has_internal_overlap helper function (#17926)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17926
ghimport-source-id: 9f7572b5d43e474492363fa17dcb86a6c27ca13c

Stack:
* **#17926 Implement at::has_internal_overlap helper function**
* #17927 Error out on in-place (unary) ops on tensors that have internal overlap

On the way to #17935.

Checks if a tensor's sizes/strides indicate that multiple elements share
the same memory location. This problem in general is hard so
at::has_internal_overlap implements two heuristics and avoids solving
the general problem:

if a tensor is contiguous, it cannot have internal overlap
if a tensor has any zero strides, it does have internal overlap
otherwise, return MemOverlap::kTooHard to indicate that there might be
overlap, but we don't know.

Reviewed By: ezyang

Differential Revision: D14438858

fbshipit-source-id: 607ab31771315921ab6165b2a1f072ac3e75925a",76.0,0.0,"aten/src/ATen/MemoryOverlap.cpp,aten/src/ATen/MemoryOverlap.h,aten/src/ATen/native/Memory.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py",5.0,5,2,1.952253839,41.0,15686.0,3.0,2794034.6666666665,7521.0,22931.83333,0.0,Preventative,0.0,1
pytorch,f09027bc2913d8ab010085040c189eecd5681c12,a45ad7cfbaa336b8d264e9f97b9bfc8d026359fa,Trevor Killeen,killeentm@gmail.com,Tue May 02 19:32:09 2017 -0700,1493753529.0,Advanced Indexing Part 1 -- Purely Integer Array Indexing,791.0,4.0,"test/test_autograd.py,test/test_cuda.py,test/test_torch.py,torch/autograd/_functions/tensor.py,torch/autograd/variable.py,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/methods/Tensor.cwrap",7.0,7,2,1.602989693,33.0,10352.0,1.0,12356.0,1031.0,13688.75544,0.0,,0.0,1
pytorch,817f6cc59dae9fc5df540c75aaa28957c3079f9c,a461804a655d7e7495557f03493fc8dee0ff4eac,Soumith Chintala,soumith@fb.com,Tue Jan 03 07:51:11 2017 -0500,1483429871.0,adding docs for more torch.* functions,895.0,5.0,"docs/source/torch.rst,torch/docs.py",2.0,3,2,0.012506305,8.0,2613.0,1.0,0.0,298.0,6381.724559,0.0,Feature Addition,0.0,1
pytorch,d625637c7cd80311ead1aa21d4e1013093b40811,a4647cc1fab1e207926b07f4d0c8dd31c7dbb0f2,Huy Do,huydhn@gmail.com,Sat Jul 16 03:52:25 2022 +0000,1657943545.0,"Apply ufmt linter to all py files under torchgen (#81570)

Previous batches:
* https://github.com/pytorch/pytorch/pull/81285
* https://github.com/pytorch/pytorch/pull/81335

We have multiple batches here to minimize merge conflicts and reviewing process. Once everything has been formatted by ufmt (black+usort), the current black linter will be removed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/81570
Approved by: https://github.com/ezyang",406.0,404.0,".lintrunner.toml,torchgen/api/autograd.py,torchgen/api/cpp.py,torchgen/api/dispatcher.py,torchgen/api/functionalization.py,torchgen/api/lazy.py,torchgen/api/native.py,torchgen/api/python.py,torchgen/api/structured.py,torchgen/api/translate.py,torchgen/api/types.py,torchgen/api/ufunc.py,torchgen/api/unboxing.py,torchgen/code_template.py,torchgen/context.py,torchgen/dest/__init__.py,torchgen/dest/lazy_ir.py,torchgen/dest/native_functions.py,torchgen/dest/register_dispatch_key.py,torchgen/dest/ufunc.py,torchgen/gen.py,torchgen/gen_backend_stubs.py,torchgen/gen_functionalization_type.py,torchgen/gen_lazy_tensor.py,torchgen/local.py,torchgen/operator_versions/gen_mobile_upgraders.py,torchgen/selective_build/operator.py,torchgen/selective_build/selector.py,torchgen/shape_functions/gen_jit_shape_functions.py,torchgen/static_runtime/config.py,torchgen/static_runtime/gen_static_runtime_ops.py,torchgen/static_runtime/generator.py,torchgen/utils.py",33.0,7,1,4.511636677,3.0,15518.0,18.0,4103250.0303030303,5412.0,12778.5,0.0,,0.0,1
pytorch,2a64a78e7b24835511c86f7ca0e6a444fc982aa5,a47749cb28bb630bc99601a64e24ad453d9583a9,"Gao, Xiang",qasdfgtyuiop@gmail.com,Thu Dec 20 22:09:09 2018 -0800,1545343749.0,"Add at::one_hot (#15208)

Summary: Closes: https://github.com/pytorch/pytorch/issues/15060

Differential Revision: D13528014

Pulled By: ezyang

fbshipit-source-id: 5a18689a4c5638d92f9390c91517f741e5396293",170.0,25.0,"aten/src/ATen/native/Onehot.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/nn.rst,test/test_nn.py,test/test_torch.py,torch/distributions/one_hot_categorical.py,torch/nn/functional.py",7.0,10,4,2.375477782,44.0,25194.0,4.0,2285450.6666666665,6169.0,19181.33333,0.0,Feature Addition,0.0,1
pytorch,1a53e45558127755a151a8925be25c1f696b07e3,a4ab83045d992650381278dc03eac70b592789a0,gchanan,gregchanan@gmail.com,Fri Apr 20 02:03:25 2018 -0400,1524189805.0,"Fix cross device indexing for more than 1 cuda device. (#6781)

* Fix cross device indexing for more than 1 cuda device.

Cross device indexing is attempted from ATen, which doesn't work well because ATen doesn't have AutoGPU, etc.
Instead, before dispatching to ATen we do type conversion on the indices; it would probably be better if we
pushed all this down to ATen, but that will take some work.

* Small cleanup.",60.0,28.0,"test/test_cuda.py,torch/csrc/autograd/python_variable_indexing.cpp",2.0,4,2,0.811278124,38.0,2020.0,2.0,493577.0,928.0,2238.805292,0.0,Corrective,1.0,1
pytorch,3d43a8244091ff4ffa70404110263e202fd94a0c,a4c59a9dabe2dcbf06c247928e0978bb80bbedf8,Johannes M Dieterich,johannes.dieterich@amd.com,Thu Aug 23 22:20:02 2018 -0700,1535062802.0,"MIOpen integration, more tests enabled, bug fixes (#10612)

Summary:
* first integration of MIOpen for batch norm and conv on ROCm
* workaround a ROCm compiler bug exposed by elementwise_kernel through explicit capture of variables in the densest packing
* workaround a ROCm compiler bug exposed by having `extern ""C"" __host__` as a definition and just `__host__` in the implementation through the hipify script
* use fabs() in accordance with C++11 for double absolute, not ::abs() which is integer-only on ROCm
* enable test_sparse set on CI, skip tests that don't work currently on ROCm
* enable more tests in test_optim after the elementwise_bug got fixed
* enable more tests in test_dataloader
* improvements to hipification and ROCm build

With this, resnet18 on CIFAR data trains without hang or crash in our tests.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10612

Reviewed By: bddppq

Differential Revision: D9423872

Pulled By: ezyang

fbshipit-source-id: 22c0c985217d65c593f35762b3eb16969ad96bdd",1948.0,97.0,"aten/src/ATen/CMakeLists.txt,aten/src/ATen/cuda/CUDAConfig.h.in,aten/src/ATen/cuda/detail/CUDAHooks.cpp,aten/src/ATen/cuda/detail/CUDAHooks.h,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/cudnn/Handle.cpp,aten/src/ATen/cudnn/Handle.h,aten/src/ATen/cudnn/Handles.cpp,aten/src/ATen/cudnn/Handles.h,aten/src/ATen/cudnn/Utils.h,aten/src/ATen/detail/CUDAHooksInterface.h,aten/src/ATen/miopen/Descriptors.cpp,aten/src/ATen/miopen/Descriptors.h,aten/src/ATen/miopen/Exceptions.h,aten/src/ATen/miopen/Handle.cpp,aten/src/ATen/miopen/Handle.h,aten/src/ATen/miopen/Types.cpp,aten/src/ATen/miopen/Types.h,aten/src/ATen/miopen/Utils.h,aten/src/ATen/miopen/miopen-wrapper.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/cuda/Loops.cuh,aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/cudnn_test.cpp,aten/src/THC/THCNumerics.cuh,cmake/Dependencies.cmake,cmake/Modules/FindMIOpen.cmake,setup.py,test/cpp_extensions/cudnn_extension.cpp,test/run_test.py,test/test_dataloader.py,test/test_optim.py,test/test_sparse.py,tools/amd_build/disabled_features.yaml,tools/amd_build/pyHIPIFY/hipify-python.py,tools/autograd/derivatives.yaml,tools/setup_helpers/miopen.py",42.0,23,4,3.27825758,52.0,14896.0,16.0,1585767.2962962964,3624.0,9909.333333,0.0,Corrective,1.0,1
pytorch,077db3de92f34cff3187b61de4b18900a927b3fd,a4dca9822dfabcdbd1b36a12c013764f2af87613,kshitij12345,kshitijkalambarkar@gmail.com,Tue Sep 20 08:03:36 2022 +0000,1663661016.0,"[composite compliance] prod (#81969)

Ref: #69991

Also fixes #82644 (fix similar to #81617)

For CompositeCompliance, we can't use `item` to choose a special fast-path when Tensor is a Subclass. Instead we always dispatch to the slower but safer implementation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81969
Approved by: https://github.com/zou3519",30.0,26.0,"functorch/test/test_ops.py,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/_masked.py",4.0,9,2,1.829639574,7.0,27116.0,4.0,132118.0,7489.0,17532.5,0.0,Corrective,1.0,1
pytorch,6909d65613c8bec5c660a5f8debafa44a65bad29,a4f544ca1411f299cf78e7404ca96c1d52857310,Adam Paszke,adam.paszke@gmail.com,Tue Jul 26 17:36:15 2016 -0400,1469554575.0,Converted nn modules,4292.0,44.0,"test/legacy/nn.py,torch/legacy/nn/AbsCriterion.py,torch/legacy/nn/Add.py,torch/legacy/nn/AddConstant.py,torch/legacy/nn/BCECriterion.py,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/Bilinear.py,torch/legacy/nn/CAddTable.py,torch/legacy/nn/CDivTable.py,torch/legacy/nn/CMul.py,torch/legacy/nn/CMulTable.py,torch/legacy/nn/CSubTable.py,torch/legacy/nn/Clamp.py,torch/legacy/nn/ClassNLLCriterion.py,torch/legacy/nn/ClassSimplexCriterion.py,torch/legacy/nn/Contiguous.py,torch/legacy/nn/Copy.py,torch/legacy/nn/Cosine.py,torch/legacy/nn/CosineDistance.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/Criterion.py,torch/legacy/nn/CriterionTable.py,torch/legacy/nn/CrossEntropyCriterion.py,torch/legacy/nn/DistKLDivCriterion.py,torch/legacy/nn/DotProduct.py,torch/legacy/nn/Dropout.py,torch/legacy/nn/ELU.py,torch/legacy/nn/Euclidean.py,torch/legacy/nn/Exp.py,torch/legacy/nn/FlattenTable.py,torch/legacy/nn/GradientReversal.py,torch/legacy/nn/HardShrink.py,torch/legacy/nn/HardTanh.py,torch/legacy/nn/HingeEmbeddingCriterion.py,torch/legacy/nn/Identity.py,torch/legacy/nn/Index.py,torch/legacy/nn/JoinTable.py,torch/legacy/nn/L1Cost.py,torch/legacy/nn/L1HingeEmbeddingCriterion.py,torch/legacy/nn/L1Penalty.py,torch/legacy/nn/LeakyReLU.py,torch/legacy/nn/Linear.py,torch/legacy/nn/Log.py,torch/legacy/nn/LogSigmoid.py,torch/legacy/nn/LogSoftMax.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/MM.py,torch/legacy/nn/MSECriterion.py,torch/legacy/nn/MarginCriterion.py,torch/legacy/nn/MarginRankingCriterion.py,torch/legacy/nn/MaskedSelect.py,torch/legacy/nn/Max.py,torch/legacy/nn/Mean.py,torch/legacy/nn/Min.py,torch/legacy/nn/MixtureTable.py,torch/legacy/nn/Module.py,torch/legacy/nn/Mul.py,torch/legacy/nn/MulConstant.py,torch/legacy/nn/MultiCriterion.py,torch/legacy/nn/__init__.py,torch/legacy/nn/convert.vim,torch/legacy/nn/ffi.py,torch/legacy/nn/utils.py",63.0,5,2,5.32081692,1.0,451.0,1.0,314086.0,51.0,176.0,0.0,,0.0,1
pytorch,0c4b3f42713de5f818a759d3ddcbefa6213a1914,a51a094200e82a73059528f8994242fdc6037f11,Samuel,albanie@users.noreply.github.com,Sat Dec 23 19:07:22 2017 +0000,1514056042.0,fix MaxPool2d __repr__ missing ceil_mode summary (#4335),2.0,1.0,torch/nn/modules/pooling.py,1.0,3,1,0,36.0,1055.0,1.0,161976.0,2219.0,24280.85823,0.0,Corrective,1.0,1
pytorch,94da8b9816689e563f7983822b62bb518bcf120f,a52001f9236825a0249bbc0ccacf1e8e0e639f82,Xiang,qasdfgtyuiop@gmail.com,Thu Feb 25 23:32:37 2021 -0800,1614295957.0,"Improve test_reference_numerics (#51604)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/50749
ci-all version of https://github.com/pytorch/pytorch/pull/50550

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51604

Reviewed By: anjali411

Differential Revision: D26666951

Pulled By: mruberry

fbshipit-source-id: b87db68f1d2a0f6c151edbc5c7809bbceece69b0",287.0,119.0,"test/test_unary_ufuncs.py,torch/testing/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,1.360793946,5.0,5943.0,3.0,528930.0,9209.0,20522.5,0.0,Corrective,1.0,1
pytorch,8a7c0d082f8581d24a35f66988ed1b26a8e0c001,a524ee00ca8474e8d9b8bb83814316f54100acf8,albanD,desmaison.alban@gmail.com,Fri Jun 11 02:26:51 2021 -0700,1623378411.0,"Forward AD formulas batch 3 (#59711)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59711

This is the exact same PR as before.
This was reverted before the PR below was faulty.

Test Plan: Imported from OSS

Reviewed By: zou3519

Differential Revision: D28995762

Pulled By: albanD

fbshipit-source-id: 65940ad93bced9b5f97106709d603d1cd7260812",172.0,14.0,"tools/autograd/derivatives.yaml,torch/autograd/gradcheck.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",5.0,8,2,1.952267816,29.0,14813.0,4.0,111999.6,12933.0,29306.5,0.0,,0.0,1
pytorch,6185478edbe3b7cb58dd18f7479ef6d0baba031e,a52bfe2c5d8588b8f9e83e0beecdd18a1d672d0e,Lukas Hoenig,l.hoenig@campus.tu-berlin.de,Tue May 24 20:09:45 2022 +0000,1653422985.0,"Convert MPS Tensor data using MPSGraph API (#78092)

Fixes #78091
If you are already working on this, simply disregard this or take what may be helpful. This is my attempt at MPS-native Tensor datatype conversion. It works for everything tested ~~but is currently only implemented for MPS-to-MPS copy, not MPS-to-X or X-to-MPS, but the same approach could easily be used~~.

Before:
```python
In [5]: pt.full((40,), -10.3, device=""mps"")
Out[5]:
tensor([-10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000], device='mps:0')

In [6]: pt.full((40,), -10.3, device=""mps"").int()
Out[6]:
tensor([-1054552883, -1054552883, -1054552883, -1054552883, -1054552883,
        -1054552883, -1054552883, -1054552883, -1054552883, -1054552883,
        -1054552883, -1054552883, -1054552883, -1054552883, -1054552883,
        -1054552883, -1054552883, -1054552883, -1054552883, -1054552883,
        -1054552883, -1054552883, -1054552883, -1054552883, -1054552883,
        -1054552883, -1054552883, -1054552883, -1054552883, -1054552883,
        -1054552883, -1054552883, -1054552883, -1054552883, -1054552883,
        -1054552883, -1054552883, -1054552883, -1054552883, -1054552883],
       device='mps:0', dtype=torch.int32)

In [7]: pt.full((40,), -10.3, device=""mps"").int().float()
Out[7]:
tensor([-10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000], device='mps:0')

In [8]: pt.full((40,), -10.3, device=""mps"").int().float().bool()
Out[8]:
tensor([ True, False, False,  True,  True, False, False,  True,  True, False,
        False,  True,  True, False, False,  True,  True, False, False,  True,
         True, False, False,  True,  True, False, False,  True,  True, False,
        False,  True,  True, False, False,  True,  True, False, False,  True],
       device='mps:0')
```

After:
```python
In [3]: pt.full((40,), -10.3, device=""mps"")
Out[3]:
tensor([-10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000, -10.3000,
        -10.3000, -10.3000, -10.3000, -10.3000, -10.3000], device='mps:0')

In [4]: pt.full((40,), -10.3, device=""mps"").int()
Out[4]:
tensor([-10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,
        -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,
        -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10],
       device='mps:0', dtype=torch.int32)

In [5]: pt.full((40,), -10.3, device=""mps"").int().float()
Out[5]:
tensor([-10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,
        -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,
        -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,
        -10., -10., -10., -10.], device='mps:0')

In [6]: pt.full((40,), -10.3, device=""mps"").int().float().bool()
Out[6]:
tensor([True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True, True, True, True, True, True, True, True, True,
        True, True, True, True], device='mps:0')
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/78092
Approved by: https://github.com/kulinseth, https://github.com/malfet",104.0,17.0,"aten/src/ATen/native/mps/operations/Copy.mm,test/test_mps.py",2.0,7,2,0.734955759,1.0,4609.0,2.0,375483.5,3603.0,8540.5,0.0,Corrective,1.0,1
pytorch,98afdcf409ebefa7754ce37d1f5c27d6e0dd8a9f,a53cde09b522d1b05e806f61fe7349b924722ed2,Adam Paszke,adam.paszke@gmail.com,Mon Jun 05 16:39:37 2017 -0400,1496680777.0,Rename masked_copy_ to masked_scatter_,30.0,16.0,"docs/source/tensors.rst,test/test_autograd.py,test/test_torch.py,torch/_tensor_docs.py,torch/autograd/_functions/tensor.py,torch/autograd/variable.py,torch/csrc/generic/methods/Tensor.cwrap,torch/tensor.py",8.0,9,3,2.704703328,29.0,10007.0,3.0,34172.375,824.0,10456.75421,0.0,,0.0,1
pytorch,cbf572671d4c46fa7bd5cd0f8a01ca40f623d6c6,a54acd37554fd07b26124f1d24e99ecab246960b,iurii zdebskyi,47012416+izdeby@users.noreply.github.com,Tue Jul 02 00:49:45 2019 -0700,1562028585.0,"Update the way boolean tensor are being printed (#22238)

Summary:
In case when the boolean tensor gets printed out, no need to specify the dtype.

Example:
```
>> x = torch.tensor([[True, True, True], [True, True, True]])
>> print(x)
tensor([[True, True, True],
        [True, True, True]])

>> x = torch.tensor([True])
>> print(x)
tensor([True])

>> x = torch.tensor(True)
>> print(x)
tensor(True)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22238

Differential Revision: D15996304

Pulled By: izdeby

fbshipit-source-id: 5699acf3e00abca8a2bbb5384f8271eeb063dce7",11.0,2.0,"test/test_torch.py,torch/_tensor_str.py",2.0,2,2,0.89049164,40.0,12544.0,2.0,323073.0,9720.0,28266.33333,0.0,,0.0,1
pytorch,2aedd17661c600758228b694be061d4ca7906357,a5a10fe353bef5f9c79c5482ececc6ab1a21447e,Rong Rong (AI Infra),rongr@fb.com,Mon Jul 12 18:20:12 2021 -0700,1626114012.0,"Move all downloading logic out of common_utils.py (#61479)

Summary:
and into tools/ folder

Currently run_tests.py invokes tools/test_selections.py
1. download and analyze what test_file to run
2. download and parse S3 stats and pass the info to local files.
3. common_utils.py uses download S3 stats to determine what test cases to run.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61479

Reviewed By: janeyx99

Differential Revision: D29661986

Pulled By: walterddr

fbshipit-source-id: bebd8c474bcc2444e135bfd2fa4bdd1eefafe595",130.0,93.0,".gitignore,test/run_test.py,tools/stats/import_test_stats.py,tools/testing/test_selections.py,torch/testing/_internal/common_utils.py",5.0,7,3,1.454535071,44.0,4259.0,3.0,434133.25,13785.0,31089.5,0.0,,0.0,1
pytorch,06f94a7d59e95f375eed73ac3f67b9c7d2ed0e7d,a5a8ab10b0fce28ba760d48493cc349d60369770,Soumith Chintala,soumith@gmail.com,Thu Jul 13 14:09:07 2017 -0400,1499954947.0,fix Hardtanh argument names to be consistent between functional and Module,16.0,5.0,torch/nn/modules/activation.py,1.0,3,1,0,32.0,714.0,1.0,10064.0,1166.0,15843.15174,0.0,Corrective,1.0,1
pytorch,91f1d79d1b264aa21f5120e5eb7abeb89e8baf1f,a5d0d762fa4943aec9453e313a592d989b5f707e,Vasiliy Kuznetsov,vasiliy@fb.com,Thu Apr 16 02:44:12 2020 -0700,1587005052.0,"redo of add quantized layer norm implementation (#36593)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36593

This is a redo of https://github.com/pytorch/pytorch/pull/35329 with a
better test.

Adds a quantized implementation of LayerNorm for server.

A future PR will add the Python wrapper.

Test Plan:
numerics match the floating point implementation

benchmarks by input size:
v1 (mean+var non-vectorized): https://gist.github.com/vkuzo/f6d72c04742608112f4c2e612c74bd13
v2 (mean+var vectorized in float): https://gist.github.com/vkuzo/4dd95657c5b5f3654e0965db00eff8d2
v3 (mean+var vectorized in int, current): https://gist.github.com/vkuzo/57a75f75629da9f23b64b38ca0e3d34b

Differential Revision: D21030268

Pulled By: vkuzo

fbshipit-source-id: b3594c3393cfce37a881319e2e0560620d51080f",410.0,6.0,"aten/src/ATen/native/layer_norm.cpp,aten/src/ATen/native/layer_norm.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,benchmarks/operator_benchmark/benchmark_all_other_test.py,benchmarks/operator_benchmark/benchmark_all_quantized_test.py,benchmarks/operator_benchmark/pt/layernorm_test.py,benchmarks/operator_benchmark/pt/qlayernorm_test.py,test/quantization/test_quantized.py,torch/_overrides.py",10.0,13,4,2.466558627,11.0,12549.0,3.0,380512.0,1107.0,2903.0,0.0,Feature Addition,0.0,1
pytorch,77c08aa46c3f3460b95b89cbe357b99180bc824d,a5f697619cf8179ba1297c31ef9d49c48b4ff0f2,Geovanni Zhang,850734033@qq.com,Mon Aug 12 13:55:12 2019 -0700,1565618112.0,"Add interfaces in lr_scheduler.pyi (#23934)

Summary:
Some interfaces of schedulers defined in lr_scheduler.py are missing in lr_scheduler.pyi.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23934

Differential Revision: D16726622

Pulled By: ezyang

fbshipit-source-id: 45fd2d28fbb658c71f6fcd33b8997d6ee8e2b17d",9.0,2.0,torch/optim/lr_scheduler.pyi,1.0,2,1,0,1.0,32.0,1.0,1699766.0,10542.0,29987.33333,0.0,Feature Addition,0.0,1
pytorch,d546e857c2f70a336ec42d472199ebecd5b74a12,a60ff6985dfa3ed108f862ee1443740c0bc7a1da,Samantha Andow,samdow@fb.com,Tue Jul 12 14:47:16 2022 -0400,1657637236.0,"[functorch] Generate n^2 not n^3 inputs for batch and instance norm; small batch norm fix (pytorch/functorch#951)

* refactor batch norm exhaustive inputs

* fix typo in batch rule

* fix expand issue, add without cudnn xfail",71.0,80.0,"functorch/functorch/csrc/BatchRulesNorm.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py",3.0,4,1,0.645213714,1.0,2526.0,3.0,3.333333333333333,1157.0,1556.0,0.0,Corrective,1.0,1
pytorch,66dc97e51ca872e01d6430363770256c76f94361,a615baa51f5d78837f140b334aa1a3792abf2f88,Xiang Gao,qasdfgtyuiop@gmail.com,Sun Jul 08 23:37:28 2018 -0700,1531093048.0,"move unbind to ATen

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/8587

Differential Revision: D8764086

Pulled By: soumith

fbshipit-source-id: 7f311cf13c341040e1f2cf4a8f05723e32d38947",74.0,14.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_autograd.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/VariableType.cpp,torch/_tensor_docs.py,torch/_torch_docs.py,torch/functional.py",11.0,9,4,3.173237955,43.0,26319.0,9.0,728178.9090909091,2844.0,6515.833333,0.0,,0.0,1
pytorch,6899e901cc471642f1307aaf6858fdb3189bfdd4,a6170573c898a1367517d8daf8e777abaf96f752,bhushan,bhushan.s.94@gmail.com,Fri Mar 01 16:38:06 2019 -0800,1551458286.0,"Adding support for 0-d tensor for transpose (.t()) (#17535)

Summary:
- Test updates
1. test_torch: added 0-d test case and t_() test cases
2. test_jit  : updated error message for TestAsync.test_async_script_error

- Updating documentation for torch.t()
Adding information regarding new support of 0-D and 1-D tenso

Fixes #17520
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17535

Differential Revision: D14269984

Pulled By: gchanan

fbshipit-source-id: 38b723f31484be939261c88edb33575d242eca65",39.0,10.0,"aten/src/ATen/native/TensorShape.cpp,test/test_jit.py,test/test_torch.py,torch/_torch_docs.py",4.0,6,3,1.735904317,42.0,31356.0,4.0,133891.25,7289.0,22315.83333,0.0,Corrective,1.0,1
pytorch,7a23dd343ed601d48c21552f7190cfe2eda98488,a6347f54673b3c299c202378c93022938433e8b7,Kulin Seth,kulinseth@gmail.com,Tue Jun 07 18:22:10 2022 +0000,1654626130.0,"MPS: Fixes (#78930)

Cast integer to float in UnaryOps
Add tensor dtype in key generation
Enable FP16 scalars and use placeholder for alpha tensor in add/sum ops

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78930
Approved by: https://github.com/albanD",345.0,122.0,"aten/src/ATen/mps/MPSAllocator.h,aten/src/ATen/mps/MPSAllocator.mm,aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Activation.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/mps/operations/UnaryOps.mm,aten/src/ATen/native/native_functions.yaml,test/test_mps.py",10.0,8,2,2.792885781,14.0,21299.0,7.0,781282.4,4041.0,9357.5,0.0,Corrective,1.0,1
pytorch,5d9de014bdf47368f1e55f3084b04d48687bc338,a64daf2c5975bfae53c43ede88c3a84aa4eadad7,Mark Neumann,markn@allenai.org,Mon Oct 02 00:33:03 2017 -0700,1506904383.0,support dictionary return types in nn.Module's __call__ (#2037),23.0,1.0,"test/test_nn.py,torch/nn/modules/module.py",2.0,4,2,0.738284866,36.0,4894.0,2.0,690315.0,1876.0,24948.55562,0.0,,0.0,1
pytorch,cb849524f33b36605a4a78e7a552cf27610fd31a,a681f6759bccc7ccb9ef6ff9e1c87d6a3378bd7d,Adam Paszke,adam.paszke@gmail.com,Thu Dec 01 19:16:03 2016 +0100,1480619763.0,Raise correct error types when indexing tensors,45.0,29.0,"test/test_torch.py,torch/csrc/generic/Storage.cpp,torch/csrc/generic/Tensor.cpp",3.0,4,2,1.314517947,17.0,3730.0,3.0,547.0,317.0,2407.512243,0.0,Corrective,0.0,1
pytorch,dd00c2997f4336cb02d0dd6c9d3c8cf7beada8f5,a6949abb158d1408c5c53d9eeafde16543160c94,Michael Antonov,michael.antonov@oculus.com,Tue Oct 23 23:19:23 2018 -0700,1540336763.0,"Guard all Caffe2 protobuf string serializations with CAFFE_ENFORCE (fixed reverted bug) (#12848)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12848

Updated all non-test uses of protobuf::MessageLite::SerializeAsString to call
SerializeAsString_EnforceCheck so that the return value is checked and can
throw an exception if failing.

Most of the affected code was called from classes derived from  BlobSerializeBase.
Didn't touch most tests and ENFORCE calls because they usually do checks
anyway.

Original commit changeset: c0760e73ecc7

Reviewed By: dzhulgakov

Differential Revision: D10453456

fbshipit-source-id: d2f2b7b4578e721924354149f08f627c7e3bf070",58.0,18.0,"binaries/convert_caffe_image_db.cc,caffe2/core/blob_serialization.cc,caffe2/core/blob_serialization.h,caffe2/core/blob_test.cc,caffe2/core/db.cc,caffe2/core/int8_serialization.cc,caffe2/core/qtensor_serialization.h,caffe2/db/protodb.cc,caffe2/operators/counter_ops.cc,caffe2/operators/dataset_ops.cc,caffe2/operators/index_ops.cc,caffe2/operators/map_ops.h,caffe2/python/pybind_state.cc,caffe2/sgd/iter_op.cc",14.0,7,2,3.139769744,15.0,6881.0,2.0,416667.28571428574,4803.0,14150.33333,0.0,Corrective,1.0,1
pytorch,c1af91a13aa82661d0d15ed467b9c68dc36b914b,a69a78daa2a53e2c4b1088b3acc4ed340aca3c2c,Sam Estep,sestep@fb.com,Tue Oct 06 20:20:14 2020 -0700,1602015614.0,"Use smaller N to speed up TestForeach (#45785)

Summary:
Between September 25 and September 27, approximately half an hour was added to the running time of `pytorch_linux_xenial_cuda10_2_cudnn7_py3_gcc7_test`. Judging from the CircleCI data, it looks like the majority of the new time was added by the following PRs:

- https://github.com/pytorch/pytorch/issues/44550
- https://github.com/pytorch/pytorch/issues/45298

I'm not sure what to do about https://github.com/pytorch/pytorch/issues/44550, but it looks like https://github.com/pytorch/pytorch/issues/45298 increased the `N` for `TestForeach` from just 20 to include both 30 and 300. This PR would remove the 300, decreasing the test time by a couple orders of magnitude (at least when running it on my devserver), from over ten minutes to just a few seconds.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45785

Reviewed By: malfet

Differential Revision: D24094782

Pulled By: samestep

fbshipit-source-id: 2476cee9d513b2b07bc384de751e08d0e5d8b5e7",15.0,13.0,test/test_foreach.py,1.0,1,1,0,1.0,766.0,1.0,452268.0,5763.0,13441.5,0.0,Feature Addition,0.0,1
pytorch,76147b897cd1ec1486430a6bc7a9ef8521c2da86,a6a811f23aa0eed68022b06591c6021215d0dee4,BowenBao,bowbao@microsoft.com,Fri Mar 12 10:42:06 2021 -0800,1615545726.0,"[ONNX] Add repeat_interleave symbolic (#52855) (#53312)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/53312

- Add support for aten::repeat_interleave
- NOTE: Also adds fix for cases with split op where input tensor sizes are not known but _outputs is provided

Test Plan: Imported from OSS

Reviewed By: pbelevich, malfet

Differential Revision: D26922422

Pulled By: SplitInfinity

fbshipit-source-id: 5362d0d8ccfdc14c15e1ae73fd70c4c113f823e6",120.0,3.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",5.0,4,2,1.597322472,3.0,11856.0,4.0,739099.4,9712.0,21500.5,0.0,Corrective,1.0,1
pytorch,bdd27ea9567675f41eb7000e29f02a841718d25e,a6bfa16c17c2e2847dbec2ccc1b2b60741ae4c65,gchanan,gregchanan@gmail.com,Fri Apr 27 19:11:45 2018 -0400,1524856305.0,"torch.arange: add numpy-style type inference. (#7016)

* torch.arange: add numpy-style type inference.

This is a backwards-compatibility breaking change.

* Fix flake8.

* Use at::optional.

* Remove unneeded header files.

* Use reference wrapper.

* Update arange for test.

* Address review comments.",191.0,63.0,"test/test_autograd.py,test/test_indexing.py,test/test_jit.py,test/test_multiprocessing.py,test/test_nn.py,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/utils/python_arg_parser.cpp",11.0,7,3,2.712750814,41.0,30542.0,8.0,239024.54545454544,988.0,2402.305292,0.0,Corrective,1.0,1
pytorch,94b6fa6f8b0cd730ad7401dc52880a9eb8a6e405,a6c0edff1a60e299d3f9794407f0a1c9a269fd19,Alban Desmaison,albandes@fb.com,Wed Nov 10 11:05:58 2021 -0800,1636542358.0,"fix gradcheck to generate valid input for forward AD complex (#68001)

Summary:
This fixed a few of the linalg checks that we disabled before!

This also seems to break sgn, abs and angle (sending on CI here to see if there are more). These two functions used to only ever get pure imaginary or real values.
This is very much likely that something is wrong with their formula.
But they are implemented as element-wise, so not sure where the error can come from. I tried to look at it but nothing obvious seems wrong there (especially because it is correct in backward mode).

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68001

Reviewed By: soulitzer

Differential Revision: D32280475

Pulled By: albanD

fbshipit-source-id: e68b1ce0e2e97f8917c3d393141d649a7669aa9d",28.0,26.0,"torch/autograd/gradcheck.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,1,0.898653376,28.0,14286.0,2.0,617799.0,16991.0,39984.0,0.0,Corrective,1.0,1
pytorch,9152f2f73ac40d518d57cfa04e190bd3c03c17eb,a6c8730045bc19ec94ed8b15bfabc69d17b3c4f7,BowenBao,bowbao@microsoft.com,Fri Aug 07 03:30:41 2020 -0700,1596771041.0,"[ONNX] Add preprocess pass for onnx export (#41832)

Summary:
in `_jit_pass_onnx`, symbolic functions are called for each node for conversion. However, there are nodes that cannot be converted without additional context. For example, the number of outputs from split (and whether it is static or dynamic) is unknown until the point where it is unpacked by listUnpack node. This pass does a preprocess, and prepares the nodes such that enough context can be received by the symbolic function.
* After preprocessing, `_jit_pass_onnx` should have enough context to produce valid ONNX nodes, instead of half baked nodes that replies on fixes from later postpasses.
* `_jit_pass_onnx_peephole` should be a pass that does ONNX specific optimizations instead of ONNX specific fixes.
* Producing more valid ONNX nodes in `_jit_pass_onnx` enables better utilization of the ONNX shape inference https://github.com/pytorch/pytorch/issues/40628.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41832

Reviewed By: ZolotukhinM

Differential Revision: D22968334

Pulled By: bzinodev

fbshipit-source-id: 8226f03c5b29968e8197d242ca8e620c6e1d42a5",217.0,225.0,"aten/src/ATen/core/aten_interned_strings.h,test/onnx/test_pytorch_onnx_onnxruntime.py,tools/build_variables.bzl,torch/csrc/jit/passes/onnx/helper.cpp,torch/csrc/jit/passes/onnx/helper.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp,torch/csrc/jit/passes/onnx/preprocess_for_onnx.h,torch/csrc/jit/python/init.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",13.0,14,4,2.302866814,10.0,12640.0,7.0,1150193.3636363635,4142.0,9650.0,0.0,Corrective,1.0,1
pytorch,0d91048639e3498ec41ed59b44d5af2390ea85d0,a74fb22b9ac5bed4b0b06e4fee54423b05794f5b,albanD,alban@robots.ox.ac.uk,Wed Jul 12 15:37:55 2017 +0100,1499873875.0,fix inplace division for python3 (#2063),8.0,0.0,"test/test_torch.py,torch/tensor.py",2.0,2,2,0.543564443,33.0,4539.0,2.0,33303.5,1122.0,18473.4346,0.0,Corrective,1.0,1
pytorch,ff501c30af6f1a781394f2b2a3255aa8d5d9f361,a769fae91d8184dd1cadce527b9ffa1559bd8c12,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Fri Jul 06 01:47:04 2018 -0700,1530841624.0,"Fix TestAutograd.test_pinverse not actually testing (#9192)

Summary:
cc vishwakftw

Also added a check if none of the input tensors in `gradcheck` have `requires_grad=True`.
Closes https://github.com/pytorch/pytorch/pull/9192

Differential Revision: D8739401

Pulled By: SsnL

fbshipit-source-id: 81bb3aa0b5c04eb209b137a4bd978e040e76cbcd",37.0,20.0,"test/test_autograd.py,test/test_distributions.py,test/test_torch.py,torch/autograd/gradcheck.py,torch/distributions/relaxed_categorical.py",5.0,4,2,2.193494758,42.0,15526.0,5.0,576921.4,2822.0,6420.833333,0.0,Corrective,1.0,1
pytorch,dc84ff1e5a74164d668fa1afac9e0e0f2315f3b2,a7796bc24d2fd25774f44dca1262bbb22d640dc3,Jacie Fan,jaciefan@gmail.com,Wed Jan 30 19:20:44 2019 -0800,1548876044.0,"CUDA histogram implementation

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/15842

Reviewed By: zou3519

Differential Revision: D13868982

Pulled By: jaciefan

fbshipit-source-id: bce81dc121c4538d204047506f8f14d0b4d8f905",190.0,29.0,"aten/src/ATen/native/LegacyDefinitions.cpp,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorMoreMath.cpp,test/test_cuda.py,test/test_torch.py",6.0,8,2,1.458416463,42.0,20683.0,6.0,1066043.8333333333,6718.0,20793.33333,0.0,,0.0,1
pytorch,232b96b6e244a594286be14be8b7196a1684237a,a788365d14771bc055d3eac4de9786ec5af0e34c,Edward Z. Yang,ezyang@meta.com,Wed Jul 19 14:42:23 2023 -0400,1689777743.0,"Switch UFMT to denylist rather than allowlist (#105536)

The new denylist was generated with this script: https://gist.github.com/ezyang/851589ac4694ed131feee7ad59281ca9

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/105536
Approved by: https://github.com/malfet, https://github.com/albanD",2105.0,45.0,.lintrunner.toml,1.0,0,0,0,3.0,1030.0,1.0,52937.0,17813.0,40219.5,0.0,,0.0,1
pytorch,ad4848565e1d9f4d408c60614f213acb52035181,a7ae73a2380c3e45394998d2d1d9bceb14f2ee55,Thomas J. Fan,thomasjpfan@gmail.com,Mon Aug 30 22:03:40 2021 -0700,1630361020.0,"BUG Fixes regression for nllloss gradcheck (#64203)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/64163

This PR includes the fix and the opinfo from https://github.com/pytorch/pytorch/pull/63854/ for non-regression testing.

cc albanD mruberry jbschlosser

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64203

Reviewed By: albanD

Differential Revision: D30647522

Pulled By: jbschlosser

fbshipit-source-id: 2974d299763505908fa93532aca2bd5d5b71f2e9",52.0,5.0,"aten/src/ATen/native/cuda/Loss.cu,torch/testing/_internal/common_methods_invocations.py",2.0,8,2,0.669996032,2.0,9905.0,2.0,272120.5,15073.0,34526.0,0.0,Corrective,1.0,1
pytorch,cb7197ce3f7c6c3fe009b798950aa720e4cad26b,a7ba0f08f381c833896e63f7bf58b86e1ef2297a,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu May 06 15:25:04 2021 -0700,1620314704.0,"Update internal code for torch.lu_solve (#56611)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56611

The goal of this refactoring is to make the `torch.linalg.solve`
to be a composition of calls to `lu_stub` and `lu_solve_stub`.
Once `lu_stub` and `lu_solve_stub` have cuSOLVER-based codepath,
`torch.linalg.solve` will have it as well.

Replaced lu_solve_helper with DECLARE_DISPATCH for lu_solve_stub.
Removed unnecessary copy improving the performance (see https://github.com/pytorch/pytorch/pull/56611#issuecomment-824303673).
Split MAGMA-based `apply_lu_solve` into `apply_lu_solve_looped_magma`
and `apply_lu_solve_batched_magma`. This simplifies future dispatch to
cuSOLVER and cuBLAS.

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D28142279

Pulled By: mruberry

fbshipit-source-id: 9d4baf650ca7a40b800616794408b34342d8d68f",204.0,126.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py",6.0,7,2,1.664861171,12.0,17512.0,4.0,76830.33333333333,11735.0,26491.0,0.0,Perfective,1.0,1
pytorch,527b10c2d1b605828df3b4844436ef0636d2c5e6,a7de545c6374cccd27cb871e284f774bc2395bbe,Mike Ruberry,mruberry@devfair044.maas,Fri Oct 04 09:39:26 2019 -0700,1570181966.0,"Makes test_cuda.py's generated tensor op tests generic (#27210)

Summary:
- The tensor op tests generated in test_cuda.py are now generic and appear in test_torch,py
- Data previously held in auxiliary data structures and files, like test_cuda_ignores.txt, is inlined

Previously the tensor op tests used several auxiliary data structures, a file, and exception handling to filter the test suite. If a function wasn't implemented, for example, that exception would be caught. This let functions like trigamma, which isn't callable, appear to be tested. See https://github.com/pytorch/pytorch/issues/27230. Filtering from additional data stores is error prone, too. It requires developers understand what data stores are used and how they're used. The existing sources are also sometimes incorrect. The txt file claims that dist_ doesn't work on half tensors, for example, but the updated tests verify it does.

In addition to making these tests generic, this PR removes those auxiliary data structures and does not catch any exceptions. Exceptions are errors. (This also means that if something implemented breaks it will now report as an error. Previously the test suite would have reported a pass.) The test infrastructure was also simplified to not perform computations with CPU half tensors since they do not support many operations. This introduces a float<->half conversion quirk but eliminates awkward functions that would first convert cpu tensors to float, perform an operation, and convert them back.

With this change test_cuda.py is almost entirely CUDA-specific.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27210

Differential Revision: D17757907

Pulled By: mruberry

fbshipit-source-id: b3c191c379667b1a7d5361087bdf82f397f77f65",539.0,784.0,"test/common_device_type.py,test/data/test_cuda_ignores.txt,test/test_cuda.py,test/test_torch.py",4.0,2,1,1.367470733,41.0,16448.0,3.0,4725564.0,12026.0,33766.83333,0.0,Corrective,0.0,1
pytorch,313960d52e2ea6aa6b3c8da84ae8355edb1c73fa,a7ec889de4d81d9750f22a1fed8b2c4c804debac,Pieter Noordhuis,pietern@fb.com,Mon Jun 24 14:27:25 2019 -0700,1561386445.0,"Add sparse tensor allreduce (#22036)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22036

Implemented only on ProcessGroupGloo, as an allgather of metadata
(sparse_dim, dense_dim, and nnz), followed by an allgather of indices,
followed by an allgather of values. Once these operations have
finished, all ranks locally compute a reduction over these sparse
tensors. Works for both CPU and CUDA tensors.

This surfaced a problem with the existing assumption of only modifying
tensors that are passed at the call site, because for sparse tensors
we don't know the dimensions of the output tensors before we run the
collective. To deal with this unknown, this commit adds a `result`
function to the `c10d::ProcessGroup::Work` class that returns a vector
of tensors.

It's a bit odd to have to retrieve the result through this function
only for operations on sparse tensors. To make this work irrespective
of tensor layout, we can create a follow-up commit to make all in
place operations make their results accessible through this function
as well. This doesn't break any existing contracts but does have the
potential to add interface ambiguity.

This is a resubmission of #19146.

Reviewed By: mrshenli

Differential Revision: D15926384

fbshipit-source-id: b6ee5d81606bfa8ed63c3d63a9e307613491e0ae",474.0,9.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/ProcessGroup.cpp,torch/lib/c10d/ProcessGroup.hpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/Utils.hpp",6.0,7,2,1.24285926,4.0,5660.0,2.0,335037.8333333333,9584.0,27895.33333,0.0,Feature Addition,0.0,1
pytorch,314d645e05074e491e24d1cc64123884e6346e20,a80dd02a224ea0f2c9582f428bc12d4579deaa92,Pritam Damania,pritam.damania@fb.com,Fri Jun 19 22:41:14 2020 -0700,1592606474.0,"[Resubmit] Ensure NCCL_BLOCKING_WAIT=1 works for dist.barrier() (#40249)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/40249

Blocking wait didn't work for dist.barrier() since we performed a
cudaDeviceSynchronize() before we performed any of the timeout checks. As a
result, in case of failures/desync the barrier() call would get stuck on
cudaDeviceSynchrnonize() and would never return a timeout error to the user.

To fix this, I've moved the device synchronization after the timeout checks.
ghstack-source-id: 106250153
ghstack-source-id: 106250153

Test Plan: waitforbuildbot

Differential Revision: D22126152

fbshipit-source-id: d919a7a6507cca7111d8ad72e916777b986d0d67",28.0,5.0,"test/distributed/test_c10d.py,torch/lib/c10d/ProcessGroupNCCL.cpp",2.0,5,2,0.98337619,4.0,4148.0,1.0,94053.0,3026.0,7326.5,0.0,Corrective,1.0,1
pytorch,0b44e1a74c7b66804ef313ff9d8bd5a13e827445,a8319698b3ba7c858fa3e4f3aac88d3fe9dc00d1,"Wu, Chunyuan",chunyuan.wu@intel.com,Thu Jul 18 08:07:01 2024 -0700,1721290021.0,"[inductor] [cpp] improve cache blocking with CPU info (#129348)

## Description
For single thread case, this PR improves the cache blocking in CPP GEMM template with the CPU info (the L1 and L2 cache size). `Mc_blocks` and `Kc_blocks` are calculated based on the below condition:
     - size_of_B < L1
     - size_of_A < 0.5 * L2

For multi-thread, we need to tune the task decomposition among threads together with cache blocking. We disabled the cache blocking change for now and will submit a follow-up PR for multi-thread optimizations.

## Performance
No regressions. Models with > 3% performance speedup are listed below:

### BF16 single thread (measured on CPU with AMX support)
- static shape

| Model Family | Model Name | Speedup |
|--------------|------------|---------|
torchbench | detectron2_fasterrcnn_r_101_dc5| 4%

- dynamic shape

| Model Family | Model Name | Speedup |
|--------------|------------|---------|
torchbench | detectron2_fasterrcnn_r_101_dc5| 4%

### FP32 single thread (measured on Ice Lake)
- static shape

| Model Family | Model Name | Speedup |
|--------------|------------|---------|
torchbench | basic_gnn_edgecnn| 10%

- dynamic shape

| Model Family | Model Name | Speedup |
|--------------|------------|---------|
torchbench | basic_gnn_edgecnn| 10%

### Next step
The E2E level improvement is limited due to the below reasons:

- For several HF models, we can observe performance improvement at kernel level for the gemm template kernel but since the performance is either still worse than ATen kernel (thus won't be selected during autotune) or improved from worse than ATen to similar to ATen, so we don't see E2E level performance change.

- There're models where the gemm template kernel could get > 10% performance improvement with this PR but since the kernel time is only about 3% of the E2E time, we don't observe significant E2E level improvement.

We will continue to find possible optimizations in the gemm template kernel in follow-up PRs.

Co-authored-by: Jiong Gong <jiong.gong@intel.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/129348
Approved by: https://github.com/jgong5, https://github.com/jansel
ghstack dependencies: #130675, #130690",113.0,4.0,"aten/src/ATen/cpu/Utils.cpp,aten/src/ATen/cpu/Utils.h,torch/_C/_cpu.pyi,torch/_inductor/codegen/cpp_gemm_template.py,torch/csrc/cpu/Module.cpp",5.0,10,2,1.535914407,1.0,760.0,3.0,1926966.2,31446.0,79097.0,0.0,Feature Addition,0.0,1
pytorch,239d3b246174f5aa17ff376cf86b7fe3d5d60e10,a83c240644116c204e93bbbffba8b8d948601cd0,Richard Zou,zou3519@users.noreply.github.com,Tue Feb 06 02:14:25 2018 -0500,1517883265.0,"Fix maxpool3d / avgpool3d crashs (#5052)

* Replace downcastOuter with newFoldBatchDim

* Fix double free

* Address comments",150.0,90.0,"aten/src/THC/generic/THCTensor.c,aten/src/THC/generic/THCTensor.h,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,test/test_nn.py",6.0,7,2,2.1296415,37.0,7775.0,5.0,4272137.5,2339.0,24512.35823,0.0,Corrective,1.0,1
pytorch,63ac3633f53966d038d5983e83254b7524300680,a88a8ec8278e19f52cfd6e75a685ce0e9200b96b,Edward Z. Yang,ezyang@mit.edu,Wed Dec 20 19:19:27 2017 -0500,1513797567.0,"Convolution derivatives in ATen (#4116)

* Convolution derivatives in ATen

This PR introduces ATen implementation of convolution, which dispatches to
THNN/CuDNN/nnpack based on input parameters. The general strategy is to compose
this function out of the various forward-backward pairs of specific
implementations, rather than write a monolithic function with backwards (which
is what we did before because the boilerplate of doing it otherwise would have
been very high.) The new API provides the following functions:

  - _convolution, which is a fully generic, native convolution implementation
    that dispatches to various other convolution implementations depending on
    input characteristics. This is prefixed with an underscore because it
    explicitly takes benchmark, deterministic and cudnn_enabled which are
    implementation details for CuDNN. The intent is to eventually provide a
    convolution that reads these parameters out of the context using #4104.
  - _convolution_nogroup is a convolution implementation for non-CuDNN
    algorithms which don't support group convolution natively.
  - _convolution_double_backward is the generic double-backwards implementation
    for convolution.

In more detail:

- Most functionality from torch/csrc/autograd/functions/convolution.cpp has been
  moved into aten/src/ATen/native/Convolution.cpp
- We continue to make use of ConvParams, but we now construct the parameters
  upon entry to a function from the function signature (which does not use
  ConvParams; having convolution take ConvParams directly would require teaching
  the code generator how to accept these as parameters, complicating ATen's API
  model) and destruct them when making subprocedure calls.
- I introduce a new idiom, input_r, which represents a const Tensor& reference,
  which will subsequently be assigned to a local Tensor input. This is helpful
  because a lot of the existing algorithms relied on being able to assign to
  locals, which is not permitted with a const reference.
- The native argument parser now supports std::array<bool,2> inputs (NB: there
  MUST NOT be a space; this is the same hack as is applied to derivatives.yaml)
- Native parser now supports Tensor? arguments, which indicates a nullable
  tensor. Previously this function was only used by NN methods.
- Documentation updates on THNN library
- I added an extra fgradInput argument to VolumetricConvolutionMM_updateOutput
  and VolumetricConvolutionMM_accGradParameters so that its buffer list lines up
  with the backward argument list. This makes it possible to write derivative
  for conv3d which previously was not supported (commented out in
  derivatives.yaml)
- Extra double_backward declarations for all convolution backwards functions was
  added.
- You can now use the syntax Tensor? in native_functions.yaml to indicate that a
  tensor argument is nullable.  There are adjustments to propagate this to the
  Python argument parser.
- NNPACK was ported to ATen, and ATen now builds and links against ATen if
  possible. New AT_NNPACK_ENABLED macro.  The nnpack functions are
  nnpack_spatial_convolution.
- Some modest CuDNN convolution refactoring to remove _forward from names.
- There's a new cudnn_convolution_backward function to deal with the fact that
  CuDNN convolution double backward requires you to have computed all gradients
  in one go.
- Variable set_flags now checks if the tensor is undefined, fixing a silent memory
  corruption.
- checkSameType updated to not raise an exception if called with Variable arguments
- ""no ATen declaration found for"" error message is improved to say what available declarations are
- make_variable now accepts undefined tensors, and returns an undefined tensor in this case.",1585.0,3592.0,"aten/CMakeLists.txt,aten/cmake/FindNNPACK.cmake,aten/src/ATen/CMakeLists.txt,aten/src/ATen/Check.cpp,aten/src/ATen/Config.h.in,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/NNPACK.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/README.md,aten/src/THNN/doc/api_reference.md,aten/src/THNN/doc/generate_reference.lua,aten/src/THNN/generic/THNN.h,aten/src/THNN/generic/VolumetricConvolutionMM.c,setup.py,test/expect/TestJit.test_c_function.expect,test/expect/TestJit.test_conv.expect,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,tools/setup_helpers/nnpack.py,torch/csrc/autograd/functions/batch_normalization.cpp,torch/csrc/autograd/functions/convolution.cpp,torch/csrc/autograd/functions/convolution.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/onnx/convolution.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/jit/export.cpp,torch/csrc/nnpack/NNPACK.cpp,torch/csrc/nnpack/NNPACK.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/legacy/nn/VolumetricConvolution.py,torch/nn/functional.py,torch/onnx/symbolic.py",38.0,29,4,3.23391049,39.0,17613.0,17.0,1405140.4571428571,385.0,1211.405869,0.0,Corrective,1.0,1
pytorch,d63bb72d89723d0d913ba38b7a98e288236391b0,a8b1755de6c967bd8e9d777b6a0dddf4e7a6eed1,Gregory Chanan,gchanan@fb.com,Mon Sep 17 15:15:19 2018 -0700,1537197319.0,"Check device argument makes sense for legacy tensor constructors. (#11669)

Summary:
Fixes: https://github.com/pytorch/pytorch/issues/11427.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11669

Differential Revision: D9817881

Pulled By: gchanan

fbshipit-source-id: 77dc5b0e6bc9884d2616210b96c07e4734058bb6",118.0,13.0,"aten/src/ATen/DeviceGuard.h,test/test_sparse.py,test/test_torch.py,torch/csrc/utils/tensor_new.cpp",4.0,7,3,1.712036983,40.0,11022.0,4.0,825057.75,4149.0,11599.33333,0.0,Corrective,1.0,1
pytorch,524002be3415ed2e83603f666910c8ea7662200d,a8d54b4029a09ac348f793dd74775bc9645eec13,Kshiteej K,kshitijkalambarkar@gmail.com,Tue Jan 18 17:31:07 2022 +0500,1642527067.0,"[functorch] [pytree] register return_types (pytorch/functorch#350)

* [pytree] register return_types

* add comment for binding the class in lambda

* remove tree_flatten_hack and update

* add tests

* make flake8 happy

* retrigger CI

* address review

* remove incorrectly added test

* add namedtuple test",39.0,34.0,"functorch/functorch/_src/pytree_hacks.py,functorch/functorch/_src/vmap.py,functorch/test/test_vmap.py",3.0,4,1,1.53734303,1.0,3843.0,2.0,1.3333333333333333,721.0,993.0,0.0,Corrective,0.0,1
pytorch,dd0fe720649c62c2ee16e468258bcae7afc7fdc0,a8e01d4bc325bf1edf9c9f3e11d30f2c5937f779,Sam Andow,samdow@fb.com,Tue Jan 04 21:15:38 2022 +0000,1641330938.0,[functorch] made tests that work on either cpu or cuda but not both skips,15.0,8.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1171.0,1.0,0.0,697.0,954.5,0.0,,0.0,1
pytorch,1d7627955bc196e810ec90550907b89a3ba3a5ad,a90f006fe54d6e83e0d3bcdde7ceb48e42e62a8f,George Qi,georgeqi94@gmail.com,Fri Jun 10 03:02:28 2022 +0000,1654830148.0,"add strides to slow path

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78610

Approved by: https://github.com/ezyang",125.0,30.0,"c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,c10/core/impl/PyInterpreter.cpp,c10/core/impl/PyInterpreter.h,test/test_python_dispatch.py,torch/csrc/autograd/python_variable.cpp,torch/overrides.py,torchgen/packaged/README.md",8.0,9,4,2.567562996,30.0,9472.0,7.0,442706.5,4202.0,9744.0,0.0,Feature Addition,0.0,1
pytorch,8d363d37da99c4205226e87757d4aa569117e867,a911c4fc1cbd6317991eeaa993b7a79f3f1adffa,Alexander,aocsa.cs@gmail.com,Fri May 07 12:35:27 2021 -0700,1620390927.0,"New: Initial support for sparse complex tensors constructors for CPU/CUDA (#57125)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57125

I'm opening this PR, solving the last issued reported before merging PR #54153

https://github.com/pytorch/pytorch/pull/54153#issuecomment-827997616,

Solves gh-50690

Test Plan: Imported from OSS

Reviewed By: astaff

Differential Revision: D28112702

Pulled By: ezyang

fbshipit-source-id: 915681954edb14b7c19c3ffe641af2d2e6649576",198.0,151.0,"aten/src/ATen/LegacyTHFunctionsCPU.cpp,aten/src/ATen/native/cuda/Nonzero.cu,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,aten/src/TH/THTensor.h,aten/src/TH/THTensorEvenMoreMath.cpp,aten/src/TH/THVector.h,aten/src/TH/generic/THTensorEvenMoreMath.cpp,test/test_autograd.py,test/test_sparse.py,test/test_type_promotion.py,tools/autograd/gen_variable_type.py,torch/autograd/gradcheck.py",15.0,14,4,2.265747782,42.0,19457.0,8.0,626711.5333333333,11775.0,26590.5,0.0,Non Functional,0.0,1
pytorch,b41988c71ed7d40af7a314b2049a4b0d5909fed2,a9469c9c8ab046a7961c1c357d84f60063507c4b,Ailing Zhang,ailzhang@fb.com,Wed Aug 29 17:48:04 2018 -0700,1535564884.0,"Fill eigenvector with zeros if not required (#10645)

Summary:
Fix #10345, which only happens in CUDA case.

* Instead of returning some random buffer, we fill it with zeros.

* update torch.symeig doc.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10645

Reviewed By: soumith

Differential Revision: D9395762

Pulled By: ailzhang

fbshipit-source-id: 0f3ed9bb6a919a9c1a4b8eb45188f65a68bfa9ba",38.0,21.0,"aten/src/THC/generic/THCTensorMathMagma.cu,test/test_cuda.py,test/test_torch.py,torch/_torch_docs.py",4.0,6,3,1.768141475,41.0,17324.0,4.0,730538.25,3708.0,10199.33333,0.0,Corrective,1.0,1
pytorch,519c086418a3818e27d55a3727c15c9128ae4f73,a9a9d0b181c28d065227250cc58f747bd036c96f,KyleCZH,kylechen@amd.com,Mon Oct 05 22:06:19 2020 -0700,1601935579.0,"Rocm skip test cases (#45782)

Summary:
Skip the following test cases for rocm (When PYTORCH_TEST_WITH_ROCM=1):
- test_reference_numerics_tan_cuda_float64 (__main__.TestUnaryUfuncsCUDA)
- test_addmv_cuda_float16 (__main__.TestTorchDeviceTypeCUDA)
- test_logspace_cuda_float64 (__main__.TestTensorCreationCUDA)
- test_gloo_backend_2gpu_module (__main__.DistributedDataParallelTest)
jeffdaily
pruthvistony

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45782

Reviewed By: VitalyFedyunin

Differential Revision: D24115581

Pulled By: xw285cornell

fbshipit-source-id: 4043a9fa19e242301b5007813c15b6b3873889c5",11.0,3.0,"test/distributed/test_c10d.py,test/test_tensor_creation_ops.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,2,1.788450457,43.0,28043.0,4.0,406371.0,5727.0,13393.5,0.0,Feature Addition,0.0,1
pytorch,3cf97bc23c741606c04f0ef9515fad1b73743485,a9cef05f5d6ca2a74f993feb191531a03e445d40,Natalia Gimelshein,ngimel@fb.com,Tue Feb 25 05:35:51 2020 -0800,1582608951.0,"improve EmbeddingBag performance on cuda (#33589)

Summary:
This PR improves performance of EmbeddingBag on cuda by removing 5 kernel launches (2 of those are synchronizing memcopies).
- 2 memcopies are checking values of offsets[0] and offsets[-1] to be in expected range (0 for the former, less than number of indices for the latter). It seems strange to check only those 2 values, if users are providing invalid offsets, invalid values can be anywhere in the array, not only the first and last element. After this PR, the checks are skipped on cuda, the first value is forced to 0, if the last value is larger than expected, cuda kernel will assert. It is less nice than ValueError, but then again, the kernel could have asserted if other offset values were invalid. On the cpu, the checks are moved inside the cpu implementation from functional.py, and will throw RuntimeError instead of ValueError.
- 3 or 4 initializations (depending on the mode) of the output tensors with .zeros() are unnecessary, because every element of those tensors is written to, so their data can be uninitialized on the start.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33589

Reviewed By: jianyuh

Differential Revision: D20078011

Pulled By: ngimel

fbshipit-source-id: 2fb2e2080313af64adc5cf1b9fc6ffbdc6efaf16",23.0,23.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,test/test_nn.py,torch/nn/functional.py",4.0,8,3,1.995780848,44.0,15983.0,4.0,1496487.5,14951.0,40067.33333,0.0,Perfective,0.0,1
pytorch,117885128073c9c2b32f4b33c6c79df3895b7071,a9e6a673aec6c479447c61f3bcc5c10ddd1a099f,Christian Puhrsch,cpuhrsch@fb.com,Mon Sep 24 17:39:10 2018 -0700,1537810750.0,"Remove caffe2::Tensor::capacity_nbytes, at::Tensor::to##name##Data, (#11876)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11876

Modern C++ api instead of macros, item() is aligned with Python frontend. caffe2::Tensor::capacity_nbytes is effecitvely unused and confusing w.r.t. caffe2::Tensor::nbytes().

codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCByte   ""item<uint8_t>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCLong   ""item<int64_t>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCInt    ""item<int32_t>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCDouble ""item<double>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCFloat  ""item<float>""

codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toByteData   ""data<uint8_t>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toLongData   ""data<int64_t>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toIntData    ""data<int32_t>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toDoubleData ""data<double>""
codemod -d caffe2           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toFloatData  ""data<float>""

codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCByte   ""item<uint8_t>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCLong   ""item<int64_t>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCInt    ""item<int32_t>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCDouble ""item<double>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCFloat  ""item<float>""

codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toByteData   ""data<uint8_t>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toLongData   ""data<int64_t>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toIntData    ""data<int32_t>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toDoubleData ""data<double>""
codemod -d hphp           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toFloatData  ""data<float>""

codemod -d caffe2 --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCComplexDouble ""item<std::complex<double>>""

codemod -d tc           --extensions cc,cpp,cu,cuh,h,py,hpp,mm toCFloat  ""item<float>""

Reviewed By: ezyang

Differential Revision: D9948572

fbshipit-source-id: 70c9f5390d92b82c85fdd5f8a5aebca338ab413c",146.0,167.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/templates/Tensor.h,aten/src/ATen/templates/TensorMethods.h,aten/src/ATen/test/atest.cpp,aten/src/ATen/test/basic.cpp,aten/src/ATen/test/scalar_test.cpp,caffe2/core/tensor.cc,caffe2/core/tensor.h,caffe2/core/tensor_impl.h,caffe2/mobile/contrib/opengl/test/opengl_test.cc,test/cpp/api/any.cpp,test/cpp/api/integration.cpp,test/cpp/api/jit.cpp,test/cpp/api/misc.cpp,test/cpp/api/module.cpp,test/cpp/api/modules.cpp,test/cpp/api/optim.cpp,test/cpp/api/parallel.cpp,test/cpp/api/rnn.cpp,test/cpp/api/serialize.cpp,test/cpp/api/tensor.cpp,tools/autograd/templates/Functions.cpp,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/api/include/torch/optim/serialize.h,torch/csrc/api/src/optim/lbfgs.cpp,torch/csrc/api/src/optim/serialize.cpp,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/jit/batched/BatchTensor.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/test_jit.cpp,torch/csrc/utils/pybind.h,torch/csrc/utils/python_arg_parser.h",41.0,33,5,5.002992954,44.0,19857.0,17.0,505344.512195122,4284.0,12277.33333,0.0,,0.0,1
pytorch,b99523832bbcb14f79c01875fe340e9523bf25b3,aa06bc0731f1ea33831e7011f43bb76122c0d45e,Saketh Are,saketh.are@gmail.com,Fri Jun 04 16:52:22 2021 -0700,1622825542.0,"OpInfo: minor fix in sample_inputs_diff (#59181)

Summary:
sample_inputs_diff constructs all five positional arguments for [diff ](https://pytorch.org/docs/stable/generated/torch.diff.html) but uses only the first three. This doesn't seem to be intentional.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59181

Test Plan: This change expands coverage of diff's OpInfo sample inputs. Related tests still pass.

Reviewed By: mruberry

Differential Revision: D28878359

Pulled By: saketh-are

fbshipit-source-id: 1466f6c6c341490885c85bc6271ad8b3bcdf3a3e",1.0,1.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,7315.0,1.0,79804.0,12721.0,28882.0,0.0,Corrective,1.0,1
pytorch,341fb6d11dec4321f8b910760903d5a81651bd96,aa3c8717392071f78110d1a4144f05f3bfd549c8,Mike Ruberry,mruberry@devfair044.maas,Tue Feb 04 19:08:23 2020 -0800,1580843303.0,"Adds TestViewOps, updates documentation (#32512)

Summary:
Understanding which ops return views and which return tensors with new storage is a common user issue, and an issue for developers connecting accelerators to PyTorch, too. This generic test suite verifies that ops which should return views do (and a few ops that shouldn't don't).  The documentation has also been updated for .t(), permute(), unfold(), and select() to clarify they return views.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32512

Differential Revision: D19659454

Pulled By: mruberry

fbshipit-source-id: b4334be9b698253a979e1bb8746fdb3ca24aa4e3",267.0,7.0,"test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/functional.py",4.0,2,2,0.384475127,41.0,26780.0,4.0,841557.25,14575.0,39373.33333,0.0,Feature Addition,0.0,1
pytorch,35be57970143236d74661f2415d66d496aab5476,aa4ea6e1f30f69a0a2438a4229946b26d4f3f183,eqy,eddiey@nvidia.com,Wed Mar 29 23:14:05 2023 +0000,1680131645.0,"[cuDNN][cuDNN V8 API] Fix incorrect use of `emplace` in the benchmark cache (#97838)

`emplace` does not overwrite the existing mapped value in a map if it already exists, which can lead to repeated execution of a plan that e.g., tries to allocate an OOM-inducing workspace size and retriggers either a heuristic run (or worse, a benchmark run).

CC @ptrblck @ngimel @Fuzzkatt @syed-ahmed

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97838
Approved by: https://github.com/ngimel",6.0,5.0,aten/src/ATen/native/cudnn/Conv_v8.cpp,1.0,5,1,0,2.0,792.0,1.0,595981.0,13897.0,31946.5,0.0,Corrective,1.0,1
pytorch,d868c97580f678d739ea2bf6b58f5defdcf242a8,aa6403bae6aeb452bd464a6ec4913d1b69548aca,iurii zdebskyi,47012416+izdeby@users.noreply.github.com,Tue Apr 30 17:28:33 2019 -0700,1556645313.0,"Added .bool() method

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/19928

Differential Revision: D15131923

Pulled By: izdeby

fbshipit-source-id: 3909cf4623fe85e98ceaf57fbb57745919899445",26.0,3.0,"docs/source/tensors.rst,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/_tensor_docs.py",4.0,7,4,1.722652953,41.0,15823.0,3.0,494231.0,8405.0,25077.83333,0.0,Feature Addition,0.0,1
pytorch,3e9e3ef383dbe2dcf68470a1e77b18e5b5a3257a,aa8a9fa5fc2479b0f93e73d7e77300c73dd732f7,Sam Gross,sgross@fb.com,Mon Jul 23 20:31:32 2018 -0700,1532377892.0,"Extend DispatchStub to support CUDA dispatch (#9664)

Summary:
This is a modification of the strategy from https://github.com/pytorch/pytorch/pull/8919 and https://github.com/pytorch/pytorch/pull/9579.

```
Previously, the CPU architecture-specific kernels self-registered with
the DispatchStub. When linking as part of a static library, this requires
the flag --whole-archive to be passed to the linker to ensure that the
object files for the kernels are included. Caffe2 and TensorFlow use that
strategy.

We ran into some issues with --whole-archive blowing up the binary size
of some downstream projects in Facebook. This PR avoids --whole-archive
for CPU kernels. The downside is that the generic code needs to be aware
of whether kernels are compiled with AVX and with AVX2 (via
HAVE_AVX_CPU_DEFINITION and HAVE_AVX2_CPU_DEFINITION).

The CUDA kernels still self-register with DispatchStub because the CPU
library is not aware of whether the CUDA library will be available at
runtime.

There are a few major changes to DispatchStub

 - The environment variable ATEN_CPU_CAPABILITY overrides the CPU
   capability detection code (Previous ATEN_DISABLE_AVX/AVX2)

 - DispatchStub is defined in the generic native code instead of the
   CPU_CAPABILITY_DEFAULT kernel.
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9664

Differential Revision: D8943350

Pulled By: colesbury

fbshipit-source-id: 329229b0ee9ff94fc001b960287814bd734096ef",252.0,152.0,".jenkins/pytorch/test.sh,aten/src/ATen/native/DispatchStub.cpp,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/CapabilityDispatch.h,aten/src/ATen/native/cpu/ReduceOpsKernel.h,aten/src/ATen/native/cpu/SoftmaxKernel.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h",11.0,7,2,2.766421264,7.0,1544.0,1.0,270912.0,3054.0,7372.833333,0.0,Preventative,1.0,1
pytorch,1d57a2d54c257f31f94c562feca43a687577d733,aa911939a328eff55c9b28b39ed3c43507ba8a2a,peterjc123,peter_jiachen@163.com,Wed Nov 08 18:51:35 2017 +0800,1510167095.0,Improve Windows Compatibility (for csrc/scripts) (#2941),842.0,379.0,".gitignore,cmake/FindCUDA/FindCUDA.cmake,setup.py,test/common.py,test/run_test.bat,test/test_multiprocessing.py,test/test_utils.py,tools/autograd/templates/Functions.cpp,tools/cwrap/cwrap.py,tools/cwrap/plugins/Broadcast.py,tools/cwrap/plugins/CuDNNPlugin.py,tools/cwrap/plugins/THPPlugin.py,tools/nnwrap/generate_wrappers.py,tools/setup_helpers/cuda.py,tools/setup_helpers/cudnn.py,tools/setup_helpers/nccl.py,tools/setup_helpers/nvtoolext.py,torch/__init__.py,torch/backends/cudnn/__init__.py,torch/csrc/Exceptions.h,torch/csrc/Generator.h,torch/csrc/Module.cpp,torch/csrc/Size.cpp,torch/csrc/Size.h,torch/csrc/Storage.cpp,torch/csrc/THP.h,torch/csrc/THP_export.h,torch/csrc/Types.h,torch/csrc/assertions.h,torch/csrc/autograd/functions/batch_normalization.cpp,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/init.cpp,torch/csrc/cuda/AutoGPU.cpp,torch/csrc/cuda/AutoGPU.h,torch/csrc/cuda/Module.cpp,torch/csrc/cudnn/Conv.cpp,torch/csrc/dl.c,torch/csrc/generic/SparseTensor.cpp,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/StorageSharing.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/Tensor.h,torch/csrc/generic/methods/SparseTensor.cwrap,torch/csrc/generic/methods/Tensor.cwrap,torch/csrc/generic/methods/TensorCompare.cwrap,torch/csrc/generic/methods/TensorCuda.cwrap,torch/csrc/generic/methods/TensorMath.cwrap,torch/csrc/generic/methods/TensorRandom.cwrap,torch/csrc/generic/serialization.cpp,torch/csrc/utils.h,torch/csrc/utils/auto_gpu.h,torch/csrc/utils/python_numbers.h,torch/csrc/utils/tuple_parser.cpp,torch/cuda/__init__.py,torch/cuda/nvtx.py,torch/lib/build_libs.bat,torch/multiprocessing/__init__.py,torch/utils/serialization/read_lua_file.py",60.0,26,4,5.033851665,41.0,21190.0,16.0,838336.1607142857,792.0,6518.672317,0.0,Perfective,0.0,1
pytorch,28fc59d13d57bd12b8eca25f54b38665f704aff8,aac2e685157348df3ded8c9b6d6f49808a4b202b,Akshit Khurana,axit@fb.com,Wed Apr 28 21:52:37 2021 -0700,1619646757.0,"Add inplace hardswish xnnpack op (#56715)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56715

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55801

Refactor to add inplace version of xnnpack hardswish op

Test Plan: buck test //xplat/caffe2:pt_xnnpack_test

Reviewed By: kimishpatel

Differential Revision: D27712305

fbshipit-source-id: ed1dba22b026251f891fe7b88fbaa9a42985ef2c",45.0,19.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/xnnpack/Activation.cpp,aten/src/ATen/native/xnnpack/Engine.h,aten/src/ATen/test/xnnpack_test.cpp",4.0,6,1,1.083897541,7.0,1130.0,1.0,59.0,11409.0,25788.0,0.0,Feature Addition,0.0,1
pytorch,daba5519220b0143b825b31b8f10786a6d111255,aac3c7bd063815f1ad73d830b8271fc737b85490,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Thu Aug 12 16:45:17 2021 -0700,1628786717.0,"[reland] OpInfo: `adaptive_avg_pool2d` (#62935)

Summary:
This PR is an attempt to reland https://github.com/pytorch/pytorch/pull/62704.

**What has changed?**

The op has non-deterministic behavior, hence an appropriate `gradcheck` wrapper had to be added.

cc: mruberry zou3519 heitorschueroff kshitij12345

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62935

Reviewed By: anjali411

Differential Revision: D30225095

Pulled By: zou3519

fbshipit-source-id: 644873cc21d44b19c8b68f9edff691913778de0e",27.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8595.0,1.0,172745.0,14584.0,33305.5,0.0,Feature Addition,0.0,1
pytorch,da79697d454aefe5d2b551edb6738a99f47ac1d4,ab18aaeba7c97d210a4c2bb615e962bb43228f17,Richard Zou,zou3519@users.noreply.github.com,Tue Feb 13 18:11:14 2018 -0500,1518545474.0,Clarify output shapes of reduce=False losses (#5082),15.0,14.0,"torch/nn/functional.py,torch/nn/modules/loss.py",2.0,3,1,0.929363626,36.0,2961.0,1.0,8357.0,440.0,2272.5,0.0,,0.0,1
pytorch,2e88a78d2ec5bb268ab8dc987800691ed2c49d70,ab2297dfe6a76383c3cec050d5aad8c3226eded4,Ailing Zhang,ailzhang@fb.com,Wed Mar 11 05:25:55 2020 -0700,1583904355.0,"Add Tensor overload for start in narrow. (#34317)

Summary:
https://github.com/pytorch/pytorch/issues/31558
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34317

Differential Revision: D20294333

Pulled By: ailzhang

fbshipit-source-id: 47c6646ae298e04a455923bd5048db026a5e3c7c",22.0,0.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py",3.0,5,2,1.528504079,41.0,23754.0,3.0,336891.0,15360.0,41110.33333,0.0,Feature Addition,0.0,1
pytorch,71f6cca992826356b6b9a7bd78c6c7070306e36a,ab44002ac8792845acbc58a3e2ed8290bfee12d3,theweiho,theweiho@users.noreply.github.com,Tue May 01 00:01:27 2018 -0700,1525132887.0,"Open-source extractMetaNetDef & runGlobalInitialization, add new Predictor constructor from db file, and add run_map_outputs (#7063)

* Refactor extractMetaNetDef and runGlobalInitialization into open...

* Fix test by making get output blobs optional

* Update test instead of making output blobs optional",169.0,12.0,"caffe2/core/predictor.cc,caffe2/core/predictor.h,caffe2/core/predictor_test.cc,caffe2/core/predictor_utils.cc,caffe2/core/predictor_utils.h",5.0,2,1,1.934602228,5.0,422.0,2.0,2171636.333333333,1.0,2.5,0.0,Corrective,1.0,1
pytorch,e4a3aa9295533d7e502a5d7dd5eceb53dfb6c54a,ab5776449cf6b2829edcd553f150fec51f510c2f,Sam Gross,colesbury@gmail.com,Fri Dec 30 22:01:47 2016 -0500,1483135307.0,Add documentation for some torch.xxx functions (#382),342.0,28.0,"docs/source/torch.rst,torch/csrc/Module.cpp,torch/docs.py",3.0,4,2,0.187413009,19.0,1538.0,2.0,67213.0,107.0,74.40530417,0.0,Feature Addition,0.0,1
pytorch,df1d68d52e15dffabc0ef714e8e064eda7105cd7,ab75d64e6ec2d9eb6dc1e8c95b4341c9f87c5c71,Pritam Damania,pritam.damania@fb.com,Wed Feb 05 23:26:06 2020 -0800,1580945166.0,"Add ability to abort NCCL communicators from the store. (#32895)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32895

When a particular rank calls `ncclCommAbort` on a communicator, it is
important to ensure all other ranks call `ncclCommAbort` on their respective
communicators. If this is not done, the other ranks could get stuck causing the
GPU to spin with 100% utilization.

To alleviate this issue, whenever any rank calls `ncclCommAbort` we put the
unique communicator id in the store. The NCCL watchdog thread then monitors the
store and aborts any communicators found in the store as ""aborted"".

A few more general fixes in this PR:

1) Use std::shared_ptr for the store in PrefixStore. PrefixStore was using a
reference to the store and when that reference went out of scope the store
object it was holding onto was invalid. This caused a segfault in the watchdog
thread.
2) Enhanced logging for the watchdog thread.

Test Plan: waitforbuildbot

Differential Revision: D19638159

fbshipit-source-id: 596cd87c9fe6d4aeaaab4cb7319cc37784d06eaa",215.0,92.0,"test/distributed/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/NCCLUtils.hpp,torch/lib/c10d/PrefixStore.cpp,torch/lib/c10d/PrefixStore.hpp,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/lib/c10d/test/FileStoreTest.cpp,torch/lib/c10d/test/HashStoreTest.cpp,torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp,torch/lib/c10d/test/TCPStoreTest.cpp",11.0,9,2,2.613906308,4.0,6132.0,8.0,12209359.454545457,14610.0,39412.83333,0.0,Corrective,1.0,1
pytorch,6d7fe76317a08a7bd50daed77216632128116653,ac9e79e561e8691a8301f6529809b8267cc6ee20,BowenBao,bowbao@microsoft.com,Thu May 13 20:37:10 2021 -0700,1620938230.0,"Add a new operator for fill_() function. (#56859) (#57596)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57596

Add the corresponding symbolic function and test for fill_() function.

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D28393520

Pulled By: SplitInfinity

fbshipit-source-id: 3e177f88d3776d0d4a9d5e7ec7df4e6629738799

Co-authored-by: Jay Zhang <jiz@microsoft.com>",22.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,1,3.0,11723.0,1.0,74.0,12047.0,27311.0,0.0,Feature Addition,0.0,1
pytorch,6470b5bd2122e48b1675886e6f0b58b53517bca5,acb0ce8885ca92f2d8652a99a80ec81a5319891b,Adam Paszke,adam.paszke@gmail.com,Tue Jan 31 20:47:54 2017 +0100,1485895674.0,Add LongTensor indexing support,58.0,4.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.740865686,23.0,3654.0,1.0,154206.0,441.0,4345.116645,0.0,Feature Addition,0.0,1
pytorch,28f3d50f9d9e3123a439de370d6fd4bf01ee989a,acb73c729b296547b3219cbb3c508a25c5bad233,vfdev,vfdev.5@gmail.com,Mon Oct 30 17:45:37 2017 +0100,1509385537.0,"Space is missing in __repr___ of conv (#3229)

* - Remove spaces in `__repr__` of layers
- Replace `size` by `kernel_size` in `__repr__` of a pooling layer

* Fix flake8 errors",72.0,64.0,"torch/nn/modules/activation.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/linear.py,torch/nn/modules/module.py,torch/nn/modules/normalization.py,torch/nn/modules/padding.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py",9.0,3,1,2.492604633,37.0,3776.0,3.0,289540.22222222225,2021.0,23887.35823,0.0,Corrective,1.0,1
pytorch,aa99aa1cb81fa4f5ff98785fcaba2cb88bedb57f,acb7df11a29c9c4de39ac863b0f25fd762cdfad1,Naman Jain,naman1205jain@gmail.com,Tue Apr 10 16:08:21 2018 +0500,1523376501.0,"Add torch.randint and torch.randint_like functions (#6136)

Adds randint and randint_like to TensorFactories.cpp",123.0,0.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,torch/_torch_docs.py",4.0,6,3,1.964252825,39.0,13561.0,4.0,433936.25,190.0,12921.22461,0.0,Feature Addition,0.0,1
pytorch,de53de39d74236b06b59222ba66e9b88cfa7115a,aceceb3d5c4f8b0632c01b75d6c51c08609cb131,Winston Smith,76181208+imaginary-person@users.noreply.github.com,Tue Apr 13 20:20:01 2021 -0700,1618345201.0,"Reland #50999 (Added pow() on CPU for float16 & bfloat16) (#55280)

Summary:
#### Reason for relanding
Line 1607 of `torch/testing/_internal/common_methods_invocations.py` of https://github.com/pytorch/pytorch/issues/50999  had `dtype` instead of `dtype=torch.bool`, so 4 of the 9 sample inputs for `bool` had incorrect dtype. This bug was caught by https://github.com/pytorch/pytorch/issues/54949.

1. Added support for pow() on CPU for `float16` (`Half`) and `bfloat16` types.
Both `pow(Tensor, Scalar)` and `pow(Tensor, Tensor)` are now supported for the aforementioned types.
However autograd isn't supported for `Float16` on CPU yet, as `log_vml_cpu` can't be enabled for it.
2. heitorschueroff added `pow_tensor_scalar_optimized_kernel` to refactor & simplify `PowKernel.cpp`.
It provides a common path for all the complex types & floating point types (except Float16, due to lack of complete AVX2 vectorization support for it).  It replaced code that had previously been duplicated for (float, double) and complex types,
so PowKernel.cpp looks a lot cleaner now.
3. Enabled (unskipped) some tests for `erf`, `erfc`,`erfinv`, `tan` and `linalg.vector.norm` which were being skipped earlier due to `pow()` not having been implemented for `float16` & `bfloat16`.
4. Added an OpInfo for `pow()` & enabled some test cases for `pow()`.
5. Extended the coverage of existing tests for `pow` in `test_binary_ufuncs.py` in order to enable comparison with `numpy`, even with discontiguous tensors, and added a test to ensure that a runtime error is raised for `pow`'s inplace variant if resizing the base tensor is required during its invocation.
6. Added `float16` & `bfloat16` to `square`'s dtype lists in its `UnaryUfuncInfo`.
7. Removed redundant `dtypesIfCPU` and `dtypesIfCUDA` from `OpInfo`s where they are equal to `dtypes`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55280

Reviewed By: jbschlosser

Differential Revision: D27591772

Pulled By: heitorschueroff

fbshipit-source-id: c7420811b32595bb3353149a61e54a73f2eb352b",321.0,254.0,"aten/src/ATen/native/cpu/PowKernel.cpp,test/test_binary_ufuncs.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",4.0,9,3,1.661673348,43.0,15570.0,4.0,355841.25,10734.0,23730.0,0.0,Corrective,1.0,1
pytorch,cfd18e105fe795072edafe54c1f5861967ca746a,ad44670fa1ce2dad7e2cdc3f90d27668e88e9548,Edward Z. Yang,ezyang@fb.com,Mon Aug 29 13:08:43 2022 -0700,1661778523.0,"Back out ""Revert D38984222: Don't introduce new overload for SymInt (#83628)"" (#84173)

Also Back out ""Revert D39075159: [acc_tensor] Use SymIntArrayRef for overloaded empty.memory_format's signature""

Original commit changeset: dab4a9dba4fa
Original commit changeset: dcaf16c037a9

Original Phabricator Diff: D38984222
Original Phabricator Diff: D39075159

Also update Metal registrations for C++ registration changes.

Also update NNPI registration to account for tightened schema checking

Differential Revision: [D39084762](https://our.internmc.facebook.com/intern/diff/D39084762/)

**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D39084762/)!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84173
Approved by: https://github.com/Krovatkin",864.0,749.0,".github/ci_commit_pins/xla.txt,aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/FunctionalInverses.cpp,aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h,aten/src/ATen/core/custom_class.cpp,aten/src/ATen/core/dispatch/OperatorEntry.cpp,aten/src/ATen/core/dynamic_type.cpp,aten/src/ATen/core/dynamic_type.h,aten/src/ATen/core/function_schema.cpp,aten/src/ATen/core/function_schema.h,aten/src/ATen/core/jit_type.h,aten/src/ATen/core/op_registration/infer_schema.cpp,aten/src/ATen/core/op_registration/infer_schema.h,aten/src/ATen/native/MathBitFallThroughLists.h,aten/src/ATen/native/MetaTensor.cpp,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cudnn/ConvShared.cpp,aten/src/ATen/native/metal/MetalAten.mm,aten/src/ATen/native/metal/ops/MetalReshape.mm,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/ATen/native/mkldnn/TensorFactories.cpp,aten/src/ATen/native/mps/TensorFactory.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/TensorFactories.cpp,aten/src/ATen/native/sparse/SparseCsrTensor.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/ts_native_functions.yaml,aten/src/ATen/native/vulkan/ops/Factory.cpp,aten/src/ATen/native/vulkan/ops/Shape.cpp,aten/src/ATen/templates/CompositeViewCopyKernels.cpp,aten/src/ATen/test/ExclusivelyOwned_test.cpp,aten/src/ATen/test/MaybeOwned_test.cpp,aten/src/ATen/test/extension_backend_test.cpp,aten/src/ATen/test/math_kernel_test.cpp,functorch/functorch/csrc/BatchRulesFactory.cpp,functorch/functorch/csrc/BatchRulesViews.cpp,test/cpp/jit/test_custom_class.cpp,test/cpp/jit/test_custom_class_registrations.cpp,test/cpp/jit/test_custom_class_registrations.h,test/cpp/lazy/test_lazy_ops.cpp,test/cpp_extensions/open_registration_extension.cpp,test/cpp_extensions/ort_extension.cpp,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/test_decomp.py,test/test_dynamic_shapes.py,test/test_meta.py,test/test_nn.py,test/test_profiler_tree.py,test/test_proxy_tensor.py,tools/autograd/derivatives.yaml,tools/autograd/gen_inplace_or_view_type.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_trace_type.py,tools/autograd/gen_variable_factories.py,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py,tools/test/test_codegen.py,torch/_subclasses/fake_tensor.py,torch/csrc/jit/codegen/cuda/interface.cpp,torch/csrc/jit/frontend/schema_type_parser.cpp,torch/csrc/jit/python/pybind_utils.cpp,torch/csrc/jit/python/pybind_utils.h,torch/csrc/jit/runtime/static/ops.cpp,torch/csrc/lazy/core/ir_builder.h,torch/csrc/lazy/ts_backend/ts_native_functions.cpp,torchgen/api/autograd.py,torchgen/api/cpp.py,torchgen/api/dispatcher.py,torchgen/api/lazy.py,torchgen/api/native.py,torchgen/api/python.py,torchgen/api/structured.py,torchgen/api/translate.py,torchgen/api/types.py,torchgen/api/ufunc.py,torchgen/api/unboxing.py,torchgen/dest/lazy_ir.py,torchgen/dest/register_dispatch_key.py,torchgen/gen.py,torchgen/gen_backend_stubs.py,torchgen/gen_functionalization_type.py,torchgen/model.py,torchgen/static_runtime/generator.py",89.0,53,7,5.471113389,49.0,103649.0,14.0,2842153.404494382,6907.0,16284.5,0.0,Feature Addition,0.0,1
pytorch,0cb5943be8e1581f0ee2b2a76d0b8eec81757654,ad5fdef6acb11862b74bb711734d00f46035896f,Sam Gross,colesbury@gmail.com,Mon Oct 31 16:12:22 2016 -0400,1477930342.0,Make every user-visible Tensor have a Storage (#179),207.0,232.0,"setup.py,test/test_torch.py,tools/cwrap/plugins/THPPlugin.py,torch/csrc/Generator.cpp,torch/csrc/cuda/Module.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/Tensor.h,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/serialization.cpp,torch/serialization.py",10.0,8,3,1.895566264,11.0,8637.0,4.0,250291.2,54.0,63.39007937,0.0,,0.0,1
pytorch,199435af9096ea03f95482b17fe1eae6ac368f6d,ad7a2eb1c9f71331afcbcd10f0b39cb2d6fa4259,Raghavan Raman,raghavanr@fb.com,Mon Sep 14 20:18:42 2020 -0700,1600114722.0,"Simplify nested Min and Max patterns. (#44142)

Summary:
Improve simplification of nested Min and Max patterns.

Specifically, handles the following pattern simplications:
  * `Max(A, Max(A, Const)) => Max(A, Const)`
  * `Max(Min(A, B), Min(A, C)) => Min(A, Max(B, C))`
  * `Max(Const, Max(A, OtherConst) => Max(A, Max(Const, OtherConst))`
     - This case can have an arbitrarily long chain of Max ops. For example: `Max(5, Max(x, Max(y, Max(z, 8)))) => Max(Max(Max(x, 8), y), z)`

Similarly, for the case of Min as well.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44142

Reviewed By: albanD

Differential Revision: D23644486

Pulled By: navahgar

fbshipit-source-id: 42bd241e6c2af820566744c8494e5dee172107f4",1111.0,14.0,"test/cpp/tensorexpr/test_boundsinference.cpp,test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/hash_provider.cpp,torch/csrc/jit/tensorexpr/hash_provider.h,torch/csrc/jit/tensorexpr/ir.h,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_mutator.h,torch/csrc/jit/tensorexpr/ir_printer.cpp,torch/csrc/jit/tensorexpr/ir_printer.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/ir_visitor.cpp,torch/csrc/jit/tensorexpr/ir_visitor.h",15.0,7,2,1.986164275,2.0,9360.0,7.0,1408734.4666666666,5098.0,11646.0,0.0,Perfective,0.0,1
pytorch,60263e0f5acb7c0dbb15c824a711cf7ec8c478e1,ad823888a10c35fe4e60e5b3c1f16375ae60ec32,James Reed,jamesreed@fb.com,Thu Apr 08 17:58:41 2021 -0700,1617904721.0,"[FX] Speed up _Namespace.create_name (#55580)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55580

Test Plan: Imported from OSS

Reviewed By: ansley

Differential Revision: D27641156

Pulled By: jamesr66a

fbshipit-source-id: d2443d41c8d84dddb1794a7901e2d09ae3639846",13.0,6.0,torch/fx/graph.py,1.0,2,1,0,1.0,1094.0,1.0,678168.0,10570.0,23369.0,0.0,,0.0,1
pytorch,9e6b7e6e6e10ad0a6f42b6d73e16158db2e95f95,adaf80bcbe918c0c1fe33f25dfddd906d77e4ab5,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri May 07 10:25:58 2021 -0700,1620383158.0,"Update internal code for at::_lu_with_info (#56612)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56612

The goal of this refactoring is to make the `torch.linalg.solve`
to be a composition of calls to `lu_stub` and `lu_solve_stub`.
Once `lu_stub` and `lu_solve_stub` have cuSOLVER-based codepath,
`torch.linalg.solve` will have it as well.

Replaced `lu_with_info_{cpu, cuda}` with one function that calls
to `lu_stub`.
Split MAGMA-based `apply_lu` into `apply_lu_looped_magma`
and `apply_lu_batched_magma`. This simplifies the future switch to
cuSOLVER and cuBLAS libraries.

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D28248756

Pulled By: mruberry

fbshipit-source-id: 40e02b5be4ff5f78885bcc95685aba581043e096",197.0,114.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml",5.0,5,1,1.47832774,12.0,17306.0,3.0,60960.2,11769.0,26549.5,0.0,Perfective,0.0,1
pytorch,5b0be9de591a2b4131d605155034f9c6940ed2cf,adbcb3c1dc7058ef35d9be455971c2099e2928d6,Adam Paszke,adam.paszke@gmail.com,Fri Aug 10 21:46:56 2018 -0700,1533937616.0,"Move dropout and alpha dropout to ATen (#10384)

Summary:
zdevito ezyang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10384

Reviewed By: ezyang

Differential Revision: D9272583

Pulled By: apaszke

fbshipit-source-id: ed5d37b28ce9ff25800bbaa0daf066cfbf1f9921",233.0,175.0,"aten/src/ATen/native/Dropout.cpp,aten/src/ATen/native/native_functions.yaml,test/expect/TestJit.test_alexnet.expect,test/expect/TestJit.test_dropout.expect,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/nn/_functions/dropout.py,torch/nn/backends/thnn.py,torch/nn/functional.py,torch/onnx/symbolic.py",10.0,14,4,2.494128721,43.0,9023.0,8.0,2306370.4444444445,3405.0,9172.333333,0.0,,0.0,1
pytorch,9757ad35b0b56cf955f294e751de9b437f9bb4ff,addebf110f2dfa86dd42e2a19ae8ae170d31605c,Lu Fang,lufang@fb.com,Tue Jan 22 04:13:07 2019 -0800,1548130387.0,"Move away from ConstantFill (#16214)

Summary:
Prerequisite of https://github.com/onnx/onnx/pull/1434
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16214

Reviewed By: BIT-silence

Differential Revision: D13755116

Pulled By: houseroad

fbshipit-source-id: a46be8d7df959b5ede93e1f9c911a9a9326e6879",19.0,24.0,"caffe2/onnx/backend.cc,test/onnx/expect/TestOperators.test_full.expect,torch/onnx/symbolic.py",3.0,7,3,1.537128182,14.0,3476.0,3.0,1357525.0,6570.0,20379.33333,0.0,,0.0,1
pytorch,45e980a243d8a751f6d4cc3e4590398f303fd46d,adfb8a4888c5c471aae0201f5a5a62ee1def8f0b,Brian Vaughan,bvaughan@fb.com,Mon Nov 18 20:37:04 2019 -0800,1574109424.0,"Fix bug in atomicAdd for int16_t (#29231)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29231

Fixes: https://github.com/pytorch/pytorch/issues/29153

Bug is that atomicAdd doesn't correctly add values for some dtypes due to incorrect casting. Was returning zeros.

Incorrect behavior before this PR:

```
In [23]: sparse=torch.sparse_coo_tensor(indices=torch.tensor([[0,0],[1,1]]), values=torch.tensor([5, 6], dtype=torch.int16), size=(2,2), device='cuda', dtype=torch.int16 )

In [24]: sparse
Out[24]:
tensor(indices=tensor([[0, 0],
                       [1, 1]]),
       values=tensor([5, 6]),
       device='cuda:0', size=(2, 2), nnz=2, dtype=torch.int16,
       layout=torch.sparse_coo)

In [25]: sparse.coalesce()
Out[25]:
tensor(indices=tensor([[0],
                       [1]]),
       values=tensor([11]),
       device='cuda:0', size=(2, 2), nnz=1, dtype=torch.int16,
       layout=torch.sparse_coo)

In [26]: sparse.to_dense()
Out[26]:
tensor([[0, 0],
        [0, 0]], device='cuda:0', dtype=torch.int16)

In [27]: sparse.coalesce().to_dense()
Out[27]:
tensor([[ 0, 11],
        [ 0,  0]], device='cuda:0', dtype=torch.int16)

In [30]: torch.add(torch.zeros([2,2],dtype=torch.int16, device='cuda'), sparse)
Out[30]:
tensor([[0, 0],
        [0, 0]], device='cuda:0', dtype=torch.int16)
```

Test Plan: Imported from OSS

Differential Revision: D18575666

Pulled By: nairbv

fbshipit-source-id: 9b193b386bf4a9615014aa890d2e9f4f694940ac",38.0,13.0,"aten/src/THC/THCAtomics.cuh,test/test_torch.py",2.0,4,2,0.95260921,40.0,14740.0,2.0,3937996.0,13245.0,36320.33333,0.0,Corrective,1.0,1
pytorch,e4c303f37327506e6120208d74f51d573dd002a4,ae0c04c773d9fe784d04d452cd289b200e906670,gchanan,gregchanan@gmail.com,Fri Mar 09 20:29:29 2018 -0500,1520627369.0,"Add torch.empty, torch.full and new_ size Tensor factory methods. (#5668)

* Add torch.empty, torch.full and new_ size Tensor factory methods.

This adds torch.full, torch.empty equivalents of np.full, np.empty.
In addition, this adds size-based Tensor factory methods new_empty, new_ones, new_full, new_zeros,
which is meant to complete the separation of the legacy ""new"" method into data-based and size-based
functions.

This also fixes an issue in sparse zeros_like when the dtype didn't match the argument dtype.

* Get rid of unnecessary zero in sparse tensor zeros_like.

* Fix test if only 1 cuda device.",279.0,22.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,test/test_sparse.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h,torch/testing/__init__.py",8.0,12,4,2.673875284,38.0,8391.0,7.0,668771.625,598.0,1841.405869,0.0,Corrective,1.0,1
pytorch,647569e546ea1866fd0be1f086d588f07947ab5a,ae214f67a54079f6daf3a581f7505f213929933a,anjali411,chourdiaanjali123@gmail.com,Fri Dec 27 22:38:19 2019 -0800,1577486299.0,"updated code to ensure error check for negative dims

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31636

Differential Revision: D19233031

Pulled By: anjali411

fbshipit-source-id: c29265ddd1f887f1a0b98aca56a2691d7584353d",5.0,2.0,"aten/src/ATen/native/ReduceOps.cpp,test/test_torch.py",2.0,5,2,0.863120569,40.0,15847.0,2.0,532361.0,14006.0,38123.33333,0.0,,0.0,1
pytorch,7820bab01a05ed3c83a27dfd749976facd893a84,ae24147211e1ebd7b34e4c29d390fef3aca38982,Samantha Andow,samdow@fb.com,Fri Jun 10 16:27:52 2022 -0400,1654878472.0,"[functorch] Fix cuda tests, skip mac unexpected successes (pytorch/functorch#864)

* a terrible hack that makes cuda work again

* add mac test skips

* reinstall with cuda pytorch--idk why this is needed

* add higher tolerance for conv_transpose3d in vmap exhaustive

* try without pip

* use only conda, xfail vjpvmap clamp",11.0,11.0,"functorch/.circleci/unittest/linux/scripts/install.sh,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,6,1,1.240670532,1.0,5721.0,2.0,0.0,1104.0,1498.0,0.0,Corrective,1.0,1
pytorch,f4cf1d6d18a895eceab0765b8ce112721a7a9cf5,ae2b2cbbec09bb27bccb3318f5505a73326f59f7,Gregory Chanan,gchanan@fb.com,Wed May 03 22:50:44 2017 -0700,1493851844.0,Make keepdim work with autograd.,106.0,78.0,"test/common_nn.py,test/test_autograd.py,test/test_torch.py,torch/autograd/_functions/reduce.py,torch/autograd/_functions/stochastic.py,torch/autograd/_functions/tensor.py,torch/autograd/variable.py,torch/nn/_functions/loss.py,torch/nn/functional.py",9.0,6,2,2.126326722,29.0,8397.0,1.0,711.0,748.0,9949.060487,0.0,,0.0,1
pytorch,96910251e0930e68e99e422075fbadc5e39429bb,ae342fd0760d66ca870484553bce805cc466a4bc,Syed Tousif Ahmed,syed.ahmed.emails@gmail.com,Wed Jun 12 19:51:19 2019 -0700,1560369079.0,"Refactor Random Number Generators in ATen (#21364)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21364
ghimport-source-id: ca7d37e10190ba46dc8512f437404ca9216d3369

Differential Revision: D15696497

Pulled By: ezyang

fbshipit-source-id: 2e713b8566ae915e175b5a79ac1dd9b86cc2a23d",1992.0,824.0,"aten/src/ATen/CPUGenerator.cpp,aten/src/ATen/CPUGenerator.h,aten/src/ATen/CPUTypeDefault.cpp,aten/src/ATen/CPUTypeDefault.h,aten/src/ATen/CheckGenerator.h,aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,aten/src/ATen/UndefinedType.cpp,aten/src/ATen/UndefinedType.h,aten/src/ATen/Utils.h,aten/src/ATen/core/Array.h,aten/src/ATen/core/DeprecatedTypeProperties.cpp,aten/src/ATen/core/DeprecatedTypeProperties.h,aten/src/ATen/core/DistributionsHelper.h,aten/src/ATen/core/Generator.cpp,aten/src/ATen/core/Generator.h,aten/src/ATen/core/MT19937RNGEngine.h,aten/src/ATen/core/PhiloxRNGEngine.h,aten/src/ATen/core/Type.h,aten/src/ATen/cuda/Array.h,aten/src/ATen/cuda/CUDAGenerator.cpp,aten/src/ATen/cuda/CUDATypeDefault.h,aten/src/ATen/cuda/detail/OffsetCalculator.cuh,aten/src/ATen/function_wrapper.py,aten/src/ATen/gen.py,aten/src/ATen/native/Distributions.cpp,aten/src/ATen/native/Distributions.h,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Distributions.cu,aten/src/ATen/native/cuda/IndexKernel.cu,aten/src/ATen/native/cuda/Reduce.cuh,aten/src/ATen/templates/GeneratorDerived.h,aten/src/ATen/templates/LegacyTHFunctions.cpp,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeDerived.h,aten/src/ATen/templates/TypeExtension.cpp,aten/src/ATen/templates/TypeExtension.h,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/cpu_generator_test.cpp,aten/src/ATen/test/scalar_test.cpp,aten/src/TH/CMakeLists.txt,aten/src/TH/TH.h,aten/src/TH/THGenerator.hpp,aten/src/TH/THRandom.cpp,aten/src/TH/THRandom.h,aten/src/TH/THTensor.h,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/TH/generic/THTensorRandom.cpp,aten/src/TH/generic/THTensorRandom.h,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.cpp,aten/src/TH/generic/THVectorDispatch.cpp,aten/src/TH/vector/AVX2.cpp,aten/src/TH/vector/AVX2.h,aten/src/THCUNN/generic/RReLU.cu,aten/src/THNN/generic/RReLU.c,aten/src/THNN/generic/THNN.h,aten/tools/run_tests.sh,docs/source/torch.rst,test/test_torch.py,tools/autograd/templates/VariableType.h,tools/cwrap/plugins/NNExtension.py,torch/__init__.py,torch/__init__.pyi.in,torch/_torch_docs.py,torch/csrc/Generator.cpp,torch/csrc/Generator.h,torch/csrc/Module.cpp,torch/csrc/Module.h,torch/csrc/autograd/VariableTypeManual.cpp,torch/random.py",73.0,30,5,4.689147547,43.0,33624.0,42.0,7412259.621212121,9371.0,27206.33333,0.0,Perfective,0.0,1
pytorch,0a159b0a3a78a80fb0f9082087a98f87f2dea986,ae392a77a685984ae301530fea6794bc1bdb6f86,SsnL,tongzhou.wang.1994@gmail.com,Thu May 14 16:03:52 2020 -0700,1589472232.0,"Add better device idx parse checks (#37376)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/32079
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37376

Differential Revision: D21476036

Pulled By: zou3519

fbshipit-source-id: 86907083c23cbaf165b645307fb340f2656b814e",88.0,44.0,"c10/core/Device.cpp,c10/util/string_utils.h,test/test_cuda.py,test/test_torch.py",4.0,4,2,1.598725105,42.0,21234.0,4.0,1246340.25,2057.0,5201.5,0.0,Corrective,1.0,1
pytorch,bcd9f189f406117ec5f11db16a1fd3c4deec3c22,ae4ec7de1e6f60aa7dcf4d5187c720f2f40543c3,Xuehai Pan,XuehaiPan@pku.edu.cn,Wed Jan 18 19:16:32 2023 +0000,1674069392.0,"Fix and update type hints for `make_functional.py` (#91579)

Changes in details:

- Fix and update some out-of-date type hints in `_functorch/make_functional.py`.
- ~Explicitly use `OrderedDict` for order-sensitive mappings.~

	In `create_names_map()`, `_swap_state()`, and `FunctionalModuleWithBuffers.__init__()`, the unordered `dict` was used. The key order should be preserved for `dict.items()` while it is required to `zip` with a tuple of `params`/`buffers`. Although since Python 3.6, the built-in dictionary is insertion ordered ([PEP 468](https://peps.python.org/pep-0468)). Explicit is better than implicit.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91579
Approved by: https://github.com/zou3519",176.0,72.0,".lintrunner.toml,torch/_functorch/make_functional.py",2.0,2,1,0.067669825,3.0,1424.0,2.0,2020867.5,11401.0,26151.5,0.0,Corrective,1.0,1
pytorch,2ab4c9dbec9468d3ddc0b3d2688f30dda776e7cc,ae55865a3b8d4189482dc7a4cdd46476be514c02,Wei Yang,38509346+weiyangfb@users.noreply.github.com,Thu Jun 14 20:42:20 2018 -0700,1529008940.0,"Migrated hardshrink() to ATen and deprecated nn.Hardshrink() (#8117)

* 1. added hardshrink() to ATen (CPU + GPU); 2. removed nn.Hardshrink(); 3. reusing previous tests for nn.Hardshrink() and included CUDA tests at test_nn; 4. default parameter lambda=0.5 is not working yet

* optimized memory read/write

* 1. pass in lambd as scalar for CPU/CUDA_apply*; 2. removed tests for hardshrink at test_legacy_nn

* fixes test_utils

* 1. replace zeros_like with empty_like; 2. use scalar_cast in cuda

* 1. printing lambd value; 2. default lambd=0.5 is still failing

* getting around Scalar bug buy removing default value of lambd from native_functions.yaml, and declare it at nn/functional.py

* cleaned up debug printf",164.0,77.0,"aten/doc/Functions.h,aten/doc/Type.h,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/nn.yaml,aten/src/THNN/generic/THNN.h,aten/src/THNN/init.cpp,test/test_legacy_nn.py,test/test_nn.py,test/test_torch.py,test/test_utils.py,tools/autograd/derivatives.yaml,torch/nn/functional.py",14.0,13,4,3.332603717,42.0,28394.0,10.0,1873078.6153846155,2731.0,25219.85823,0.0,Corrective,1.0,1
pytorch,1f391a42f7ba00d27cf178943355c533398c7cfc,ae61f3ff4254447f3c98f85678626ffddde7e32f,Leonid Vlasenkov,leo.vlasenkov@gmail.com,Tue Jun 27 14:04:54 2017 +0300,1498572294.0,adds poisson NLL loss (#1779),103.0,2.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/loss.py",5.0,6,3,1.99103063,30.0,5979.0,3.0,331374.8,1049.0,12571.54911,0.0,Feature Addition,0.0,1
pytorch,b4bc0d249f782d3afc877e61189f7427f6d55968,ae68e455be3c264b2b3bcc61819dda5627f751a9,Peter Bell,peterbell10@live.co.uk,Fri Aug 19 02:32:16 2022 +0100,1660876336.0,"Enable formatting in all of testing/_internal/opinfo (#83559)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83559
Approved by: https://github.com/albanD",604.0,305.0,".lintrunner.toml,torch/testing/_internal/opinfo/core.py,torch/testing/_internal/opinfo/utils.py",3.0,4,1,0.325179152,3.0,3268.0,1.0,35934.0,6603.0,15255.5,0.0,,0.0,1
pytorch,52cc0c2c373ecc669e3e08139c2dec665845606a,ae6a68ad09f55593d1d6a40e9d0df771e834a431,Luca Wehrstedt,lcw@fb.com,Fri Jun 12 14:08:05 2020 -0700,1591970885.0,"[TensorPipe] Add extensive logging (#39781)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39781

Use a new feature of TensorPipe where a pipe can tell you the name of the remote endpoint, in order to make the logging messages more informative: whenever there is a failure on a pipe, say which worker this was to/from, and the ID of the message involved.

Also, add plenty of verbose logging, to help with debugging. This is off by default, but can be enabled by setting the `GLOG_v` env var to a value of 1 or higher.
ghstack-source-id: 105777704

Test Plan: Builds.

Differential Revision: D21973150

fbshipit-source-id: 9e3ce1b9977e1e9ecd91ff4a6fe82786dc79a702",88.0,23.0,torch/csrc/distributed/rpc/tensorpipe_agent.cpp,1.0,4,1,0,1.0,785.0,1.0,302540.0,2785.0,6809.5,0.0,Corrective,1.0,1
pytorch,0fb58d76a1dd7b0ff6cdd374aff83e9a294b3047,ae9f39eb580c4d92157236d64548b055f71cf14b,James Reed,jamesreed@fb.com,Sun Dec 06 01:22:00 2020 -0800,1607217720.0,"[FX][1/2] Make docstrings pretty when rendered (#48738)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48738

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D25280867

Pulled By: jamesr66a

fbshipit-source-id: d08641c19a6c69b4042389c800a48e699f0be628",454.0,286.0,"torch/fx/__init__.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/fx/node.py,torch/fx/proxy.py,torch/fx/symbolic_trace.py",6.0,2,1,1.816946033,1.0,1839.0,6.0,916386.6666666666,7243.0,16359.0,0.0,Non Functional,0.0,1
pytorch,61063ebadec3b795cdbb224c7a5e61733d61c2a0,aea6ba4bcd850ecc1b1280c9bc229679b859f030,Sam Gross,colesbury@gmail.com,Tue Nov 29 17:35:03 2016 -0500,1480440903.0,"Support pinned memory in the DataLoader (#265)

DataLoader now supports the constructor argument 'pin_memory'. When set
to true, tensors in the sample are copied to pinned memory. This happens
in a background thread when num_workers > 1.",64.0,3.0,"test/test_dataloader.py,torch/utils/data/dataloader.py",2.0,4,2,0.817138776,14.0,322.0,1.0,260511.0,80.0,50.41491189,0.0,,0.0,1
pytorch,d612855b91d4d9bf57e4f81d1721dd44286ee4b5,aeb60945384c283c8a1e362b0bac0097535e8714,Christian Puhrsch,cpuhrsch@fb.com,Wed Sep 05 15:44:37 2018 -0700,1536162277.0,"Unify opt flag for cmake codegen (#11227)

Summary:
Also enables debug for non-MSVC for kernel codegen
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11227

Differential Revision: D9656506

Pulled By: cpuhrsch

fbshipit-source-id: 667195cb55de1a1a9042b6b1c4436e9c6c743333",24.0,25.0,cmake/Codegen.cmake,1.0,1,1,0,3.0,188.0,1.0,1340947.0,3854.0,10749.83333,0.0,Corrective,1.0,1
pytorch,18b9d6b20a852de9e57839986fbd12cf15531b31,aeb6cf356faa21f5286ebfc8686d87d8cac4b4da,Mike Ruberry,mruberry@devfair044.h1.fair,Fri Apr 15 22:20:40 2022 +0000,1650061240.0,"Adds multilabel_soft_margin_loss opinfo

Per title
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75883
Approved by: https://github.com/ngimel",45.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,16721.0,1.0,2056.0,2336.0,5490.5,0.0,Feature Addition,0.0,1
pytorch,8612b0bbd8ad5052de7a182c397b76330842dcd5,aeb7a3668df6c0829cb4084ba4e0ff37a94b7c41,Sam Gross,colesbury@gmail.com,Mon Dec 11 20:45:43 2017 -0500,1513025143.0,Implement Variable.new (#4080),313.0,39.0,"aten/src/ATen/UndefinedType.cpp,aten/src/ATen/UndefinedType.h,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeDerived.cpp,aten/src/ATen/templates/TypeDerived.h,setup.py,test/test_cuda.py,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/python_scalars.h,torch/csrc/utils/tensor_list.cpp,torch/csrc/utils/tensor_list.h,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h",21.0,11,4,3.16460869,39.0,9267.0,9.0,793227.7222222222,371.0,1120.905869,0.0,,0.0,1
pytorch,94439d7df4d158023ea964db9afacaa2e1370074,aebf3b47aef7c7bbdfe30f74dc35ed0ba727bdc1,Jerry Zhang,jerryzh@fb.com,Fri Jul 27 17:50:54 2018 -0700,1532713854.0,"Remove template parameter from Tensor (#9939)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9939

Pull Request resolved: https://github.com/facebookresearch/weakly-supervised-action-detection/pull/13

Pull Request resolved: https://github.com/pytorch/translate/pull/166

Pull Request resolved: https://github.com/pytorch/pytorch/pull/9125

Closes https://github.com/pytorch/pytorch/pull/9125

Use inheritance for polymorphism, and remove template parameter
This is to change the templating in call sites, the core implementations will change later

Before Caffe2 Tensor class was compile-time fixed to bind to a particular device/context. With this change, we're making it a runtime property (stored inside the tensor), but preserve the same semantics. For example, one has to specify device type in order to create a Tensor - there are no uninitialized tensors. More specifically the changes are:

1. We added an extra argument *DeviceType* to most of the constructors of the tensor, e.g. (Tensor(DeviceType type)),
2. Semantics of constructor Tensor(const Tensor<SrcContext>& src, ContextForCopy* context); is changed, in this constructor, the second context is passed in to enable us to call the templated Copy function, it could be in a different context as source and target previously, now we'll enforce that the context should have same device type as src, if it is provided.
3. To preserve 'get-or-construct' semantics of Blob, we added specialized getter Blob::GetMutableTensor that verifies both that Blob contains a Tensor and that it's of a correct type
4. Specifically, Tensor type is not default-constructible any more (as we don't have unknown device tensors) and thus some of the code handling STL containers needs to change

Note: Some changes are postponed just to keep this diff a bit smaller. Please see `TODO`s.

Reviewed By: ezyang, houseroad

Differential Revision: D9024330

fbshipit-source-id: e0b8295d2dc6ebe2963383ded5af799ad17164ba",4187.0,3515.0,"binaries/benchmark_helper.cc,binaries/benchmark_helper.h,binaries/core_overhead_benchmark.cc,binaries/print_core_object_sizes.cc,binaries/speed_benchmark.cc,caffe2/contrib/aten/aten_op_template.h,caffe2/contrib/gloo/common.cc,caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,caffe2/contrib/nervana/nervana_fc_op_gpu_test.cc,caffe2/contrib/warpctc/ctc_op.h,caffe2/core/allocator.cc,caffe2/core/blob.h,caffe2/core/blob_gpu_test.cc,caffe2/core/blob_serialization.cc,caffe2/core/blob_serialization.h,caffe2/core/blob_serialization_gpu.cc,caffe2/core/blob_test.cc,caffe2/core/context.cc,caffe2/core/context.h,caffe2/core/context_base.cc,caffe2/core/context_base.h,caffe2/core/context_gpu.cu,caffe2/core/context_gpu.h,caffe2/core/context_test.cc,caffe2/core/dispatch/CMakeLists.txt,caffe2/core/dispatch/OpSchema.h,caffe2/core/dispatch/OpSchema_test.cpp,caffe2/core/hip/blob_serialization_hip.cc,caffe2/core/hip/context_hip.cc,caffe2/core/hip/context_hip.h,caffe2/core/int8_serialization.cc,caffe2/core/operator.h,caffe2/core/plan_executor.cc,caffe2/core/tensor.cc,caffe2/core/tensor.h,caffe2/core/tensor_int8.h,caffe2/core/typeid.cc,caffe2/core/typeid.h,caffe2/core/workspace.h,caffe2/experiments/operators/fully_connected_op_decomposition.h,caffe2/experiments/operators/fully_connected_op_prune.h,caffe2/experiments/operators/fully_connected_op_sparse.h,caffe2/experiments/operators/sparse_matrix_reshape_op.h,caffe2/ideep/operators/concat_split_op.cc,caffe2/ideep/operators/operator_fallback_ideep.h,caffe2/ideep/operators/utility_ops.cc,caffe2/ideep/utils/ideep_context.h,caffe2/ideep/utils/ideep_register.cc,caffe2/image/image_input_op.h,caffe2/image/transform_gpu.cu,caffe2/image/transform_gpu.h,caffe2/mkl/mkl_utils_test.cc,caffe2/mkl/operators/conv_op.cc,caffe2/mkl/operators/conv_op_mkldnn.cc,caffe2/mkl/operators/operator_fallback_mkl.h,caffe2/mkl/operators/packed_fc_op.cc,caffe2/mkl/operators/pool_op.cc,caffe2/mkl/operators/utility_ops.cc,caffe2/mkl/utils/mkl_context.cc,caffe2/mkl/utils/mkl_context.h,caffe2/mobile/contrib/CMakeLists.txt,caffe2/mobile/contrib/arm-compute/operators/copy_op.cc,caffe2/mobile/contrib/arm-compute/test/gl_operator_test.h,caffe2/mobile/contrib/ios/ios_caffe.cc,caffe2/mobile/contrib/ios/ios_caffe_predictor.cc,caffe2/mobile/contrib/ios/mpscnn/mpscnn.mm,caffe2/mobile/contrib/ios/pool_test.cc,caffe2/mobile/contrib/ios/resize_test.cc,caffe2/mobile/contrib/nnapi/nnapi.cc,caffe2/mobile/contrib/nnapi/nnapi_benchmark.cc,caffe2/mobile/contrib/nnapi/nnapi_test.cc,caffe2/mobile/contrib/opengl/CMakeLists.txt,caffe2/mobile/contrib/opengl/test/opengl_test.cc,caffe2/mobile/contrib/snpe/snpe_op.cc,caffe2/mobile/contrib/snpe/snpe_op_benchmark.cc,caffe2/mobile/contrib/ulp2/ulp.cc,caffe2/mobile/contrib/ulp2/ulp_neon.cc,caffe2/mobile/contrib/ulp2/ulp_test.cc,caffe2/mpi/mpi_gpu_test.cc,caffe2/mpi/mpi_ops.h,caffe2/observers/profile_observer_gpu.cc,caffe2/operators/accuracy_op.cc,caffe2/operators/accuracy_op.cu,caffe2/operators/affine_channel_op.cc,caffe2/operators/affine_channel_op.cu,caffe2/operators/apmeter_op.cc,caffe2/operators/assert_op.h,caffe2/operators/atomic_ops.cc,caffe2/operators/batch_gather_ops.cu,caffe2/operators/batch_gather_ops.h,caffe2/operators/batch_matmul_op.h,caffe2/operators/batch_matmul_op_gpu_test.cc,caffe2/operators/batch_matmul_op_test.cc,caffe2/operators/bbox_transform_op.cc,caffe2/operators/boolean_mask_ops.cc,caffe2/operators/boolean_mask_ops.cu,caffe2/operators/boolean_unmask_ops.cu,caffe2/operators/boolean_unmask_ops_test.cc,caffe2/operators/box_with_nms_limit_op.cc,caffe2/operators/ceil_op.cu,caffe2/operators/channel_backprop_stats_op.cc,caffe2/operators/channel_backprop_stats_op.cu,caffe2/operators/channel_backprop_stats_op.h,caffe2/operators/channel_shuffle_op_gpu.cu,caffe2/operators/channel_stats_op.cc,caffe2/operators/channel_stats_op.cu,caffe2/operators/channel_stats_op.h,caffe2/operators/clip_op.cc,caffe2/operators/clip_op.cu,caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc,caffe2/operators/concat_split_op.h,caffe2/operators/conditional_op.cc,caffe2/operators/conv_op.h,caffe2/operators/conv_op_impl.h,caffe2/operators/conv_op_shared.cc,caffe2/operators/conv_op_shared.h,caffe2/operators/conv_op_shared_gpu.cc,caffe2/operators/conv_pool_op_base.h,caffe2/operators/conv_transpose_op.h,caffe2/operators/conv_transpose_op_impl.h,caffe2/operators/conv_transpose_op_mobile.h,caffe2/operators/conv_transpose_op_mobile_impl.h,caffe2/operators/conv_transpose_op_mobile_test.cc,caffe2/operators/conv_transpose_unpool_op_base.h,caffe2/operators/cosine_embedding_criterion_op.cc,caffe2/operators/cosine_embedding_criterion_op.cu,caffe2/operators/counter_ops.h,caffe2/operators/cross_entropy_op.cc,caffe2/operators/cross_entropy_op.cu,caffe2/operators/ctc_beam_search_decoder_op.cc,caffe2/operators/ctc_greedy_decoder_op.cc,caffe2/operators/dataset_ops.cc,caffe2/operators/dataset_ops.h,caffe2/operators/deform_conv_op.h,caffe2/operators/deform_conv_op_impl.h,caffe2/operators/depthwise_3x3_conv_op_cudnn.cu,caffe2/operators/distance_op.cc,caffe2/operators/distance_op.cu,caffe2/operators/distance_op.h,caffe2/operators/dropout_op.cc,caffe2/operators/dropout_op.cu,caffe2/operators/dropout_op_cudnn.cc,caffe2/operators/elementwise_linear_op.cc,caffe2/operators/elementwise_linear_op.cu,caffe2/operators/elementwise_logical_ops.cc,caffe2/operators/elementwise_logical_ops.h,caffe2/operators/elementwise_op_test.h,caffe2/operators/elementwise_ops.cu,caffe2/operators/elementwise_ops.h,caffe2/operators/elementwise_ops_utils.cc,caffe2/operators/elementwise_ops_utils.h,caffe2/operators/enforce_finite_op.cu,caffe2/operators/enforce_finite_op.h,caffe2/operators/ensure_cpu_output_op.h,caffe2/operators/expand_op.h,caffe2/operators/feature_maps_ops.h,caffe2/operators/filler_op.cc,caffe2/operators/filler_op.cu,caffe2/operators/filler_op.h,caffe2/operators/find_op.cu,caffe2/operators/flatten_op.h,caffe2/operators/floor_op.cu,caffe2/operators/fully_connected_op.h,caffe2/operators/gather_fused_8bit_rowwise_op.h,caffe2/operators/gather_ranges_to_dense_op.h,caffe2/operators/generate_proposals_op.cc,caffe2/operators/generate_proposals_op_test.cc,caffe2/operators/given_tensor_fill_op.h,caffe2/operators/group_norm_op.h,caffe2/operators/gru_unit_op.h,caffe2/operators/h_softmax_op.cc,caffe2/operators/h_softmax_op.h,caffe2/operators/half_float_ops.cu,caffe2/operators/if_op.h,caffe2/operators/index_ops.cc,caffe2/operators/instance_norm_op.cu,caffe2/operators/instance_norm_op.h,caffe2/operators/integral_image_op.cu,caffe2/operators/integral_image_op.h,caffe2/operators/jsd_op.cc,caffe2/operators/last_n_window_collector.cc,caffe2/operators/layer_norm_op.cu,caffe2/operators/layer_norm_op.h,caffe2/operators/leaky_relu_op.cc,caffe2/operators/leaky_relu_op.cu,caffe2/operators/lengths_pad_op.h,caffe2/operators/lengths_tile_op.h,caffe2/operators/listwise_l2r_op.cc,caffe2/operators/listwise_l2r_op.h,caffe2/operators/load_save_op.h,caffe2/operators/local_response_normalization_op.cc,caffe2/operators/local_response_normalization_op.cu,caffe2/operators/local_response_normalization_op.h,caffe2/operators/locally_connected_op.h,caffe2/operators/locally_connected_op_impl.h,caffe2/operators/logit_op.cu,caffe2/operators/lp_pool_op.cc,caffe2/operators/lp_pool_op.cu,caffe2/operators/lpnorm_op.cc,caffe2/operators/lstm_unit_op.h,caffe2/operators/map_ops.h,caffe2/operators/margin_ranking_criterion_op.cc,caffe2/operators/margin_ranking_criterion_op.cu,caffe2/operators/max_pool_with_index.cu,caffe2/operators/mem_query_op.cu,caffe2/operators/multi_class_accuracy_op.cc,caffe2/operators/multi_class_accuracy_op.cu,caffe2/operators/norm_planar_yuv_op.cc,caffe2/operators/normalize_ops.cu,caffe2/operators/numpy_tile_op.h,caffe2/operators/one_hot_ops.cc,caffe2/operators/one_hot_ops.cu,caffe2/operators/one_hot_ops.h,caffe2/operators/onnx_while_op.h,caffe2/operators/onnxifi_op.cc,caffe2/operators/operator_fallback_gpu.h,caffe2/operators/operator_fallback_gpu_test.cc,caffe2/operators/order_switch_ops.cc,caffe2/operators/order_switch_ops.cu,caffe2/operators/pack_rnn_sequence_op.h,caffe2/operators/pack_segments.cc,caffe2/operators/pack_segments.cu,caffe2/operators/pack_segments.h,caffe2/operators/pad_op.cc,caffe2/operators/pad_op_gpu.cu,caffe2/operators/partition_ops.h,caffe2/operators/percentile_op.h,caffe2/operators/perplexity_op.cc,caffe2/operators/perplexity_op.cu,caffe2/operators/piecewise_linear_transform_op.cc,caffe2/operators/piecewise_linear_transform_op.cu,caffe2/operators/piecewise_linear_transform_op.h,caffe2/operators/pool_op.cu,caffe2/operators/pool_op_cudnn.cu,caffe2/operators/prelu_op.cc,caffe2/operators/prelu_op.cu,caffe2/operators/prepend_dim_op.h,caffe2/operators/quant_decode_op.h,caffe2/operators/reducer_functors.h,caffe2/operators/reduction_front_back_ops.h,caffe2/operators/reduction_ops.cc,caffe2/operators/reduction_ops.cu,caffe2/operators/reduction_ops.h,caffe2/operators/relu_n_op.cc,caffe2/operators/remove_data_blocks_op.h,caffe2/operators/reservoir_sampling.cc,caffe2/operators/reshape_op.h,caffe2/operators/reshape_op_gpu_test.cc,caffe2/operators/resize_op.cc,caffe2/operators/resize_op.cu,caffe2/operators/reverse_packed_segs_op.h,caffe2/operators/rmac_regions_op.cc,caffe2/operators/rmac_regions_op.cu,caffe2/operators/rmac_regions_op.h,caffe2/operators/rnn/recurrent_network_blob_fetcher_op.h,caffe2/operators/rnn/recurrent_network_executor.h,caffe2/operators/rnn/recurrent_network_op.h,caffe2/operators/rnn/recurrent_op_cudnn.cc,caffe2/operators/rnn/recurrent_op_cudnn.h,caffe2/operators/roi_align_gradient_op.cc,caffe2/operators/roi_align_gradient_op.cu,caffe2/operators/roi_align_op.cc,caffe2/operators/roi_align_op.cu,caffe2/operators/roi_align_op_gpu_test.cc,caffe2/operators/roi_pool_op.cc,caffe2/operators/roi_pool_op.cu,caffe2/operators/scale_op.cc,caffe2/operators/segment_reduction_op.h,caffe2/operators/segment_reduction_op_gpu.cu,caffe2/operators/selu_op.cc,caffe2/operators/selu_op.cu,caffe2/operators/sequence_ops.cc,caffe2/operators/sequence_ops.cu,caffe2/operators/sequence_ops.h,caffe2/operators/shape_op.h,caffe2/operators/sinusoid_position_encoding_op.h,caffe2/operators/slice_op.cu,caffe2/operators/slice_op.h,caffe2/operators/softmax_op.cc,caffe2/operators/softmax_op.h,caffe2/operators/softmax_ops.cu,caffe2/operators/softmax_shared.cc,caffe2/operators/softmax_with_loss_op.cc,caffe2/operators/softmax_with_loss_op.h,caffe2/operators/softplus_op.cc,caffe2/operators/softplus_op.cu,caffe2/operators/space_batch_op.h,caffe2/operators/space_batch_op_gpu.cu,caffe2/operators/sparse_to_dense_mask_op.h,caffe2/operators/sparse_to_dense_op.h,caffe2/operators/spatial_batch_norm_gradient_op.cc,caffe2/operators/spatial_batch_norm_op.cc,caffe2/operators/spatial_softmax_with_loss_op.cc,caffe2/operators/spatial_softmax_with_loss_op.h,caffe2/operators/stats_ops.cc,caffe2/operators/string_ops.cc,caffe2/operators/string_ops_test.cc,caffe2/operators/stump_func_op.cu,caffe2/operators/stylizer_ops.cc,caffe2/operators/summarize_op.cc,caffe2/operators/summarize_op.cu,caffe2/operators/swish_op.cc,caffe2/operators/tensor_protos_db_input.h,caffe2/operators/thresholded_relu_op.cc,caffe2/operators/thresholded_relu_op.cu,caffe2/operators/tile_op.h,caffe2/operators/top_k.cu,caffe2/operators/tt_linear_op.h,caffe2/operators/unique_ops.cu,caffe2/operators/unique_ops.h,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.cu,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_gpu_test.cc,caffe2/operators/utility_ops_test.cc,caffe2/operators/weighted_multi_sampling_op.cc,caffe2/operators/weighted_sample_op.cc,caffe2/operators/weighted_sample_op.cu,caffe2/operators/weighted_sample_op.h,caffe2/operators/while_op.h,caffe2/operators/workspace_ops.cc,caffe2/opt/fusion.cc,caffe2/opt/onnxifi_transformer.cc,caffe2/predictor/predictor.cc,caffe2/predictor/predictor_test.cc,caffe2/python/pybind_state.cc,caffe2/python/pybind_state.h,caffe2/python/pybind_state_dlpack.h,caffe2/python/pybind_state_gpu.cc,caffe2/python/pybind_state_hip.cc,caffe2/python/pybind_state_int8.cc,caffe2/queue/blobs_queue_db.h,caffe2/queue/queue_ops.h,caffe2/queue/rebatching_queue.cc,caffe2/queue/rebatching_queue_ops.h,caffe2/sgd/adam_op.h,caffe2/sgd/adam_op_gpu.cu,caffe2/sgd/fp16_momentum_sgd_op.h,caffe2/sgd/fp32_momentum_sgd_op.h,caffe2/sgd/iter_op.h,caffe2/sgd/learning_rate_op.h,caffe2/sgd/momentum_sgd_op.h,caffe2/sgd/yellowfin_op.h,caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc,caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc,caffe2/share/contrib/nnpack/conv_op.cc,caffe2/share/contrib/nnpack/nnpack_test.cc,caffe2/utils/filler.h,caffe2/utils/hip/math_blas_hip_test.cc,caffe2/utils/hip/math_hip.cc,caffe2/utils/math.h,caffe2/utils/math_cpu.cc,caffe2/utils/math_gpu.cu,caffe2/utils/math_gpu_test.cc,caffe2/utils/math_test.cc,caffe2/utils/smart_tensor_printer.cc,caffe2/utils/smart_tensor_printer.h,caffe2/utils/smart_tensor_printer_test.cc,caffe2/video/video_input_op.h,modules/detectron/group_spatial_softmax_op.h,modules/detectron/select_smooth_l1_loss_op.h,modules/detectron/sigmoid_cross_entropy_loss_op.h,modules/detectron/sigmoid_focal_loss_op.h,modules/detectron/smooth_l1_loss_op.h,modules/detectron/softmax_focal_loss_op.h",365.0,50,3,6.869605031,29.0,106548.0,6.0,74640.04931506849,3153.0,8272.333333,0.0,Corrective,1.0,1
pytorch,c4fd76e7b479d69938920cdfd16fafc8ee8106ef,aecbaa5d45558514f9ec16979cf08037bdf62d0f,kshitij12345,kshitijkalambarkar@gmail.com,Thu May 04 07:23:35 2023 +0000,1683185015.0,"[vmap] bucketize (#95783)

Ref: https://github.com/pytorch/pytorch/issues/96740
Pull Request resolved: https://github.com/pytorch/pytorch/pull/95783
Approved by: https://github.com/zou3519",44.0,1.0,"aten/src/ATen/functorch/BatchRulesReduceOps.cpp,test/functorch/test_vmap.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.422872232,7.0,25964.0,3.0,283724.6666666667,15419.0,34750.5,0.0,,0.0,1
pytorch,298129dd1d6761399308b751f8cdda9c1179ebc1,aeeb259812c628a65797ae52eb7c6e63f50f488c,Samantha Andow,samdow@fb.com,Wed Feb 23 16:06:03 2022 -0500,1645632363.0,"[functorch] normal, floats only (pytorch/functorch#510)

[ghstack-poisoned]",19.0,18.0,"functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/test/test_vmap.py",3.0,4,1,0.480433457,1.0,3839.0,1.0,0.0,823.0,1146.5,0.0,,0.0,1
pytorch,30a8ba93b137ed791febbd70cf782cce3b23607d,aeefe2ce31cac554d48de1372eb426dcf4a3a301,Spandan Tiwari,sptiwari@microsoft.com,Wed Jan 13 21:49:15 2021 -0800,1610574555.0,"[ONNX] ONNX dev branch merge 01-06-2021 (#50163)

Summary:
[ONNX] ONNX dev branch merge 01-06-2021
- [ONNX] Support onnx if/loop sequence output in opset 13 - (https://github.com/pytorch/pytorch/issues/49270)
- Symbolic function for torch.square (https://github.com/pytorch/pytorch/issues/49446)
- [ONNX] Add checks in ONNXSetDynamicInputShape (https://github.com/pytorch/pytorch/issues/49783) â¦
- [ONNX] Enable export af aten::__derive_index (https://github.com/pytorch/pytorch/issues/49514) â¦
- [ONNX] Update symbolic for unfold (https://github.com/pytorch/pytorch/issues/49378) â¦
- [ONNX] Update the sequence of initializers in exported graph so that it is as same as inputs. (https://github.com/pytorch/pytorch/issues/49798)
- [ONNX] Enable opset 13 ops (https://github.com/pytorch/pytorch/issues/49612) â¦
- [ONNX] Improve error message for supported model input types in ONNX export API. (https://github.com/pytorch/pytorch/issues/50119)
- [ONNX] Add a post-pass for If folding (https://github.com/pytorch/pytorch/issues/49410)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50163

Reviewed By: pbelevich

Differential Revision: D25821059

Pulled By: SplitInfinity

fbshipit-source-id: 9f511a93d9d5812d0ab0a49d61ed0fa5f8066948",1511.0,168.0,".jenkins/caffe2/test.sh,aten/src/ATen/core/interned_strings.h,scripts/onnx/test.sh,test/onnx/expect/TestOperators.test_batchnorm.expect,test/onnx/expect/TestOperators.test_batchnorm_1d.expect,test/onnx/expect/TestOperators.test_batchnorm_onnx_irv4.expect,test/onnx/expect/TestOperators.test_batchnorm_training.expect,test/onnx/expect/TestOperators.test_layer_norm_aten.expect,test/onnx/expect/TestOperators.test_linear.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,tools/build_variables.bzl,torch/_C/__init__.pyi.in,torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.h,torch/csrc/jit/passes/onnx/fold_if_node.cpp,torch/csrc/jit/passes/onnx/fold_if_node.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp,torch/csrc/jit/passes/onnx/scalar_type_analysis.h,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/python/init.cpp,torch/csrc/jit/python/python_arg_flatten.cpp,torch/csrc/jit/serialization/export.cpp,torch/onnx/__init__.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",30.0,21,6,2.729536712,15.0,19038.0,17.0,5884276.444444444,8082.0,18314.5,0.0,Feature Addition,0.0,1
pytorch,785054d3a9d0cf3d528511c42d81a9f09e36f1c6,af0c339f00094c4c2f3c260b55e04e0e3654776a,Sergii Dymchenko,sdym@fb.com,Sat Oct 29 00:23:47 2022 +0000,1667003027.0,"Disable slow-gradcheck tests (#88008)

Disable because slow-gradcheck tests take > 4 hrs and time out. Will need to figure out if and how to re-enable later.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88008
Approved by: https://github.com/seemethere, https://github.com/huydhn",6.0,0.0,.github/workflows/periodic.yml,1.0,2,1,0,1.0,224.0,1.0,204552.0,8894.0,21023.0,0.0,,0.0,1
pytorch,bd8feb33d4345e6a98d42c0f55fb74d9e17a5a1d,af1bd88fc488c09f7fc1ad04f609431c31e448c2,Eddie Yan,eddiey@nvidia.com,Thu Nov 04 16:29:49 2021 -0700,1636043389.0,"Allow scalars for aliased binary ops {`multiply`, `subtract`, `divide`} (#65937)

Summary:
https://github.com/pytorch/pytorch/issues/65868 pointed out that the ""long-form"" versions of some binary ops like `mul`, `sub`, and `div` don't match their alias's behavior when it comes to handling scalar inputs. This PR adds the missing registration in `python_arg_parser.cpp` to resolve this.

CC ptrblck ngimel

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65937

Reviewed By: malfet

Differential Revision: D32156580

Pulled By: ngimel

fbshipit-source-id: b143cf7119a8bb51609e1b8734204edb750f0210",8.0,5.0,"torch/csrc/utils/python_arg_parser.cpp,torch/testing/_internal/common_methods_invocations.py",2.0,5,1,0.779349837,13.0,13685.0,2.0,2150545.5,16843.0,39532.0,0.0,Feature Addition,0.0,1
pytorch,62847a2b9c98ce7c9a9007b173e76a12a0ebd2e6,af49805a73fb0a4b0459f25511598e5abfb54a96,Peter Bell,peterbell10@live.co.uk,Mon Nov 29 23:10:06 2021 -0800,1638227406.0,"Port lerp to structured kernels (#68924)

Summary:
Ref https://github.com/pytorch/pytorch/issues/55070

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68924

Reviewed By: jbschlosser

Differential Revision: D32697409

Pulled By: bdhirsh

fbshipit-source-id: b098533e46f8bdbb995c76db0e6a124ab2b076b8",59.0,158.0,"aten/src/ATen/native/Lerp.cpp,aten/src/ATen/native/Lerp.h,aten/src/ATen/native/cpu/LerpKernel.cpp,aten/src/ATen/native/cuda/Lerp.cu,aten/src/ATen/native/native_functions.yaml",5.0,6,1,2.069137787,12.0,11346.0,4.0,9877079.4,17344.0,40779.5,0.0,,0.0,1
pytorch,578507cb7b258b0be56e13cfce5d2550f3ca1775,af564e73b8116bbde2b090aaf8bf039a542674c5,Pearu Peterson,pearu.peterson@gmail.com,Sat Nov 20 00:26:55 2021 -0800,1637368015.0,"Strided masked log_softmax. (#68461)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68461

Test Plan: Imported from OSS

Reviewed By: dagitses, zou3519

Differential Revision: D32569961

Pulled By: cpuhrsch

fbshipit-source-id: 5d262adacf239dace4a28de85af4b602e36f17f0",47.0,4.0,"test/test_masked.py,torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.063432204,2.0,14144.0,2.0,11438.333333333334,17235.0,40569.0,0.0,,0.0,1
pytorch,3fe00f0c906ab4166058a8619b3c2edf121b2537,af638ad5d7c8fc9ff97f0ad1cd2bbcfa3ced514e,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Wed Jul 31 04:11:36 2019 -0700,1564546296.0,"pin_memory should not copy on already pinned tensors (#23484)

Summary:
fixes https://github.com/pytorch/pytorch/issues/21076
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23484

Differential Revision: D16546264

Pulled By: ezyang

fbshipit-source-id: 8058e0bbc6336751f36b884d71234feef498a982",119.0,47.0,"aten/src/ATen/Context.h,aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/cuda/detail/CUDAHooks.cpp,aten/src/ATen/cuda/detail/CUDAHooks.h,aten/src/ATen/detail/CUDAHooksInterface.h,aten/src/ATen/native/Memory.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/THC/THCCachingHostAllocator.cpp,test/test_cuda.py,test/test_cuda_primary_ctx.py,test/test_torch.py,torch/__init__.pyi.in,torch/_tensor_docs.py,torch/csrc/generic/StorageMethods.cpp,torch/tensor.py",16.0,13,3,3.217513986,44.0,29243.0,7.0,803323.625,10311.0,29535.83333,0.0,Corrective,1.0,1
pytorch,493234236326b9c7b4f3255a11a60e099eb81f3b,afa1ff8e04dad60dee9f979451a36713903f521d,Nikita Vedeneev,nik@quansight.com,Fri Mar 12 21:23:13 2021 -0800,1615584193.0,"Implements `torch.linalg.lstsq` (#49093)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/44378 by providing a wider range of drivers similar to what SciPy is doing.

The supported CPU drivers are `gels, gelsy, gelsd, gelss`.
The CUDA interface has only `gels` implemented but only for overdetermined systems.

The current state of this PR:
- [x] CPU interface
- [x] CUDA interface
- [x] CPU tests
- [x] CUDA tests
- [x] Memory-efficient batch-wise iteration with broadcasting which fixes https://github.com/pytorch/pytorch/issues/49252
- [x] docs

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49093

Reviewed By: albanD

Differential Revision: D26991788

Pulled By: mruberry

fbshipit-source-id: 8af9ada979240b255402f55210c0af1cba6a0a3c",1498.0,1.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,test/test_namedtuple_return_api.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/linalg.h,torch/csrc/autograd/utils/wrap_outputs.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",14.0,20,5,2.088526443,16.0,32152.0,9.0,478681.8571428572,9753.0,21561.0,0.0,Corrective,1.0,1
pytorch,432858c9601be87facc9d4aac4c209c583893948,afa8cbf8c21a080f43acfdf3cac6a07a4c606841,Brian Stark,brstark@microsoft.com,Fri Feb 07 02:16:40 2020 -0800,1581041800.0,"Modifed randNLike for scripting (#32830)

Summary:
the rand N like function had required args which were not being used.
As such modified the method signature to give default values so when
scripting does not provide these arguments which are not even being
used, no error is thrown.

Additionally modified the const checker for handling prim::Constant as
well
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32830

Reviewed By: hl475

Differential Revision: D19731715

Pulled By: houseroad

fbshipit-source-id: a3cacb3977eecb88b122e0ceb654fdbf1c8286c1",3.0,2.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.521928095,2.0,5569.0,3.0,122133.66666666669,14626.0,39441.83333,0.0,Feature Addition,0.0,1
pytorch,b6bbb41fd8aacdcf4cf29c3002cec225fcb94e8c,afb560065c842e7c07cb71c9ea6dc0fcc774c412,kshitij12345,kshitijkalambarkar@gmail.com,Mon Mar 22 16:38:01 2021 -0700,1616431081.0,"[testing] OpInfo for sgn and sign (#53885)

Summary:
Reference https://github.com/pytorch/pytorch/issues/42515

TODO:
* [x] Check rendered docs. https://11525594-65600975-gh.circle-artifacts.com/0/docs/generated/torch.sgn.html

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53885

Reviewed By: ejguan

Differential Revision: D27114318

Pulled By: mruberry

fbshipit-source-id: 678179d87741aacd3b50f03dc460207c5aa29589",73.0,83.0,"test/test_torch.py,test/test_unary_ufuncs.py,torch/_torch_docs.py,torch/testing/__init__.py,torch/testing/_internal/common_methods_invocations.py",5.0,4,2,1.543348155,44.0,24773.0,4.0,527095.4,9955.0,22045.5,0.0,Non Functional,0.0,1
pytorch,0aed86a17576c3711dc692efb62f70ad19af401c,afbf3458073500635fe77dfb9a99f94fcf4d0751,Andres Lugo-Reyes,Andy.LugoReyes@amd.com,Thu Nov 09 22:26:02 2023 +0000,1699568762.0,"[ROCm] Unskip functorch tests that now work (#110760)

This issue unskips some of the working tests that were skipped as a result of https://github.com/pytorch/pytorch/issues/96560

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110760
Approved by: https://github.com/zou3519, https://github.com/jeffdaily",3.0,23.0,"test/functorch/test_ops.py,test/functorch/test_vmap.py",2.0,2,1,0.619382195,1.0,7588.0,2.0,323117.0,21802.0,49791.0,0.0,,0.0,1
pytorch,0629785645e0693ff5586248e48464d5fc19c0a0,affe742d31a98be938ea9d2536b8dd9b532a01a6,gchanan,gregchanan@gmail.com,Thu Feb 08 18:53:24 2018 -0500,1518116004.0,"Add scalar module tests for test_nn. (#5116)

* Add scalar module tests for test_nn.

* Properly return from glu.

* Guard scalar test with skipIf.",153.0,22.0,"test/common.py,test/common_nn.py,test/test_nn.py,torch/nn/_functions/loss.py,torch/nn/functional.py",5.0,4,2,1.240840874,38.0,9465.0,5.0,128262.2,517.0,1526.905869,0.0,Feature Addition,0.0,1
pytorch,aac3c7bd063815f1ad73d830b8271fc737b85490,b0043072529b81276a69df29e00555333117646c,Zsolt Dollenstein,zsol@fb.com,Thu Aug 12 17:56:55 2021 -0700,1628791015.0,"[codemod][lint][fbcode/c*] Enable BLACK by default

Test Plan: manual inspection & sandcastle

Reviewed By: zertosh

Differential Revision: D30279364

fbshipit-source-id: c1ed77dfe43a3bde358f92737cd5535ae5d13c9a",57161.0,29030.0,"android/pytorch_android/generate_test_torchscripts.py,android/test_app/make_assets.py,android/test_app/make_assets_custom.py,binaries/bench_gen/bench_gen.py,caffe2/core/nomnigraph/op_gen.py,caffe2/distributed/file_store_handler_op_test.py,caffe2/distributed/redis_store_handler_op_test.py,caffe2/distributed/store_ops_test_util.py,caffe2/perfkernels/hp_emblookup_codegen.py,docs/caffe2/process.py,docs/cpp/source/conf.py,docs/source/conf.py,docs/source/scripts/build_activation_images.py,ios/TestApp/benchmark/trace_model.py,ios/TestApp/custom_build/custom_build.py,modules/detectron/upsample_nearest_op_test.py,mypy_plugins/check_mypy_version.py,setup.py,test/ao/sparsity/test_kernels.py,test/ao/sparsity/test_parametrization.py,test/ao/sparsity/test_pruner.py,test/ao/sparsity/test_scheduler.py,test/ao/sparsity/test_sparsifier.py,test/backward_compatibility/check_backward_compatibility.py,test/backward_compatibility/dump_all_function_schemas.py,test/benchmark_utils/test_benchmark_utils.py,test/bottleneck_test/test_args.py,test/bottleneck_test/test_cuda.py,test/cpp/api/init_baseline.py,test/cpp/api/optim_baseline.py,test/cpp/jit/tests_setup.py,test/cpp_extensions/setup.py,test/custom_backend/backend.py,test/custom_backend/test_custom_backend.py,test/custom_operator/test_custom_classes.py,test/custom_operator/test_custom_ops.py,test/distributions/test_constraints.py,test/distributions/test_distributions.py,test/distributions/test_transforms.py,test/distributions/test_utils.py,test/error_messages/storage.py,test/jit_hooks/model.py,test/linear.py,test/mobile/custom_build/prepare_model.py,test/mobile/test_bytecode.py,test/mobile/test_lite_script_module.py,test/optim/test.py,test/run_test.py,test/scripts/cuda_memcheck_common.py,test/scripts/run_cuda_memcheck.py,test/simulate_nccl_errors.py,test/test_ao_sparsity.py,test/test_autocast.py,test/test_autograd.py,test/test_binary_ufuncs.py,test/test_buffer_protocol.py,test/test_bundled_images.py,test/test_bundled_inputs.py,test/test_complex.py,test/test_cpp_api_parity.py,test/test_cpp_extensions_aot.py,test/test_cpp_extensions_jit.py,test/test_cuda.py,test/test_cuda_primary_ctx.py,test/test_dataloader.py,test/test_datapipe.py,test/test_determination.py,test/test_dispatch.py,test/test_foreach.py,test/test_function_schema.py,test/test_functional_autograd_benchmark.py,test/test_functional_optim.py,test/test_futures.py,test/test_fx.py,test/test_fx_experimental.py,test/test_gen_backend_stubs.py,test/test_import_time.py,test/test_indexing.py,test/test_jit.py,test/test_jit_cuda_fuser.py,test/test_jit_disabled.py,test/test_jit_fuser.py,test/test_jit_fuser_legacy.py,test/test_jit_fuser_te.py,test/test_jit_legacy.py,test/test_jit_profiling.py,test/test_jit_simple.py,test/test_jit_string.py,test/test_kernel_launch_checks.py,test/test_license.py,test/test_linalg.py,test/test_logging.py,test/test_metal.py,test/test_mkldnn.py,test/test_mobile_optimizer.py,test/test_model_dump.py,test/test_module_init.py,test/test_modules.py,test/test_multiprocessing.py,test/test_multiprocessing_spawn.py,test/test_namedtensor.py,test/test_namedtuple_return_api.py,test/test_native_functions.py,test/test_nn.py,test/test_nnapi.py,test/test_numba_integration.py,test/test_numpy_interop.py,test/test_openmp.py,test/test_ops.py,test/test_optim.py,test/test_overrides.py,test/test_package.py,test/test_profiler.py,test/test_pruning_op.py,test/test_public_bindings.py,test/test_python_dispatch.py,test/test_pytree.py,test/test_quantization.py,test/test_reductions.py,test/test_serialization.py,test/test_set_default_mobile_cpu_allocator.py,test/test_shape_ops.py,test/test_show_pickle.py,test/test_sort_and_select.py,test/test_sparse.py,test/test_sparse_csr.py,test/test_spectral_ops.py,test/test_tensor_creation_ops.py,test/test_tensorboard.py,test/test_tensorexpr.py,test/test_tensorexpr_pybind.py,test/test_testing.py,test/test_throughput_benchmark.py,test/test_torch.py,test/test_type_hints.py,test/test_type_info.py,test/test_type_promotion.py,test/test_typing.py,test/test_unary_ufuncs.py,test/test_utils.py,test/test_view_ops.py,test/test_vmap.py,test/test_vulkan.py,test/test_xnnpack_integration.py,torch/_VF.py,torch/__config__.py,torch/__future__.py,torch/__init__.py,torch/_appdirs.py,torch/_autograd_functions.py,torch/_classes.py,torch/_deploy.py,torch/_jit_internal.py,torch/_linalg_utils.py,torch/_lobpcg.py,torch/_lowrank.py,torch/_namedtensor_internals.py,torch/_ops.py,torch/_python_dispatcher.py,torch/_six.py,torch/_sources.py,torch/_storage_docs.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_torch_docs.py,torch/_utils.py,torch/_utils_internal.py,torch/_vmap_internals.py,torch/autocast_mode.py,torch/contrib/_tensorboard_vis.py,torch/cpu/amp/autocast_mode.py,torch/fft/__init__.py,torch/functional.py,torch/futures/__init__.py,torch/hub.py,torch/linalg/__init__.py,torch/overrides.py,torch/profiler/__init__.py,torch/profiler/profiler.py,torch/quasirandom.py,torch/random.py,torch/serialization.py,torch/sparse/__init__.py,torch/special/__init__.py,torch/storage.py,torch/torch_version.py,torch/types.py",188.0,52,9,5.517817166,54.0,176393.0,151.0,11138902.707446808,14585.0,33399.5,0.0,,0.0,1
pytorch,638f0b5d78fe5ff2e484dc573c35b97a4bcf4e82,b043a749191e32bf6c1d0fdee3170c45dce9d3d9,Hugh Perkins,hughperkins@gmail.com,Wed Nov 01 12:47:51 2017 +0000,1509540471.0,fix softmax doc (#3337),1.0,1.0,torch/nn/functional.py,1.0,2,1,0,28.0,1678.0,1.0,32.0,2051.0,23927.85823,0.0,Corrective,1.0,1
pytorch,d98516026e86a94c9824fe8635a999bc0bd75ac8,b0479506a8014d7907d520c2b5f0f22a29e0fced,Lingyi Liu,lingyiliu@fb.com,Sat Mar 07 01:41:51 2020 -0800,1583545311.0,"Add the 3d avg pool for video related model (#33339)

Summary:
```
import torch, time

for dtype in [torch.qint8, torch.quint8, torch.qint32]:
    print('****', str(dtype), '*****')
    x = torch.rand(1, 5, 56, 56, 256)

    q_x = torch.quantize_per_tensor(x, 0.5, 1, dtype)
    q_x = q_x.permute([0, 4, 1, 2, 3])

    x = x.permute([0, 4, 1, 2, 3])

    NITER = 10

    s = time.time()
    for i in range(NITER):
        float_out = torch.nn.functional.avg_pool3d(x, kernel_size=3, stride=None, padding=0)
    time_per_iter_float = (time.time() - s) / NITER

    s = time.time()
    for i in range(NITER):
        quant_out = torch.nn.quantized.functional.avg_pool3d(q_x, kernel_size=3, stride=None, padding=0)
    time_per_iter_quant = (time.time() - s) / NITER
    print('time/iter ms (float)', 'time/iter ms (quant)', 'quant/float', sep='\t')
    print(time_per_iter_float * 1000, time_per_iter_quant * 1000, time_per_iter_quant / time_per_iter_float, sep='\t')
```

```
**** torch.qint8 *****
time/iter ms (float)  time/iter ms (quant)  quant/float
16.286182403564453  0.7308721542358398  0.04487682479080417
**** torch.quint8 *****
time/iter ms (float)  time/iter ms (quant)  quant/float
15.364313125610352  0.6497383117675781  0.042288796541418254
**** torch.qint32 *****
time/iter ms (float)  time/iter ms (quant)  quant/float
15.649032592773438  13.879132270812988  0.8869003363966556
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33339

Differential Revision: D19900904

Pulled By: lly-zero-one

fbshipit-source-id: 4522cc6b4a0751aeda6c7edc258e0cb3f55a8fe3",696.0,33.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool3d.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py,torch/nn/quantized/functional.py",7.0,11,3,1.991769778,10.0,10821.0,5.0,1106349.8333333333,15283.0,40952.83333,0.0,Feature Addition,0.0,1
pytorch,b5193b6a8116dec81a8ed88a424be8e8ceba3de6,b0545aa85f7302be5b9baf8320398981365f003d,Ailing Zhang,ailzhang@fb.com,Thu Feb 14 22:55:44 2019 -0800,1550184944.0,"maskrcnn & bert AD coverage part 1 (#16689)

Summary:
- Moved a few functions from `autograd` namespace to `aten` namespace to be visible from JIT nativeResolver.
- Added a hack to loop up keyword only argument. Will add proper support for kw only later
- Simulate function overload in aten using `_<number>` as function name suffix.
- Even `forward` returns multiple outputs like in `kthvalue`, there's at most one requires grad that we currently support.
- Removed the `TensorList` related ops here since partial `TensorList` support is prone to bugs. Our symbolic diff for `cat` was never tested with autodiff, and it seems broken. Need to find another proper way to support these ops(either by properly supporting `TensorList` or sth like `prim::ConstantChunk`  and leave them for next PR.

Ops supported in this PR:
```
erf
expand_as
index
kthvalue
mean
permute
pow
rsub
select
sqrt
squeeze
t
to
topk
transpose
view
var
embedding
logsumexp
// grad is None
_dim_arange
contiguous
nonzero
ones_like
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16689

Differential Revision: D14020806

Pulled By: ailzhang

fbshipit-source-id: a5e2c144a7be5a0d39d7ac5f93cb402ec12503a5",725.0,283.0,"aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/common_methods_invocations.py,test/cpp/jit/test_misc.h,test/expect/TestFuser.test_lstm_cuda-backward.expect,test/expect/TestFuser.test_lstm_cuda-forward.expect,test/expect/TestFuser.test_milstm_cuda-backward.expect,test/expect/TestFuser.test_milstm_cuda-forward.expect,test/expect/TestJit.test_cpp_cuda.expect,test/test_jit.py,third_party/onnx,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/csrc/jit/autodiff.cpp,torch/csrc/jit/graph_executor.cpp,torch/csrc/jit/passes/graph_fuser.cpp,torch/csrc/jit/symbolic_script.cpp,torch/nn/functional.py",20.0,17,5,3.321292951,37.0,32619.0,14.0,419757.8,7031.0,21598.33333,0.0,Corrective,1.0,1
pytorch,2aea8077f9a7e04cf3ef145eed864bb74784cccb,b07358b329974dc528b3acc1e3d7af2ab22c1ef8,soumith,soumith@fb.com,Tue Dec 27 21:34:09 2016 -0800,1482874449.0,renaming test to avoid dot in test name,3.0,3.0,"test/test_autograd.py,test/test_cuda.py",2.0,1,1,0.918295834,18.0,1573.0,2.0,1164.0,196.0,8685.289675,0.0,Preventative,0.0,1
pytorch,ed4a1d54a79aa726801ff629ecb29e42377a12ea,b074a24394003ecfe184c81d5ef7daee1a757814,Xiong Wei,xiongw.fnst@cn.fujitsu.com,Fri Apr 02 03:28:51 2021 -0700,1617334131.0,"Port torch.copysign method_tests() to OpInfo (#54945)

Summary:
Related https://github.com/pytorch/pytorch/issues/54261

This PR ports the method_tests() entries of `torch.copysign` to OpInfo.

While porting the tests, the `test_out` cases from `test_ops.py` would fail as the out variant of `torch.copysign` does not support scalar inputs.
```python
>>> x = torch.randn(2)
>>> y = torch.empty_like(x)
>>> torch.copysign(x, 1.)
tensor([1.4836, 1.2156])
>>> torch.copysign(x, 1., out=y)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: copysign(): argument 'other' (position 2) must be Tensor, not float
```
This PR fixes the tests by adding an overload `native_functions` entry and re-dispatching scalar inputs to the existing `copysign_out` function.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54945

Reviewed By: gchanan

Differential Revision: D27505300

Pulled By: mruberry

fbshipit-source-id: f68250fa52f8dcfd45426039ec178ca5e883e206",29.0,9.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,4818.0,1.0,1122.0,10352.0,22870.5,0.0,Corrective,1.0,1
pytorch,40bedf620686e374646095d9e11cafa6515dcd6b,b07a11929d182a507b784d903e5fab10cb4bd998,Anirudh Dagar,anirudhdagar6@gmail.com,Thu Nov 11 20:46:54 2021 -0800,1636663614.0,"Array API: Add torch.linalg.cross (#63285)

Summary:
### Create `linalg.cross`

Fixes https://github.com/pytorch/pytorch/issues/62810

As discussed in the corresponding issue, this PR adds `cross` to the `linalg` namespace (**Note**: There is no method variant) which is slightly different in behaviour compared to `torch.cross`.

**Note**: this is NOT an alias as suggested in mruberry's [https://github.com/pytorch/pytorch/issues/62810 comment](https://github.com/pytorch/pytorch/issues/62810#issuecomment-897504372) below
> linalg.cross being consistent with the Python Array API (over NumPy) makes sense because NumPy has no linalg.cross. I also think we can implement linalg.cross without immediately deprecating torch.cross, although we should definitely refer users to linalg.cross. Deprecating torch.cross will require additional review. While it's not used often it is used, and it's unclear if users are relying on its unique behavior or not.

The current default implementation of `torch.cross` is extremely weird and confusing. This has also been reported multiple times previously. (See https://github.com/pytorch/pytorch/issues/17229, https://github.com/pytorch/pytorch/issues/39310, https://github.com/pytorch/pytorch/issues/41850, https://github.com/pytorch/pytorch/issues/50273)

- [x] Add `torch.linalg.cross` with default `dim=-1`
- [x] Add OpInfo and other tests for `torch.linalg.cross`
- [x] Add broadcasting support to `torch.cross` and `torch.linalg.cross`
- [x] Remove out skip from `torch.cross` OpInfo
- [x] Add docs for `torch.linalg.cross`. Improve docs for `torch.cross` mentioning `linalg.cross` and the difference between the two. Also adds a warning to `torch.cross`, that it may change in the future (we might want to deprecate it later)

 ---

### Additional Fixes to `torch.cross`
- [x] Fix Doc for Tensor.cross
- [x] Fix torch.cross in `torch/overridres.py`

While working on `linalg.cross` I noticed these small issues with `torch.cross` itself.

[Tensor.cross docs](https://pytorch.org/docs/stable/generated/torch.Tensor.cross.html) still mentions `dim=-1` default which is actually wrong. It should be `dim=None` after the behaviour was updated in PR https://github.com/pytorch/pytorch/issues/17582 but the documentation for the `method` or `function` variant wasnât updated. Later PR https://github.com/pytorch/pytorch/issues/41850 updated the documentation for the `function` variant i.e `torch.cross` and also added the following warning about the weird behaviour.
> If `dim` is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.

But still, the `Tensor.cross` docs were missed and remained outdated. Iâm finally fixing that here. Also fixing `torch/overrides.py` for `torch.cross` as well now, with `dim=None`.

To verify according to the docs the default behaviour of `dim=-1` should raise, you can try the following.

```python
a = torch.randn(3, 4)
b = torch.randn(3, 4)
b.cross(a)  # this works because the implementation finds 3 in the first dimension and the default behaviour as shown in documentation is actually not true.
>>> tensor([[ 0.7171, -1.1059,  0.4162,  1.3026],
        [ 0.4320, -2.1591, -1.1423,  1.2314],
        [-0.6034, -1.6592, -0.8016,  1.6467]])

b.cross(a, dim=-1)  # this raises as expected since the last dimension doesn't have a 3
>>> RuntimeError: dimension -1 does not have size 3
```

Please take a closer look (particularly the autograd part, this is the first time I'm dealing with `derivatives.yaml`). If there is something missing, wrong or needs more explanation, please let me know. Looking forward to the feedback.

cc mruberry Lezcano IvanYashchuk rgommers

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63285

Reviewed By: gchanan

Differential Revision: D32313346

Pulled By: mruberry

fbshipit-source-id: e68c2687c57367274e8ddb7ef28ee92dcd4c9f2c",232.0,60.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/Cross.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",12.0,14,5,2.520725414,33.0,56596.0,10.0,836918.4166666666,17034.0,40057.0,0.0,Corrective,1.0,1
pytorch,32b67b5a6babff8c69b02e353fa49a92e686b620,b0816e47149f5bfcea60f84af35debb49026355d,Bin Bao,binbao@meta.com,Sun Jul 23 21:09:34 2023 +0000,1690146574.0,"[inductor] Fix AOTInductor output issues (#105773)

Summary: This is a follow-up on https://github.com/pytorch/pytorch/pull/105496. There are several issues with the previous fix,
1) It explicitly does copy for every output at the end of the main function;
2) When an output is ReinterpretView, no as_strided was generated for it;
3) There can be duplicated buffer declarations.

This PR fixes by making sure can_reuse behave consistently between two AOTIndcutor passes, and thus always generate the same set of kernels. It also adds handling of ReinterpretView.

Differential Revision: [D47692214](https://our.internmc.facebook.com/intern/diff/D47692214)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/105773
Approved by: https://github.com/jansel",124.0,88.0,"test/inductor/test_aot_inductor.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/compile_fx.py,torch/_inductor/graph.py,torch/_inductor/scheduler.py",5.0,5,2,1.051083836,1.0,5136.0,3.0,193408.6,17939.0,40668.0,0.0,Corrective,1.0,1
pytorch,123297a8c02e0ebbf8b0ae3d3cb16d0dc2e350b4,b09769992f83f94150eaef2ab9d03c37b36da159,Mike Ruberry,mruberry@devfair044.h1.fair,Fri Apr 15 06:16:01 2022 +0000,1650003361.0,"Improves the OpInfo out= tests

Edit: OpInfos separated into their own PRs to debug an ASAN failure that doesn't identify the failing test properly. This PR now just updates the out tests.

Adds OpInfos for:

- nn.functional.smooth_l1_loss
- nn.functional.l1_loss
- nn.functional.pdist
- nn.functional.binary_cross_entropy
- nn.functional.triplet_margin_loss
- nn.functional.triplet_margin_with_distance_loss
- nn.functional.max_unpool{1, 2, 3}D
- nn.functional.alpha_dropout
- nn.functional.soft_margin_loss
- nn.functional.multilabel_soft_margin_loss
- nn.functional.multilabel_margin_loss
- nn.functional.multi_margin_loss
- nn.functional.margin_ranking_loss

These OpInfos were taken from https://github.com/pytorch/pytorch/pull/67560, https://github.com/pytorch/pytorch/pull/67823, https://github.com/pytorch/pytorch/pull/68625, and https://github.com/pytorch/pytorch/pull/67079. The sample input update from https://github.com/pytorch/pytorch/pull/67017 is also rolled into this PR.

cc @zou3519 @nikitaved @pmeier @vfdev-5 @dagitses
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75782
Approved by: https://github.com/ngimel",391.0,285.0,"test/test_binary_ufuncs.py,test/test_ops.py,torch/nn/functional.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,2,1.065797781,37.0,26472.0,3.0,259193.25,2316.0,5455.5,0.0,Corrective,1.0,1
pytorch,2300234c9ca290abf61b84795f653e7b3c4f93f6,b09d66e60dd2a55877e1c1fc6fc34cbbc756f952,Sam Gross,colesbury@gmail.com,Wed Nov 15 20:19:41 2017 -0500,1510777181.0,"Fix a reference cycle when in-place ops on views save the output (#3679)

Previously, an in-place operation that saves its output (such as
relu/threshold) would create a reference cycle when applied to the a
view. There were two cycles created:

1) The cycle base.grad_fn.fn.input_.base
   base.grad_fn is a CopySlices
   base.grad_fn.fn is ThresholdBackward
   base.grad_fn.fn.input_ is a SavedVariable with base pointing to base

2) The cycle base.grad_fn.fn.input_.grad_fn.next_functions[0]
   base.grad_fn.fn.input_.grad_fn is AsStridedBackward
   and next_functions[0] points to base.grad_fn

Generally, we avoid cycles because the AD graph is mostly immutable. Two
notable exceptions are:

a) Variable.grad_fn can change to point to a new grad_fn
b) SavedVariables in a function can be set after the function is created

The first case is not a problem if grad_fns do not hold strong references
to Variables. Removing ""base"" from SavedVariable removes the strong ref.

For the second case, we need to avoid saving the grad_fn of outputs. We
were incorrectly saving the grad_fns of outputs when they were the
result of in-place ops on views.",70.0,36.0,"test/test_autograd.py,test/test_nn.py,tools/autograd/gen_variable_type.py,torch/csrc/autograd/functions/batch_normalization.h,torch/csrc/autograd/functions/convolution.h,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/functions/tensor.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/saved_variable.cpp,torch/csrc/autograd/saved_variable.h",10.0,7,3,2.990503956,38.0,10136.0,6.0,313218.5,334.0,938.4058694,0.0,Corrective,1.0,1
pytorch,d561aa944b7e777eb0575be2427e26a86df85f11,b0c447e954a335b0df60307a2e7c720320af7231,Richard Zou,zou3519@gmail.com,Fri Sep 16 21:51:34 2022 -0700,1663365094.0,"[functorch] add batch rule for linalg.lu_solve (#85175)

Fixes https://github.com/pytorch/functorch/issues/1022
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85175
Approved by: https://github.com/Chillee",55.0,10.0,"functorch/csrc/BatchRulesLinearAlgebra.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,3,1,0.799001465,2.0,6715.0,3.0,319759.6666666667,7458.0,17487.5,0.0,Corrective,1.0,1
pytorch,7328710cbce459bbd77dfe424acc94010725d979,b10625004707eceb46e3af3c3bb842719ab30137,vfdev,vfdev.5@gmail.com,Tue Feb 02 08:07:44 2021 -0800,1612253264.0,"Introduced AliasInfo for OpInfo (#50368)

Summary:
Introduced AliasInfo for OpInfo.

Context: Split of https://github.com/pytorch/pytorch/issues/49158

cc mruberry , please let me know if you'd like to see here more code to cover

> [ ] fold test_op_aliases.py into OpInfo-based testing in test_ops.py

from https://github.com/pytorch/pytorch/issues/50006

and/or add `UnaryUfuncInfo('abs')` as discussed https://github.com/pytorch/pytorch/pull/49158/files#r548774221

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50368

Reviewed By: ngimel

Differential Revision: D26177261

Pulled By: mruberry

fbshipit-source-id: 2e3884a387e8d5365fe05945375f0a9d1b5f5d82",166.0,15.0,"test/run_test.py,test/test_ops.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,1.036309276,43.0,11721.0,3.0,202448.75,8544.0,19252.5,0.0,Feature Addition,0.0,1
pytorch,d9686145021248ae77b643adec678e547358c48b,b10c94b5072f288ca915adb24fe1545ca64a773d,Matthew Inkawhich,matthewinkawhich@gmail.com,Fri Jun 15 23:02:24 2018 -0400,1529103744.0,"Update operator documentation with markdown descriptions and interfaces (#8085)

* Update operator documentation with markdown descriptions and interfaces

* Added rest of updated operator documentation to source files

* Commiting local changes for rebase

* fixed bracket typo in sqrt_op.cc file

* Added updated markdown documentation to remaining completed ops",3664.0,456.0,"caffe2/operators/abs_op.cc,caffe2/operators/arg_ops.cc,caffe2/operators/cast_op.cc,caffe2/operators/ceil_op.cc,caffe2/operators/clip_op.cc,caffe2/operators/concat_split_op.cc,caffe2/operators/cos_op.cc,caffe2/operators/counter_ops.cc,caffe2/operators/distance_op.cc,caffe2/operators/dropout_op.cc,caffe2/operators/elementwise_ops_schema.cc,caffe2/operators/elementwise_sum_op.cc,caffe2/operators/exp_op.cc,caffe2/operators/filler_op.cc,caffe2/operators/flatten_op.cc,caffe2/operators/floor_op.cc,caffe2/operators/load_save_op.cc,caffe2/operators/local_response_normalization_op.cc,caffe2/operators/log_op.cc,caffe2/operators/matmul_op.cc,caffe2/operators/mean_op.cc,caffe2/operators/minmax_ops.cc,caffe2/operators/mod_op.cc,caffe2/operators/negative_op.cc,caffe2/operators/pool_op.cc,caffe2/operators/prepend_dim_op.h,caffe2/operators/reshape_op.cc,caffe2/operators/reshape_op.h,caffe2/operators/sequence_ops.cc,caffe2/operators/shape_op.cc,caffe2/operators/sigmoid_op.cc,caffe2/operators/sin_op.cc,caffe2/operators/softmax_op.cc,caffe2/operators/softmax_with_loss_op.cc,caffe2/operators/sqr_op.cc,caffe2/operators/sqrt_op.cc,caffe2/operators/transpose_op.cc,caffe2/operators/utility_ops.cc",38.0,2,1,4.720796514,15.0,7439.0,13.0,3831952.447368421,716.0,3825.5,0.0,Corrective,1.0,1
pytorch,bb119d957e331a1dbca91fbdf00f298088218e5b,b141754b7fa1f048fb27dfbd5cb3491419156273,Edward Yang,ezyang@fb.com,Mon Nov 11 16:09:29 2019 -0800,1573488569.0,"Give a better error message when people accidentally use unsupported devices (#29409)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29409

Fixes #27875

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Differential Revision: D18396828

Pulled By: ezyang

fbshipit-source-id: 3f53cbbe620cd3445852273be90ff5744aa7a8cb",8.0,1.0,"c10/core/impl/DeviceGuardImplInterface.h,test/test_torch.py",2.0,4,2,0.99107606,40.0,14552.0,2.0,2736691.0,13004.0,35900.83333,0.0,Corrective,1.0,1
pytorch,797544c47a4e9bdff02137a127f883a6df9b3dfe,b14d6318f8ea634fe5e3acfe2e5d8acdfc6cca90,Pavan Yalamanchili,pyalamanchili@twitter.com,Fri Feb 17 01:25:33 2017 -0800,1487294733.0,"Convert real to accreal in libTHCUNN

- This reverts commit 0d85922d116879448485ef88ae21e83a9255a0b0.
- Includes fixes for TemporalRowConvolution",164.0,115.0,"SparseLinear.cu,generic/BatchNormalization.cu,generic/ELU.cu,generic/HardTanh.cu,generic/LeakyReLU.cu,generic/LookupTable.cu,generic/MarginCriterion.cu,generic/MultiMarginCriterion.cu,generic/PReLU.cu,generic/SoftPlus.cu,generic/SoftShrink.cu,generic/SparseLinear.cu,generic/SpatialConvolutionLocal.cu,generic/SpatialConvolutionMM.cu,generic/SpatialCrossMapLRN.cu,generic/SpatialDilatedConvolution.cu,generic/SpatialFullConvolution.cu,generic/SpatialSubSampling.cu,generic/Sqrt.cu,generic/THCUNN.h,generic/TemporalConvolution.cu,generic/TemporalRowConvolution.cu,generic/Threshold.cu,generic/VolumetricConvolution.cu,generic/VolumetricDilatedConvolution.cu,generic/VolumetricFullConvolution.cu",26.0,1,1,3.685547011,12.0,7067.0,1.0,-310963.0,17.0,2255.333333,0.0,Corrective,1.0,1
pytorch,0eb2c830068f257e21eeafc99909b568e32b2529,b14f2e899c92f541644e393b2513aa60aeb9196f,Will Feng,willfeng@fb.com,Thu Aug 23 16:59:36 2018 -0700,1535043576.0,"Preserve sparse tensor shape and dim invariants, and add scalar tensor support (#9279)

Summary:
When 0-sized dimension support is added, we expect an empty sparse tensor to be a 1-dimensional tensor of size `[0]`, with `sparseDims == 1` and `denseDims == 0`. Also, we expect the following invariants to be preserved at all times:

```
_sparseDims + _denseDims = len(shape)
_indices.shape: dimensionality: 2,  shape: (_sparseDims, nnz)
_values.shape:  dimensionality: 1 + _denseDims.  shape: (nnz, shape[_sparseDims:])
```

This PR fixes various places where the invariants are not strictly enforced when 0-sized dimension support is enabled.

Tested and `test_sparse.py` passes locally on both CPU and CUDA with the `USE_TH_SIZE_ZERO_DIM` flag.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9279

Differential Revision: D8936683

Pulled By: yf225

fbshipit-source-id: 12f5cd7f52233d3b26af6edc20b4cdee045bcb5e",408.0,205.0,"aten/doc/Functions.h,aten/doc/Tensor.h,aten/doc/Type.h,aten/src/ATen/SparseTensorImpl.cpp,aten/src/ATen/SparseTensorImpl.h,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/LegacyBridge.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/sparse/SparseUtils.h,aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cpp,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,test/test_nn.py,test/test_sparse.py,test/test_torch.py,tools/autograd/gen_python_functions.py,torch/csrc/utils/tensor_new.cpp",20.0,13,4,3.077466167,44.0,30558.0,13.0,1254659.65,3615.0,9871.333333,0.0,Corrective,1.0,1
pytorch,4026593240fcd6844e73fd4142d73f6faf666ee5,b16a352a3bb9c80b5510002b0087daaae1173138,Rudy Bunel,bunel.rudy@gmail.com,Sat Apr 01 23:47:51 2017 -0700,1491090471.0,Fix remainder and cremainder for integer types,32.0,4.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.309543429,28.0,4057.0,1.0,1239.0,302.0,23188.59567,0.0,Corrective,1.0,1
pytorch,dbc354b262f7f5e49aa781785cfce6299fdc2aa8,b189a7444da8b17c535e7d04c9ab705289ec53e1,Khushi,khushiagrawal411@gmail.com,Tue Nov 22 00:15:30 2022 +0000,1669076130.0,"[fix] tril & tril : out of bound check (#89384)

Fixes #83326

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89384
Approved by: https://github.com/ngimel",14.0,0.0,"aten/src/ATen/native/TriangularOps.cpp,test/functorch/test_vmap.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.378783493,7.0,23741.0,3.0,993222.3333333334,9746.0,22581.0,0.0,Corrective,1.0,1
pytorch,e54cbb82503a233ada5323b4e776ee7329d0acf4,b18eeaa80a94eb27a820b64847dbf2cc319d75dd,Jeffrey Wan,jw3468@fb.com,Wed Feb 03 04:20:15 2021 -0800,1612326015.0,"Implement `np.diff` for single order differences (#50569)

Summary:
Implements `np.diff` for single order differences only:
 - method and function variants for `diff` and function variant for `diff_out`
 - supports out variant, but not in-place since shape changes
 - adds OpInfo entry, and test in `test_torch`
 - automatic autograd because we are using the `Math` dispatch

_Update: we only support Tensors for prepend and append in this PR. See discussion below and comments for more details._

Currently there is a quirk in the c++ API based on how this is implemented: it is not possible to specify scalar prepend and appends without also specifying all 4 arguments.

That is because the goal is to match NumPy's diff signature of `diff(int n=1, int dim=-1, Union[Scalar, Tensor] prepend=None, Union[Scalar, Tensor] append)=None` where all arguments are optional, positional and in the correct order.
There are a couple blockers. One is c++ ambiguity. This prevents us from simply doing `diff(int n=1, int dim=-1, Scalar? prepend=None, Tensor? append=None)` etc for all combinations of {Tensor, Scalar} x {Tensor, Scalar}.

Why not have append, prepend not have default args and then write out the whole power set of {Tensor, Scalar, omitted} x {Tensor, Scalar, omitted} you might ask. Aside from having to write 18 overloads, this is actually illegal because arguments with defaults must come after arguments without defaults. This would mean having to write `diff(prepend, append, n, dim)` which is not desired. Finally writing out the entire power set of all arguments n, dim, prepend, append is out of the question because that would actually involve 2 * 2 * 3 * 3 = 36 combinations. And if we include the out variant, that would be 72 overloads!

With this in mind, the current way this is implemented is actually to still do `diff(int n=1, int dim=-1, Scalar? prepend=None, Tensor? append=None)`. But also make use of `cpp_no_default_args`. The idea is to only have one of the 4 {Tensor, Scalar} x {Tensor, Scalar} provide default arguments for the c++ api, and add `cpp_no_default_args` for the remaining 3 overloads. With this, Python api works as expected, but some calls such as `diff(prepend=1)` won't work on c++ api.

We can optionally add 18 more overloads that cover the {dim, n, no-args} x {scalar-tensor, tensor-scalar, scalar-scalar} x {out, non-out} cases for c++ api. _[edit: counting is hard - just realized this number is still wrong. We should try to count the cases we do cover instead and subtract that from the total: (2 * 2 * 3 * 3) - (3 + 2^4) = 17. 3 comes from the 3 of 4 combinations of {tensor, scalar}^2 that we declare to be `cpp_no_default_args`, and the one remaining case that has default arguments has covers 2^4 cases. So actual count is 34 additional overloads to support all possible calls]_

_[edit: thanks to https://github.com/pytorch/pytorch/issues/50767 hacky_wrapper is no longer necessary; it is removed in the latest commit]_
 hacky_wrapper was also necessary here because `Tensor?` will cause dispatch to look for the `const optional<Tensor>&` schema but also generate a `const Tensor&` declaration in Functions.h. hacky_wrapper allows us to define our function as `const Tensor&` but wraps it in optional for us, so this avoids both the errors while linking and loading.

_[edit: rewrote the above to improve clarity and correct the fact that we actually need 18 more overloads (26 total), not 18 in total to complete the c++ api]_

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50569

Reviewed By: H-Huang

Differential Revision: D26176105

Pulled By: soulitzer

fbshipit-source-id: cd8e77cc2de1117c876cd71c29b312887daca33f",261.0,1.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",10.0,11,4,2.290639437,45.0,39919.0,9.0,463665.5,8591.0,19348.0,0.0,Corrective,1.0,1
pytorch,8b61ee522e63a0c11c4196a7c42694efb12398b8,b1ae7f90d5a8b09f0d5962b1c0c564fdc01556de,Christian Sarofeen,csarofeen@nvidia.com,Sun Mar 05 01:35:46 2017 -0800,1488677746.0,Added functionality for data parallel table (#843),155.0,50.0,"test/test_nn.py,torch/cuda/comm.py,torch/nn/parallel/data_parallel.py,torch/nn/parallel/parallel_apply.py,torch/nn/parallel/scatter_gather.py,torch/tensor.py",6.0,5,2,1.910480607,23.0,3130.0,1.0,31045.0,505.0,4678.978466,0.0,Feature Addition,0.0,1
pytorch,03962bc7f1b98b796dee794f68c343fd7a6947e2,b232659765cada47a688c352755078adc56f05ac,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu Apr 29 16:09:03 2021 -0700,1619712543.0,"Replaced _lstsq_helper with internal dispatch (#54724)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54724

Removed at::_lstsq_helper; it is replaced with DEFINE/DECLARE_DISPATCH.

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D27993747

Pulled By: mruberry

fbshipit-source-id: dc8b884fd33b3dd18d9a8e4c582b869ac5391de5",367.0,445.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml",5.0,5,1,1.794294273,12.0,15994.0,3.0,86702.0,11437.0,25869.5,0.0,,1.0,1
pytorch,196a0b1722d9e6c77e6e999b59c169551749c3cb,b23b6e7108b8173f9e03805704c8ea0c1e8a2ea5,Aaron Orenstein,aorenste@fb.com,Mon May 06 18:14:42 2024 +0000,1715019282.0,"Ensure that vmap is restored properly if an exception is thrown during frame eval (#122074)

We save and restore the DynamicLayerStack during frame eval but since fx graph has no way to express a try/finally we just assume it will happen. If we throw an exception between the push and pop to the stack then we're left in a state that affects following operations poorly.  Make sure that if it's in a bad state we restore it after frame eval.

Repro:
before:
```
$ rm test/dynamo_skips/TestSparseCPU.test_log1p_cpu_uint8
$ rm test/dynamo_expected_failures/FuncTorchHigherOrderOpTests.test_vmap_free_tensor
$ PYTORCH_TEST_WITH_DYNAMO=1 pytest test/jit/test_sparse.py test/dynamo/test_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_sparse.py -k 'test_log1p_cpu_uint8'
============= 1 passed, 8588 deselected in 9.75s =============
$ PYTORCH_TEST_WITH_DYNAMO=1 pytest test/jit/test_sparse.py test/dynamo/test_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_sparse.py -k
'test_vmap_free_tensor_dynamic_shapes or test_log1p_cpu_uint8'
================== short test summary info ===================
FAILED [0.0632s] test/test_sparse.py::TestSparseCPU::test_log1p_cpu_uint8 - AssertionError: ""only Tensors of floating point dtype can require gradients""
does not match ""You are attempting to call Tensor.requires_grad_() (or perhaps using torch.autograd.functional.* APIs) inside of a function ...
======= 1 failed, 1 skipped, 8587 deselected in 10.99s =======
```
(Note that adding test_vmap_free_tensor_dynamic_shapes causes test_vmap_free_tensor_dynamic_shapes to fail)
after:
```
$ rm test/dynamo_skips/TestSparseCPU.test_log1p_cpu_uint8
$ rm test/dynamo_expected_failures/FuncTorchHigherOrderOpTests.test_vmap_free_tensor
$ PYTORCH_TEST_WITH_DYNAMO=1 pytest test/jit/test_sparse.py test/dynamo/test_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_sparse.py -k 'test_log1p_cpu_uint8'
============= 1 passed, 8588 deselected in 9.89s =============
$ PYTORCH_TEST_WITH_DYNAMO=1 pytest test/jit/test_sparse.py test/dynamo/test_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_sparse.py -k
'test_vmap_free_tensor_dynamic_shapes or test_log1p_cpu_uint8'
======= 1 passed, 1 skipped, 8587 deselected in 11.34s =======
```
(test_vmap_free_tensor_dynamic_shapes passes either way)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/122074
Approved by: https://github.com/oulgen",49.0,1.0,"torch/_C/_functorch.pyi,torch/_dynamo/eval_frame.py,torch/csrc/functorch/init.cpp",3.0,5,1,0.978854149,2.0,2176.0,2.0,2677646.333333333,28345.0,67330.5,0.0,Feature Addition,0.0,1
pytorch,862e35af2590b8cafa66593d2cfca61deea8fc50,b23e51d467e76e2ae4db8accc6f342595b46ca1f,Yangqing Jia,jiayq84@gmail.com,Tue Sep 06 22:54:56 2016 -0700,1473202496.0,chunky sync,10172.0,3436.0,"caffe2/binaries/fb_run_plan_mpi.cc,caffe2/binaries/predictor_verifier.cc,caffe2/contrib/nccl/cuda_nccl_op_gpu.cc,caffe2/contrib/nccl/nccl_ops_test.py,caffe2/contrib/nnpack/nnpack_ops_test.py,caffe2/contrib/torch/torch_ops_test.py,caffe2/contrib/warpctc/ctc_ops_test.py,caffe2/core/blob_serializer_base.h,caffe2/core/context.h,caffe2/core/logging.cc,caffe2/core/logging_is_google_glog.h,caffe2/core/net.cc,caffe2/core/net.h,caffe2/core/net_test.cc,caffe2/core/operator.cc,caffe2/core/operator.h,caffe2/core/operator_test.cc,caffe2/core/parallel_net_test.cc,caffe2/core/predictor.cc,caffe2/core/predictor.h,caffe2/core/predictor_test.cc,caffe2/core/tensor.h,caffe2/core/types.cc,caffe2/core/types.h,caffe2/core/workspace.cc,caffe2/core/workspace.h,caffe2/mpi/mpi_common.cc,caffe2/mpi/mpi_common.h,caffe2/mpi/mpi_gpu_test.cc,caffe2/mpi/mpi_ops.cc,caffe2/mpi/mpi_ops.h,caffe2/mpi/mpi_ops_gpu.cc,caffe2/mpi/mpi_test.cc,caffe2/operators/atomic_ops.cc,caffe2/operators/boolean_mask_ops.cc,caffe2/operators/cast_op.h,caffe2/operators/communicator_op.cc,caffe2/operators/communicator_op_gpu.cc,caffe2/operators/concat_split_op.cc,caffe2/operators/concat_split_op.h,caffe2/operators/conv_op.h,caffe2/operators/conv_op_eigen.cc,caffe2/operators/conv_pool_op_base.h,caffe2/operators/conv_transpose_op.cc,caffe2/operators/conv_transpose_unpool_op_base.h,caffe2/operators/cosine_embedding_criterion_op.cc,caffe2/operators/cosine_embedding_criterion_op.cu,caffe2/operators/cosine_embedding_criterion_op.h,caffe2/operators/counter_ops.cc,caffe2/operators/counter_ops.h,caffe2/operators/counter_ops_gpu.cc,caffe2/operators/cross_entropy_op.cc,caffe2/operators/cross_entropy_op.cu,caffe2/operators/cross_entropy_op.h,caffe2/operators/dataset_ops.cc,caffe2/operators/distance_op.cu,caffe2/operators/elementwise_op.cc,caffe2/operators/elementwise_op.cu,caffe2/operators/elementwise_op.h,caffe2/operators/elementwise_op_gpu.cc,caffe2/operators/elementwise_op_schema.cc,caffe2/operators/exp_op.cc,caffe2/operators/filler_op.cc,caffe2/operators/filler_op.h,caffe2/operators/find_duplicate_elements_op.cc,caffe2/operators/find_duplicate_elements_op.h,caffe2/operators/instance_norm_op.cc,caffe2/operators/load_save_op.cc,caffe2/operators/margin_ranking_criterion_op.cc,caffe2/operators/margin_ranking_criterion_op.cu,caffe2/operators/margin_ranking_criterion_op.h,caffe2/operators/matmul_op.cc,caffe2/operators/matmul_op.cu,caffe2/operators/matmul_op.h,caffe2/operators/negative_op.cc,caffe2/operators/no_default_engine_op.h,caffe2/operators/normalize_op.cc,caffe2/operators/normalize_op.h,caffe2/operators/operator_fallback_gpu.h,caffe2/operators/order_switch_ops.cc,caffe2/operators/pad_op.h,caffe2/operators/pad_op_gpu.cu,caffe2/operators/pool_op.h,caffe2/operators/prefetch_op.h,caffe2/operators/recurrent_network_op.cc,caffe2/operators/recurrent_network_op.h,caffe2/operators/recurrent_op_cudnn.cc,caffe2/operators/relu_op.cc,caffe2/operators/remove_data_blocks_op.cc,caffe2/operators/remove_data_blocks_op.h,caffe2/operators/sequence_ops.cc,caffe2/operators/sigmoid_op.cc,caffe2/operators/sigmoid_op.cu,caffe2/operators/softsign_op.cc,caffe2/operators/softsign_op.cu,caffe2/operators/sparse_to_dense_mask_op.cc,caffe2/operators/spatial_batch_norm_op.cc,caffe2/operators/spatial_batch_norm_op.h,caffe2/operators/spatial_batch_norm_op_cudnn.cc,caffe2/operators/string_ops.cc,caffe2/operators/string_ops.h,caffe2/operators/tanh_op.cc,caffe2/operators/tanh_op.cu,caffe2/operators/text_file_reader.cc,caffe2/operators/utility_ops.cc,caffe2/operators/utility_ops.h,caffe2/operators/utility_ops_gpu.cc,caffe2/operators/utility_ops_gpu_test.cc,caffe2/operators/utility_ops_test.cc,caffe2/python/_import_c_extension.py,caffe2/python/caffe2_python.cc,caffe2/python/caffe2_python.h,caffe2/python/caffe2_python_gpu.cc,caffe2/python/caffe_translator.py,caffe2/python/cnn.py,caffe2/python/convnet_benchmarks.py,caffe2/python/core.py,caffe2/python/core_gradients_test.py,caffe2/python/dataio.py,caffe2/python/dataset.py,caffe2/python/db_test.py,caffe2/python/device_checker.py,caffe2/python/dyndep.py,caffe2/python/hypothesis_test.py,caffe2/python/hypothesis_test_util.py,caffe2/python/op/python.py,caffe2/python/op/python_op.cpp,caffe2/python/op/python_test.py,caffe2/python/operator_test/conv_test.py,caffe2/python/operator_test/conv_transpose_test.py,caffe2/python/operator_test/cosine_embedding_criterion_op_test.py,caffe2/python/operator_test/cross_entropy_ops_test.py,caffe2/python/operator_test/dataset_ops_test.py,caffe2/python/operator_test/duplicate_operands_test.py,caffe2/python/operator_test/elementwise_op_broadcast_test.py,caffe2/python/operator_test/emptysample_ops_test.py,caffe2/python/operator_test/margin_ranking_criterion_op_test.py,caffe2/python/operator_test/matmul_op_test.py,caffe2/python/operator_test/mpi_test.py,caffe2/python/operator_test/record_queue_test.py,caffe2/python/operator_test/reshape_ops_test.py,caffe2/python/operator_test/sequence_ops_test.py,caffe2/python/operator_test/spatial_bn_op_test.py,caffe2/python/operator_test/squeeze_test.py,caffe2/python/operator_test/text_file_reader_test.py,caffe2/python/pybind.cc,caffe2/python/pybind_state.cc,caffe2/python/pybind_state.h,caffe2/python/pybind_state_gpu.cc,caffe2/python/queue_util.py,caffe2/python/record_queue.py,caffe2/python/schema.py,caffe2/python/scope.py,caffe2/python/test_util.py,caffe2/python/text_file_reader.py,caffe2/python/utils.py,caffe2/python/workspace.py,caffe2/python/workspace_test.py,caffe2/queue/queue_ops.h,caffe2/sgd/adagrad_op.cc,caffe2/sgd/adagrad_op.h,caffe2/sgd/adam_op.cc,caffe2/sgd/adam_op.h,caffe2/sgd/iter_op.cc,caffe2/sgd/iter_op.h,caffe2/sgd/iter_op_gpu.cc,caffe2/sgd/learning_rate_functors.h,caffe2/sgd/learning_rate_op.h,caffe2/utils/math.h,caffe2/utils/math_cpu.cc,caffe2/utils/math_gpu.cu,caffe2/utils/proto_utils.cc,caffe2/utils/string_utils.cc,caffe2/utils/string_utils.h,caffe2/utils/string_utils_test.cc",175.0,16,1,6.342924415,6.0,26634.0,8.0,3327308.983739837,254.0,2035.333333,0.0,,0.0,1
pytorch,aa16de517df967ac7a9e2025a1361a54349e404c,b24c34426f47d6c311ad80ebba1d2575e6c7a6aa,Saketh Are,saketh@fb.com,Sat Oct 30 15:31:50 2021 -0700,1635607910.0,"Add OpInfo for torch.unique and torch.unique_consecutive (#67529)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67529

Reviewed By: pbelevich

Differential Revision: D32045941

Pulled By: saketh-are

fbshipit-source-id: fefea1ddabcd3c4b40e9374b991410626437cdb4",69.0,0.0,"test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.377308401,2.0,17670.0,3.0,255376.66666666663,16732.0,39202.0,0.0,Feature Addition,0.0,1
pytorch,481def752cc001ff8ac7e3b723ece11aa1110c77,b26eafec079a18bc331f569a7e35497129feed71,Kulin Seth,kulin_seth@apple.com,Sun Oct 02 15:27:52 2022 +0000,1664724472.0,"[MPS] Specialized memory pool for scalar values. (#85817)

- Add buffer usage and debug verbosity flags to MPSAllocator
- Add high_watermark_ration to limit the memory allocation

Pull Request resolved: https://github.com/pytorch/pytorch/pull/85817
Approved by: https://github.com/razarmehr",507.0,314.0,"aten/src/ATen/mps/MPSAllocator.h,aten/src/ATen/mps/MPSAllocator.mm,aten/src/ATen/mps/MPSDevice.mm,aten/src/ATen/mps/MPSGuardImpl.mm,aten/src/ATen/mps/MPSStream.h,aten/src/ATen/mps/MPSStream.mm,aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Activation.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/Indexing.mm,aten/src/ATen/native/mps/operations/PointwiseOps.mm,aten/src/ATen/native/mps/operations/RangeFactories.mm,aten/src/ATen/native/mps/operations/View.mm",14.0,7,1,2.518164567,1.0,5840.0,11.0,4153371.9285714286,7935.0,18811.0,0.0,Corrective,1.0,1
pytorch,b149456645a2e9e70bdac8aa9e6d47681442c200,b26f82b0ecc9f762a33fc271186daae1303aafd9,Teng Li,tengli@fb.com,Thu Nov 22 02:21:55 2018 -0800,1542853315.0,"Robust NCCL barrier improvement to cover all devices combinations (#14271)

Summary:
This covers the very edgy case when we run the same NCCL process group with multiple GPU combinations instead of the last GPU combination. We always keep track of what GPUs have been used previously in the NCCL process group and barrier() itself will synchronize on each GPU's NCCL stream.

Test covered as well. Tested on 8-GPU machine
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14271

Differential Revision: D13164993

Pulled By: teng-li

fbshipit-source-id: 81e04352740ea50b5e943369e74cfcba40bb61c1",40.0,6.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp",3.0,4,2,1.214295263,3.0,2288.0,2.0,256079.3333333333,5551.0,16851.83333,0.0,Feature Addition,0.0,1
pytorch,39ab5bcba86ddc84720b93564cac637952abd677,b2ab6891c5486f75cf6b40a29d833cac0f47b3cc,Soumith Chintala,soumith@fb.com,Wed Jan 04 17:51:55 2017 -0500,1483552315.0,fix the rest of Pool module docs for rst,138.0,52.0,torch/nn/modules/pooling.py,1.0,3,1,0,20.0,471.0,1.0,34807.0,304.0,6387.724559,0.0,Corrective,1.0,1
pytorch,6e4f501f1ae2814b1f6801c95b6cab2f297a3cf6,b2cc8a2617ccd72a11e500e2200ed130566bed91,Ksenija Stanojevic,ksenija.stanojevic@gmail.com,Tue Jul 07 03:27:56 2020 -0700,1594092476.0,"[ONNX]Fix export of full_like (#40063)

Summary:
Fix export of full_like when fill_value is of type torch._C.Value.

This PR fixes a bug when exporting GPT2DoubleHeadsModel https://github.com/huggingface/transformers/issues/4950

Pull Request resolved: https://github.com/pytorch/pytorch/pull/40063

Reviewed By: hl475

Differential Revision: D22398353

Pulled By: houseroad

fbshipit-source-id: 6980a61211fe571c2e4a57716970f474851d811e",32.0,7.0,"test/onnx/expect/TestOperators.test_full_like.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",3.0,5,2,1.236032214,3.0,6266.0,2.0,1186500.6666666667,3413.0,8123.5,0.0,Corrective,1.0,1
pytorch,f796080781e2ee9d74194ef82484f2ec343e7e38,b2cfd961d3ef63f5dc02e1e0b66756a6297d6f3a,anderspapitto,anderspapitto@gmail.com,Wed Feb 07 02:40:27 2018 -0800,1517971227.0,"Handle sequence lengths correctly when exporting RNNs to ONNX (#4695)

* PackedSequence: store batch_sizes as tensor

rather than converting to a list of python integers. This maintains
the invariant that module's inputs/outputs are collections of
Variables.

In particular, this causes the JIT to no longer choke when flattening
and unflattening arguments.

* Handle sequence lengths correctly when exporting RNNs to ONNX

- when uniform sequence lengths are provided, correctly omit the
  argument when constructing the ONNX graph, so as to not fix the
  graph to the batch size.

- handle PackedSequences by floating them through the graph and
  eliminating them in an optimization pass. ONNX does not have packed
  sequences, but operates on a representation equivalent to
  PaddedSequence, so we hide the representation-switching from ONNX

- as a preliminary step towards handling PackedSequences, not directly
  tied to ONNX export, change batch_sizes from being an argument to
  the RNN operators into being an argument to the forward() function
  of those RNN operators. This more closely models the reality that
  batch_sizes are effectively part of the input sequences.",259.0,113.0,"test/test_nn.py,torch/autograd/function.py,torch/csrc/jit/export.cpp,torch/csrc/jit/interned_strings.h,torch/csrc/jit/ir.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/nn/_functions/packing.py,torch/nn/_functions/rnn.py,torch/nn/modules/rnn.py,torch/nn/utils/rnn.py,torch/onnx/symbolic.py",11.0,12,2,2.728493451,38.0,10009.0,9.0,600314.3636363636,2348.0,24546.35823,0.0,Corrective,1.0,1
pytorch,8535418a06d75025541370cc656a8b6a0330ca0d,b37503e4527ebed05fd4ac644b47dedb4ed1c1fd,Heitor Schueroff,heitorschueroff@fb.com,Mon Sep 13 12:50:27 2021 -0700,1631537427.0,"Initial implementation of nanmean (#62671)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62671

Very crude first implementation of `torch.nanmean`. The current reduction kernels do not have good support for implementing nan* variants. Rather than implementing new kernels for each nan* operator, I will work on new reduction kernels with support for a `nan_policy` flag and then I will port `nanmean` to use that.

**TODO**

- [x] Fix autograd issue

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D30515181

Pulled By: heitorschueroff

fbshipit-source-id: 303004ebd7ac9cf963dc4f8e2553eaded5f013f0",150.0,7.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",9.0,10,3,2.164271189,36.0,42755.0,9.0,1022956.3333333334,15380.0,35172.5,0.0,Corrective,1.0,1
pytorch,cfe94009966e815ca3fd9742ef577d4627b8b989,b3a9a7a9b95961943b841a925e0d982da7b322ef,vishwakftw,cs15btech11043@iith.ac.in,Tue Jul 30 16:40:51 2019 -0700,1564504851.0,"Rename gels to lstsq (#23460)

Summary:
Changelog:
- Rename `gels` to `lstsq`
- Fix all callsites
- Rename all tests
- Create a tentative alias for `lstsq` under the name `gels` and add a deprecation warning to not promote usage.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/23460

Test Plan: - All tests should pass to confirm that the patch is correct

Differential Revision: D16547834

Pulled By: colesbury

fbshipit-source-id: b3bdb8f4c5d14c7716c3d9528e40324cc544e496",175.0,132.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_cuda.py,test/test_namedtuple_return_api.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/functional.py,torch/tensor.py",14.0,11,5,2.213595907,42.0,38766.0,10.0,453486.5714285714,10293.0,29494.83333,0.0,Corrective,1.0,1
pytorch,492580b855a1e2c5d339a7468314e30bb6c81537,b3ab25aefaffa3f14031a9ddf92e6a0451ca70e7,kshitij12345,kshitijkalambarkar@gmail.com,Wed Dec 09 18:08:24 2020 -0800,1607537304.0,"[numpy] `torch.cosh`: promote integer inputs to float (#48923)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48923

Reviewed By: zhangguanheng66

Differential Revision: D25393679

Pulled By: mruberry

fbshipit-source-id: 2151ee0467b50175f84ac492c219a46ef6bd66c3",13.0,7.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/UnaryGeometricKernels.cu,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py",4.0,11,2,1.814979821,9.0,5097.0,2.0,264162.5,7324.0,16480.5,0.0,,0.0,1
pytorch,04b33b72316bf21ea3851c764480c5d9d319e884,b3e141e84c31763bb9e9ac78bf1996e047745b77,Yi Cheng,eason@fb.com,Wed Jul 18 23:27:58 2018 -0700,1531956478.0,"Add predictor config into Predictor (#9434)

Summary:
This is the first step of refactoring the Predictor. In this diff the config struct
is introduced and the internal data structure of Predictor has been updated.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9434

Differential Revision: D8843262

Pulled By: fishbone

fbshipit-source-id: 23f5e4751614e3fedc9a04060d69331bfdecf864",72.0,32.0,"caffe2/core/predictor.cc,caffe2/core/predictor.h,caffe2/core/predictor_config.h",3.0,2,1,1.44524582,6.0,262.0,2.0,3722991.0,3004.0,7090.333333,0.0,Feature Addition,0.0,1
pytorch,9c58341809c2f9891ade71e7b0dd8e9c232a2d6c,b41449b680a6e6477e737b28430c99fca95405f0,James Cross,jcross@fb.com,Tue Mar 28 14:43:54 2017 -0700,1490712234.0,"SparseMomentumSGDUpdateOp

Summary: Creates SparseMomentumSGDUpdate, a sparse version of MomentumSGDUpdate, to make that optimization method (via in-place updating operator) compatible with GradientSlices.

Differential Revision: D4784973

fbshipit-source-id: e6330f471a4d5f53589a6ac245e38f256ca7f354",152.0,5.0,"caffe2/python/hypothesis_test.py,caffe2/python/operator_test/momentum_sgd_test.py,caffe2/sgd/momentum_sgd_op.cc,caffe2/sgd/momentum_sgd_op.h",4.0,4,1,1.662480105,4.0,2483.0,3.0,5137935.5,664.0,2058.833333,0.0,,0.0,1
pytorch,671c224b0a1cca1f9f6d9966a4064cd7f73bb5d7,b420ded66f6eda92486560705b6188ef157a6001,Pritam Damania,pritam.damania@fb.com,Wed May 26 20:20:54 2021 -0700,1622060454.0,"ShardedTensor framework for ChunkedShardingSpec (#58517)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/58517

Building upon the sharding specifications, this PR introduces the
intial skeleton of ShardedTensor and allows building a ShardedTensor by
specifying ChunkedShardingSpec.

In follow up PRs, I'll add further support for GenericShardingSpec.
ghstack-source-id: 129917841

Test Plan:
1) unit tests.
2) waitforbuildbot

Reviewed By: SciPioneer

Differential Revision: D28526012

fbshipit-source-id: 8e62847b58957d284e40f57a644302c171289138",651.0,91.0,"test/distributed/_sharded_tensor/test_sharded_tensor.py,test/distributed/_sharding_spec/test_sharding_spec.py,torch/distributed/_sharded_tensor/__init__.py,torch/distributed/_sharded_tensor/api.py,torch/distributed/_sharding_spec/__init__.py,torch/distributed/_sharding_spec/api.py",6.0,8,2,2.056051659,1.0,502.0,1.0,249498.0,12490.0,28281.5,0.0,Feature Addition,0.0,1
pytorch,2dff0b6f6a845b1cdd978f81e326d228195d14e9,b45f1b960127e3c27da0739ac27a48ec041acf61,Mike Ruberry,mruberry@devfair044.maas,Wed Oct 02 02:17:06 2019 -0700,1569982626.0,"Makes more of test_cuda.py generic and updates test_torch tests (#27135)

Summary:
- Makes more of test_cuda generic, including some serialization tests
- Updates some tests in test_torch to use latest extensibility points and patterns

Most remaining tests in test_cuda.py are either generated (to be moved in a follow-up PR) or deal with CUDA-specific features like streams, events, and querying CUDA devices.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27135

Differential Revision: D17696478

Pulled By: mruberry

fbshipit-source-id: 51ae424c8a72e725556a2f2bc92ad9a87244b3c0",628.0,655.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.7977259,41.0,15873.0,2.0,62339.5,11955.0,33581.33333,0.0,Feature Addition,0.0,1
pytorch,8c8114801b2b972bdb02e57f416f3b269f013aeb,b476d10c64d2bff5e94cdeb6d74a5e61f06d15fa,Sam Gross,colesbury@gmail.com,Wed Dec 20 01:10:11 2017 -0500,1513732211.0,Move max_pool1d to ATen (#4257),71.0,95.0,"aten/src/ATen/native/Pooling.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py,torch/onnx/symbolic.py",6.0,9,2,1.66188186,28.0,2778.0,4.0,6796.6,382.0,1187.905869,0.0,,0.0,1
pytorch,9f6b6b81010e6f2edbf5faeb521284b1276ee1de,b4b8f53a5d49402e93d73a8b5ff7be5da23df413,Mike Ruberry,mruberry@devfair044.maas,Sun Sep 15 00:09:04 2019 -0700,1568506144.0,"Ports most of test_torch.py to generic device type framework (#26232)

Summary:
This PR moves many tests in test_torch.py to the generic device type framework. This means that many CUDA tests now run in test_torch.py and there is greater consistency in how tests for many device types are written.

One change is that all MAGMA tests are run on the default stream due to intermittent instability running MAGMA on the non-default stream. This is a known issue.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26232

Test Plan:
While this PR edits the tests itself, it was validated using two independent methods:

(1) The code was reviewed and it was verified that all deleted functions were actually moved.
(2) The output of the TestTorch CI was reviewed and test outputs were matched before and after this PR.

Differential Revision: D17386370

Pulled By: mruberry

fbshipit-source-id: 843d14911bbd52e8aac6861c0d9bc3d0d9418219",10484.0,11003.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.092340241,41.0,16426.0,2.0,61571.0,11410.0,32058.33333,0.0,,0.0,1
pytorch,aaff2fecda78ca8064e313944c05a6df720ba87e,b4bc55beefda3a0724b0fb83c04b6bbd8dd46c77,Teng Li,tengli@fb.com,Fri Jan 18 10:23:51 2019 -0800,1547807031.0,"TCP init method race condition fix (#15684)

Summary:
This PR fixes a race condition for TCP init method, when master rank can exit earlier than slave ranks and thus the TCP daemon thread gets shutdown before other slaves are able to access it.

This will let every rank (process) write a special key to the store to mark that they are completed (and thus about to exit).  The master rank (who is the server) will always wait until all the ranks to complete before complete itself.

This should fix: https://github.com/pytorch/pytorch/issues/15638

Tested using the repro of https://github.com/pytorch/pytorch/issues/15638 and works fine. Also test_distributed and test_c10d should have already had this coverage.

I had to make rendezvous test in c10d the world size of 1, since it is a single process code.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15684

Differential Revision: D13570904

Pulled By: teng-li

fbshipit-source-id: 34f3bc471204bbd29320df359347ad5561c6b589",101.0,50.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/distributed/rendezvous.py,torch/lib/c10d/TCPStore.cpp,torch/lib/c10d/TCPStore.hpp,torch/lib/c10d/test/TCPStoreTest.cpp",6.0,9,2,1.769323907,2.0,2971.0,6.0,4398071.166666667,6544.0,20285.33333,0.0,Corrective,1.0,1
pytorch,906f9efc576b23b7a7229b7d1599f1d9426ccefc,b57fe3cc66cfc348d56a7f41fa74b784119fa89a,bhushan,bhushan.s.94@gmail.com,Mon Mar 11 15:55:01 2019 -0700,1552319701.0,"Introducing array-like sequence methods __contains__ (#17733)

Summary:
for tensor

Fixes: #17000
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17733

Differential Revision: D14401952

Pulled By: soumith

fbshipit-source-id: c841b128c5a1fceda1094323ed4ef1d0cf494909",23.0,0.0,"test/test_torch.py,torch/tensor.py",2.0,2,2,0.998635964,40.0,10939.0,2.0,809253.5,7418.0,22761.83333,0.0,Corrective,1.0,1
pytorch,45ddeb5ce6d0a7e2a719fac3d7e32a3643b46379,b5a2f04089fb3c82175b8768f4dea7fe4619c7f5,Pritam Damania,pritam.damania@fb.com,Mon Oct 05 18:52:09 2020 -0700,1601923929.0,"Disallow creation of ProcessGroupNCCL without GPUs. (#45642)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/45642

Prior to https://github.com/pytorch/pytorch/pull/45181, initializing a
NCCL process group would work even if no GPUs were present. Although, now since
init_process_group calls `barrier()` this would fail.

In general the problem was that we could initialize ProcessGroupNCCL without
GPUs and then if we called a method like `barrier()` the process would crash
since we do % numGPUs resulting in division by zero.
ghstack-source-id: 113490343

Test Plan: waitforbuildbot

Reviewed By: osalpekar

Differential Revision: D24038839

fbshipit-source-id: a1f1db52cabcfb83e06c1a11ae9744afbf03f8dc",52.0,7.0,"test/distributed/test_c10d.py,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/testing/_internal/common_distributed.py",3.0,7,2,0.897260036,4.0,5882.0,3.0,653192.0,5716.0,13384.5,0.0,,0.0,1
pytorch,86288265add5225be6de7870a88941937d9475de,b5d13296c65e4b3cd5aa9715cf58df0fc043454e,Adam Lerer,alerer@fb.com,Mon Oct 17 19:18:55 2016 -0700,1476731935.0,addressing comments,610.0,657.0,"test/test_autograd.py,test/test_nn.py,torch/autograd/function.py,torch/autograd/functions/tensor.py,torch/autograd/variable.py,torch/backends/cudnn/__init__.py,torch/backends/cudnn/rnn.py,torch/nn/functions/rnn.py,torch/nn/modules/__init__.py,torch/nn/modules/module.py,torch/nn/modules/rnn.py,torch/nn/modules/rnn/__init__.py,torch/nn/modules/rnn/cell.py,torch/nn/modules/rnn/rnn.py",14.0,10,2,2.63649374,10.0,4542.0,5.0,42428.78571428572,25.0,102.6666667,0.0,Feature Addition,0.0,1
pytorch,08648061f7690f6fad817677006b7159978db08d,b5e1df046e6c1a025c1339d61bccd2cd8aa127c4,moskomule,moskomule@users.noreply.github.com,Wed Jun 28 15:02:06 2017 +0900,1498662126.0,fixed typo in formula of  GRU in doc (#1921),17.0,17.0,torch/nn/modules/rnn.py,1.0,3,1,0,27.0,570.0,1.0,495544.0,1056.0,12579.04911,0.0,Corrective,1.0,1
pytorch,03f2ad9029165d20585d00b11f45fe50de6b687f,b5ee5e585ba5e36bff7a9879fa64085273942ba5,gchanan,gregchanan@gmail.com,Sat Mar 10 04:50:18 2018 -0500,1520657418.0,Only allow dense floating-point types as the default tensor type. (#5674),29.0,15.0,"test/test_torch.py,torch/csrc/tensor/python_tensor.cpp",2.0,4,2,0.684038436,38.0,6234.0,2.0,363209.5,2440.0,24761.35823,0.0,,0.0,1
pytorch,a0a2d9885ac44e6d8e48e2ebefefd48922d42014,b5f7720ab92ec44889ca0f168604febbf06f80f2,Soumith Chintala,soumith@fb.com,Fri Sep 16 09:31:36 2016 -0400,1474018296.0,docstrings for container and batchnorm,216.0,59.0,"README.md,docs/docutils/doc2md.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py",5.0,5,2,2.042070704,9.0,1323.0,2.0,32322.6,160.0,813.469697,0.0,Non Functional,0.0,1
pytorch,6ecd13dfefb446942ca6c0529a6673bd5b599dd6,b604314aa6d0c2888dcd77837a4f34c554442d4b,Michael Suo,suo@fb.com,Wed Mar 16 22:40:55 2022 -0700,1647470455.0,"Prepatory changes for GHA workflow consolidations.

This PR includes all the new files we are adding as part of the workflow consolidation work.

These changes are split out and to be landed first to maintain forward compatibilty of workflows.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74305

Approved by: https://github.com/malfet, https://github.com/seemethere",2240.0,0.0,".github/actions/build-android/action.yml,.github/actions/calculate-docker-image/action.yml,.github/actions/chown-workspace/action.yml,.github/actions/download-build-artifacts/action.yml,.github/actions/pull-docker-image/action.yml,.github/actions/setup-linux/action.yml,.github/actions/setup-rocm/action.yml,.github/actions/setup-win/action.yml,.github/actions/teardown-linux/action.yml,.github/actions/teardown-win/action.yml,.github/actions/upload-test-artifacts/action.yml,.github/scripts/display_ec2_info.sh,.github/workflows/_android-build-test.yml,.github/workflows/_android-full-build-test.yml,.github/workflows/_bazel-build-test.yml,.github/workflows/_docs.yml,.github/workflows/_ios-build-test.yml,.github/workflows/_linux-build.yml,.github/workflows/_linux-test.yml,.github/workflows/_mac-build.yml,.github/workflows/_mac-test.yml,.github/workflows/_rocm-test.yml,.github/workflows/_win-build.yml,.github/workflows/_win-test.yml",24.0,15,1,4.294166002,1.0,0.0,0.0,0.0,1476.0,3598.5,0.0,Feature Addition,0.0,1
pytorch,72fa86409843d2e00ce80d54ac18e1f1d1986354,b6379591a967bca7f55ce5c8e348555a97674c56,leslie-fang-intel,leslie.fang@intel.com,Fri Jun 28 04:41:47 2024 -0700,1719549707.0,"[Inductor][CPP] Pass weight dtype explicitly for cpp gemm template (#129221)

**Summary**
This PR mainly refactor 2 things:

1. Passing in weight's data type explicitly in `create_micro_gemm` as `input2.dtype`. When registering `CppMicroGemmConfig`, we will reuse `input.dtype` if `input2.dtype` is not explicitly registered.
2. Add an util function to get the output data type and compute data type from input data type.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/129221
Approved by: https://github.com/jgong5, https://github.com/jansel
ghstack dependencies: #128825, #129048, #129049, #129103, #129220",50.0,14.0,"torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_micro_gemm.py,torch/_inductor/codegen/cpp_utils.py,torch/_inductor/utils.py",4.0,3,1,1.676798197,1.0,3705.0,3.0,360367.75,30705.0,76782.0,0.0,Feature Addition,0.0,1
pytorch,f4f32cecfd8dbcf7cfe073c78b212e17445dc6fc,b647804a557cee584ad4c98bb08847e164c824ed,Junjie Bai,bai@in.tum.de,Thu Jun 06 20:00:18 2019 -0700,1559851218.0,"Fix embedding bag nan output when input is empty (#21400)

Summary:
```
import torch

Embed = torch.nn.EmbeddingBag(100, 10, sparse=True)

print(Embed(input=torch.LongTensor([]), offsets=torch.LongTensor([0])))
print(Embed(input=torch.LongTensor([]), offsets=torch.LongTensor([0, 0])))
```

Before this fix:
```
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
```

After this fix:
```
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21400

Differential Revision: D15643357

Pulled By: bddppq

fbshipit-source-id: 119eba38129dc0a3757c331304a18044714fcca5",24.0,3.0,"aten/src/ATen/native/EmbeddingBag.cpp,test/test_nn.py",2.0,5,2,0.825626526,42.0,9558.0,2.0,1123818.0,9211.0,26851.83333,0.0,Corrective,1.0,1
pytorch,f8ec51bd865bb488dc0c30f1e970c5dc49ce4727,b64fc3c4b5d927928770f9b343eb845123367084,Mike Ruberry,38511765+mruberry@users.noreply.github.com,Sun Apr 26 04:16:50 2020 -0700,1587874610.0,"Changes warnings generated in cpp to show point of Python origination (#36052)

Summary:
Today in PyTorch, warnings triggered in C++ are printed to Python users like this:

`../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.`

This may be unhelpful to Python users, who have complained it's difficult to relate these messages back to their programs. After this PR, warnings that go through the PyWarningHandler and allow it to add context print like this:

```
test/test_torch.py:16463: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:81.)
  cpu_result = getattr(cpu_tensor, op_str)(*cpu_args)
```

This relates the warning back to the user's program. The information about the cpp file and line number is preserved in the body of the warning message.

Some warnings, like those generated in the JIT, already account for a user's Python context, and so they specify that they should be printed verbatim and are unaffected by this change. Warnings originating in Python and warnings that go through c10's warning handler, which prints to cerr, are also unaffected.

A test is added to test_torch.py for this behavior. The test relies on uint8 indexing being deprecated and its warning originating from its current header file, which is an unfortunate dependency. We could implement a `torch.warn` function, instead.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36052

Differential Revision: D20887740

Pulled By: mruberry

fbshipit-source-id: d3515c6658a387acb7fccaf83f23dbb452f02847",133.0,23.0,"c10/util/Exception.cpp,c10/util/Exception.h,test/cpp/api/support.h,test/test_indexing.py,test/test_torch.py,torch/csrc/Exceptions.cpp,torch/csrc/Exceptions.h,torch/csrc/jit/runtime/interpreter.cpp",8.0,9,3,2.431926464,42.0,21285.0,7.0,1767642.75,1424.0,3777.5,0.0,Feature Addition,0.0,1
pytorch,dad4b2d6cc0ef6baeeb3281e9823eab52063a63b,b6a30f7edefe86faff570680fd2a668315c1da18,Sam Gross,colesbury@gmail.com,Wed Dec 20 21:32:21 2017 -0500,1513805541.0,"Move SELU to ATen (#4269)

Fuse scale multiplication into ELU",94.0,135.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/nn.yaml,aten/src/THCUNN/ELU.cu,aten/src/THCUNN/generic/ELU.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/ELU.c,aten/src/THNN/generic/THNN.h,tools/autograd/derivatives.yaml,torch/legacy/nn/ELU.py,torch/nn/_functions/thnn/__init__.py,torch/nn/_functions/thnn/activation.py,torch/nn/functional.py,torch/onnx/symbolic.py",14.0,17,3,3.046734951,39.0,7425.0,6.0,1060693.8461538462,388.0,1224.905869,0.0,,0.0,1
pytorch,3f029224cddfbea343965a4e9b36eeb7e870ea82,b6adf6871c5bdca29bf9ae1ac3dbedfb869caabd,danielsimig,ds2913@ic.ac.uk,Thu May 10 19:46:57 2018 -0700,1525981617.0,EmbeddingBag to handle empty bags in all modes (#7389),75.0,35.0,"aten/src/ATen/native/EmbeddingBag.cpp,test/test_nn.py",2.0,5,2,0.999045942,40.0,7695.0,2.0,605000.0,2631.0,25013.85823,0.0,,0.0,1
pytorch,92991d953364cdc8239792b20ffbb771b06209a1,b6b2fc7e3f50a83a873da2271578a009819b98e3,Winston Smith,76181208+imaginary-person@users.noreply.github.com,Mon Apr 19 15:54:42 2021 -0700,1618847682.0,"Added OpInfos of add & mm (#55915)

Summary:
Added `OpInfo`s of `add` & `mm`.

cc anjali411

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55915

Reviewed By: agolynski

Differential Revision: D27800077

Pulled By: heitorschueroff

fbshipit-source-id: 84be4b0930f6ef472622e6721a516cc182ac76d1",57.0,13.0,"test/test_autograd.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.315997133,42.0,14095.0,2.0,119034.0,10978.0,24220.5,0.0,Feature Addition,0.0,1
pytorch,3ba5eae72a0fa347abdee25d646d3fa1ad2c7417,b6cfd622856164cb264a386b8e705eb6863a9848,Henry Cheng,39224097+jazzysoggy@users.noreply.github.com,Thu Jan 19 14:49:54 2023 +0000,1674139794.0,"vmap support for torch.linalg.vander (#91749)

Adds vmap support for torch.linalg.vander in a similar manner to how view_as_complex is implemented.

#91700

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91749
Approved by: https://github.com/lezcano",3.0,7.0,"aten/src/ATen/functorch/BatchRulesDecompositions.cpp,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/functorch/test_vmap_registrations.py",4.0,6,2,1.970950594,1.0,7823.0,4.0,382204.75,11449.0,26239.0,0.0,Feature Addition,0.0,1
pytorch,3202028ed1ca24c91dc7192ef69b305690db7abc,b6f41bb8488e7a15705d015944cbfe2a912b6213,Mike Ruberry,mruberry@devfair044.h1.fair,Mon Dec 06 15:30:44 2021 -0800,1638804644.0,"The Jiterator (#69439)

Summary:
This PR:

- creates the ""jiterator"" pattern, allowing elementwise unary and binary kernels that don't accept scalars to be jit compiled when called
- ports the gcd and i1 CUDA kernels to use the jiterator
- extends elementwise binary systemic testing to be comparable to elementwise unary systemic testing
- separates one test case from test_out in test_ops.py
- updates more OpInfos to use expected failures instead of skips

The jiterator currently does not support half, bfloat16 or complex dtypes. It also (as mentioned above) doesn't support scalar inputs. In the future we expect to add support for those datatypes and scalars.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/69439

Reviewed By: ngimel

Differential Revision: D32874968

Pulled By: mruberry

fbshipit-source-id: d44bb9cde4f602703e75400ec5a0b209f085e9b3",2551.0,323.0,"aten/src/ATen/core/Array.h,aten/src/ATen/cudnn/Utils.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/CUDALoops.cuh,aten/src/ATen/native/cuda/GcdLcmKernel.cu,aten/src/ATen/native/cuda/Loops.cuh,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/cuda/MemoryAccess.cuh,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,aten/src/ATen/native/cuda/ZetaKernel.cu,aten/src/ATen/native/cuda/jit_utils.cu,aten/src/ATen/native/cuda/jit_utils.h,test/test_binary_ufuncs.py,test/test_ops.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",16.0,12,3,3.027418175,6.0,23354.0,10.0,7115181.928571428,17530.0,41227.5,0.0,Feature Addition,0.0,1
pytorch,908924204841ab43896627a85174efaf14fa9a6f,b6fc7af8a0fd158e4324aa9fd97dac4db2c038de,leslie-fang-intel,leslie.fang@intel.com,Fri Nov 03 02:54:17 2023 +0800,1698980057.0,"Enable oneDNN QConv FP32/BF16 output (#112010)

**Summary**

- PR 1 for enabling Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor https://github.com/pytorch/pytorch/issues/111640.
- Enable QConv (relu, add, add_relu) with BFloat16 or Float32 output.

**Test Plan**
```
python -u -m pytest -s -v test_quantized_op.py -k test_qconv1d_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv2d_pt2e
python -u -m pytest -s -v test_quantized_op.py -k test_qconv3d_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_relu_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_add_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_add_relu_pt2e
python -u -m pytest test_quantized_op.py -k test_qconv2d_add_relu_float_output_pt2e
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/112010
Approved by: https://github.com/jerryzh168, https://github.com/jgong5",191.0,84.0,"aten/src/ATen/native/quantized/cpu/qconv.cpp,aten/src/ATen/native/quantized/library.cpp,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/quantization/core/test_quantized_op.py,torch/_inductor/fx_passes/quantization.py,torch/_inductor/ir.py,torch/_inductor/lowering.py,torch/_meta_registrations.py",8.0,13,3,2.146036489,5.0,29396.0,6.0,780290.0,21502.0,49140.5,0.0,Feature Addition,0.0,1
pytorch,aeae5d602081f14f41125c4f2bc796e3f0cebfcf,b6fea4f77f4a19eef02b808469ca073b149a8d6f,t-kuha,imagingtechnerd@gmail.com,Wed Oct 09 23:05:32 2019 -0700,1570662332.0,"Removes floating_dtype decorator from test_torch and test_cuda (#27599)

Summary:
Per title. Also makes a few test_torch tests generic.

This PR removes ~half the floating_dtype decorators. Follow-up will remove the rest.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27599

Differential Revision: D17840056

Pulled By: mruberry

fbshipit-source-id: 428bb5498c452083e3608325e0b548b1d75baf2d",513.0,512.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.044481223,41.0,15861.0,2.0,76067.0,12146.0,34000.33333,0.0,,0.0,1
pytorch,5c2c40ad871544b4721955c4472fdc7a72628976,b7106429697bf6a9e374d68aef839cbcfdfd3bf6,Edward Yang,ezyang@fb.com,Wed Dec 12 03:11:02 2018 -0800,1544584262.0,"Make ATen HIPify out-of-place, but still reuse CUDA names. (#14866)

Summary:
```
    This diff changes the HIPification of ATen to be out-of-place.
    We now have the following mappings:

    - ATen/cuda => ATen/hip
    - ATen/native/cuda => ATen/native/hip
    - ATen/native/sparse/cuda => ATen/native/sparse/hip
    - THC => THH
    - THCUNN => THHUNN

    The build system is adjusted to know about these new build paths,
    and HIPify is taught how to adjust include paths and
    THC_GENERIC_FILE appropriately.  ATen_hip is now built as
    the ATen_hip library, rather than reusing ATen_cuda.

    However, despite these new filepaths, none of the identifiers in ATen
    have actually changed.  So, e.g., THHGeneral.h still defines functions
    named THC_blahblah, and HIP still shows up as CUDA in PyTorch itself.
    We'll tackle this in a subsequent PR; this diff is just to get the files
    out-of-place.

    Minor extra improvements:

    - Don't edit tmp_install when hipifying
    - HIP no longer builds native_cudnn_cpp; it was unnecessary
    - Caffe2_HIP_INCLUDES is now Caffe2_HIP_INCLUDE, for consistency
      with all the other variables.
    - HIP build now properly respects ATEN_CUDA_FILES_GEN_LIB (it
      did not previously.)
    - You can now override file extension matching in pyHIPIFY
      by explicitly specifying its full name in the matching list.
      This is used so we can HIPify CMakeLists.txt in some situations.

    A little bit of string and ceiling wax:

    - gen.py grows a --rocm flag so that it knows to generate CUDA
      files which actually refer to the HIP headers (e.g., THH.h)
      We'll get rid of this eventually and generate real HIP files,
      but not for this PR.
    - Management of HIP dependencies is now completely deleted
      from the ATen CMakeLists.txt.  The old code was dead (because
      it was shoveled in ATen_CUDA_DEPENDENCY_LIBS and promptly
      ignored by the Caffe2 build system) and didn't actually work.
```

Stacked on https://github.com/pytorch/pytorch/pull/14849 review last commit only
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14866

Differential Revision: D13419475

Pulled By: ezyang

fbshipit-source-id: cb4c843df69a1d8369314c9fab1b7719520fa3db",238.0,79.0,"aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/gen.py,aten/src/ATen/miopen/Utils.h,aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,aten/src/ATen/native/miopen/Conv_miopen.cpp,caffe2/CMakeLists.txt,cmake/Codegen.cmake,cmake/Dependencies.cmake,cmake/public/utils.cmake,third_party/sleef,tools/amd_build/build_amd.py,tools/amd_build/pyHIPIFY/hipify_python.py,tools/cwrap/plugins/NNExtension.py,torch/csrc/autograd/engine.cpp",15.0,18,6,2.811469969,50.0,7044.0,12.0,1179063.2666666666,5999.0,18640.83333,0.0,Feature Addition,0.0,1
pytorch,7188f0851d6f926b85a72ccb6a0a1a259fe2722d,b738b0960691284cf5ccb68ff76f26b5bac24244,Sam Gross,colesbury@gmail.com,Wed Sep 07 19:41:39 2016 -0400,1473277299.0,"Clean up Module forward and __call__ (#14)

* _forward is renamed forward since users should override it

 * some __call__ overrides are changed to forward

 * function which return a single variable are changed to return that
   variable instead of a one-element tuple",67.0,63.0,"test/test_autograd.py,test/test_nn.py,torch/autograd/function.py,torch/autograd/variable.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/criterion.py,torch/nn/modules/linear.py,torch/nn/modules/module.py,torch/nn/modules/pooling.py",12.0,5,2,2.787229607,2.0,1088.0,8.0,997497.1666666666,145.0,2284.528571,0.0,,0.0,1
pytorch,31952b56ebc438a2a331db4221fe03576896fa44,b744e1c8ef959b1f5089eb4b547b80b8d2fde1a3,Kulin Seth,kulin_seth@apple.com,Fri Jul 01 15:10:56 2022 +0000,1656688256.0,"Add scatter support for view operations (#79939)

* Add scatter support for view operations; #78074, #78886, #79672
* Update test_slicing_replace_column to properly test different sizes
* Handle in-place changes for binary ops; add new testcase
* Add new view ops testing scatter; add MPSDebugConfig.h config file for debugging purposes
* Merge gatherViewTensor and scatterViewTensor into a generic function
* Add scatter on demand in scatterViewOperation instead of caching it into a generic graph
* Create separate graphs for scatter and gather;
* Create scatter graph at scatter time

Fixes #ISSUE_NUMBER

Pull Request resolved: https://github.com/pytorch/pytorch/pull/79939
Approved by: https://github.com/razarmehr",1083.0,114.0,"aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/Copy.mm,test/test_mps.py",5.0,7,2,1.290427841,1.0,6602.0,5.0,882344.6,4940.0,11682.0,0.0,Corrective,1.0,1
pytorch,a234774096cb99cb8826fc56092f720e74634987,b75a214b36d74a775f3c4542f58ac8f9c9f107fd,Huy Do,huydhn@gmail.com,Mon Aug 15 21:25:05 2022 +0000,1660598705.0,"Fix windows flaky test env var (#83466)

Reland #83426 and #83436

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83466
Approved by: https://github.com/atalman",20.0,4.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,3.0,3453.0,1.0,819.0,6434.0,14937.5,0.0,Corrective,1.0,1
pytorch,c02cb7aa08bc36085da217f0527a87e1c1b1cfa6,b7b99ab0c8f82100177729b9751481852d83e77e,Negin Raoof,neginmr@utexas.edu,Sat May 30 04:18:24 2020 -0700,1590812304.0,"[ONNX] Remove Aten ops from ONNX export (#37239)

Summary:
This PR adds a new operator export type to exporter: ONNX_FALLTHROUGH
This new type allows ops that are not supported to pass through.
This PR also removes all aten ops in ONNX operator export type mode.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37239

Reviewed By: hl475

Differential Revision: D21440509

Pulled By: houseroad

fbshipit-source-id: 38b826677cf3431ea44868efebefe1ff51c9aa75",333.0,104.0,"test/onnx/expect/TestOperators.test_embedding_bags.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_utility_funs.py,test/onnx/verify.py,torch/csrc/jit/serialization/export.cpp,torch/csrc/onnx/init.cpp,torch/csrc/onnx/onnx.h,torch/onnx/__init__.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",12.0,9,2,2.663464897,14.0,9805.0,11.0,1877216.0833333333,2457.0,6122.0,0.0,Feature Addition,0.0,1
pytorch,07350da3b4403811ba52a62c2ea5747f59b1788e,b7c5d575638afeb7b61594782af438658f6234cc,kshitij12345,kshitijkalambarkar@gmail.com,Mon Mar 29 01:08:53 2021 -0700,1616980133.0,"[testing] support op with args/kwargs in test_unary_ufunc (#52194)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/51242

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52194

Reviewed By: ngimel

Differential Revision: D27385139

Pulled By: mruberry

fbshipit-source-id: 63118dee33a138ef13810465d2d2d9fa194dfb28",52.0,20.0,"test/test_ops.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,1.050522866,2.0,7163.0,3.0,292242.6666666667,10146.0,22436.5,0.0,Corrective,1.0,1
pytorch,c16b7b41f76233ba930ce7dce6d31f1d362f7e86,b805e1abefd10efabff019e9bb5e3d7d8ba85660,Richard Zou,zou3519@gmail.com,Thu Oct 13 19:44:46 2022 -0700,1665690286.0,"[functorch] Fix torch.cat batching rule (#86932)

The bug was discovered in https://github.com/pytorch/pytorch/pull/86842.

torch.cat has an edge case where it ignores all tensors of shape [0]. So
if any of the BatchedTensors have logical shape [0] but physical shape
[B, 0], then we coerce them to shape [0] by slicing them.

Why don't we just ignore those Tensors? We need to propagate
requires_grad-ness somehow (e.g. if the BatchedTensor wraps a Tensor of
shape [B, 0] that requires grad, then the output must require grad).

Test Plan:
- new tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86932
Approved by: https://github.com/Chillee",75.0,17.0,"aten/src/ATen/functorch/BatchRulesDecompositions.cpp,aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp,test/functorch/test_ops.py,test/functorch/test_vmap.py",4.0,6,2,0.807572678,1.0,7392.0,4.0,434215.5,8578.0,20446.0,0.0,Corrective,1.0,1
pytorch,edebdaf182b619b889cc74659dd926b058cd9304,b812e35a7575a4b931307fcd1ac05efa122794ee,Nikita Karetnikov,nikita@karetnikov.org,Wed Jul 26 14:50:13 2023 +0000,1690383013.0,"[pt2] add meta for `argsort.stable`, use `sort` samples in `OpInfo` (#106025)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106025
Approved by: https://github.com/ezyang, https://github.com/zou3519",7.0,5.0,"test/functorch/test_vmap.py,torch/_meta_registrations.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.325011211,7.0,31220.0,3.0,88753.0,18081.0,41045.0,0.0,Feature Addition,0.0,1
pytorch,0aedd9e8c19f9687f96c8bac8c063002c7cef81a,b815b5bc6bf32f1d7ad17e5779df948546f94211,Horace He,horacehe2007@yahoo.com,Sat Jun 26 23:15:35 2021 -0700,1624749335.0,[functorch] Added triu/tril,20.0,2.0,"functorch/functorch/csrc/BatchRulesHelper.h,functorch/functorch/csrc/BatchRulesViews.cpp,functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/test/test_vmap.py",4.0,4,1,1.789929075,1.0,3342.0,3.0,0.5,176.0,330.0,0.0,Feature Addition,0.0,1
pytorch,3562ca2da23d5e283bbfe61fc99d415b522f8bcc,b822aba8ec65133b51a40236a902ca5a3d981592,Xiang Gao,qasdfgtyuiop@gmail.com,Tue Jan 26 19:00:14 2021 -0800,1611687614.0,"Enable BFloat support for gemms on arch other than ampere (#50442)

Summary:
Fixes #{issue number}

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50442

Reviewed By: bdhirsh

Differential Revision: D26044981

Pulled By: mruberry

fbshipit-source-id: 65c42f2c1de8d24e4852a1b5bd8f4b1735b2230e",74.0,64.0,"aten/src/ATen/cuda/CUDABlas.cpp,test/test_linalg.py,torch/testing/_internal/common_cuda.py,torch/testing/_internal/common_methods_invocations.py",4.0,8,3,1.35864942,2.0,10529.0,4.0,3063117.5,8356.0,18878.5,0.0,Corrective,1.0,1
pytorch,b77e3c2ca1790cfd4414d15f0a0952a630dc22c9,b832b99afb241a8b6ea9fc34698d1f3bfd451f00,Iurii Zdebskyi,iuriiz@fb.com,Tue Apr 02 23:10:43 2019 -0700,1554246643.0,"Bool Tensor for CUDA (#18166)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18166
ghimport-source-id: a8e2ba2d966e49747a55701c4f6863c5e24d6f14

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18166 Bool Tensor for CUDA**
* #18165 Resolved comments from Bool Tensor for CPU PR
------

This PR enables bool tensor creation and some basic operations for the CPU backend. This is a part of Bool Tensor feature implementation work. The whole plan looks like this:
1. Storage Implementation [Done]
2. Tensor Creation.
a) CPU [Done]
b) CUDA [This PR]
3. Tensor Conversions.
4. Tensor Indexing.
5. Tensor Operations.
6. Back compatibility related changes.

Change:
Enable bool tensor in CUDA with the following operations:

    torch.zeros
    torch.tensor
    torch.ones
    torch.rand/rand_like/randint/randint_like
    torch.full
    torch.full_like
    torch.empty
    torch.empty_like

Tested via unit tests and local scripts.

Differential Revision: D14605104

fbshipit-source-id: b7d7340a7d70edd03a109222d271e68becba762c",268.0,166.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,aten/src/ATen/gen.py,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/cpu/CopyKernel.cpp,aten/src/ATen/native/cuda/CUDAScalar.cu,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/TensorCompare.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/ATen/preprocess_declarations.py,aten/src/THC/THCTensorMath.cu,aten/src/THC/THCTensorMath.h,aten/src/THC/THCTensorRandom.cu,aten/src/THC/THCTensorRandom.h,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMath.h,c10/util/Half.h,test/test_torch.py,torch/testing/__init__.py",20.0,13,4,2.556166231,42.0,23841.0,15.0,2412131.1,7843.0,23744.33333,0.0,Feature Addition,0.0,1
pytorch,0a39a9cfbc95afb815f716d030a256f852185d3b,b8530dc1f071b3d980aefde294276873252af104,Zeming Lin,ebetica0@gmail.com,Mon Aug 13 17:11:45 2018 -0700,1534180305.0,"A few additions (#9837)

Summary:
This PR provides 4 fixes / features:

1. torch::nn::Cloneable inherits virtually from torch::nn::Module. We want to pass around a module with new functions, and the best way to do this is to do a diamond inheritance pattern, i.e.

```c++
struct MySuperModuleImpl : virtual public torch::nn::Module {
  virtual void myFunction() = 0;
}

struct MySuperModule : public torch::nn::Cloneable<MySuperModule>, MySuperModuleImple {};

struct MyModule : public MySuperModule<MyModule> {
  void myFunction() override;
};
```

This way, we can simply pass around MySuperModuleImpl around instead of torch::nn::Module.

2. Optimizer options are public now, since there's no way to decay the LR or modify it during training otherwise
3. Serialization functions creates autograd history and calls copy_! Bad!
4. Optimizers did not create buffers after add_parameters was called.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9837

Reviewed By: goldsborough

Differential Revision: D9199746

Pulled By: ebetica

fbshipit-source-id: 76d6b22e589a42637b7cc0b5bcd3c6b6662fb299",141.0,120.0,"test/cpp/api/module.cpp,test/cpp/api/optim.cpp,test/cpp/api/sequential.cpp,test/cpp/api/serialization.cpp,torch/csrc/api/include/torch/nn/cloneable.h,torch/csrc/api/include/torch/nn/module.h,torch/csrc/api/include/torch/nn/modules/any.h,torch/csrc/api/include/torch/optim/adagrad.h,torch/csrc/api/include/torch/optim/adam.h,torch/csrc/api/include/torch/optim/lbfgs.h,torch/csrc/api/include/torch/optim/optimizer.h,torch/csrc/api/include/torch/optim/rmsprop.h,torch/csrc/api/include/torch/optim/sgd.h,torch/csrc/api/include/torch/serialization.h,torch/csrc/api/src/optim/adagrad.cpp,torch/csrc/api/src/optim/adam.cpp,torch/csrc/api/src/optim/lbfgs.cpp,torch/csrc/api/src/optim/rmsprop.cpp,torch/csrc/api/src/optim/sgd.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h",21.0,14,2,3.967729749,38.0,3982.0,9.0,2825337.4285714286,3421.0,9197.333333,0.0,Corrective,1.0,1
pytorch,bcb466fb765a9143f275a2d7f5988ffa28e80fac,b85fc35f9afbb4dd4d4cd74cb96db45bb6c23930,Francisco Massa,fvsmassa@gmail.com,Sun Oct 23 11:03:10 2016 +0200,1477220590.0,"Fix for versions compiled without CUDA support (#155)

* Fix pytorch when compiling without CUDA support
* Skip print test with CUDA types if CUDA is not available",6.0,0.0,"test/test_torch.py,torch/cuda/__init__.py",2.0,3,2,0.918295834,11.0,2717.0,2.0,241022.5,249.0,2310.64599,0.0,Corrective,1.0,1
pytorch,02317d9336f544cce3a924169acb2ece5b9cb21d,b86dc0c8ba3d4a6feb16863c04b931f43efd1ad8,Kevin Zakka,kevinzakka@users.noreply.github.com,Mon Dec 18 07:32:05 2017 +0200,1513582325.0,"add reduce arg to PoissonNLLLoss (#3770)

* add reduce arg to PoissonNLLLoss

* fixed comments except reference function

* fixed unit test

* small indentation fix

* fixing last comments by richard

* lint check

* another linting issue",30.0,9.0,"test/test_nn.py,torch/nn/functional.py,torch/nn/modules/loss.py",3.0,4,2,1.573534961,37.0,7637.0,1.0,949.0,2208.0,24237.85823,0.0,Corrective,1.0,1
pytorch,526e4aa5f88528aec02cfb166d7b50aaae046fde,b87682f555a9dd8cc87fb33a419528a911f1ed2b,Pearu Peterson,pearu.peterson@gmail.com,Tue Nov 29 22:49:36 2022 +0200,1669762176.0,"Fix gradcheck for CSR and CSC inputs. (#89786)

Partially fix-es https://github.com/pytorch/pytorch/issues/87085

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89786
Approved by: https://github.com/albanD",61.0,18.0,"test/test_autograd.py,test/test_overrides.py,torch/autograd/gradcheck.py",3.0,3,2,0.867425232,44.0,13037.0,3.0,1132229.3333333333,10123.0,23211.5,0.0,Corrective,1.0,1
pytorch,b25182971f705a2d6105c36fa5f0a61bd133145b,b87c113cf4f9bebb69b04e926556743e8f2b5f94,Eli Stevens,wickedgrey@gmail.com,Sun Feb 26 13:33:26 2017 -0800,1488116006.0,"CUDA documentation enhancement and docs versioning (#848)

* Add more detail to CUDA documentation

Also adds better cross-linking to the pages that discuss relevant topics.

* Adds recommendation to torch.save docs

* Make the version numbers for the docs dynamic

Might need tweaks for beta, 1.0, etc.",101.0,42.0,"docs/source/conf.py,docs/source/notes/cuda.rst,docs/source/notes/multiprocessing.rst,torch/cuda/__init__.py,torch/nn/modules/loss.py,torch/nn/modules/upsampling.py,torch/nn/parallel/data_parallel.py,torch/serialization.py",8.0,8,2,2.240559153,24.0,1858.0,4.0,471446.375,436.0,3909.575745,0.0,Feature Addition,0.0,1
pytorch,afe6ea884fa21d424d614efff6bd3a75ec4357af,b88340ac726e2d991c95b3448bd4af7929f69a6c,Andrew Gu,andgu@fb.com,Mon Jan 23 00:13:22 2023 +0000,1674432802.0,"[PT-D][Lint] Include nested directories to ufmt (#92779)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92779
Approved by: https://github.com/mrshenli, https://github.com/Skylion007",4.0,4.0,.lintrunner.toml,1.0,0,0,0,3.0,879.0,1.0,343439.0,11575.0,26581.5,0.0,,0.0,1
pytorch,ea367347c0d03f75251a793c72874b7207504af3,b89fda51cd3899eae1dc9e5a2f2700c4113a4400,Pearu Peterson,pearu.peterson@gmail.com,Fri Feb 24 11:22:26 2023 +0200,1677237746.0,"Implement sparse semantics support in gradcheck (2nd try) (#95405)

Replaces https://github.com/pytorch/pytorch/pull/94714 that was reverted due to https://github.com/pytorch/pytorch/pull/94714#issuecomment-1442355648

Pull Request resolved: https://github.com/pytorch/pytorch/pull/95405
Approved by: https://github.com/albanD",209.0,46.0,"test/test_autograd.py,test/test_sparse.py,torch/autograd/gradcheck.py",3.0,3,2,1.330033663,44.0,17060.0,2.0,344333.0,12810.0,29960.5,0.0,Non Functional,0.0,1
pytorch,5a7aad968164edb397ee3c4b33bf07e048c6feb7,b8b840be3d03cf2f7dc4a373f9b0a95f1411cb89,Edward Z. Yang,ezyang@meta.com,Tue Apr 11 13:15:23 2023 -0400,1681218923.0,"Convert logging f-strings to use % format, part five (#98765)

This does some annoying but simple cases by hand.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/98765
Approved by: https://github.com/wanchaol",77.0,92.0,"benchmarks/dynamo/common.py,docs/source/conf.py,functorch/benchmarks/chrome_trace_parser.py,tools/linter/adapters/s3_init.py,torch/_dynamo/symbolic_convert.py,torch/_dynamo/utils.py,torch/backends/_nnapi/serializer.py,torch/backends/xeon/run_cpu.py,torch/cuda/_sanitizer.py,torch/distributed/elastic/agent/server/api.py,torch/distributed/elastic/multiprocessing/api.py,torch/distributed/elastic/multiprocessing/errors/__init__.py,torch/distributed/elastic/multiprocessing/tail_log.py,torch/distributed/elastic/timer/file_based_local_timer.py,torch/distributed/elastic/timer/local_timer.py,torch/distributed/rpc/__init__.py,torch/fx/passes/infra/partitioner.py,torch/fx/passes/infra/pass_manager.py,torch/fx/passes/net_min_base.py,torch/package/_importlib.py,torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py",21.0,30,5,3.173158309,33.0,15702.0,14.0,3511184.8571428573,14402.0,32934.0,0.0,,0.0,1
pytorch,3fd46a2f9c56c692b242727cb146cfd464210c6a,b8de1cf0073908913dad890a4f48fa06090636b4,Xuehai Pan,XuehaiPan@pku.edu.cn,Wed Feb 08 17:31:38 2023 +0000,1675877498.0,"[functorch][nn] Refactor NN stateless APIs by swapping module tensors (#92536)

- Fixes #92295
- Resolves #86708
- Resolves #92153
- Closes #92401
- Closes #92218

- Requires #91579

Refactor NN stateless APIs by swapping module tensors.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92536
Approved by: https://github.com/jbschlosser",1095.0,229.0,".lintrunner.toml,test/test_stateless.py,torch/_functorch/functional_call.py,torch/_functorch/make_functional.py,torch/nn/utils/_named_member_accessor.py,torch/nn/utils/stateless.py",6.0,5,2,2.065860385,3.0,2458.0,3.0,1382233.4,12206.0,28542.0,0.0,Corrective,1.0,1
pytorch,12d6f79ecdb6b4ec17b1eef0e726e887636d516e,b8fb6eae88cba2267fb56d3eda2e8a4ba7f04139,Mingfei Ma,mingfei.ma@intel.com,Thu Apr 18 13:31:24 2019 -0700,1555594284.0,"Improve bmm() performance on CPU when input tensor is non-contiguous (#19338)

Summary:
This PR aims to improve Transformer performance on CPU, `bmm()` is one of the major bottlenecks now.

Current logic of `bmm()` on CPU only uses MKL batch gemm when the inputs `A` and `B` are contiguous or transposed. So when `A` or `B` is a slice of a larger tensor, it falls to a slower path.

`A` and `B` are both 3D tensors. MKL is able to handle the batch matrix multiplication on occasion that `A.stride(1) == 1 || A.stride(2) == 1` and `B.stride(1) == || B.stride(2) == 1`.

From [fairseq](https://github.com/pytorch/fairseq) implementation of Transformer, multi-head attention has two places to call bmm(), [here](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L167) and [here](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py#L197), `q`, `k`, `v` are all slices from larger tensor. So the `bmm()` falls to slow path at the moment.

Results on Xeon 6148 (20*2 cores 2.5GHz) indicate this PR improves Transformer training performance by **48%** (seconds per iteration reduced from **5.48** to **3.70**), the inference performance should also be boosted.

Before:
```
| epoch 001:   0%| | 27/25337 [02:27<38:31:26,  5.48s/it, loss=16.871, nll_loss=16.862, ppl=119099.70, wps=865, ups=0, wpb=4715.778, bsz=129.481, num_updates=27, lr=4.05e-06, gnorm=9.133,
```
After:
```
| epoch 001:   0%| | 97/25337 [05:58<25:55:49,  3.70s/it, loss=14.736, nll_loss=14.571, ppl=24339.38, wps=1280, ups=0, wpb=4735.299, bsz=131.134, num_updates=97, lr=1.455e-05, gnorm=3.908,
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19338

Differential Revision: D14986346

Pulled By: soumith

fbshipit-source-id: 827106245af908b8a4fda69ed0288d322b028f08",12.0,12.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/mkl/LinearAlgebra.cpp",2.0,5,1,0.650022422,7.0,763.0,2.0,2363468.5,8146.0,24552.83333,0.0,Perfective,0.0,1
pytorch,000d73ccde6e0440346b4b5c2afa7687228c9768,b90790ab1bb30d89aff2ab6c383e755c3b5542af,Sam Gross,colesbury@gmail.com,Fri May 17 16:03:05 2019 -0700,1558108985.0,"Don't split 256-bit AVX2 load/store intrinsics (#20609)

Summary:
Recent versions of GCC split unaligned load and store intrinsics into
two 128-bit instructions. On old processors (Sandy Bridge) this was a
bit faster for unaligned data, but bit slower for aligned data. On new
processors (Intel Haswell+, recent AMD) splitting loads is slower on
both aligned and unaligned data.

Clang, MSVC, and ICC do not split unaligned load and store intrinsics.

There's a good explanation here:
https://stackoverflow.com/questions/52626726/why-doesnt-gcc-resolve-mm256-loadu-pd-as-single-vmovupd#tab-top

Splitting load and store intrinsics makes no sense in our AVX2
configuration because the CPUs that support AVX2 instructions are the
same CPUs where splitting is disadvantageous on all data alignemnt.

Note that this doesn't change the AVX configuration (used by CPUs that
support AVX but not AVX2). It's possible this would be benficial for
that configuration too (our data is usually 32-byte aligned), but I'd
prefer the conservative change for now.

torch.add generated assembly (hot loop) (GCC 7.3.0)
before:
https://gist.github.com/colesbury/066376537bccd514daf8fe4ab54d8295

after:
https://gist.github.com/colesbury/8b4b948145001d44b225c51d2428bb91

Timing of `torch.add(x, y, out=z)` for size 10240 (1 thread, Broadwell,
no turbo):
before: 7.35 us after: 6.39 us

(Take the torch.add timings with a grain of salt. The difference in timings
is much larger than I would expect.)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20609

Differential Revision: D15385800

Pulled By: colesbury

fbshipit-source-id: 66415b148a3b19360b9de9881af594ab46547b6f",11.0,1.0,cmake/Codegen.cmake,1.0,1,1,0,3.0,193.0,1.0,1208844.0,8733.0,25895.33333,0.0,Feature Addition,0.0,1
pytorch,e4f3e5434f4e980d9b6b40d59d75b2a8c0a4c86c,b928e08f3d55cc5ff3dad7dbd7be98686ef7ecaa,Joel Schlosser,jbschlosser@meta.com,Wed Sep 06 17:49:14 2023 -0400,1694022554.0,"Initial vmap + NT support with unbind fallback (#106786)

PoC demonstrating vmap + NT based on the [design doc](https://docs.google.com/document/d/1dVVk6TOqz93PLTIneU2T3xaxCs9qZ0MaJyCvOAp_bC0). This PR:
* Allows `BatchedTensorImpl`s to contain NTs
* Introduces a `BatchedNestedTensor` dispatch key for NT-specific batching rules
* Provides a batching rule fallback that unbinds the NTs -> performs computation on constituent -> rebinds results into NT

Restrictions:
* Only supports one level of vmap
* Only supports vmapping over dim=0 for NTs
    * For operations with mixed NT / dense inputs, support is also limited to dim=0 for the dense inputs
Pull Request resolved: https://github.com/pytorch/pytorch/pull/106786
Approved by: https://github.com/zou3519",416.0,30.0,"aten/src/ATen/functorch/BatchedFallback.cpp,aten/src/ATen/functorch/BatchedFallback.h,aten/src/ATen/functorch/BatchedTensorImpl.cpp,aten/src/ATen/functorch/BatchedTensorImpl.h,aten/src/ATen/functorch/Interpreter.cpp,aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp,c10/core/DispatchKey.cpp,c10/core/DispatchKey.h,test/functorch/test_vmap.py,torch/csrc/functorch/init.cpp,torchgen/model.py",11.0,12,5,2.159149177,5.0,11249.0,9.0,5578428.181818182,19434.0,44165.0,0.0,Feature Addition,1.0,1
pytorch,335033f7182bf421d203d5eeaad598fa1102933f,b9793a66b56ec01e4ec85dce879552dfa650d0c8,Peter Bell,peterbell10@live.co.uk,Thu Sep 08 14:18:18 2022 +0100,1662646698.0,"Fix linalg.norm sample inputs function and related failures (#84452)

Due to an indentation error, the return statement happens after just 1
loop of `for test_size in test_sizes` so only one shape was ever
tested.

This also revealed several cases where the provided shapes don't work
so I've disabled the generation of those sample inputs.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84452
Approved by: https://github.com/Lezcano, https://github.com/zou3519",42.0,5.0,"functorch/functorch/csrc/BatchRulesDecompositions.cpp,functorch/test/test_vmap.py,test/test_mps.py,torch/testing/_internal/opinfo/definitions/linalg.py",4.0,10,3,0.650815836,2.0,14245.0,3.0,215638.75,7169.0,16716.0,0.0,Corrective,1.0,1
pytorch,3d9fd060f47fa623d241f4a8c2da6ea7ab6dfb72,b97ae59e29ff78829632bd4ae24edd5ecc9cf5ea,Will Constable,whc@fb.com,Thu Oct 13 15:10:46 2022 +0000,1665673846.0,"Change legacy wrap_dim to work with symint == (#86842)

- previously, sizes == vector<T>({0}) failed to hit SymInt::operator==, causing a the loop to bail out too early and make an invalid call to downstream maybe_wrap_dim helper

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86842
Approved by: https://github.com/Chillee, https://github.com/malfet, https://github.com/albanD",16.0,3.0,"aten/src/ATen/WrapDimUtils.h,test/functorch/test_ops.py,test/test_ops_gradients.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,1.924824067,10.0,24774.0,5.0,1544405.2,8322.0,19728.0,0.0,,0.0,1
pytorch,f7ab0cb56cbc55fe96b6df4dba20eb9b4495d51b,b997474a4f7dc0c1aa2e29049317169dff0a8680,Richard Zou,zou3519@users.noreply.github.com,Fri Jan 19 14:37:53 2018 -0500,1516372673.0,Adds Im2Col and Col2Im (#4729),924.0,58.0,"aten/src/THCUNN/CMakeLists.txt,aten/src/THCUNN/Col2Im.cu,aten/src/THCUNN/Im2Col.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/Im2Col.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THCUNN/im2col.h,aten/src/THNN/generic/Col2Im.c,aten/src/THNN/generic/Im2Col.c,aten/src/THNN/generic/SpatialFullDilatedConvolution.c,aten/src/THNN/generic/THNN.h,aten/src/THNN/init.c,test/test_nn.py,torch/nn/_functions/thnn/__init__.py,torch/nn/_functions/thnn/fold.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/fold.py",18.0,12,3,3.461908854,37.0,11851.0,8.0,3061443.8,2289.0,24386.35823,0.0,Feature Addition,0.0,1
pytorch,a7e7fbab82d0badaad116eab5a552f6246c3a5ac,b99f972e071d8ef42ede878b1ce7d95d4ba75859,Richard Zou,zou3519@gmail.com,Fri Aug 12 23:51:12 2022 -0700,1660348272.0,"[functorch] update functorch lagging db (#83346)

I'm planning on removing functorch lagging op db because it doesn't make
sense in the context of being a part of PyTorch. Before that happens,
this PR updates it, and a future PR will delete it.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83346
Approved by: https://github.com/samdow",179.0,3.0,"functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py,functorch/test/xfail_suggester.py",5.0,2,1,1.704231237,2.0,7118.0,4.0,822385.0,6404.0,14892.5,0.0,,0.0,1
pytorch,edd41d8d808c1f6865b211be07cc25a1d50cfb0c,b9ab26765e90468c90ebde1072036e5282ef75f8,Luca Antiga,luca.antiga@orobix.com,Mon May 29 17:02:05 2017 -0400,1496077325.0,Add 3D upsampling (nearest and trilinear) with tests,381.0,77.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/_functions/thnn/upsampling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/upsampling.py",6.0,8,3,2.207782506,30.0,5126.0,1.0,7633.0,841.0,11075.84844,0.0,Feature Addition,0.0,1
pytorch,505ecab88d569aa6e48ffe21bacf3b8c2eaa110b,b9b9ae935b32777d567a434e5144f5ec95ce10e7,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Sat Sep 08 14:43:00 2018 -0700,1536417780.0,"Make torch.randint have default dtype int64 (#11040)

Summary:
cc gchanan apaszke
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11040

Differential Revision: D9565728

Pulled By: SsnL

fbshipit-source-id: eb5be9609f30c88f52746fa7e13ad71e2856648e",152.0,4.0,"test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp",3.0,4,2,0.532821506,41.0,10043.0,3.0,75234.0,3952.0,11040.83333,0.0,,0.0,1
pytorch,f9762758583762128f1443cab8a5f41359f1d35a,b9d1ad9c78de61a7bc77bd52e01dae91e00d7767,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Tue May 25 18:51:06 2021 -0700,1621968666.0,"OpInfo: `diag_embed`, `diagonal` (#58642)

Summary:
See: https://github.com/pytorch/pytorch/issues/54261.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58642

Reviewed By: ngimel

Differential Revision: D28627226

Pulled By: mruberry

fbshipit-source-id: b96fa8410bd53937ddb72a46c02b949691ee9458",28.0,14.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,6935.0,1.0,15808.0,12448.0,28177.0,0.0,,0.0,1
pytorch,bbb30ad4ab86f06f90408fdba2d311088be7d55d,b9e89cf9fdf134affc184d36296bb6ac321aec91,Sam Gross,sgross@fb.com,Fri Jul 20 17:18:02 2018 -0700,1532107082.0,"Revert ""Extend DispatchStub to support CUDA dispatch (#9579)"" (#9614)

Summary:
This reverts commit bcf0bf42a1727c8ee788f733c28579d0e36a387c.

The commit was causing issues for some internal FB projects.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9614

Reviewed By: Yangqing

Differential Revision: D8929552

Pulled By: colesbury

fbshipit-source-id: ae9026ad8762a4c5de401273694b4c878fc241a6",117.0,201.0,".jenkins/pytorch/test.sh,aten/src/ATen/native/DispatchStub.cpp,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/CapabilityDispatch.h,aten/src/ATen/native/cpu/ReduceOpsKernel.h,aten/src/ATen/native/cpu/SoftmaxKernel.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h",11.0,7,2,2.450507219,7.0,1629.0,1.0,71971.0,3027.0,7247.833333,0.0,,1.0,1
pytorch,7c44506441f27e965e8b4bb253f5ed24592b5c8d,b9ece39685742a12c7bd599b09fde240e1087ad3,Adam Paszke,adam.paszke@gmail.com,Tue Feb 14 20:28:38 2017 -0800,1487104118.0,"Make torch.Size methods return torch.Size, not tuple",51.0,2.0,"test/test_torch.py,torch/csrc/Size.cpp",2.0,3,2,0.450791388,22.0,2955.0,1.0,146754.0,420.0,3892.575745,0.0,,0.0,1
pytorch,23a5ddd3c86b9aab916707b6ac6aaa35050b5b3f,ba046331e8cfae7c93a86a5664fcb5c25f9dbee0,Masaki Kozuki,masaki.kozuki.2014@gmail.com,Tue May 01 09:00:30 2018 +0900,1525165230.0,"add spectral normalization [pytorch] (#6929)

* initial commit for spectral norm

* fix comment

* edit rst

* fix doc

* remove redundant empty line

* fix nit mistakes in doc

* replace l2normalize with F.normalize

* fix chained `by`

* fix docs

fix typos
add comments related to power iteration and epsilon
update link to the paper
make some comments specific

* fix typo",162.0,0.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/utils/__init__.py,torch/nn/utils/spectral_norm.py",4.0,6,3,1.093947739,40.0,8537.0,3.0,469204.3333333333,3.0,3.0,0.0,Corrective,1.0,1
pytorch,5c44f2a16b8e74dfbc35f2642b78dedc045da403,ba0851326c87256a819197c133b44f3e617b0608,Anjali Chourdia,chourdiaanjali@fb.com,Tue May 12 12:19:06 2020 -0700,1589285946.0,"Revert D21449462: [CUDA] addmv for complex tensors

Test Plan: revert-hammer

Differential Revision:
D21449462

Original commit changeset: 1f2dd5a7f8a4

fbshipit-source-id: 4f5f035668d1de4469d11ddeb08a77340eb52f98",4.0,76.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cuda/CUDABlas.h,aten/src/ATen/native/cuda/Blas.cu,aten/src/ATen/test/cuda_complex_test.cu,test/test_torch.py",6.0,8,2,2.212268313,41.0,19467.0,1.0,27259.0,1953.0,5043.5,0.0,Feature Addition,0.0,1
pytorch,252e9058d45445dc44d3f818eb8bd6fa95ac826a,ba0ebe33c170a8e8aa9df8e8b8549b980b89551e,Junjie Bai,bai@in.tum.de,Thu Dec 06 02:35:21 2018 -0800,1544063721.0,"Unify device argument parsing between torch and c10

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/14786

Differential Revision: D13334501

Pulled By: bddppq

fbshipit-source-id: ae3536be1fe0dcd6a1552ec93629ecc9554c0d7c",24.0,37.0,"c10/Device.cpp,c10/Device.h,test/test_torch.py,torch/csrc/utils/python_arg_parser.h",4.0,5,3,1.702657549,40.0,10226.0,4.0,128047.75,5873.0,17772.33333,0.0,,0.0,1
pytorch,c5799766035a403a46e5ab07cebf9b481cfac9bd,ba1bd4176722f831a548dd69e34951189bc440e1,Edward Yang,ezyang@fb.com,Tue Mar 03 22:33:40 2020 -0800,1583274820.0,"Turn on strict dtype checking for test_torch.py (#33825)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33825

Partially addresses #20376

I do this by overriding assertEqual in classes that opt into
this.  This means I have to fix #33821.  The fix is a little
unsatisfactory as idiomatic Python 2 super() calls don't work
(since the class is no longer in scope); hopefully this will just
work when we go to Python 3.

General approach taken:
- A lot of dtype mismatches are because we specified tensor constants
  that infer to some dtype, but the actual dtype needed is something else.
  Those are easy, just annotate the tensor() constructor (often a legacy
  Tensor/FloatTensor call) with dtype
- There are a few cases where the promotion rules are nontrivial.  Some of them
  I just typed out the expected promotion rules manually (based on trial
  and error)
- There are some more complex cases; if it gets too hairy I just
  set exact_dtype=False and nope the fuck out

I don't have time to do it for all the other classes.  But the setup
should work if people just incrementally add the overrides to classes,
and then eventually flip the default.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Differential Revision: D20125791

Pulled By: ezyang

fbshipit-source-id: 389c2d1efbd93172af02f13e38ac5e92fe730c57",195.0,114.0,"test/test_torch.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",4.0,4,2,1.193457078,40.0,18738.0,4.0,1295130.0,15174.0,40770.33333,0.0,Corrective,1.0,1
pytorch,cab8772c6c8c3ce4bdd44b1889d7b28e6cacf0b0,ba4cff2ffcbe0150bb5d524a236feab6cc6fce58,anjali411,chourdiaanjali123@gmail.com,Mon Mar 02 19:53:27 2020 -0800,1583178807.0,"[dtype inference] Following pytorch default for float vs double (#33713)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33713

Differential Revision: D20193387

Pulled By: anjali411

fbshipit-source-id: d802ec395df4e75e2be02e91d7288ae6fb7cf8e0",7.0,2.0,"test/test_torch.py,torch/csrc/utils/tensor_new.cpp",2.0,4,2,0.918295834,40.0,16193.0,2.0,601402.5,15114.0,40681.83333,0.0,,0.0,1
pytorch,849fb1f7e3df1ce97e357127d4b50f9d41723196,ba544aa0ad28dd56f7965a98ae3bd77d78083bed,yunjey,yunjey47@naver.com,Mon Jul 17 03:04:11 2017 +0900,1500260651.0,Add comments in nn.ELU (#2111),11.0,11.0,torch/nn/modules/activation.py,1.0,3,1,0,32.0,725.0,1.0,192770.0,670.0,5393.672317,0.0,Feature Addition,0.0,1
pytorch,8ae6b0c5f98ea028b82bcf8e8ce333c407ec08e2,ba824eb2d6c37233e533042228c71eab61a1285e,BowenBao,bowbao@microsoft.com,Thu Feb 04 20:35:27 2021 -0800,1612470927.0,"[ONNX] Update unsafe_chunk() method to support new version 13 of Split operator. (#51415) (#51524)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51524

* def unsafe_chunk() support and test in ops13.

* Use _unsqueeze_helper insteadof Unsqueeze operator

* Cast the splits into long.

* Change the test to a fixed dimension.

* Update test_pytorch_onnx_onnxruntime.py

* Disable test_loop_with_list for opset 13.

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D26203123

Pulled By: SplitInfinity

fbshipit-source-id: b273aeff8339faa0e8e9f1fcfbf877d1b703209f

Co-authored-by: Negin Raoof <neginmr@utexas.edu>",37.0,2.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset13.py",2.0,4,2,0.89049164,3.0,6842.0,2.0,20.5,8684.0,19530.5,0.0,Corrective,1.0,1
pytorch,d3f784244eb422bfcd48a9d19dd80a7baaffee15,bac566bf619335d7b9f18dbc35bf663d4d95e253,kshitij12345,kshitijkalambarkar@gmail.com,Wed Mar 24 07:03:17 2021 -0700,1616569397.0,"torch.square : OpInfo and minor fixes (#52551)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Add `out` variant to be consistent with Unary Ops.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52551

Reviewed By: heitorschueroff

Differential Revision: D27233482

Pulled By: mruberry

fbshipit-source-id: fef6f241849a12c46028bd1aad8f5ecc1dc65ea1",38.0,1.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_binary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",4.0,8,3,1.559091001,13.0,17243.0,4.0,126038.25,10028.0,22208.0,0.0,Corrective,1.0,1
pytorch,30000aa3fd57c4bb91dedb002e124588d1143a51,badf84bd6bd33c19d1723b64bd72bdb715d3d3b7,Adnan Akhundov,aakhundov@meta.com,Sat Feb 17 03:59:35 2024 -0800,1708142375.0,"[inductor] Add torch.cond support to JIT Inductor (#119759)

Summary: `torch.cond` is already supported in Dynamo and Export: the `true_fn` and `false_fn` subgraphs are traced as child fx graphs of the main graph and passed to the `torch.cond` higher-order operator in the fx graph. However, this breaks in Inductor, as the latter doesn't have the ways of dealing with child fx subgraphs and properly lowering and codegen-ing them.

In this PR, we add `torch.cond` support in Inductor. This is achieved by adding subgraph lowering and codegen-ing infrastructure as well as new `Conditional` IR node type weaving the parent graph with the true and false child subgraphs.

Here we only implement `torch.cond` support in JIT Inductor (Python wrapper codegen). The implementation in AOT Inductor (C++ wrapper codegen), including ABI-compatibility mode, will follow.

Test Plan:

```
$ python test/inductor/test_control_flow.py
...
----------------------------------------------------------------------
Ran 24 tests in 86.790s
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/119759
Approved by: https://github.com/jansel, https://github.com/eellison",564.0,19.0,"test/inductor/test_control_flow.py,torch/_inductor/codegen/cpp_wrapper_cpu.py,torch/_inductor/codegen/cpp_wrapper_cuda.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/graph.py,torch/_inductor/ir.py,torch/_inductor/lowering.py",7.0,5,2,1.936854795,2.0,18005.0,3.0,83791.16666666667,25308.0,57099.5,0.0,Feature Addition,0.0,1
pytorch,f8e217a17e4d46bb461a04de42bd853d291d0c1f,baedb559e352e7766adb49e501764f2b569bf442,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Sat Aug 14 00:10:07 2021 -0700,1628899807.0,"OpInfo: `nn.functional.conv_transpose2d` (#62882)

Summary:
See https://github.com/facebookresearch/functorch/issues/78 and https://github.com/pytorch/pytorch/issues/54261.

cc: mruberry zou3519 Chillee

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62882

Reviewed By: bdhirsh

Differential Revision: D30280804

Pulled By: zou3519

fbshipit-source-id: e40cdf43e98c1f11e45df6b8bc13110b4d29c45f",40.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8692.0,1.0,25091.0,14631.0,33572.5,0.0,,0.0,1
pytorch,a061f139dccb5f56c9d14e25ef54ff821b4dd3c8,bb24185ff42bc4d774711c1e3a5752280058091d,Peter Bell,peterbell10@live.co.uk,Mon Dec 26 18:07:49 2022 +0000,1672078069.0,"Fix _check_no_differentiable_outputs for forward ad (#91391)

This `is_forward_ad` isn't propagated, which leads to this line creating a
slow-gradcheck failure on master:
```
    if not is_forward_ad and any(o.is_complex() for o in outputs):
        raise ValueError(""Expected output to be non-complex. get_numerical_jacobian no ""
                         ""longer supports functions that return complex outputs."")
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91391
Approved by: https://github.com/albanD",5.0,3.0,torch/autograd/gradcheck.py,1.0,2,1,0,29.0,1648.0,1.0,457216.0,10875.0,24763.0,0.0,Corrective,1.0,1
pytorch,ced0054a9ed13755f9513c344c1880ad9faf9711,bb353ccc176b31ac24ceaaf4fc79cea419aaae13,Brandon Amos,bdamos@vt.edu,Thu Mar 23 19:06:00 2017 -0400,1490295960.0,"Add batch triangular factorization and solves,  add IntegerTensor to cwrap (#903)",161.0,0.0,"test/test_cuda.py,test/test_torch.py,tools/cwrap/plugins/THPPlugin.py,torch/_torch_docs.py,torch/csrc/Module.cpp,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/methods/TensorMath.cwrap",7.0,8,3,2.243044784,27.0,11515.0,1.0,12377.0,564.0,5484.555248,0.0,Feature Addition,0.0,1
pytorch,7c24a3d5cf0a5cfd1d61768953cb8f8be98b77c3,bb3779efe83d43a3fdc069c4dd8ebbd1e3b1aaaa,Gregory Chanan,gchanan@fb.com,Thu Jun 22 18:22:58 2017 -0700,1498155778.0,Add broadcasting to masked_select.,38.0,13.0,"test/test_autograd.py,test/test_torch.py,torch/_torch_docs.py,torch/_utils.py,torch/autograd/_functions/tensor.py,torch/csrc/generic/methods/Tensor.cwrap",6.0,7,2,2.18755036,32.0,12793.0,1.0,145339.0,1038.0,12560.54911,0.0,Feature Addition,0.0,1
pytorch,ab834d50932e4cb34716b925c9c350db02d2541d,bb5dcaf24ffcb493a6ef961402fa65d107829140,Hong Xu,hong@topbug.net,Wed Dec 04 02:20:34 2019 -0800,1575426034.0,"Add logical_and and logical_or (#30521)

Summary:
With the CI failure caused in 8bbafa0b32d2899ef6101172d62c6049427c977b fixed (incorrect return type of the lambdas in CUDA kernels)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30521

Differential Revision: D18770151

Pulled By: ailzhang

fbshipit-source-id: 02f0fe1d5718c34d24da6dbb5884ee8b247ce39a",272.0,62.0,"aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/README.md,docs/source/tensors.rst,docs/source/torch.rst,test/test_namedtensor.py,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py",12.0,11,4,3.159165218,42.0,35928.0,3.0,419718.0,13606.0,37225.83333,0.0,Corrective,1.0,1
pytorch,dca7299b9a096613db279fb08874f2c0ba1e1ce4,bb658fe29b14484f22df9de94d48761108a8c5f0,Soumith Chintala,soumith@gmail.com,Sat Jun 04 04:45:31 2016 +0000,1465015531.0,fixes for gcc 5.xx,10.0,1.0,CMakeLists.txt,1.0,0,0,0,29.0,24.0,1.0,6690681.0,135.0,99.74199134,0.0,Corrective,1.0,1
pytorch,2c2cce73d4b32e7e87a104df7e395a480e38cab4,bb673fb1d9935eddf13be61f7f2587d6303a455a,Sean Ross-Ross,srossross@gmail.com,Tue Dec 06 05:52:09 2022 +0000,1670305929.0,"fix: update error when tensor escapes vmap (#89077)

Fixes https://github.com/pytorch/functorch/issues/1054

@zou3519, I played around with it, but I am unsure of how to repro the cases for gen_vmap_inplace_plumbing and below in gen_vmap_plumbing_no_returns

I've also seen that there are 24 other instances of the `TORCH_INTERNAL_ASSERT(maybe_layer.has_value());` assert, should I change all of these and add tests?

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89077
Approved by: https://github.com/zou3519",49.0,7.0,"aten/src/ATen/functorch/BatchRulesHelper.h,aten/src/ATen/functorch/PlumbingHelper.cpp,aten/src/ATen/functorch/PlumbingHelper.h,test/functorch/test_vmap.py,torchgen/gen_vmap_plumbing.py",5.0,7,3,1.973792823,1.0,5483.0,4.0,4813542.6,10213.0,23385.0,0.0,Corrective,1.0,1
pytorch,4d5338228fb39da7496f05da193e7b58fdf2e9cb,bb8978f605e203fbb780f03010fefbece35ac51c,Alban Desmaison,albandes@fb.com,Fri Nov 05 14:01:30 2021 -0700,1636120890.0,"Revert D32175963: Converting hardswish to strucutred kernels with metatensor support

Test Plan: revert-hammer

Differential Revision:
D32175963 (https://github.com/pytorch/pytorch/commit/57335a9ee3f3a6a34ddb11e924be9c33abc04abb)

Original commit changeset: f4d749c6aeaf

fbshipit-source-id: 6d68a96cf872c2d7b518c061875b9336bca0043a",139.0,164.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/Activation.h,aten/src/ATen/native/cpu/Activation.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/utils/Factory.cpp,aten/src/ATen/native/utils/Factory.h,aten/src/ATen/native/xnnpack/Activation.cpp,aten/src/ATen/native/xnnpack/Engine.h,test/test_nn.py",10.0,9,2,2.147900759,45.0,32625.0,1.0,43360.0,16884.0,39591.5,0.0,,0.0,1
pytorch,7035738b50341565092630343bb7aabd000f8353,bbd42c605aa31b0d2c0de1b282e8bd58d8b42e34,David Berard,dberard@fb.com,Wed Feb 09 18:55:34 2022 -0800,1644432934.0,"[JIT] Opinfo tests for nnc fusion - retry (#72486)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/72486

Retry #70465.

Test Plan: Imported from OSS

Reviewed By: mikaylagawarecki

Differential Revision: D34061628

Pulled By: davidberard98

fbshipit-source-id: e27ed315bc4ad57cdbfbc9cedffcbb7886004524
(cherry picked from commit 7937808d2ebcc758aad4eac3ae6ffe1056d13fc5)",214.0,73.0,"test/test_jit_fuser_te.py,test/test_ops.py,test/test_tensorexpr.py,torch/csrc/jit/python/script_init.cpp,torch/csrc/jit/tensorexpr/eval.cpp,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_utils.py",7.0,8,2,2.136110459,4.0,25422.0,2.0,134264.14285714287,602.0,1430.0,0.0,,0.0,1
pytorch,47e9ec401a8ca3c16f95dcc3e05399d6859f0019,bbdadab306f52dc520cb9afc199186645b0f788e,Jeffrey Wan,jw3468@fb.com,Sat May 01 00:36:00 2021 -0700,1619829360.0,"Refactor fast gradcheck (#55871)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55871

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D28096549

Pulled By: soulitzer

fbshipit-source-id: ee8b71fbd03ee581e71cdfcfd5e2258adefe15a6",56.0,34.0,torch/autograd/gradcheck.py,1.0,2,1,0,28.0,1201.0,1.0,294290.0,11533.0,26035.0,0.0,Perfective,0.0,1
pytorch,5b922918d023126ad1f468c68577c9b599ad202d,bbdc5b7bd06f9a6312243797a0d048d0e3a656f9,Hong Xu,hong@topbug.net,Thu Feb 13 22:03:05 2020 -0800,1581631385.0,"Optimize error checking in mvlgamma (#32665)

Summary:
- Clean up error checking code
- Avoid unecessary floating-point computation
- Use float instead of double when possible to avoid massive cast in the tensor
- Use bool instead of uint8_t for clear Boolean purpose
- Improve error message
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32665

Differential Revision: D19601920

Pulled By: VitalyFedyunin

fbshipit-source-id: 0c6c6b5ff227b1437a6c1bae79b2c4135a13cd37",10.0,10.0,"aten/src/ATen/native/UnaryOps.cpp,test/test_torch.py",2.0,5,2,0.468995594,41.0,15471.0,2.0,89967.0,14738.0,39629.83333,0.0,Perfective,0.0,1
pytorch,e8d8ccb34ae16d01dbea5b5c024365f4a75128eb,bbe6ef38642cda127ece82f984ee3971e8750a45,Benoit Steiner,benoitsteiner@fb.com,Mon Oct 15 20:38:59 2018 -0700,1539635939.0,"torch.finfo and torch.iinfo to mimic the numpy equivalent (#12472)

Summary:
This pull request intends to provide the functionality requested in https://github.com/pytorch/pytorch/issues/10742 by adding a new torch.finfo and torch.iinfo API.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12472

Differential Revision: D10250829

Pulled By: benoitsteiner

fbshipit-source-id: eb22ca55d5b0064bef381fa7f1eb75989977df30",488.0,102.0,"aten/src/ATen/Dispatch.h,docs/source/index.rst,docs/source/type_info.rst,setup.py,test/run_test.py,test/test_type_info.py,torch/csrc/Module.cpp,torch/csrc/TypeInfo.cpp,torch/csrc/TypeInfo.h",9.0,8,4,1.916780825,42.0,2529.0,4.0,1162175.2,4621.0,13653.33333,0.0,Feature Addition,0.0,1
pytorch,46b18aa294fea4b86ce385258d93c81041451d60,bbf6131159efa50bec1543100bfdaebcb4af274f,Joel Schlosser,jbschlosser@fb.com,Mon Aug 02 13:51:37 2021 -0700,1627912297.0,"Add factory kwargs test to test_modules (#62340)

Summary:
Adds a new `ModuleInfo`-based test to `test_modules.py`.

The test passes `device` and `dtype` to each module during instantiation, ensuring that the kwargs are applied to any newly-created parameters or buffers. Note that the `device` and `dtype` kwargs should only be present when a module creates parameters or buffers; the test uses some mock magic to identify this.

Originally lifted from `test/test_module_init.py`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62340

Reviewed By: malfet

Differential Revision: D30022543

Pulled By: jbschlosser

fbshipit-source-id: 77e5d46d6b11c16dc39d19a1c650ee48c26c54c1",95.0,1.0,"test/test_modules.py,torch/testing/_internal/common_utils.py",2.0,4,2,0.757878463,2.0,2625.0,2.0,369695.0,14315.0,32731.5,0.0,Feature Addition,0.0,1
pytorch,6d36bbde7eb2eb0aed448f694338cb49c2ae47f3,bbf7e159e02fafd81dc61fcfa771dc108fd75fc9,Sherlockk Huang,bahuang@fb.com,Tue Mar 29 23:13:37 2022 +0000,1648595617.0,"Implement torch.special.log_ndtr

Implements torch.special.log_ndtr

Issue: https://github.com/pytorch/pytorch/issues/50345

TODO:
- [x] adding proper reference to scipy implementation
- [x] double check if the changes in test/test_unary_ufuncs.py is really necessary
- [x] check setting for UnaryUfuncInfo
cc: @kshitij12345 @mruberry
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74795
Approved by: https://github.com/anjali411",152.0,4.0,"aten/src/ATen/native/Math.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_unary_ufuncs.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",14.0,19,5,3.37720607,20.0,40309.0,10.0,3306587.785714286,1796.0,4267.0,0.0,Feature Addition,0.0,1
pytorch,1011ac188f0489e57bde50f69158dd088356cb6c,bc026c05775a6504e48363941285ae381e52b931,Zhengxu Chen,zhxchen17@fb.com,Fri Jan 07 19:19:15 2022 -0800,1641583155.0,"[jit] Split Union type and Optional type to separate impl file. (#69483)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69483

To avoid accidental linking to Union type and Optional type in Edge runtimes, we can separate these types into different files, so that we don't accidentally link with them in type.cpp.
ghstack-source-id: 146670525

Test Plan: just code move.

Reviewed By: ejguan

Differential Revision: D32264607

fbshipit-source-id: c60b6246f21f3eb0a67f827a9782f70ce5200da7",466.0,446.0,"aten/src/ATen/core/jit_type.h,aten/src/ATen/core/type.cpp,aten/src/ATen/core/union_type.cpp,tools/build_variables.bzl",4.0,5,2,1.047157184,4.0,5246.0,2.0,6.666666666666667,18189.0,43051.5,0.0,Preventative,0.0,1
pytorch,0e753b28184ae3fc977d5016bc4d3ee5542c2947,bc0ab07064f576117052fbc5eea4af5041ff71ff,Xiaomeng Yang,yangxm@fb.com,Thu Feb 13 00:31:46 2020 -0800,1581553906.0,"Opitmize Unfold3d to improve performance of Conv3d (#33191)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33191

Opitmize Unfold3d to improve performance of Conv3d forward

Test Plan: buck test mode/dev-nosan //caffe2/test:nn -- ""Conv3d""

Reviewed By: houseroad

Differential Revision: D19821946

fbshipit-source-id: 937adafddb9a1aef5f1d1423dd99884c59e465f9",292.0,190.0,"aten/src/ATen/native/ConvolutionMM3d.cpp,aten/src/ATen/native/Unfold3d.cpp,aten/src/ATen/native/Unfold3d.h",3.0,4,1,0.914354411,1.0,1052.0,2.0,6103899.333333333,14713.0,39578.83333,0.0,Perfective,0.0,1
pytorch,3526b604b1c7fd3be0c6f4e898f0599081f0ee6a,bc1ce584512a860c15cb991460d8c98debd62b26,Iurii Zdebskyi,iuriiz@devfair004.maas,Thu Oct 22 16:27:49 2020 -0700,1603384069.0,"Push rocm to slow path (#46216)

Summary:
Push rocm to slow path

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46216

Reviewed By: bwasti

Differential Revision: D24263731

Pulled By: izdeby

fbshipit-source-id: 98ede2478b8f075ceed44a9e4f2aa292f523b8e2",25.0,1.0,"aten/src/ATen/native/ForeachUtils.h,test/test_foreach.py",2.0,5,2,0.706274089,1.0,1068.0,2.0,477618.0,6155.0,14172.5,0.0,,0.0,1
pytorch,952df2ba8f3cf0117b20cd3505756568ef8b9ddf,bc1d96ca985241752485ba02d0b609283192b49c,Edward Yang,ezyang@fb.com,Tue Oct 23 02:26:20 2018 -0700,1540261580.0,"Add support for inline expect tests. (#12825)

Summary:
expecttest and test_expecttest are the implementation and tests
for this functionality.  I wired it up to the --accept flag,
but there's also a new environment variable EXPECTTEST_ACCEPT
which may be more convenient to trigger.  Haven't tested if this
works in fbcode.

There may be a few expect tests which will benefit from inline
treatment, but I just did one to show it works.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12825

Reviewed By: teng-li

Differential Revision: D10448630

Pulled By: ezyang

fbshipit-source-id: 3d339f82e2d00891309620a60e13039fa1ed8b46",326.0,6.0,".jenkins/pytorch/macos-test.sh,.jenkins/pytorch/test.sh,.jenkins/pytorch/win-test.sh,test/common_utils.py,test/expecttest.py,test/run_test.py,test/test_expecttest.py,test/test_jit.py",8.0,3,2,1.397886966,12.0,10280.0,5.0,267883.3333333333,4785.0,14097.83333,0.0,Feature Addition,0.0,1
pytorch,4a4890cfb2440c497fd41e69405ad623c144e32b,bc1fef96aff4aeffe7a7b39ef8b4ff467860df28,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Jun 17 07:24:02 2022 +0000,1655450642.0,"Reference implementations for rsqrt and native_layer_norm (#79413)

This PR adds references for:
- `torch.rsqrt`
- `torch.native_layer_norm`
-  `torch.nn.functional.layer_norm`

`native_layer_norm` had a different number of dimensions if the input was 0-sized. I fixed that.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/79413
Approved by: https://github.com/mruberry, https://github.com/Chillee",245.0,52.0,"aten/src/ATen/native/cuda/layer_norm_kernel.cu,aten/src/ATen/native/layer_norm.cpp,test/test_decomp.py,torch/_decomp/decompositions.py,torch/_refs/__init__.py,torch/_refs/nn/functional/__init__.py,torch/testing/_internal/common_methods_invocations.py",7.0,13,3,2.089385458,5.0,27129.0,7.0,1517220.0,4460.0,10695.5,0.0,Corrective,1.0,1
pytorch,02b9f686a74171aeeb151a82c501957eb73ebb34,bc233fe405099dc8244fd2bcd326ba9d39a55c65,Brennan Vincent,btv@fb.com,Tue Jan 15 04:14:04 2019 -0800,1547525644.0,"`var` for multiple dimensions (#15892)

Summary:
Timings are the same as for `std` .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15892

Differential Revision: D13651173

Pulled By: umanwizard

fbshipit-source-id: a26bf1021dd972aa9e3e60fb901cd4983bfa190f",54.0,45.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOps.h,aten/src/ATen/native/SharedReduceOps.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cuda/ReduceOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/derivatives.yaml,torch/csrc/jit/passes/shape_analysis.cpp",12.0,14,4,2.777546765,41.0,20166.0,8.0,1015047.75,6461.0,19999.33333,0.0,,0.0,1
pytorch,f5ba489f9376c5edbb410c7d7506128b0206d924,bc45c47aa3d67b2c81372055f17ca81562ac14a2,Xiang Gao,qasdfgtyuiop@gmail.com,Thu Sep 03 03:50:20 2020 -0700,1599105020.0,"Expand the coverage of test_addmm and test_addmm_sizes (#43831)

Summary:
- This test is very fast and very important, so it makes no sense in marking it as slowTest
- This test is should also run on CUDA
- This test should check alpha and beta support
- This test should check `out=` support
- manual computation should use list instead of index_put because list is much faster
- precision for TF32 needs to be fixed. Will do it in future PR.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43831

Reviewed By: ailzhang

Differential Revision: D23435032

Pulled By: ngimel

fbshipit-source-id: d1b8350addf1e2fe180fdf3df243f38d95aa3f5a",37.0,44.0,"aten/src/ATen/native/cuda/LinearAlgebra.cu,test/test_torch.py",2.0,6,2,0.167026804,42.0,20945.0,2.0,26256.5,4802.0,11157.0,0.0,Corrective,1.0,1
pytorch,45d6212fd293709b3dae41510916f5ea3ca91c53,bc475cad671aafd9e1d4347801f35a12ca800edc,Sergey Zagoruyko,zagoruyko2@gmail.com,Sun Dec 25 15:28:11 2016 +0100,1482679691.0,Move max pooling construction logic to functions (#343),140.0,56.0,"torch/nn/functions/thnn/pooling.py,torch/nn/modules/pooling.py",2.0,5,1,0.827054178,16.0,603.0,1.0,73304.0,251.0,2842.784684,0.0,,0.0,1
pytorch,2e81710366279af67f3e05fe00011a99f962e61f,bc6dc8d271d6cf4d0ae381077f59fc7bb7cf024d,Kshiteej K,kshitijkalambarkar@gmail.com,Fri Sep 23 21:40:07 2022 +0000,1663969207.0,"[fix] composite compliance: cumprod, _masked.cumprod, linalg.vander (#85330)

Ref: #69991
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85330
Approved by: https://github.com/zou3519",50.0,27.0,"aten/src/ATen/native/ReduceOps.cpp,functorch/test/test_ops.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/opinfo/definitions/_masked.py,torch/testing/_internal/opinfo/definitions/linalg.py",5.0,11,3,1.958452417,11.0,25092.0,3.0,136152.6,7632.0,17934.0,0.0,Corrective,1.0,1
pytorch,03d4198a67881f8bab8e4be58ed349bafa41cd01,bc6eec1db850fd0de794129f1982dad6e2263359,James Reed,jamesreed@fb.com,Sat Sep 07 02:25:52 2019 -0700,1567823152.0,"Factor unnecesary work out of add inner loop (#25751)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25751

This PR does several things:
1) Factor unnecessary scale inversion out of quantize function in the inner loop. This saves cycles in the inner kernel (unfortunately the compiler couldn't hoist it out automatically for some reason)
2) Use FMA in the dequantize routine when possible. This also necessitates having the user pass in a pre-multiplied (scale * -zero_point) vector.

Benchmark Script
```
import torch
import time

x = torch.rand(1, 256, 56, 56)
y = torch.rand(1, 256, 56, 56)

print('dtype', 'ms/iter (float)', 'ms/iter (quant)', 'quant / float', sep='\t')

for dtype in [torch.quint8, torch.qint8, torch.qint32]:
    qX = torch.quantize_linear(x, 0.1, 5, dtype).permute([0, 3, 1, 2])
    qY = torch.quantize_linear(y, 0.1, 5, dtype).permute([0, 3, 1, 2])

    _x = x.permute([0, 3, 1, 2])
    _y = y.permute([0, 3, 1, 2])

    NITER = 10000

    # Test float
    s = time.time()
    for i in range(NITER):
        _x + _y
    elapsed_float = time.time() - s
    ms_per_iter_float = elapsed_float / NITER * 1000

    # Test quantized
    s = time.time()
    for i in range(NITER):
        torch.ops.quantized.add(qX, qY, 0.1, 5)
    elapsed = time.time() - s
    ms_per_iter = elapsed / NITER * 1000

    print(str(dtype), ms_per_iter_float, ms_per_iter, ms_per_iter / ms_per_iter_float, sep='\t')
    print('float gbps', 'quant gbps', sep='\t')
    print((x.numel() + 2 * y.numel()) * x.element_size() / ms_per_iter_float / 1e6,
          (qX.numel() + 2 * qX.numel()) * qX.element_size() / ms_per_iter / 1e6,
          sep = '\t')

```

Before this change
```
dtype	ms/iter (float)	ms/iter (quant)	quant / float
torch.quint8	0.47297704219818115	0.1909616231918335	0.403743958278252
float gbps	quant gbps
20.368413560257675	12.612209509659206
torch.qint8	0.4638909578323364	0.18829500675201416	0.40590359344764254
float gbps	quant gbps
20.767363185988053	12.79082245219568
torch.qint32	0.4605833768844605	4.219791603088379	9.161840862847583
float gbps	quant gbps
20.916499560114787	2.2830018413585225

```

After this change
```
dtype	ms/iter (float)	ms/iter (quant)	quant / float
torch.quint8	0.465389084815979	0.1516613483428955	0.3258807593282176
float gbps	quant gbps
20.70051128038237	15.880433784319726
torch.qint8	0.4630591154098511	0.15664465427398683	0.3382821956443757
float gbps	quant gbps
20.804669812996085	15.375232631861083
torch.qint32	0.4726278781890869	4.103795266151429	8.682931023610927
float gbps	quant gbps
20.38346116380751	2.347532314650444

```

Test Plan: Imported from OSS

Differential Revision: D17222302

Pulled By: jamesr66a

fbshipit-source-id: fffc819f565dfd3b85fb6496c7c6635ec2c237a4",128.0,77.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",2.0,9,1,0.429100031,1.0,1074.0,2.0,39129.5,11222.0,31594.83333,0.0,Feature Addition,0.0,1
pytorch,1d763810ba0eb67d924f438f1bbb5c70cdaab6f8,bc7bd7a8b357308709272ae34af30053be4fc494,Adam Paszke,adam.paszke@gmail.com,Thu Jul 21 17:46:59 2016 -0400,1469123219.0,Add unit tests and fix detected bugs,2800.0,125.0,"setup.py,test/test.py,tools/cwrap/config.py,tools/cwrap/functions.py,torch/Tensor.py,torch/TensorPrinting.py,torch/csrc/Module.cpp,torch/csrc/generic/Storage.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/TensorMethods.cwrap.cpp,torch/csrc/utils.h",11.0,6,3,1.199291306,2.0,5093.0,5.0,611189.6,47.0,129.5,0.0,Corrective,1.0,1
pytorch,78c9b2948ad00b1c853838b95b51e1c4f5a035aa,bc7f3efb0987ed02599a34bd688f04e10a8308a9,Yang Chen,yangche@fb.com,Thu Feb 15 22:43:36 2024 -0800,1708037016.0,"[aot_inductor] move CppWrapperCodeGen into a separate file (#119871)

This reverts commit d8e319a961bb872027f0abdc413d6beb7502ac9b.

Differential Revision: [D53817853](https://our.internmc.facebook.com/intern/diff/D53817853)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/119871
Approved by: https://github.com/albanD, https://github.com/khabinov
ghstack dependencies: #119870",1713.0,1695.0,"tools/amd_build/build_amd.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/cpp_wrapper_cpu.py,torch/_inductor/codegen/cpp_wrapper_cuda.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/graph.py",6.0,5,2,1.028762311,6.0,8664.0,3.0,144648.5,25263.0,57012.0,0.0,,0.0,1
pytorch,6db721b5dda11638aa2eaf6aaea3af341274ef21,bcb466fb765a9143f275a2d7f5988ffa28e80fac,Soumith Chintala,soumith@gmail.com,Sat Oct 22 15:56:18 2016 -0400,1477151778.0,fix bug with numpy conversion and storageOffset > 0 (#154),70.0,3.0,"test/common.py,test/test_torch.py,torch/csrc/generic/TensorMethods.cwrap",3.0,4,2,0.660925374,10.0,6387.0,1.0,426397.0,49.0,54.65263348,0.0,Corrective,1.0,1
pytorch,e2e20e79fb753b675f45e85b72141bc4b465eec0,bcd301a457301777037c2e6cc36f44bf3e0fd4db,Erjia Guan,erjia@fb.com,Fri Oct 29 16:07:29 2021 -0700,1635523649.0,"Add OpInfor for `nn.functional.ctc_loss` (#67464)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67464

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D32001919

Pulled By: ejguan

fbshipit-source-id: f277a8e9c9887ed62e871e8a0c8549e853e34356",55.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,11928.0,1.0,3.0,16709.0,39144.0,0.0,Feature Addition,0.0,1
pytorch,ef36046ad747bc19615f5f0554be4a92ef31e8d7,bcfa023a00d49e88a9df53bc1f1815cae01aab82,Pavel Belevich,pbelevich@fb.com,Wed Jul 17 01:43:26 2019 -0700,1563327806.0,"hardshrink_cpu and hardshrink_backward_cpu refactoring with at::native::cpu_kernel

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22459

Differential Revision: D16132625

Pulled By: pbelevich

fbshipit-source-id: d7eb1cd6ed04eba3d0c54feaca1e5ab2836211b5",62.0,24.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/Activation.h,aten/src/ATen/native/cpu/Activation.cpp,test/test_torch.py",4.0,6,2,1.769044623,40.0,13044.0,4.0,2481423.0,9971.0,28892.33333,0.0,Perfective,0.0,1
pytorch,baeb359e7af363ed2654d715abe8c97fe2360c4c,bcfd3488580a3b88cda521f3ba6f9aa1176eb4af,neginraoof,neginmr@utexas.edu,Mon Mar 09 17:32:10 2020 -0700,1583775130.0,"[ONNX] Export new_zeros (#34077)

Summary:
ONNX export for new_zeros op added.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34077

Reviewed By: hl475

Differential Revision: D20332074

Pulled By: houseroad

fbshipit-source-id: 4235c4f2c279c37aa8dde6d13c1b26f621967768",17.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.997502546,2.0,5323.0,1.0,315568.0,15301.0,41003.33333,0.0,Feature Addition,0.0,1
pytorch,187955b9599c162f75d19ceb65fde02bd85253b3,bd0cc7d3649473fe1b38f4867cbfbd40149c81f4,Thomas Viehmann,tv.github@beamnet.de,Wed Apr 18 11:41:27 2018 +0200,1524051687.0,"Implement torch.einsum (fixes #1889) (#6307)

* start at generic trilinear

* Implement einsum (fixes #1889)

This provides a simple implementation of einsum. It is built on
top of the work for computing bilinear (#6110).
It uses a naive left-to-right resolution at the moment.
Autograd is able to differentiate by itself.
The obvious unsupported feature is taking diagonals (einsum('ii->i',(a,)).

* add tests and docs

* fix flake8

* clean diff

* rebase on current master to resolve conflicting String wrapping

* clean up after rebase

* better commentary in einsum and sumproduct_pair

* don't say fixme if it's fixed and rename num_outputs to num_output_dims

* adapt python wrapper to use std::string instead of String to avoid typedef at::String

* typos and some vector to array conversion

* fix accidental python<->python3 change

* really fix bad rebase",284.0,12.0,"aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/torch.rst,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/VariableType.cpp,torch/_torch_docs.py,torch/csrc/Device.cpp,torch/csrc/utils/python_arg_parser.cpp",9.0,13,5,1.605075914,39.0,16364.0,7.0,258641.5555555556,15.0,367.1166667,0.0,Corrective,1.0,1
pytorch,ad2ce1cd3de235035fa21e33b8dcbee522f15d6d,bd2b450b15668017632bf3be1f0375f91e02d902,Richard Zou,zou3519@users.noreply.github.com,Wed Mar 09 22:14:44 2022 -0500,1646864084.0,"[functorch] Fix not cleaning up DynamicLayer dispatch keys after transforms. (pytorch/functorch#584)

It turns out we've had this problem since the beginning of the time but
it wasn't causing any problems. This PR makes it so that after a
transform, like grad(f)(x), the DynamicLayer{Front, Back}Mode keys are
properly disabled.",9.0,1.0,"functorch/functorch/csrc/DynamicLayer.cpp,functorch/test/test_ops.py",2.0,4,1,0.721928095,1.0,1916.0,2.0,1.0,870.0,1219.0,0.0,Corrective,1.0,1
pytorch,0a4e5e07db5db9c86466ada4684bb170b6f4b9e9,bd34f85fe5ebf84c3ae7716f2008f15031bb41f1,Nikita Karetnikov,nikita@karetnikov.org,Fri Aug 04 14:35:21 2023 +0200,1691159721.0,"[pt2] meta for `searchsorted.Scalar`, tests, and out support (#106283)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106283
Approved by: https://github.com/ezyang",53.0,16.0,"aten/src/ATen/native/Bucketization.cpp,aten/src/ATen/native/cuda/Bucketization.cu,aten/src/ATen/native/native_functions.yaml,test/functorch/test_vmap.py,torch/_meta_registrations.py,torch/testing/_internal/common_methods_invocations.py",6.0,10,3,2.126199166,17.0,46790.0,6.0,3022788.1666666665,18387.0,41657.0,0.0,,0.0,1
pytorch,16d2c3d7b3b4d694ffce2b6ef4ebc4b69eb0447d,bd5303010dbe3b682d65e198f6ff5dfc3cb6c54a,Sam Gross,colesbury@gmail.com,Tue Feb 14 00:00:16 2017 -0800,1487030416.0,"Refactor autograd package to separate Python dependencies. (#662)

The core autograd Variable, Function, and Engine no longer depend on the
Python API. This let's us implement functions in C++. In the future, we
can also multithread engine and release the GIL for most of the
non-Python backwards.",2970.0,1767.0,"setup.py,test/test_autograd.py,test/test_multiprocessing.py,torch/autograd/function.py,torch/autograd/variable.py,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Exceptions.h,torch/csrc/Module.cpp,torch/csrc/ModuleSparse.cpp,torch/csrc/Tensor.cpp,torch/csrc/autograd/autograd.h,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/engine.h,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/functions/batch_normalization.cpp,torch/csrc/autograd/functions/batch_normalization.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/grad_buffer.cpp,torch/csrc/autograd/grad_buffer.h,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_cpp_function.h,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_engine.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/autograd/saved_variable.h,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Tensor.cpp,torch/csrc/cudnn/BatchNorm.cpp,torch/csrc/cudnn/Types.cpp,torch/csrc/cudnn/Types.h,torch/csrc/generic/SparseTensor.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/Tensor.h,torch/csrc/utils.cpp,torch/multiprocessing/reductions.py,torch/nn/modules/module.py,torch/nn/parameter.py",44.0,12,2,3.809679727,24.0,8755.0,2.0,352403.25,172.0,753.2774931,0.0,Perfective,0.0,1
pytorch,8399197df67f90d7e5cb52d735eb55c803664b35,bd88fd079310204e409e4995c351842a9bf3ec1c,Iurii Zdebskyi,iuriiz@fb.com,Mon Jul 15 16:25:19 2019 -0700,1563207919.0,"Added .bfloat16() (#22852)

Summary:
Add conversion method for bfloat16
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22852

Differential Revision: D16256760

Pulled By: izdeby

fbshipit-source-id: 01d75495f9df513a0cdf78791c3eb013ab92bd95",15.0,2.0,"docs/source/tensors.rst,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/_tensor_docs.py",4.0,7,4,1.713078843,41.0,16932.0,2.0,359664.5,9930.0,28789.33333,0.0,Feature Addition,0.0,1
pytorch,eeb0d67b92e76ab9f4aca43503fe34cc3e1621a6,bdaa0e38b8a6b6de1e7e8a964e1cc07f4850b154,Lin Huang,lhuang04@fb.com,Thu Nov 29 21:54:19 2018 -0800,1543528459.0,"Fix tautological-compare in aten/src/ATen/native/cuda/SummaryOps.cu (#14540)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14540

refactor the HANDLE_SWITCH_CASE to avoid tautological-compare in macro

Reviewed By: ezyang

Differential Revision: D13255725

fbshipit-source-id: cfa64bb7bc53d19c93a693015202f207567690b4",12.0,12.0,aten/src/ATen/native/cuda/SummaryOps.cu,1.0,5,1,0,3.0,294.0,1.0,5717837.0,5698.0,17273.83333,0.0,Corrective,1.0,1
pytorch,cf975dde0d22a01442b4b99e6006c255650fa311,bdacc0856c25214868a1aa361262b5419b8d79bb,Gary Miguel,garymiguel@microsoft.com,Fri May 13 21:41:31 2022 +0000,1652478091.0,"[ONNX] handle equality checks on devices (#77203)

Previously the newly added `test_device_eq` would fail since the inputs
to `Equal` were invalid. Handle this by replacing its inputs with a
fixed tensor `Constant`. This is OK since ONNX doesn't have the concept
of different devices.

Discovered during investigation of
https://github.com/microsoft/onnx-converters-private/issues/9
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77203
Approved by: https://github.com/BowenBao",43.0,16.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.989822056,4.0,17864.0,2.0,41590.0,3215.0,7730.0,0.0,Corrective,1.0,1
pytorch,e8102b0a9bf02f3c5e1c211c4ccfb90c14ed8570,bdfef2975c5accb86d60134e197575d734e45c47,Soumith Chintala,soumith@fb.com,Wed Jan 11 03:58:26 2017 -0800,1484107106.0,adding more docs for torch.* functions,454.0,33.0,"torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorCompare.cwrap,torch/docs.py",3.0,4,1,0.107123807,22.0,4722.0,3.0,588888.6666666666,316.0,6402.724559,0.0,Feature Addition,0.0,1
pytorch,0d30f7788978374cb535648bbc9fc5ff31a5317d,bef70aa377fe6996567d97f49e71c0f95f17d379,Adam Paszke,adam.paszke@gmail.com,Tue Dec 27 16:04:53 2016 +0100,1482854693.0,Make type checking more strict and fix topk arguments,11.0,6.0,"test/test_torch.py,tools/cwrap/plugins/THPPlugin.py,torch/csrc/generic/methods/TensorCompare.cwrap,torch/csrc/utils.h",4.0,8,3,1.903967813,19.0,3888.0,1.0,6100.0,345.0,2991.196975,0.0,Corrective,1.0,1
pytorch,92f376147c174bd8c5f700648d4d645540deb4df,befab0d9d48d154b8ec5eab0cc3092c6c8eee5a2,David,jiafa@microsoft.com,Thu Dec 03 17:33:09 2020 -0800,1607016789.0,"[ONNX] Cast Gather index to Long if needed (#47653)

Summary:
Onnx op Gather index need be int32 or int64. However, we don't have this Cast in our converter.
Therefore, it fails the following UT (for opset 11+)
`seq_length.type().scalarType()` is None, so `_arange_cast_helper()` cannot treat it as all integral, then it will cast all to float. Then this float value will be used as Gather index, hence it throws error in ORT about float type index.
The fix is that we need cast Gather index type to Long if it is not int/long.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47653

Reviewed By: heitorschueroff

Differential Revision: D25298056

Pulled By: mruberry

fbshipit-source-id: 05e3a70ccfd74612233c63ec5bb78e060b211909",19.0,12.0,"torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",2.0,2,1,0.99323382,3.0,3368.0,1.0,112168.0,7172.0,16228.5,0.0,Corrective,1.0,1
pytorch,d26c8f26d1b2d524c810c4ac272da998b71e72fd,bf059e3925a2565cb05a407f541cc11d762eef05,Nikita Shulga,nshulga@meta.com,Wed May 24 03:09:55 2023 -0700,1684897795.0,"[Typing] Export `torch.backends` as subpackage (#102099)

So that `pyright` is happy.

Do a little refactor in `mps/__init__.py` to avoid cyclical dependency on `torch.fx` by calling `mps._init()` implicitly.

Fixes https://github.com/pytorch/pytorch/issues/101686
Pull Request resolved: https://github.com/pytorch/pytorch/pull/102099
Approved by: https://github.com/Skylion007",20.0,11.0,"torch/__init__.py,torch/backends/__init__.py,torch/backends/mps/__init__.py",3.0,3,1,1.583476643,42.0,1787.0,3.0,5717273.666666667,16150.0,36569.5,0.0,Corrective,1.0,1
pytorch,"5b3ccec10d3ba1fa46a58dd6735c24a5e501d240,bb1019d1ec1503718b97d17366902f96f349f472",bf0e185bd689c8358abad690fa64f592f3ee959c,Sam Gross,sgross@fb.com,Thu Dec 01 21:47:20 2016 -0800,1480628840.0,Merge commit 'bb1019d1ec1503718b97d17366902f96f349f472',94.0,10.0,"torch/lib/THCUNN/CMakeLists.txt,torch/lib/THCUNN/generic/BatchNormalization.cu,torch/lib/THCUNN/generic/LookupTable.cu,torch/lib/THCUNN/generic/SpatialConvolutionLocal.cu,torch/lib/THCUNN/generic/SpatialConvolutionMM.cu,torch/lib/THCUNN/generic/SpatialDilatedConvolution.cu,torch/lib/THCUNN/generic/SpatialFullConvolution.cu,torch/lib/THCUNN/generic/TemporalMaxPooling.cu,torch/lib/THCUNN/generic/VolumetricDilatedConvolution.cu,torch/lib/THCUNN/generic/VolumetricFullConvolution.cu",10.0,4,1,2.877053544,15.0,3050.0,1.0,315899.0,78.0,5214.51526,0.0,,0.0,1
pytorch,e425bdb832ae547f6486da84c51badeb2673e52a,bf1d957dc8f64fb6dd006ce523cf844c722afa7f,Ethan Steinberg,ethan.steinberg@gmail.com,Thu Sep 26 21:09:01 2019 -0700,1569532141.0,"Fix the Bernoulli distribution sampler (#26864)

Summary:
The current Bernoulli distribution sampler is slightly off in that it returns true slightly too often. This is most obvious at very low p values, like p = 0, although it theoretically occurs at every probability. See  https://github.com/pytorch/pytorch/issues/26807.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26864

Differential Revision: D17610459

Pulled By: ezyang

fbshipit-source-id: 28215ff820a6046822513f284793e7b850d38438",11.0,1.0,"aten/src/ATen/core/DistributionsHelper.h,test/test_torch.py",2.0,5,2,0.650022422,40.0,13252.0,2.0,4619528.0,11794.0,33073.33333,0.0,Corrective,1.0,1
pytorch,3cb34744db55fd4c6381704e5c8219bdec540dd5,bf4c269bee8a7c0687122a2f59a35153432bd9e8,Richard Zou,zou3519@users.noreply.github.com,Wed Nov 01 10:29:34 2017 -0400,1509532174.0,"Implement reduce keyword for SmoothL1Loss (#3382)

* Implement reduce keyword for SmoothL1Loss",208.0,38.0,"test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/legacy/nn/SmoothL1Criterion.py,torch/lib/ATen/nn.yaml,torch/lib/THCUNN/SmoothL1Criterion.cu,torch/lib/THCUNN/generic/SmoothL1Criterion.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THNN/generic/SmoothL1Criterion.c,torch/lib/THNN/generic/THNN.h,torch/nn/modules/loss.py",12.0,15,3,3.154807334,39.0,11013.0,5.0,341002.5,2046.0,23919.35823,0.0,,0.0,1
pytorch,6575e674ceaa94473f92d3f3a8b28d08969fdab3,bf6a156f6412c203af8fba8a3945171431600b55,Heitor Schueroff,heitorschueroff@fb.com,Wed Nov 11 01:19:51 2020 -0800,1605057591.0,"Fix kthvalue error for scalar input (#47600)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/47600

fixes https://github.com/pytorch/pytorch/issues/30818

Note that the median case was already fixed by https://github.com/pytorch/pytorch/pull/45847

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D24860337

Pulled By: heitorschueroff

fbshipit-source-id: 69ccbbb6c7c86671e5712b1c2056c012d898b4f2",10.0,7.0,"aten/src/ATen/native/cuda/Sorting.cu,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.332820405,43.0,23737.0,3.0,378578.3333333333,6635.0,15068.5,0.0,Corrective,1.0,1
pytorch,8430cf6e865ffa823205e3e61d7bf4c8e502ae68,bfbd1bbb5094ddc148af7874dbeb7eb1e248594e,Allen Ye,allenye0119@gmail.com,Tue Sep 05 04:05:44 2017 +0800,1504584344.0,Update torch.triu/torch.tril doc (#2619),16.0,16.0,torch/_torch_docs.py,1.0,1,1,0,24.0,4901.0,1.0,46305.0,1436.0,22098.09738,0.0,Non Functional,0.0,1
pytorch,cfc87cad02e47a3a62b4328454e825748c7be4fd,c003494754510d5e0e0c2a34ff816f570a5f0910,mingfeima,mingfei.ma@intel.com,Fri May 13 00:46:37 2022 +0800,1652402797.0,"add channels last support for PixelShuffle and PixelUnshuffle

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50573

Approved by: https://github.com/VitalyFedyunin",380.0,25.0,"aten/src/ATen/native/PixelShuffle.cpp,aten/src/ATen/native/cpu/PixelShuffleKernel.cpp,aten/src/ATen/native/cpu/PixelShuffleKernel.h,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/build_variables.bzl",8.0,8,3,1.57741315,48.0,39569.0,4.0,5431309.666666667,3317.0,7952.0,0.0,Feature Addition,0.0,1
pytorch,a2a6171859c1e3f0ef7cfc6b9f95b1589749d3ca,c01d3a486f488836edf200d40e7964f00934acd4,Richard Zou,zou3519@users.noreply.github.com,Mon Jun 20 17:06:55 2022 -0400,1655744815.0,"[functorch] Cleanup jvp testing (pytorch/functorch#890)

- deleted test_vmapjvp (test_vmapjvpall is a superset of test_vmapjvp)
- added additional_op_db to things tested by test_vmapjvpall
- did some accounting in discover_coverage",12.0,93.0,"functorch/test/discover_coverage.py,functorch/test/functorch_additional_op_db.py,functorch/test/test_ops.py",3.0,2,1,0.586997419,1.0,2892.0,3.0,0.3333333333333333,1123.0,1516.5,0.0,Feature Addition,0.0,1
pytorch,e3dcd175f7aacb6aa2fb150b01335afd508c9d52,c031643e392d2d5dda14c5876761265ac6bcf32a,Mike Ruberry,mruberry@fb.com,Sat May 07 03:42:24 2022 +0000,1651894944.0,"Adds decorators for Python References and extends Python Reference testing (#76945)

This PR does the following...

Tests:
- fixes test_type_promotion in test_binary_ufuncs to correctly generate scalar cpu tensors
- fixes test_python_reference_consistency to use the Python Reference's reference inputs
- extends Python reference testing to test_conj_view, test_neg_view, and test_neg_conj_view
- adds a NaN propagation sample input for elementwise unary and binary operations
- fixes the UnaryUfuncInfo class to properly register its reference inputs
- Updates the Python Reference OpInfos to skip error inputs when their behavior on scalar inputs is inconsistent with their reference operators

Code organization:
- moves elementwise type promotion functionality to prims.utils

Prims & Refs:
- fixes scalar cpu tensor handling by having them pass through broadcasting and device and shape checks
- adds two decorators, `elementwise_type_promotion_wrapper` and `out_wrapper`, the former allows for elementwise type promotion to be automated and the latter automatically adds the out kwarg and handles it properly

cc @ezyang who also had some thoughts on cpu scalar tensor handling
cc @chillee -- might want to use this new decorator as we converge decompositions and references
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76945
Approved by: https://github.com/ngimel",715.0,632.0,"test/test_binary_ufuncs.py,test/test_ops.py,torch/_decomp/decompositions.py,torch/_prims/__init__.py,torch/_prims/utils.py,torch/_prims/wrappers.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",8.0,7,2,1.90689061,5.0,29030.0,6.0,106420.14285714286,2956.0,7137.0,0.0,Corrective,1.0,1
pytorch,c9ac6d6246020fdaaf6d817dd26ddb3315bf61d4,c048d197720943d7a5eb127ab13c7b3c6e26ac57,Samantha Andow,samdow@fb.com,Wed May 18 17:34:15 2022 -0400,1652895255.0,[functorch] fix CI (pytorch/functorch#816),8.0,109.0,"functorch/test/test_eager_transforms.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py",3.0,2,1,0.195582996,1.0,5075.0,3.0,0.3333333333333333,1074.0,1463.0,0.0,Corrective,1.0,1
pytorch,9acce7524652af05429b87cc38f7b7aa56d9a089,c07e0a6166e75b567b41c508a1ab08276ec09fd2,Francisco Massa,fvsmassa@gmail.com,Sat Feb 27 18:20:05 2016 +0100,1456597205.0,Add missing declarations to THNN.h,63.0,2.0,generic/THNN.h,1.0,1,1,0,10.0,945.0,1.0,10687.0,7.0,17.83333333,0.0,Feature Addition,0.0,1
pytorch,5a406c023e8f23166d52421799dcdb9ab3ef3bfc,c0966914bc8be75c331696c62fc76d6902f0c734,Jeffrey Wan,jw3468@fb.com,Fri Jan 29 17:04:55 2021 -0800,1611939895.0,"Internal gradcheck wrapper in testing._internal that sets certain flags to True (#51133)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/49409

There are many call sites where, gradcheck/gradgradcheck is now being implicitly invoked with `check_batched_grad` as True, but they were previously False. Cases fall into two basic categories:
1) the call site was previously using `torch.autograd.gradcheck` but is now changed to use the globally imported function instead
3) the call site was already using globally imported function, but does not explicitly pass `check_batched_grad` flag

Only in the _assertGradAndGradgradChecks cases, which are infrequent, I assumed that the the author is aware that omitting the flag means not applying check_batched_grad=True. (but maybe that is not the case?)

Overall this PR in its current state assumes that unless the author explicitly specified `check_batched_grad=False`, they were just probably not aware of this flag and did not mean to have this flag as False.

So far exceptions to the above (as discovered by CI) include:
 - Mkldnn (opaque tensors do not have strides) https://app.circleci.com/pipelines/github/pytorch/pytorch/264416/workflows/e4d87886-6247-4305-8526-2696130aa9a4/jobs/10401882/tests
 - all cases in test_sparse (https://app.circleci.com/pipelines/github/pytorch/pytorch/264553/workflows/3c1cbe30-830d-4acd-b240-38d833dccd9b/jobs/10407103)
 - all cases in test_overrides (https://app.circleci.com/pipelines/github/pytorch/pytorch/264553/workflows/3c1cbe30-830d-4acd-b240-38d833dccd9b/jobs/10407236)
 - test_autograd (test_LSTM_grad_and_gradgrad) - (https://app.circleci.com/pipelines/github/pytorch/pytorch/264553/workflows/3c1cbe30-830d-4acd-b240-38d833dccd9b/jobs/10407235)
 - test_data_parallel (test_data_parallel_buffers_requiring_grad) - *SIGSEGV* (https://app.circleci.com/pipelines/github/pytorch/pytorch/264820/workflows/14d89503-040d-4e3d-9f7b-0bc04833589b/jobs/10422697)
 - test_nn (https://app.circleci.com/pipelines/github/pytorch/pytorch/264919/workflows/df79e3ed-8a31-4a8e-b584-858ee99686ff/jobs/10427315)

Possible TODO is to prevent new tests from invoking external gradcheck.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51133

Reviewed By: ezyang

Differential Revision: D26147919

Pulled By: soulitzer

fbshipit-source-id: dff883b50f337510a89f391ea2fd87de2d531432",77.0,57.0,"test/distributed/test_data_parallel.py,test/distributions/test_distributions.py,test/test_autograd.py,test/test_cpp_extensions_jit.py,test/test_linalg.py,test/test_mkldnn.py,test/test_nn.py,test/test_overrides.py,test/test_sparse.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_nn.py,torch/testing/_internal/common_utils.py",12.0,6,2,3.127222928,44.0,49549.0,11.0,966344.6666666666,8476.0,19120.0,0.0,Corrective,1.0,1
pytorch,ec4a0f332e114ff84cfd2d3f5b5a8015b6f00da8,c0a419e6baec7b2b10dbe4e76612ea5008cc224b,Tongzhou Wang,SsnL@users.noreply.github.com,Mon Jun 04 22:46:52 2018 -0400,1528152412.0,"Add non_blocking to Tensor/Module.to (#7312)

* Add non_blocking to Tensor/Module.to

* flake8

* Add argparse tests

* cpp parse

* Use C++ parser

* use a commong parse function with Tensor.to

* fix test_jit

* use THPObjectPtr

* increase refcount for None, True, and False

* address comments

* address comments",178.0,112.0,"test/test_jit.py,test/test_nn.py,test/test_torch.py,tools/autograd/templates/python_nn_functions.cpp,tools/autograd/templates/python_variable_methods.cpp,torch/_tensor_docs.py,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/utils/python_arg_parsing.h,torch/csrc/tensor/python_tensor.cpp,torch/csrc/tensor/python_tensor.h,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_conversion_dispatch.cpp,torch/csrc/utils/tensor_conversion_dispatch.h,torch/nn/modules/module.py",14.0,12,3,3.194839931,43.0,23597.0,11.0,2112331.6153846155,1244.0,3457.305292,0.0,Corrective,1.0,1
pytorch,69631c3ee320ae48ae2bec336264c01fe60481a6,c0aa6a01cea3c7f8076010c52fe391c05f59e350,James Reed,jamesreed@fb.com,Mon Sep 23 20:17:39 2019 -0700,1569269859.0,"NHWC specialization for quantized::cat (#26524)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26524

This creates an NHWC specialization for `quantized::cat` that kicks in when all inputs are `NHWC`. This ensures the correct layout is propagated downstream as well as is an optimized implementation specifically for this data layout

Benchmark script based on Squeezenet shapes:
```
import torch, time

torch.manual_seed(0)

# NHWC
sizes = [
    (1, 54, 54, 64),
    (1, 54, 54, 128),
    (1, 26, 26, 128),
    (1, 26, 26, 256),
    (1, 12, 12, 256)
]

for size in sizes:
    x = torch.rand(*size)
    y = torch.rand(*size)
    qX = torch.quantize_linear(x, 0.01, 3, torch.qint8).permute([0, 3, 1, 2])
    qY = torch.quantize_linear(y, 0.01, 3, torch.qint8).permute([0, 3, 1, 2])

    ref = torch.cat([qX.dequantize(), qY.dequantize()], dim=1)

    NITER = 1000
    s = time.time()
    for i in range(NITER):
        out = torch.ops.quantized.cat([qX, qY], dim=1, scale=0.01, zero_point=3)
    time_per_iter = (time.time() - s) / NITER

    print('time per iter ms', time_per_iter * 1000)
    print('gb/s', (qX.numel() + qY.numel() + out.numel()) * qX.element_size() / time_per_iter / 1e9)

    torch.testing.assert_allclose(out.dequantize(), ref)
```

Before this change

```
time per iter ms 0.6898486614227295
gb/s 1.0821156026605054
time per iter ms 1.5480577945709229
gb/s 0.9644291093239284
time per iter ms 0.3180875778198242
gb/s 1.0881028500775023
time per iter ms 0.6702737808227539
gb/s 1.032748139350315
time per iter ms 0.13010454177856445
gb/s 1.1333655073392244
```
After this change
```
time per iter ms 0.11604785919189453
gb/s 6.432656364350577
time per iter ms 0.15956878662109375
gb/s 9.356416324360508
time per iter ms 0.040181636810302734
gb/s 8.613685939027139
time per iter ms 0.06564664840698242
gb/s 10.544696748392909
time per iter ms 0.018549680709838867
gb/s 7.949247337814738
```

Test Plan: Imported from OSS

Differential Revision: D17503593

Pulled By: jamesr66a

fbshipit-source-id: ec5d57ad8fbcb3fd9379e8bd370abd29d386f953",204.0,8.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qconcat.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py",5.0,10,2,1.577627146,1.0,3385.0,3.0,188232.6,11640.0,32774.83333,0.0,Corrective,0.0,1
pytorch,9b465313cf366f603e9fd68bc8561d8ba9c0c5a5,c0dfe2370340a215d3bdf441b42c1a2d275df6d1,gchanan,gregchanan@gmail.com,Thu Jun 21 13:12:16 2018 -0400,1529586736.0,"Support n-dimensional empty tensors in (most of) THCUNN. (#8722)

* Support n-dimensional empty tensors in (most of) THCUNN.

* Fix incorrect parens.",305.0,306.0,"aten/src/THCUNN/common.h,aten/src/THCUNN/generic/ClassNLLCriterion.cu,aten/src/THCUNN/generic/Col2Im.cu,aten/src/THCUNN/generic/Im2Col.cu,aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu,aten/src/THCUNN/generic/MultiMarginCriterion.cu,aten/src/THCUNN/generic/SparseLinear.cu,aten/src/THCUNN/generic/SpatialAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/SpatialAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/SpatialAveragePooling.cu,aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu,aten/src/THCUNN/generic/SpatialConvolutionLocal.cu,aten/src/THCUNN/generic/SpatialConvolutionMM.cu,aten/src/THCUNN/generic/SpatialCrossMapLRN.cu,aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu,aten/src/THCUNN/generic/SpatialFractionalMaxPooling.cu,aten/src/THCUNN/generic/SpatialFullDilatedConvolution.cu,aten/src/THCUNN/generic/SpatialGridSamplerBilinear.cu,aten/src/THCUNN/generic/SpatialMaxUnpooling.cu,aten/src/THCUNN/generic/SpatialReflectionPadding.cu,aten/src/THCUNN/generic/SpatialReplicationPadding.cu,aten/src/THCUNN/generic/SpatialSubSampling.cu,aten/src/THCUNN/generic/SpatialUpSamplingBilinear.cu,aten/src/THCUNN/generic/SpatialUpSamplingNearest.cu,aten/src/THCUNN/generic/TemporalConvolution.cu,aten/src/THCUNN/generic/TemporalMaxPooling.cu,aten/src/THCUNN/generic/TemporalReflectionPadding.cu,aten/src/THCUNN/generic/TemporalReplicationPadding.cu,aten/src/THCUNN/generic/TemporalRowConvolution.cu,aten/src/THCUNN/generic/TemporalUpSamplingLinear.cu,aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricAdaptiveAveragePooling.cu,aten/src/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu,aten/src/THCUNN/generic/VolumetricAveragePooling.cu,aten/src/THCUNN/generic/VolumetricConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFractionalMaxPooling.cu,aten/src/THCUNN/generic/VolumetricFullDilatedConvolution.cu,aten/src/THCUNN/generic/VolumetricGridSamplerBilinear.cu,aten/src/THCUNN/generic/VolumetricMaxUnpooling.cu,aten/src/THCUNN/generic/VolumetricReplicationPadding.cu,aten/src/THCUNN/generic/VolumetricUpSamplingNearest.cu,aten/src/THCUNN/generic/VolumetricUpSamplingTrilinear.cu",46.0,4,1,5.337545636,6.0,11068.0,2.0,514577.54347826086,725.0,3868.5,0.0,Corrective,1.0,1
pytorch,b726a1bbf8a0295bc1abff0bf0a348d0dbf27c91,c134f3283515ce135c71049adfe923e3fb5581fa,Heitor Schueroff,heitorschueroff@fb.com,Thu Dec 03 19:36:04 2020 -0800,1607024164.0,"Implemented torch.inner (#46716)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/46716

Implemented torch.inner similar to [numpy.inner](https://numpy.org/doc/stable/reference/generated/numpy.inner.html). For now it's implemented as a composite op.

TODO

- [x] Add documentation

Test Plan: Imported from OSS

Reviewed By: malfet

Differential Revision: D24860351

Pulled By: heitorschueroff

fbshipit-source-id: de5c82f285893495491fdba73b35634f4d00bac8",158.0,0.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_linalg.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",10.0,11,4,2.163673765,36.0,37744.0,3.0,10553.0,7179.0,16254.5,0.0,Feature Addition,0.0,1
pytorch,1bde5a216f2b4745bca353c578b9d30993d71a73,c147aa306c6386a753fdff24b48d04e803070a63,Sam Estep,sestep@fb.com,Wed Jan 20 23:50:50 2021 -0800,1611186650.0,"Use doctest directly to get docstring examples (#50596)

Summary:
This PR addresses [a two-year-old TODO in `test/test_type_hints.py`](https://github.com/pytorch/pytorch/blame/12942ea52b6b36f34bf331e58c8d695b3be64bed/test/test_type_hints.py#L21-L22) by replacing most of the body of our custom `get_examples_from_docstring` function with [a function from Python's built-in `doctest.DocTestParser` class](https://docs.python.org/3/library/doctest.html#doctest.DocTestParser.get_examples). This mostly made the parser more strict, catching a few errors in existing doctests:

- missing `...` in multiline statements
- missing space after `>>>`
- unmatched closing parenthesis

Also, as shown by [the resulting diff of the untracked `test/generated_type_hints_smoketest.py` file](https://pastebin.com/vC5Wz6M0) (also linked from the test plan below), this introduces a few incidental changes as well:

- standalone comments are no longer preserved
- indentation is now visually correct
- [`example_torch_promote_types`](https://github.com/pytorch/pytorch/blob/4da9ceb74388bb3df26a3581f281caf1cc554890/torch/_torch_docs.py#L6753-L6772) is now present
- an example called `example_torch_tensor___array_priority__` is added, although I can't tell where it comes from
- the last nine lines of code from [`example_torch_tensor_align_as`](https://github.com/pytorch/pytorch/blob/5d45140d6874be04c22c8abba55e4438c25d2fdb/torch/_tensor_docs.py#L386-L431) are now present
- the previously-misformatted third line from [`example_torch_tensor_stride`](https://github.com/pytorch/pytorch/blob/5d45140d6874be04c22c8abba55e4438c25d2fdb/torch/_tensor_docs.py#L3508-L3532) is now present

Pull Request resolved: https://github.com/pytorch/pytorch/pull/50596

Test Plan:
Checkout the base commit, typecheck the doctests, and save the generated file:
```
$ python test/test_type_hints.py TestTypeHints.test_doc_examples
$ cp test/generated_type_hints_smoketest.py /tmp
```
Then checkout this PR, do the same thing, and compare:
```
$ python test/test_type_hints.py TestTypeHints.test_doc_examples
$ git diff --no-index {/tmp,test}/generated_type_hints_smoketest.py
```
The test should succeed, and the diff should match [this paste](https://pastebin.com/vC5Wz6M0).

Reviewed By: walterddr

Differential Revision: D25926245

Pulled By: samestep

fbshipit-source-id: 23bc379ff438420e556263c19582dba06d8e42ec",43.0,73.0,"test/test_type_hints.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/functional.py,torch/serialization.py",5.0,2,2,1.652518024,41.0,17886.0,5.0,448106.8,8230.0,18597.0,0.0,Corrective,0.0,1
pytorch,789e0e066a531084e76303486a1be5849c52b7e8,c14b62fca2f348d0ab3b901698a841f2c72c3ac6,Peter Goldsborough,peter@goldsborough.me,Wed Apr 18 22:07:03 2018 +0100,1524089223.0,"Create FileBaton to synchronize distributed JIT C++ extension builds (#6684)

* Create FileBaton to synchronize distributed JIT C++ extension builds

* Move FileBaton to its own file

* Autoformat code

* Respect verbose flag in cpp_extension._prepare_ldflags",140.0,56.0,"torch/utils/cpp_extension.py,torch/utils/file_baton.py",2.0,2,1,0.794700734,7.0,699.0,1.0,9371.0,2576.0,24931.85823,0.0,,0.0,1
pytorch,12c1465d763ede0fb9b1b56d11f32493cbbeb306,c14c4efc0eb97ccc471ac6cbdcc659417227ae97,Oguz Ulgen,oulgen@meta.com,Sat Oct 28 02:33:46 2023 -0700,1698460426.0,"[Inductor] Add triton.autotune support for user defined triton kernels with constant/simple grids (#112228)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/112228
Approved by: https://github.com/jansel",141.0,24.0,"test/dynamo/test_functions.py,torch/_dynamo/variables/builder.py,torch/_dynamo/variables/functions.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py,torch/_inductor/triton_heuristics.py",6.0,7,2,2.230259856,2.0,15369.0,3.0,50437.66666666666,21272.0,48508.0,0.0,Feature Addition,0.0,1
pytorch,c56a7cfc37aae8b5e1515da37b314ed03bff8641,c1dce21fd5505085ef148aa9d70a312d8d14257f,Thomas Viehmann,tv.code@beamnet.de,Tue Sep 11 20:06:47 2018 -0700,1536696407.0,"Cuda TensorAccessor (#11373)

Summary:
Provide a TensorAccessor-Like interface for CUDA as discussed in #8366.

Compared to TensorAccessor
- the CUDATensorAccessor copies the sizes and strides while on the host (I didn't implement a host indexing function, though) to enable transfer to the device, on the device, `[]` works like for TensorAccessors,
- instantiation is from TensorAccessors in order to allow using `.accessor<..>`. The drawback is that it you cannot use `auto` for the variable declaration, but the alternative would be a cuda-specific `.accessor`-like function,
- there is a PtrTraits argument to enable `__restrict__`,

Example for the intended use:
```
...
template <typename scalar_t>
__global__ void
apply_homography_2d_kernel(cuda::CUDATensorAccessor<scalar_t, 4> dest_a,
			   cuda::CUDATensorAccessor<scalar_t, 4> src_a,
			   cuda::CUDATensorAccessor<float, 2> transform) {
...
}

template <typename scalar_t>
Tensor apply_homography_2d_template(Tensor& res, const Tensor& image, const Tensor& transform) {
  ...
  cuda::CUDATensorAccessor<scalar_t, 4> image_a(image.accessor<scalar_t, 4>());
  cuda::CUDATensorAccessor<scalar_t, 4> res_a(res.accessor<scalar_t, 4>());
  cuda::CUDATensorAccessor<float, 2> transform_a(transform.accessor<float, 2>());
  auto stream = at::cuda::getCurrentCUDAStream();

  apply_homography_2d_kernel<scalar_t>
    <<<grid, block, 0, stream>>>(res_a, image_a, transform_a);
  return res;
}

...
```

I could use a hint where to put a test for this (e.g. doing a plain vanilla matrix multiplication with a custom kernel) and comparing with the aten mm.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11373

Differential Revision: D9735573

Pulled By: ezyang

fbshipit-source-id: 482b218a0d514e19a8b692bbc77c0e37082cfded",179.0,17.0,"aten/src/ATen/core/TensorAccessor.h,aten/src/ATen/templates/Tensor.h,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/cuda_packedtensoraccessor_test.cu",4.0,6,1,1.266663157,7.0,382.0,3.0,714412.0,4012.0,11220.33333,0.0,,0.0,1
pytorch,ca0ef5238208e446125eae945a6c4a07e73ce725,c1f0e6e763b6720789e54cb83477bb353139a12f,ganler,jaway.liu@gmail.com,Mon Apr 11 23:26:44 2022 +0000,1649719604.0,"[ONNX] Make Non-Float Op Exportation Compatible to Avoid Invalid ONNX Models

There are a few ONNX operators do not support non-float (e.g., integer) inputs at early versions. For example, Clip supports non-float types until [opset 12](https://github.com/onnx/onnx/blob/main/docs/Changelog.md#type-constraints-280), that said older versions like [opset 6](https://github.com/onnx/onnx/blob/main/docs/Changelog.md#type-constraints-107) cannot deal with integer types.

I initially find such a bug in Clip (https://github.com/pytorch/pytorch/pull/70584), but later found more:
1. Clip < 12;
2. Min/Max < 12;
3. ReLU < 14;
4. Pad < 11;

In PyTorch, if we export Max-11 with integer inputs, actually the exportation will succeed; however, fail when imported by other frameworks like ONNXRuntime.

```python
import torch

class Net(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()

    def forward(self, x: torch.Tensor):
        return torch.max(x, x + 1)

net = Net()
onnx_model = 'test.onnx'

torch.onnx.export(net, (torch.zeros((3, 3), dtype=torch.int32),),
                  onnx_model, verbose=True, opset_version=11)
```

This is an unexpected behavior as we want to ensure that every model exported by PyTorch is valid (https://github.com/pytorch/pytorch/pull/70584#issuecomment-1020636579). Theoretically, we can simply forbid such cases (e.g., `Clip<int>` < 12, `ReLU<int>` < 14). But actually we can enhance the compatibility and flexibility of PyTorch by simply casting inputs of those operators into float tensors, which allows the float operator functions, and then casting it back to original types.

This PR implements the second approach to achieve better compatibility in PyTorch.

@garymm  @thiagocrepaldi

Pull Request resolved: https://github.com/pytorch/pytorch/pull/72401
Approved by: https://github.com/garymm, https://github.com/thiagocrepaldi",148.0,45.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.30790743,4.0,15993.0,3.0,1456312.5,2182.0,5216.0,0.0,Corrective,1.0,1
pytorch,8f7cf796eaafc8e1b77c2d7815291f2721ccb377,c22f51ce7c77ef23d9add8fc7c525145efd52230,Jiong Gong,jiong.gong@intel.com,Thu Aug 15 03:13:46 2024 -0700,1723691626.0,"[inductor][cpp][gemm] improve large bs perf with better cache blocking (#132729)

Improve the cache blocking by reducing Mc_blocks to make A reside in L2 and reused by B as much as possible. This improves large bs perf for both scenarios: 1) N is large and K is of medium sizes; 2) K is large. Different strategies are used to handle these scenarios. Check the notes in `get_cache_blocking` in the changes.

Measured with 56-core Intel (R) Xeon (R) CPU Max 9480, jemalloc 5.1 and intel omp, bf16. Run with code cache of B matrix (weights).

Model Shapes | Before Optimization | After Optimization | Speedup | onednn linear | Speedup over onednn
-- | -- | -- | -- | -- | --
M=1024, N=12288, K=4096 (Llama2-8b) | 5.69 ms | 3.71 ms | 1.53 | 4.53 ms | 1.22
M=1024, N=4096, K=4096 (Llama2-8b) | 1.69 ms | 1.63 ms | 1.04 | 2.05 ms | 1.26
M=1024, N=22016, K=4096 (Llama2-8b) | 10.32 ms | 6.57 ms | 1.57 | 8.46 ms | 1.29
M=1024, N=4096, K=11008 (Llama2-8b) | 5.21 ms | 3.26 ms | 1.60 | 4.65 ms | 1.43
M=1024, N=5120, K=4096 (Llama3-8b) | 1.99 ms | 1.78 ms | 1.12 | 2.31 ms | 1.30
M=1024, N=28672, K=4096 (Llama3-8b) | 13.41 ms | 8.56 ms | 1.57 | 10.96 ms | 1.28
M=1024, N=4096, K=14336 (Llama3-8b) | 6.93 ms | 4.31 ms | 1.61 | 6.24 ms | 1.45

Pull Request resolved: https://github.com/pytorch/pytorch/pull/132729
Approved by: https://github.com/leslie-fang-intel, https://github.com/chunyuan-w, https://github.com/jansel",117.0,38.0,"test/inductor/test_cpu_select_algorithm.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_template_kernel.py,torch/_inductor/config.py",4.0,5,2,1.063709937,1.0,3221.0,4.0,433356.75,32793.0,82984.0,0.0,Perfective,0.0,1
pytorch,e06af7913692641dbcad1cf4d7931f32b8b4c2a7,c236247826bbf49d4f491d97bac6df1da6c1abe8,Nikita Vedeneev,nik@quansight.com,Wed Dec 08 03:48:23 2021 -0800,1638935303.0,"OpInfo tests for `(svd|pca)_lowrank` (#69107)

Summary:
As per title.

While working on this I have discovered several issues with these methods related to grad instabilities. I will file them and link here later. These were quite painful to force to pass all the tests with these discovered issues, sorry for the delay, mruberry!

cc jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano

Pull Request resolved: https://github.com/pytorch/pytorch/pull/69107

Reviewed By: zou3519

Differential Revision: D32920341

Pulled By: mruberry

fbshipit-source-id: 15b33e2b46acdcbff8a37d8e43e381eb55d1a296",146.0,3.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.102713254,2.0,16563.0,2.0,191690.5,17597.0,41430.0,0.0,,0.0,1
pytorch,f17cfe42936310a2e3fd573e1f4dec8c684d4003,c238ee368165b5cfd0ff59a5d6479cf6393c719b,Adam Paszke,adam.paszke@gmail.com,Fri Mar 03 19:23:51 2017 +0100,1488569031.0,Fix issues with lazy grad initialization (#912),17.0,5.0,"test/test_nn.py,torch/nn/modules/module.py,torch/nn/utils/clip_grad.py",3.0,5,2,0.865856617,23.0,2772.0,3.0,96931.33333333331,507.0,3361.471519,0.0,Corrective,1.0,1
pytorch,7848c229b87e28899030f076ae3521dc041b1201,c258e4732a2f8a6b6992ee5d9283e90c45a60201,xiaobingsuper,xiaobing.zhang@intel.com,Sun Mar 15 23:09:54 2020 -0700,1584313794.0,"solve conv3d backward get incorrect result problem (#34358)

Summary:
Fix https://github.com/pytorch/pytorch/issues/34344.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34358

Differential Revision: D20461698

Pulled By: ngimel

fbshipit-source-id: 472624d0037ab65d9dcc221f647ec68818be5fc9",8.0,13.0,aten/src/ATen/native/ConvolutionMM3d.cpp,1.0,4,1,0,2.0,684.0,1.0,2334411.0,118.0,382.0,0.0,Corrective,1.0,1
pytorch,"ef6a76450963855a43f1bf979cd1b9f66559f473,64c8a1377335799b322ca41d323dee13118be0ab",c279a91c03b73d5ddb6c8fe9290524c89653c2ac,soumith,soumith@fb.com,Mon Nov 14 05:54:27 2016 -0800,1479102867.0,Merge commit '64c8a1377335799b322ca41d323dee13118be0ab',11794.0,9216.0,"torch/lib/THCUNN/Abs.cu,torch/lib/THCUNN/AbsCriterion.cu,torch/lib/THCUNN/BCECriterion.cu,torch/lib/THCUNN/BatchNormalization.cu,torch/lib/THCUNN/CMakeLists.txt,torch/lib/THCUNN/ClassNLLCriterion.cu,torch/lib/THCUNN/DistKLDivCriterion.cu,torch/lib/THCUNN/ELU.cu,torch/lib/THCUNN/HardTanh.cu,torch/lib/THCUNN/L1Cost.cu,torch/lib/THCUNN/LeakyReLU.cu,torch/lib/THCUNN/LogSigmoid.cu,torch/lib/THCUNN/LogSoftMax.cu,torch/lib/THCUNN/LookupTable.cu,torch/lib/THCUNN/MSECriterion.cu,torch/lib/THCUNN/MarginCriterion.cu,torch/lib/THCUNN/MultiLabelMarginCriterion.cu,torch/lib/THCUNN/MultiMarginCriterion.cu,torch/lib/THCUNN/PReLU.cu,torch/lib/THCUNN/RReLU.cu,torch/lib/THCUNN/SharedMem.cuh,torch/lib/THCUNN/Sigmoid.cu,torch/lib/THCUNN/SmoothL1Criterion.cu,torch/lib/THCUNN/SoftMarginCriterion.cu,torch/lib/THCUNN/SoftMax.cu,torch/lib/THCUNN/SoftPlus.cu,torch/lib/THCUNN/SoftShrink.cu,torch/lib/THCUNN/SparseLinear.cu,torch/lib/THCUNN/SpatialAdaptiveMaxPooling.cu,torch/lib/THCUNN/SpatialAveragePooling.cu,torch/lib/THCUNN/SpatialClassNLLCriterion.cu,torch/lib/THCUNN/SpatialConvolutionLocal.cu,torch/lib/THCUNN/SpatialConvolutionMM.cu,torch/lib/THCUNN/SpatialCrossMapLRN.cu,torch/lib/THCUNN/SpatialDilatedConvolution.cu,torch/lib/THCUNN/SpatialDilatedMaxPooling.cu,torch/lib/THCUNN/SpatialFractionalMaxPooling.cu,torch/lib/THCUNN/SpatialFullConvolution.cu,torch/lib/THCUNN/SpatialMaxPooling.cu,torch/lib/THCUNN/SpatialMaxUnpooling.cu,torch/lib/THCUNN/SpatialReflectionPadding.cu,torch/lib/THCUNN/SpatialReplicationPadding.cu,torch/lib/THCUNN/SpatialSubSampling.cu,torch/lib/THCUNN/SpatialUpSamplingBilinear.cu,torch/lib/THCUNN/SpatialUpSamplingNearest.cu,torch/lib/THCUNN/Sqrt.cu,torch/lib/THCUNN/Square.cu,torch/lib/THCUNN/THCHalfAutoNumerics.cuh,torch/lib/THCUNN/THCUNN.h,torch/lib/THCUNN/Tanh.cu,torch/lib/THCUNN/TemporalConvolution.cu,torch/lib/THCUNN/TemporalMaxPooling.cu,torch/lib/THCUNN/Threshold.cu,torch/lib/THCUNN/VolumetricAveragePooling.cu,torch/lib/THCUNN/VolumetricConvolution.cu,torch/lib/THCUNN/VolumetricDilatedConvolution.cu,torch/lib/THCUNN/VolumetricDilatedMaxPooling.cu,torch/lib/THCUNN/VolumetricFullConvolution.cu,torch/lib/THCUNN/VolumetricMaxPooling.cu,torch/lib/THCUNN/VolumetricMaxUnpooling.cu,torch/lib/THCUNN/VolumetricReplicationPadding.cu,torch/lib/THCUNN/cmake/select_compute_arch.cmake,torch/lib/THCUNN/common.h,torch/lib/THCUNN/generic/Abs.cu,torch/lib/THCUNN/generic/AbsCriterion.cu,torch/lib/THCUNN/generic/BCECriterion.cu,torch/lib/THCUNN/generic/BatchNormalization.cu,torch/lib/THCUNN/generic/ClassNLLCriterion.cu,torch/lib/THCUNN/generic/DistKLDivCriterion.cu,torch/lib/THCUNN/generic/ELU.cu,torch/lib/THCUNN/generic/HardTanh.cu,torch/lib/THCUNN/generic/L1Cost.cu,torch/lib/THCUNN/generic/LeakyReLU.cu,torch/lib/THCUNN/generic/LogSigmoid.cu,torch/lib/THCUNN/generic/LogSoftMax.cu,torch/lib/THCUNN/generic/LookupTable.cu,torch/lib/THCUNN/generic/MSECriterion.cu,torch/lib/THCUNN/generic/MarginCriterion.cu,torch/lib/THCUNN/generic/MultiLabelMarginCriterion.cu,torch/lib/THCUNN/generic/MultiMarginCriterion.cu,torch/lib/THCUNN/generic/PReLU.cu,torch/lib/THCUNN/generic/RReLU.cu,torch/lib/THCUNN/generic/Sigmoid.cu,torch/lib/THCUNN/generic/SmoothL1Criterion.cu,torch/lib/THCUNN/generic/SoftMarginCriterion.cu,torch/lib/THCUNN/generic/SoftMax.cu,torch/lib/THCUNN/generic/SoftPlus.cu,torch/lib/THCUNN/generic/SoftShrink.cu,torch/lib/THCUNN/generic/SparseLinear.cu,torch/lib/THCUNN/generic/SpatialAdaptiveMaxPooling.cu,torch/lib/THCUNN/generic/SpatialAveragePooling.cu,torch/lib/THCUNN/generic/SpatialClassNLLCriterion.cu,torch/lib/THCUNN/generic/SpatialConvolutionLocal.cu,torch/lib/THCUNN/generic/SpatialConvolutionMM.cu,torch/lib/THCUNN/generic/SpatialCrossMapLRN.cu,torch/lib/THCUNN/generic/SpatialDilatedConvolution.cu,torch/lib/THCUNN/generic/SpatialDilatedMaxPooling.cu,torch/lib/THCUNN/generic/SpatialFractionalMaxPooling.cu,torch/lib/THCUNN/generic/SpatialFullConvolution.cu,torch/lib/THCUNN/generic/SpatialMaxPooling.cu,torch/lib/THCUNN/generic/SpatialMaxUnpooling.cu,torch/lib/THCUNN/generic/SpatialReflectionPadding.cu,torch/lib/THCUNN/generic/SpatialReplicationPadding.cu,torch/lib/THCUNN/generic/SpatialSubSampling.cu,torch/lib/THCUNN/generic/SpatialUpSamplingBilinear.cu,torch/lib/THCUNN/generic/SpatialUpSamplingNearest.cu,torch/lib/THCUNN/generic/Sqrt.cu,torch/lib/THCUNN/generic/Square.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THCUNN/generic/Tanh.cu,torch/lib/THCUNN/generic/TemporalConvolution.cu,torch/lib/THCUNN/generic/TemporalMaxPooling.cu,torch/lib/THCUNN/generic/Threshold.cu,torch/lib/THCUNN/generic/VolumetricAveragePooling.cu,torch/lib/THCUNN/generic/VolumetricConvolution.cu,torch/lib/THCUNN/generic/VolumetricDilatedConvolution.cu,torch/lib/THCUNN/generic/VolumetricDilatedMaxPooling.cu,torch/lib/THCUNN/generic/VolumetricFullConvolution.cu,torch/lib/THCUNN/generic/VolumetricMaxPooling.cu,torch/lib/THCUNN/generic/VolumetricMaxUnpooling.cu,torch/lib/THCUNN/generic/VolumetricReplicationPadding.cu,torch/lib/THCUNN/im2col.h,torch/lib/THCUNN/vol2col.h",123.0,5,1,6.400443632,13.0,13296.0,1.0,464002.0,156.0,7221.155843,0.0,,0.0,1
pytorch,e9422b1fb0692dcddb28517b8ced690743a3b085,c2a248bdb3a1b70e30e526505f1504803de7737b,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Fri Oct 20 23:04:49 2023 +0000,1697843089.0,"Revert ""[ROCm] Unskip functorch tests that now work (#110760)""

This reverts commit 71b35862d3f4ebf0285370d2224b0d0efb118321.

Reverted https://github.com/pytorch/pytorch/pull/110760 on behalf of https://github.com/izaitsevfb due to Lint failure ([comment](https://github.com/pytorch/pytorch/pull/110760#issuecomment-1773490896))",24.0,4.0,"test/functorch/test_ops.py,test/functorch/test_vmap.py",2.0,2,1,0.591672779,1.0,7571.0,1.0,5453.0,20950.0,47870.5,0.0,Non Functional,0.0,1
pytorch,52bf38007b39c84a03374a85a2feda85cf48ad67,c2df54d6d02301dd7ec038aad4b5f3e68dfd973b,Igor Fedan,ifedan@fb.com,Thu Jul 18 02:46:31 2019 -0700,1563417991.0,"avg_pool2d avg_pool3d for LongTensor (#22433)

Summary:
Generate avg_pool2d/avg_pool3d for LongTensor for CPU.
Added divisor_override parameter.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22433

Differential Revision: D16108809

Pulled By: ifedan

fbshipit-source-id: 8de7ff585a0479702cceafb5ccf9dfea62a9cc50",570.0,218.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/AveragePool2d.cpp,aten/src/ATen/native/AveragePool3d.cpp,aten/src/ATen/native/cuda/AveragePool2d.cu,aten/src/ATen/native/cuda/AveragePool3d.cu,aten/src/ATen/native/mkldnn/Pooling.cpp,aten/src/ATen/native/native_functions.yaml,test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/symbolic_script.cpp,torch/nn/functional.py,torch/nn/modules/pooling.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py",16.0,17,4,2.975995335,43.0,32996.0,13.0,922121.0625,9995.0,28940.83333,0.0,Feature Addition,0.0,1
pytorch,9fe82435363e2e6b6f59d0049de5b2777608f71f,c2f787ce7708ba81c1d24b4481024510379d37de,Edward Yang,ezyang@fb.com,Fri May 08 03:31:55 2020 -0700,1588908715.0,"Give _VariableFunctions class a different name, so pickling works (#38033)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38033

Pickles require class names to be actually accessible from the module
in question.  _VariableFunction was not!  This fixes it.

Fixes https://github.com/pytorch/pytorch/issues/37703

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Differential Revision: D21458068

Pulled By: ezyang

fbshipit-source-id: 2a5ac41f9d1972e300724981b9b4b84364ddc18c",14.0,1.0,"test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp",2.0,4,2,0.996791632,41.0,18721.0,2.0,1091467.5,1849.0,4778.0,0.0,Corrective,1.0,1
pytorch,6468bc46375e4cb73fe3b59c7b81dff559bb0967,c314e0deb56a549f217a6b407e8efdef4c599751,Zafar,cc.rafaz@zafar.cc,Wed Jun 24 01:10:31 2020 -0700,1592961031.0,"[quant] Quantized adaptive_avg_pool3d (#40271)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/40271

Closes #40244

Test Plan: Imported from OSS

Reviewed By: vkuzo

Differential Revision: D22134318

Pulled By: z-a-f

fbshipit-source-id: 0489b6c083a3cbc21a1d81d8bfcc499372308088",463.0,163.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/quantization/test_quantized_op.py,torch/nn/quantized/functional.py",6.0,12,3,1.894757651,12.0,13481.0,6.0,957525.5,3149.0,7642.5,0.0,,0.0,1
pytorch,52f8d320b3af297c05c200e4b8908697cab95a91,c31ced42469cd5f0b4e758f5484be40a9515bc38,Nikita Vedeneev,nik@quansight.com,Fri Oct 23 17:11:05 2020 -0700,1603473065.0,"make `torch.lu` differentiable. (#46284)

Summary:
As per title. Limitations: only for batches of squared full-rank matrices.

CC albanD

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46284

Reviewed By: zou3519

Differential Revision: D24448266

Pulled By: albanD

fbshipit-source-id: d98215166268553a648af6bdec5a32ad601b7814",161.0,2.0,"test/test_autograd.py,test/test_jit.py,torch/_autograd_functions.py,torch/functional.py,torch/tensor.py,torch/testing/_internal/common_methods_invocations.py",6.0,4,2,2.076866119,44.0,27577.0,5.0,572153.4,6192.0,14255.5,0.0,,0.0,1
pytorch,b34ae77be86502936a8022f9f1517dad2165356b,c345212c867f9670a6475ffdb274fd8e3298ab6c,Du Phan,fehiepsi@gmail.com,Tue Apr 17 12:33:39 2018 +0900,1523968419.0,"Support gpu triangle solve (#6648)

* add cuda trtrs

* remove queue

* add test trtrs",55.0,4.0,"aten/src/ATen/Declarations.cwrap,aten/src/THC/generic/THCTensorMathMagma.cu,aten/src/THC/generic/THCTensorMathMagma.h,test/test_cuda.py,test/test_torch.py",5.0,6,2,1.452928923,39.0,13256.0,3.0,694241.6,1039.0,6959.172317,0.0,Feature Addition,0.0,1
pytorch,de2295f507d107788d009ca0494abe1e023c8e52,c34c2e2de12dc40e0e38b4106c63fccfca337de2,Ronan Collobert,ronan@collobert.com,Wed Sep 26 17:55:16 2012 +0200,1348682116.0,fixed some thread safety issues,21.0,29.0,generic/THTensorMath.c,1.0,1,1,0,2.0,1193.0,1.0,17546617.0,25.0,168.0,0.0,Corrective,1.0,1
pytorch,e1be08fcf5b48511b8b4546ef552d8fb472b677e,c36552c4cbc8fd9c9346d213b11654c126615d5d,Ryan J. Evans,me@rjevans.net,Thu Oct 17 16:10:54 2019 -0700,1571328654.0,"Fixing dispatch error in windows debug builds (#24360)

Summary:
nullptr initialization values for dispatch pointers were overwriting values set using the REGISTER_DISPATCH macro.

Relevant issue: https://github.com/pytorch/pytorch/issues/22681
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24360

Differential Revision: D17952241

Pulled By: ezyang

fbshipit-source-id: 4bf86dc24153e504bbeacb526c58fd8230bb972a",8.0,0.0,aten/src/ATen/native/DispatchStub.h,1.0,4,1,0,1.0,193.0,1.0,4325051.0,12370.0,34531.33333,0.0,Corrective,1.0,1
pytorch,183b3aacd2db811e8a6e7509f8f306ab5f454272,c367e0b64e9871a87105958089091c5880e96680,Sam Gross,colesbury@gmail.com,Thu Dec 29 23:20:32 2016 -0500,1483053632.0,"Support dilated 1d and 3d convolutions (#372)

Fixes #367",411.0,398.0,"test/test_nn.py,torch/nn/backends/thnn.py,torch/nn/functional.py,torch/nn/functions/conv.py,torch/nn/functions/thnn/auto.py,torch/nn/modules/conv.py,torch/nn/modules/utils.py",7.0,7,2,1.440713267,18.0,2627.0,5.0,32466.85714285714,104.0,73.65764778,0.0,Corrective,1.0,1
pytorch,bac566bf619335d7b9f18dbc35bf663d4d95e253,c371542efc31b1abfe6f388042aa3ab0cef935f2,kshitij12345,kshitijkalambarkar@gmail.com,Wed Mar 24 07:24:20 2021 -0700,1616570660.0,"testing: dont skip test_ops suite for operators testing against scipy (#54186)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/54152

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54186

Reviewed By: ngimel

Differential Revision: D27287024

Pulled By: mruberry

fbshipit-source-id: 3e19b94b138fb56a7cb2c1c13af3587a5b6d937a",182.0,160.0,"test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.246565888,2.0,6445.0,2.0,70393.0,10029.0,22209.0,0.0,Corrective,1.0,1
pytorch,05bc877a05e8621cd5c41cce404b7dfe4ddc5a7e,c39d48ea7dff977f6dc3f53e4319e4c7bfa12e79,Adam Lerer,alerer@fb.com,Sat May 20 22:45:25 2017 -0700,1495320325.0,Fast transposed copy,10.0,0.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.721928095,30.0,4232.0,2.0,295536.0,787.0,12873.56049,0.0,,0.0,1
pytorch,7449b467d9d4655321e833675633955e2ac3154f,c3b7baecea07314d28259d410aad970f41680c0b,Edward Z. Yang,ezyang@fb.com,Tue Jan 09 22:33:32 2018 -0800,1515537212.0,"Fix #4422, use grad for cudnn_batch_norm derivative / don't use toTensor()

This commit fixes double-backwards on batch norm.  There were two
bugs:

- Returned buffers from batchnorm backwards were being marked as differentiable
  when they shouldn't be.  The fix for this is ""easy"": use 'grad' instead of
  'grads[0]' in cudnn_batch_norm's backward definition.  (More on this below.)

- I was using toTensor on a Scalar, which gives me a Tensor of the wrong
  type when I'm in CUDA world.  Using the Scalar add() overload directly
  solves the problem.

The differentiability of returned buffers was annoyingly subtle and I nearly
went off and implemented a big pile of infrastructure to ""tell"" the codegen how
to distinguish between differentiable and non-differentiable outputs before
realizing that there must be a way we do this legitimately, because it works for
THNN.  I documented this in derivatives.yaml, and also added tests for the
problem in load_derivatives.py to catch the various ways you could ""get it
wrong"".  Hope this helps someone else.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",83.0,27.0,"test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/load_derivatives.py,tools/autograd/templates/Functions.cpp",5.0,4,2,1.672892398,37.0,8850.0,5.0,338469.8,416.0,2238.0,0.0,Corrective,1.0,1
pytorch,ad5d421554f37697d1e1ae058b01228009cb8ac9,c3f7e5ff5576084c160389f7612cacffb674805d,Peter Goldsborough,peter@goldsborough.me,Tue Apr 10 18:31:23 2018 -0700,1523385083.0,"Install signal handler for SIGCHLD in run_test.py (#6436)

Handle exit signal in run_test.py",32.0,16.0,test/run_test.py,1.0,1,1,0,6.0,258.0,1.0,693478.0,855.0,2003.805292,0.0,,0.0,1
pytorch,1e35ef07e9fa6781e85c3e915295adeca53bceb1,c406bf20a00ba69e4336a8ff9df18ca33ce57af9,Brian Vaughan,bvaughan@fb.com,Thu May 09 14:04:23 2019 -0700,1557410663.0,"error instead of crashing on attempt to subclass typed tensors (#20283)

Summary:
https://github.com/pytorch/pytorch/issues/20052

typed tensors (e.g. torch.FloatTensor) can't be subclassed. Was causing
crashes and other errors.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20283

Differential Revision: D15278138

Pulled By: nairbv

fbshipit-source-id: 8493eac4d34dfb76b054362bf0acec02146cd0e2",16.0,1.0,"test/test_torch.py,torch/csrc/tensor/python_tensor.cpp",2.0,4,2,0.787126586,40.0,11847.0,2.0,859507.0,8570.0,25378.33333,0.0,,0.0,1
pytorch,ecffe53ef002addd49a7d68f2d57b560d0a14642,c40b99f9ae3450a0ea47cce6f2082556bc6163e8,Martin Raison,martinraison@users.noreply.github.com,Thu Mar 15 19:46:53 2018 +0100,1521143213.0,"speed up CPU EmbeddingBag (indexSelectAdd op) (#5433)

* speed up CPU EmbeddingBag (indexSelectAdd op)

* keep operator inside EmbeddingBag + speedup

* comment

* update checkScalarTypes signature

* enforce type in embedding_bag_backward_cpu",71.0,13.0,"aten/src/ATen/TensorUtils.cpp,aten/src/ATen/TensorUtils.h,aten/src/ATen/native/EmbeddingBag.cpp",3.0,4,1,0.944713457,3.0,540.0,2.0,1393888.0,2468.0,24795.35823,0.0,Feature Addition,0.0,1
pytorch,eb0889cf7d084244ef9521d4728ccd93e458d51d,c42c594315cba86e618caa248531de1e502bde6b,lezcano,lezcano-93@hotmail.com,Tue Jul 05 09:20:27 2022 +0000,1657012827.0,"Fix rrelu on CUDA (#80434)

Rrelu had a massive bug on CUDA, that would zero out the gradient in
the part where the function is the identity.

Fixes https://github.com/pytorch/pytorch/issues/80205
Pull Request resolved: https://github.com/pytorch/pytorch/pull/80434
Approved by: https://github.com/albanD",16.0,11.0,"aten/src/ATen/native/cuda/RreluWithNoise.cu,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",3.0,10,3,0.824980257,17.0,24511.0,3.0,2583943.6666666665,4966.0,11751.0,0.0,Corrective,1.0,1
pytorch,3858e1684bf4ce0cf6ac76392ead0a67ca5a0329,c46c6a4fe68a8ed6a8adb35837e87209cf020547,Brennan Vincent,btv@fb.com,Fri May 24 22:07:56 2019 -0700,1558735676.0,"Zero slice bug (#20914)

Summary:
Bug reported internally at FB:

```python
>>> t=torch.from_numpy(np.empty((0,4)))
>>> t[:,1::2]*=1
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: Trying to resize storage that is not resizable at ../aten/src/TH/THStorageFunctions.cpp:76
```

This happens because the storage offset of `t[:, 1::2]` is 1, and it has 0 elements. We can fix this by avoiding resizing the storage for no-element arrays.

(We could *also* have avoided it by not modifying the storage index in this case, but I felt this way was more semantically correct -- in general, we should not be assuming it's okay to do anything to the storage when it has zero elements).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20914

Differential Revision: D15497860

Pulled By: umanwizard

fbshipit-source-id: 6af61d73a05edfc5c07ce8be9e530f15bf72e6a9",20.0,2.0,"aten/src/ATen/native/Resize.h,aten/src/ATen/native/cuda/Resize.cuh,test/test_torch.py",3.0,6,2,1.582023941,40.0,11864.0,3.0,3400279.6666666665,8897.0,26249.83333,0.0,Corrective,1.0,1
pytorch,3afb4e5cf7b0162c532449fb5c9e7c7058a4c803,c46fc46dba8636823a54391b7bce1e87a486d946,drisspg,drisspguessous@gmail.com,Sat Nov 11 01:22:56 2023 -0800,1699665776.0,"expose mem-eff to autograd (#110495)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110495
Approved by: https://github.com/jbschlosser",254.0,30.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp,aten/src/ATen/native/transformers/cuda/attention.cu,aten/src/ATen/native/transformers/cuda/attention_backward.cu,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,tools/autograd/derivatives.yaml,torch/_inductor/lowering.py,torch/_meta_registrations.py,torch/_subclasses/fake_utils.py,torch/testing/_internal/common_dtype.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/composite_compliance.py",15.0,19,4,2.687011602,21.0,63835.0,13.0,1835381.2,21913.0,50053.0,0.0,,0.0,1
pytorch,e124790cb2b6675a4b6edf64620a7eb7f7228b29,c4742fd1285241a04ae253154c7d85ebcc4c2907,Gregory Chanan,gchanan@fb.com,Mon May 08 18:52:12 2017 -0700,1494269532.0,"Explicitly pass keepdim=False for tests that require it.

If we change the default to False, reverting this commit is optional.",22.0,23.0,"test/test_legacy_nn.py,test/test_torch.py",2.0,1,1,0.67519144,29.0,4594.0,2.0,1008.0,132.0,4490.410678,0.0,,0.0,1
pytorch,a32916190d260d86507d2b146d0649ee4321c1a4,c47bdd7522c7146c091c95ff5b4ce354d7a8870c,Brian Hirsh,hirsheybar@meta.com,Thu Dec 22 15:37:24 2022 +0000,1671723444.0,"*_scatter ops should preserve input stride/storage_offset (#91029)

It turns out that we *do* need to update *_scatter ops to return the exact same strides as their inputs. I added a test to `test/test_functionalization.py`, which now trips thanks to Ed's functionalization stride debugging check. It only actually ends up tripping silent correctness if you try to .backward() on that function.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91029
Approved by: https://github.com/ezyang",353.0,184.0,"aten/src/ATen/FunctionalTensorWrapper.cpp,aten/src/ATen/MemoryOverlap.cpp,aten/src/ATen/functorch/BatchRulesScatterOps.cpp,aten/src/ATen/functorch/BatchedFallback.cpp,aten/src/ATen/functorch/BatchedFallback.h,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/TensorShape.h,test/functorch/test_aotdispatch.py,test/functorch/test_eager_transforms.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/test_functionalization.py,test/test_fx_reinplace_pass.py,torch/_functorch/eager_transforms.py,torch/_meta_registrations.py,torch/_prims/__init__.py,torch/_prims_common/__init__.py,torch/_prims_common/wrappers.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py,torchgen/gen_functionalization_type.py",22.0,15,4,2.501077652,11.0,55743.0,17.0,2271273.8181818184,10847.0,24727.5,0.0,Corrective,1.0,1
pytorch,53e6aca8b3672877fea15ba8b1566c204ba2951f,c4bf196334bea0579b4ce6083b292be8712c7561,Pearu Peterson,pearu.peterson@gmail.com,Mon Nov 01 23:10:03 2021 -0700,1635808203.0,"Strided masked reduction: mean (2nd try) (#67088)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67088

Stack from [ghstack](https://github.com/ezyang/ghstack):
* __->__ #67088

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D32070264

Pulled By: cpuhrsch

fbshipit-source-id: 08a91550dd24fb0f51abf06591a0e26186c4f9f9",60.0,2.0,"torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,1,0.869137581,2.0,12655.0,2.0,274899.0,16755.0,39279.5,0.0,,0.0,1
pytorch,379ae6d865970fd31bd554e82560ae3a67e9bdb7,c4d131866230425c090f663689ee44d04c9ac14c,Sam Gross,colesbury@gmail.com,Wed Mar 15 20:54:19 2017 -0400,1489611259.0,Fix map_location in torch.load (#1006),47.0,7.0,".gitignore,test/test_torch.py,test/test_utils.py,torch/_utils.py,torch/serialization.py,torch/tensor.py",6.0,2,2,2.155674348,26.0,4319.0,2.0,544194.8333333334,198.0,661.6909143,0.0,Corrective,1.0,1
pytorch,0f99b28431012d3a146727a2c1cf4995bc2263b6,c4f56e9685c6243fb9dc9c32cef9215d0a48e98b,Di Wu,allwu@fb.com,Thu Apr 02 18:10:15 2020 -0700,1585851015.0,"[pytorch][PR] Optimize qavg_pool3d_nhwc (#35740)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35740

For one of the quantized CV model, the avg_pool3d operation is more than 6x slower than C2 implementation. The reason behind this comes from the following aspects:
- function access inside the loop (such as ```q_scale()``` and ```q_zero_point()```)
- additional data copy in ```Vec256::store``` and ```at::quantize_vec```

This diff resolves the above issue with the following measures:
- lift function access outside the loops
- add an 8-lane path in ```QuantizeAvx2``` to replace ```at::quantize_vec```
- in addition, interchanges c-loop to the innermost for better memory locality.

Test Plan:
buck test //caffe2/test:quantized

Performance Before (n x h x w x c = 4 x 56 x 56 x ch):
```
type            c=2             c=4             c=15            c=24            c=48            c=128           c=256
torch.qint8     903.08 us       1373.39 us      2297.97 us      636.72 us       864.98 us       1618.72 us      2908.47 us
torch.quint8    911.93 us       1429.39 us      2315.59 us      623.08 us       844.17 us       1522.28 us      2711.08 us
torch.qint32    897.77 us       1346.97 us      3846.41 us      6211.92 us      11977.74 us     34348.23 us     62927.48 us
```
Performance After:
```
type            c=2             c=4             c=15            c=24            c=48            c=128           c=256
torch.qint8     123.29 us       176.00 us       348.90 us       99.02 us        132.73 us       267.17 us       513.43 us
torch.quint8    123.76 us       171.90 us       338.17 us       97.92 us        131.06 us       260.09 us       521.16 us
torch.qint32    102.97 us       172.57 us       559.31 us       814.03 us       1606.11 us      4164.89 us      10041.52 us
```

Reviewed By: lly-zero-one

Differential Revision: D20711888

fbshipit-source-id: a71dd55639500f4a036eee96c357737cff9d33db",213.0,73.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp",2.0,9,1,0.760677935,2.0,3005.0,2.0,689856.0,719.0,2076.5,0.0,Feature Addition,0.0,1
pytorch,e2aa28a2d080c7f4f546766d40b87d1d2367bee4,c51b53d4ef474c0192a616f10c29ac7a5e54cbc3,Natalia Gimelshein,ngimel@fb.com,Wed May 04 02:50:00 2022 +0000,1651632600.0,"[WIP] sum reference

Per title

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76714
Approved by: https://github.com/mruberry",50.0,18.0,"test/test_ops.py,torch/_prims/__init__.py,torch/_prims/utils.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",5.0,6,2,1.789491281,5.0,22090.0,2.0,107526.6,2834.0,6801.0,0.0,,0.0,1
pytorch,047ae6b7138d006e772f1ea4f6937cfcc1c87f33,c524448dd1bf9fcbfced6a6d9c13d8f297c63275,Nick Korovaiko,villedepommes@fb.com,Fri May 14 02:36:38 2021 -0700,1620959798.0,"init hardshrink (#57749)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57749

add to a fx test

Test Plan: Imported from OSS

Reviewed By: huiguoo

Differential Revision: D28425974

fbshipit-source-id: 195c7a1944decb7a2a99c2831cab38485f32be17",42.0,3.0,"test/test_fx.py,test/test_jit_fuser_te.py,torch/csrc/jit/passes/tensorexpr_fuser.cpp,torch/csrc/jit/runtime/symbolic_script.cpp,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_metaprogramming_utils.py",7.0,9,2,2.012554762,2.0,18129.0,6.0,153695.57142857142,12070.0,27359.0,0.0,Feature Addition,0.0,1
pytorch,0ca8f7a15f16f75514515251f016fb2ff8c40137,c561ef540692a92ba866b8add8c3ed401a1315e9,Richard Zou,zou3519@gmail.com,Wed Apr 10 01:08:59 2019 -0700,1554858539.0,"Refactor CPU embedding_bag implementation (#18734)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18734
ghimport-source-id: e0e50d4b47f2fb8c86e464aacb950521d601f8d3

Reviewed By: cpuhrsch

Differential Revision: D14851413

Pulled By: zou3519

fbshipit-source-id: 8ac4e4de590a363e9807dc552fe4ca52b92652ed",97.0,67.0,aten/src/ATen/native/EmbeddingBag.cpp,1.0,4,1,0,6.0,357.0,1.0,2766648.0,8009.0,24158.33333,0.0,Perfective,0.0,1
pytorch,fb1e6835cc15e3f6536d2223c0e86ce02ffec757,c562ebca233c3ad16357753accdd04dc1ab2ab88,Natalia Gimelshein,ngimel@fb.com,Mon Sep 20 17:11:29 2021 -0700,1632157889.0,"Revert ""Revert D30558877: Ported std/var to ReductionOpInfo (#65262)

Summary:
Reland of https://github.com/pytorch/pytorch/issues/63978

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65262

Reviewed By: mruberry

Differential Revision: D31037360

Pulled By: ngimel

fbshipit-source-id: 1c60f40c547229767cba3bbe7e11ca0fbbc8f95f",145.0,49.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,10019.0,1.0,259444.0,15583.0,35854.0,0.0,,0.0,1
pytorch,5952acc041db96b99dc0d463a28d74da9d3b28e5,c56a7cfc37aae8b5e1515da37b314ed03bff8641,vishwakftw,cs15btech11043@iith.ac.in,Tue Sep 11 19:40:56 2018 -0700,1536694856.0,"More use of AT_CHECK and AT_ERROR (#11457)

Summary: Considering these increase the size of the message stack, I didn't touch the code outside `ATen/native`

Differential Revision: D9754283

Pulled By: soumith

fbshipit-source-id: 04198ec4fd0c4abae09eeba92c493a783408537a",274.0,423.0,"aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/RoiPooling.cpp,aten/src/ATen/native/SpectralOpsUtils.h,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/CuFFTPlanCache.h,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/cudnn/AffineGridGenerator.cpp,aten/src/ATen/native/cudnn/BatchNorm.cpp,aten/src/ATen/native/cudnn/Conv.cpp,aten/src/ATen/native/cudnn/GridSampler.cpp,aten/src/ATen/native/cudnn/LossCTC.cpp,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/miopen/BatchNorm_miopen.cpp,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/mkldnn/Conv.cpp",18.0,9,1,3.177473586,7.0,7915.0,11.0,3701493.722222222,4011.0,11218.33333,0.0,,0.0,1
pytorch,156fe2866665c55557e41bb46603fef3e39bfb35,c57f0530e73da090a12a8ca9bcf26a8edda1df3e,Po-Hsien Chu,stegben.benjamin@gmail.com,Thu May 18 23:31:36 2017 -0500,1495150296.0,"let long_args False for param ""size"" of set_ (#1568)

* fix #1524, let long_args False for param ""size"" of set_",5.0,9.0,"test/test_torch.py,torch/csrc/generic/methods/Tensor.cwrap",2.0,5,2,0.749595257,29.0,4341.0,2.0,280995.0,772.0,12846.06049,0.0,Corrective,1.0,1
pytorch,c9f380df02624dbb943df98dcc2abe397e3e539d,c5845c44821bb3e9f8847544c122cca42aaeba6d,Max Wang,mwang@fb.com,Fri Apr 26 20:28:45 2019 -0700,1556310525.0,"Add support for reduce-scatter in c10d (#18844)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18844
ghimport-source-id: c6b2f0032c7c2212be2000a9c1f262f63d878a97

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18844 Add support for reduce-scatter in c10d**
* #18820 Refactor ProcessGroupNCCL collective primitives

Reviewed By: mrshenli

Differential Revision: D14768369

fbshipit-source-id: a9def7a0da6e9cd995e982371cc1e22f3df1a156",398.0,25.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/distributed/distributed_c10d.py,torch/lib/c10d/ProcessGroup.hpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/ProcessGroupGloo.hpp,torch/lib/c10d/ProcessGroupMPI.cpp,torch/lib/c10d/ProcessGroupMPI.hpp,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/lib/c10d/Types.hpp,torch/lib/c10d/test/ProcessGroupNCCLTest.cpp",12.0,9,2,2.63091608,6.0,8277.0,7.0,2740419.833333333,8361.0,24994.83333,0.0,Feature Addition,0.0,1
pytorch,ca39e3679ff834d67da20abaa3b313664c935d8a,c5a4844085ea4db27912f5174be2585aebf7079a,Driss Guessous,drisspg@fb.com,Fri Oct 07 03:52:46 2022 +0000,1665114766.0,"Xformer SDP forward/backward kernel (#86157)

# Summary
Include xformer kernel code and make header updates to successfully build. Need to update the kernel calling code and dispatch system to clean this up.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86157
Approved by: https://github.com/cpuhrsch",14246.0,83.0,"aten/src/ATen/CMakeLists.txt,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/transformers/cuda/attention.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/attention_backward_generic.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/attention_forward_generic.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/attention_scaling_coefs_updater.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/debug_utils.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue_pipelined.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue_rescale_output.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/epilogue_thread_apply_logsumexp.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/find_default_mma.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm/custom_mma.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm/custom_mma_base.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm/custom_mma_multistage.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm/custom_mma_pipelined.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/gemm_kernel_utils.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/iterators/epilogue_predicated_tile_iterator.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/iterators/make_residual_last.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/iterators/predicated_tile_access_iterator_residual_last.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/iterators/predicated_tile_iterator_residual_last.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_backward.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernel_forward.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_bf16.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_bf16_aligned.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k128.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_bf16_aligned_k64.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_bf16_k128.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_bf16_k64.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f16.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f16_aligned.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f16_aligned_k128.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f16_aligned_k64.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f16_k128.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f16_k64.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f32.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f32_aligned.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f32_aligned_k128.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f32_aligned_k64.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f32_k128.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/backward_f32_k64.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/forward_bf16.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/forward_bf16_aligned.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/forward_f16.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/forward_f16_aligned.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/forward_f32.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/forward_f32_aligned.cu,aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/generate_kernels.sh,aten/src/ATen/native/transformers/cuda/mem_eff_attention/mma_from_smem.h,aten/src/ATen/native/transformers/cuda/mem_eff_attention/mma_simt_tile_iterator_residual.h,aten/src/ATen/native/transformers/cuda/sdp_utils.h,test/test_transformers.py,torch/testing/_internal/common_methods_invocations.py",52.0,14,3,4.070653322,20.0,34342.0,5.0,146399.33333333334,8113.0,19227.0,0.0,,1.0,1
pytorch,bccc26f365d8b795e2931797d283e32c5f47aa0f,c5a8946e40d6cda42aa38dda2705ea4e9930c2cb,Edward Z. Yang,ezyang@fb.com,Sat Sep 10 04:00:38 2022 -0400,1662782438.0,"Revert ""Revert ""Redo how custom/python_custom methods on TensorImpl work  (#84796)"" (#84806)

This reverts commit ca3b2bfbe3945c756a67a784aaa7d9891698c59b.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84806
Approved by: https://github.com/Chillee",464.0,282.0,"aten/src/ATen/BatchedTensorImpl.cpp,aten/src/ATen/EmptyTensor.cpp,aten/src/ATen/FunctionalTensorWrapper.cpp,aten/src/ATen/FunctionalTensorWrapper.h,aten/src/ATen/NestedTensorImpl.cpp,aten/src/ATen/NestedTensorImpl.h,aten/src/ATen/OpaqueTensorImpl.h,aten/src/ATen/SparseCsrTensorImpl.cpp,aten/src/ATen/SparseCsrTensorImpl.h,aten/src/ATen/SparseTensorImpl.cpp,aten/src/ATen/functorch/BatchedTensorImpl.cpp,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,c10/core/UndefinedTensorImpl.cpp,c10/core/UndefinedTensorImpl.h,c10/core/impl/PyInterpreter.cpp,c10/core/impl/PyInterpreter.h,functorch/test/test_ops.py,test/test_dynamic_shapes.py,test/test_fake_tensor.py,test/test_proxy_tensor.py,test/test_sparse.py,test/test_sparse_csr.py,torch/_subclasses/fake_tensor.py,torch/csrc/autograd/python_variable.cpp,torch/csrc/lazy/core/tensor_impl.cpp,torch/csrc/lazy/core/tensor_impl.h",27.0,16,5,1.977804946,33.0,21665.0,1.0,21562.0,7235.0,16907.5,0.0,,0.0,1
pytorch,27deefb5e1cf501b51694b0b6067813c0bb3bcc8,c5b3727e5e3efd63889f1c60ea20a54397c712e3,David Berard,dberard@fb.com,Fri Apr 01 20:22:43 2022 -0700,1648844563.0,"[JIT] OpInfo tests for nvfuser (#71299)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/71299

These tests verify that for the same inputs, the eager version of an op
and a traced, fused version of the op return the same output.

Currently the tests don't check whether or not fusion actually occurred.

Test Plan: Imported from OSS

Reviewed By: eellison

Differential Revision: D33595299

Pulled By: davidberard98

fbshipit-source-id: 26fdacf44941808c134953e7a883a02d13a43f19
(cherry picked from commit 8cd084e2e3130fcd5f9c99302d6d9bf4e21c25bb)",81.0,9.0,"test/test_jit_cuda_fuser.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.595761097,5.0,20718.0,2.0,5923.5,1915.0,4585.0,0.0,,0.0,1
pytorch,67eb2d5952741f2024c826d008ed35b8a1cc56d9,c609768896ead0b4bb439a0b03e58360a5c00023,lezcano,lezcano-93@hotmail.com,Wed Oct 05 10:13:17 2022 +0000,1664964797.0,"Add refs for torch.unfold and a decomposition for its backward. (#85629)

It's not clear to me what's the difference between `unfold` and `unfold_copy`, as this latter one is codegen'd

I also took this chance to clean the implementation of unfold and its reference
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85629
Approved by: https://github.com/mruberry",95.0,59.0,"aten/src/ATen/functorch/BatchRulesDecompositions.cpp,aten/src/ATen/native/TensorShape.cpp,functorch/test/test_aotdispatch.py,functorch/test/test_ops.py,test/test_ops.py,test/test_proxy_tensor.py,tools/autograd/gen_variable_type.py,torch/_decomp/decompositions.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",10.0,15,5,2.204259971,18.0,37445.0,7.0,136035.9,8035.0,18977.5,0.0,Feature Addition,0.0,1
pytorch,f38e1d2d609092342eec407882f6b0232c3e7115,c60dacd4cfc064b8ac4c3b356e04b7b9b105de38,Hong Xu,hong@topbug.net,Wed Feb 03 22:30:53 2021 -0800,1612391453.0,"Replace all AT_ASSERTM in ATen/native (#51147)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51147

Test Plan: Imported from OSS

Reviewed By: H-Huang

Differential Revision: D26206403

Pulled By: ezyang

fbshipit-source-id: 6a5c331337bf03b3dc29ceef8f7eeb4539b22c7f",18.0,18.0,"aten/src/ATen/native/ChanelShuffle.cpp,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/SoftMax.cpp",4.0,4,1,1.612197223,8.0,1157.0,4.0,13722829.0,8625.0,19425.0,0.0,,0.0,1
pytorch,24ae50bcc72588f043c05074cc1f1df9cbac9233,c620ece726a197740538c1de3d8e0ff62253ac73,mingfeima,mingfei.ma@intel.com,Fri Feb 10 03:12:35 2023 +0800,1675998755.0,"port sparse_mm.reduce to pytorch and optimize it on CPU (#83727)

### Motivation of this PR

This patch is to migrate `spmm_reduce` from `torch-sparse` (a 3rd party dependency for PyG) to `torch`, which is a response to the initial proposal for fusion of **Gather, Apply Scatter** in Message Passing of GNN inference/training. https://github.com/pytorch/pytorch/issues/71300

**GAS** is the major step for Message Passing, the behavior of **GAS** can be classified into 2 kinds depending on the storage type of `EdgeIndex` which records the connections of nodes:

* COO: the hotspot is `scatter_reduce`
* CSR: the hotspot is `spmm_reduce`

The reduce type can be choose from: ""max"", ""mean"", ""max"",  ""min"".

extend `torch.sparse.mm` with an `reduce` argument, maps to `torch.sparse_mm.reduce` internally.
`sparse_mm_reduce` is registered under the TensorTypeId of `SparseCsrCPU`, and this operator requires an internal interface `_sparse_mm_reduce_impl` which has dual outputs:
* `out` - the actual output
* `arg_out` - records output indices in the non zero elements if the reduce type is ""max"" or ""min"", this is only useful for training. So for inference, it will not be calculated.

### Performance

Benchmark on GCN for obgn-products on Xeon single socket, the workload is improved by `4.3x` with this patch.

Performance benefit for training will be bigger, the original backward impl for `sum|mean` is sequential; the original backward impl for `max|min` is not fused.

#### before:
```
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
       torch_sparse::spmm_sum        97.09%       56.086s        97.09%       56.088s        6.232s             9
                 aten::linear         0.00%      85.000us         1.38%     795.485ms      88.387ms             9
                 aten::matmul         0.00%      57.000us         1.38%     795.260ms      88.362ms             9
                     aten::mm         1.38%     795.201ms         1.38%     795.203ms      88.356ms             9
                   aten::relu         0.00%      50.000us         0.76%     440.434ms      73.406ms             6
              aten::clamp_min         0.76%     440.384ms         0.76%     440.384ms      73.397ms             6
                   aten::add_         0.57%     327.801ms         0.57%     327.801ms      36.422ms             9
            aten::log_softmax         0.00%      23.000us         0.10%      55.503ms      18.501ms             3
```

#### after
```
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------
               aten::spmm_sum        87.35%       11.826s        87.36%       11.827s        1.314s             9
                 aten::linear         0.00%      92.000us         5.87%     794.451ms      88.272ms             9
                 aten::matmul         0.00%      62.000us         5.87%     794.208ms      88.245ms             9
                     aten::mm         5.87%     794.143ms         5.87%     794.146ms      88.238ms             9
                   aten::relu         0.00%      53.000us         3.35%     452.977ms      75.496ms             6
              aten::clamp_min         3.35%     452.924ms         3.35%     452.924ms      75.487ms             6
                   aten::add_         2.58%     348.663ms         2.58%     348.663ms      38.740ms             9
                 aten::argmax         0.42%      57.473ms         0.42%      57.475ms      14.369ms             4
            aten::log_softmax         0.00%      22.000us         0.39%      52.605ms      17.535ms             3
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/83727
Approved by: https://github.com/jgong5, https://github.com/cpuhrsch, https://github.com/rusty1s, https://github.com/pearu",941.0,34.0,"aten/src/ATen/native/cpu/SpmmReduceKernel.cpp,aten/src/ATen/native/cpu/SpmmReduceKernel.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp,aten/src/ATen/native/sparse/SparseCsrTensorMath.h,aten/src/ATen/native/sparse/SparseTensorMath.cpp,build_variables.bzl,test/distributed/_tensor/test_dtensor_ops.py,test/expect/HasDecompTest.test_has_decomposition.expect,test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,test/test_proxy_tensor.py,test/test_sparse_csr.py,tools/autograd/derivatives.yaml,torch/sparse/__init__.py,torch/testing/_internal/common_methods_invocations.py",18.0,18,4,2.346548833,37.0,60039.0,10.0,1811890.9375,12325.0,28824.5,0.0,Feature Addition,1.0,1
pytorch,68251fb93196a4c28a0c9f9ce37896b21c6c04e0,c638f379b3550556d9ec706d6dc39c23cc6799c3,Brennan Vincent,btv@fb.com,Wed Nov 28 14:50:49 2018 -0800,1543416649.0,"Make `mean` function work across multiple dimensions. (#14252)

Summary:
Multi-dimensional `sum` is already implemented, and it's trivial to implement `mean` in terms of `sum`, so just do it.

Bonus: Fix incomplete language in the `torch.sum` documentation which doesn't take into account multiple dimensions when describing `unsqueeze` (at the same time as introducing similar language in `torch.mean`).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14252

Differential Revision: D13161157

Pulled By: umanwizard

fbshipit-source-id: c45da692ba83c0ec80815200c5543302128da75c",103.0,41.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_torch_docs.py,torch/csrc/jit/passes/shape_analysis.cpp",10.0,13,4,2.957487216,42.0,27493.0,6.0,89286.0,5660.0,17197.33333,0.0,Corrective,1.0,1
pytorch,e31cd462781ee3c26e1d7a7a186ca5f678e0b8ce,c64594f5cc7f17f9f0638fe2c2d3d1a583412cec,Mike Ruberry,mruberry@devfair044.maas,Fri Aug 21 04:59:49 2020 -0700,1597985989.0,"Extends test_unary_ufunc.py with numerics, contiguity, domain tests (#42965)

Summary:
This PR:

- ports the tests in TestTorchMathOps to test_unary_ufuncs.py
- removes duplicative tests for the tested unary ufuncs from test_torch.py
- adds a new test, test_reference_numerics, that validates the behavior of our unary ufuncs vs. reference implementations on empty, scalar, 1D, and 2D tensors that are contiguous, discontiguous, and that contain extremal values, for every dtype the unary ufunc supports
- adds support for skipping tests by regex, this behavior is used to make the test suite pass on Windows, MacOS, and ROCm builds, which have a variety of issues, and on Linux builds (see https://github.com/pytorch/pytorch/issues/42952)
- adds a new OpInfo helper, `supports_dtype`, to facilitate test writing
- extends unary ufunc op info to include reference, domain, and extremal value handling information
- adds OpInfos for `torch.acos` and `torch.sin`

These improvements reveal that our testing has been incomplete on several systems, especially with larger float values and complex values, and several TODOs have been added for follow-up investigations. Luckily when writing tests that cover many ops we can afford to spend additional time crafting the tests and ensuring coverage.

Follow-up PRs will:

- refactor TestTorchMathOps into test_unary_ufuncs.py
- continue porting tests from test_torch.py to test_unary_ufuncs.py (where appropriate)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42965

Reviewed By: pbelevich

Differential Revision: D23238083

Pulled By: mruberry

fbshipit-source-id: c6be317551453aaebae9d144f4ef472f0b3d08eb",527.0,76.0,"test/test_torch.py,test/test_unary_ufuncs.py,torch/testing/__init__.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py",5.0,4,2,1.45122151,42.0,23575.0,3.0,532465.0,4438.0,10357.5,0.0,Feature Addition,0.0,1
pytorch,630bd85aae958495682fb5959f5a97832c2223d7,c6500bcf1494aadf7bd86adb554fdad376b7f105,Yanli Zhao,yanlizhao@fb.com,Fri Sep 25 03:52:17 2020 -0700,1601005937.0,"[reland] Make grad point to bucket buffer in DDP to save memory usage (#44344)

Summary:
[test all]
Pull Request resolved: https://github.com/pytorch/pytorch/pull/44344

reland #41954

Add one argument in DDP API to enable/disable letting grads pointing  to views. When it is disabled, behavior is the same as DDP right now; when it is enabled, Make both variable.grad() and grad in distautograd context point to bucket buffer in DDP to save memory usage.
In this case, grad will be view of bucket buffer tensors, in order to make it compatiable with optimizer.zero_grad(), we
made changes in #41283.

Also be noted that we can not make variable.grad() pointing to bucket buffer during construction time, because we want to
keep grad undefined for unused parameters.
ghstack-source-id: 112845787

Test Plan:
1. When grad_is_view=false:
a. roberta_base, peak memory usage 8250MB, p50 per iteration latency 0.923second, https://www.internalfb.com/intern/fblearner/details/218029699/?notif_channel=cli
b. resnet, peak memory usage 3089MB, p50 per iteration latency 0.120second, https://www.internalfb.com/intern/fblearner/details/218029035/?notif_channel=cli
c. accuracy benchmark, distributed=false, .accuracy 40.914535522461, .loss: 1.6370717287064; distributed=true, .accuracy: 39.966053009033, .loss: 1.6849111318588
https://www.internalfb.com/intern/fblearner/details/218035688/?notif_channel=cli
d. classy vision uru production flow, https://www.internalfb.com/intern/fblearner/details/219065811/?notif_channel=cli
e. pytext flow, https://www.internalfb.com/intern/fblearner/details/219137458/?notif_channel=cli

2. When grad_is_view=true:
a. roberta_base, peak memory usage 7183MB, p50 per iteration latency 0.908second, https://www.internalfb.com/intern/fblearner/details/217882539?tab=operator_details
b. resnet, peak memory usage 2988 MB, p50 per iteration latency 0.119second, https://www.internalfb.com/intern/fblearner/details/218028479/?notif_channel=cli
c. accuracy benchmark, distributed=false, .accuracy 41.713260650635, .loss: 1.69939661026; distributed=true, .accuracy: 39.966053009033, .loss: 1.6849111318588, https://www.internalfb.com/intern/fblearner/details/218037058/?notif_channel=cli
d. classy vision uru production flow, expected, can not work well with apex.amp https://www.internalfb.com/intern/fblearner/details/219205218/?notif_channel=cli
e. pytext flow, detach_() related error, expected, as pytext zero_grad depends on apex repo where detach_() is called. also seeing the warning in finalize_bucket_dense due to tied weights, which is expected. https://www.internalfb.com/intern/fblearner/details/219150229/?notif_channel=cli

Reviewed By: mrshenli

Differential Revision: D23588186

fbshipit-source-id: f724d325b954ef6f06ede31759bf01dd29a6f5e5",425.0,120.0,"test/distributed/test_c10d.py,torch/csrc/autograd/VariableTypeManual.cpp,torch/csrc/autograd/functions/accumulate_grad.h,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h,torch/nn/parallel/distributed.py,torch/testing/_internal/distributed/distributed_test.py",8.0,13,2,1.971946955,22.0,11505.0,6.0,808441.875,5426.0,12662.0,0.0,Feature Addition,0.0,1
pytorch,7daa96a3ce14bfee5b5c830d4954fc29438a85f1,c68119387d7b1b24c796a041bdcfa1f6b627e5fa,Ailing,ailzhang@users.noreply.github.com,Tue Jun 25 17:41:16 2019 -0700,1561484476.0,"serialize torch.Size object (#20952)

Summary:
fixes https://github.com/pytorch/pytorch/issues/20823
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20952

Differential Revision: D15514274

Pulled By: ailzhang

fbshipit-source-id: 8340a40fadfd06063f7f33b0d99d693e74d5defb",34.0,0.0,"test/test_torch.py,torch/csrc/Size.cpp",2.0,3,2,0.733537929,40.0,12341.0,2.0,4691082.5,9616.0,27952.33333,0.0,Corrective,1.0,1
pytorch,80c8635a7eaf374b651f479606d0edc651459c39,c681b03d374e4010a875018c8a8a0485c40ceb9b,Tongzhou Wang,SsnL@users.noreply.github.com,Fri Dec 01 18:22:46 2017 -0500,1512152566.0,"Add determinant function on variable; Add backward on svd (#3816)

* determinant on variable

* svd bwd",424.0,20.0,"CONTRIBUTING.md,aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/THC/generic/THCTensorMathMagma.cu,aten/src/THC/generic/THCTensorMathMagma.h,docs/source/torch.rst,test/test_autograd.py,test/test_cuda.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/jit/templates/aten_dispatch.cpp,torch/_torch_docs.py,torch/csrc/autograd/utils/wrap_outputs.h,torch/csrc/generic/methods/TensorMath.cwrap,torch/functional.py",20.0,20,5,3.283495284,39.0,24387.0,17.0,871586.3,2167.0,24173.85823,0.0,Feature Addition,0.0,1
pytorch,1e76ade9dc144d797a0daf2fa9190822a14eb390,c6adee0807ce816a821118658bed917628800ebb,Will Feng,yf225@cornell.edu,Thu Jan 04 21:58:13 2018 -0500,1515103093.0,disable CUDA HalfTensor tests in test_cuda for Windows (#4482),2.0,3.0,test/test_cuda.py,1.0,1,1,0,37.0,1221.0,1.0,212690.0,895.0,6687.672317,0.0,,0.0,1
pytorch,49f679d0e94ceb81ed171a37f0d4b400a65b7feb,c6d7e1e6bfd52a8f93debd44aaaecde4daf759ac,Alykhan Tejani,alykhan.tejani@gmail.com,Sun Jul 09 19:31:24 2017 +0100,1499628684.0,added input size checks to batchnorm (#2020),56.0,9.0,"test/test_nn.py,torch/csrc/autograd/functions/batch_normalization.cpp,torch/nn/modules/batchnorm.py,torch/nn/modules/instancenorm.py",4.0,7,2,1.49120725,31.0,4084.0,2.0,369422.5,1104.0,16359.4346,0.0,Feature Addition,0.0,1
pytorch,ea8b09365c0f0ce88fcc147098eb0adcb09d4951,c6ea6ed8ffa76c398db076a6a8efc198b9379787,David Pollack,sent@da3.net,Mon Sep 18 18:48:49 2017 +0200,1505760529.0,"Add Nd Padding, Pad1d functions and ConstantPad3d (#2657)",1139.0,106.0,"test/test_nn.py,torch/lib/THCUNN/TemporalReflectionPadding.cu,torch/lib/THCUNN/TemporalReplicationPadding.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THCUNN/generic/TemporalReflectionPadding.cu,torch/lib/THCUNN/generic/TemporalReplicationPadding.cu,torch/lib/THNN/generic/THNN.h,torch/lib/THNN/generic/TemporalReflectionPadding.c,torch/lib/THNN/generic/TemporalReplicationPadding.c,torch/lib/THNN/init.c,torch/nn/_functions/padding.py,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/auto_double_backwards.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/padding.py",16.0,11,2,3.362960067,38.0,9818.0,1.0,137464.0,1752.0,24780.55562,0.0,Feature Addition,0.0,1
pytorch,216961b7bf59577cee7d3e47c2eef6243ae187d8,c6fc3ab557fd9498eb719d015f1b63ea1601743d,pbialecki,bialecki.emb@gmail.com,Mon Aug 13 23:19:46 2018 -0700,1534202386.0,"fixes printing non-contiguous tensors

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/10405

Differential Revision: D9302794

Pulled By: soumith

fbshipit-source-id: e4a7db8d33400a5a050d05fd1679de8bc3cbcf30",28.0,2.0,"test/expect/TestTorch.test_print-non_contiguous.expect,test/test_torch.py,torch/_tensor_str.py",3.0,3,2,1.294820355,40.0,8821.0,2.0,1142174.0,3430.0,9231.333333,0.0,Corrective,1.0,1
pytorch,3894de569e64f7ae3277b7ce394a123bffde3992,c72ab194581f1b352524edfbd18f3fd7b48b38c2,anjali411,chourdiaanjali123@gmail.com,Mon Jun 22 17:52:02 2020 -0700,1592848322.0,"Add addmv for complex dtypes (#40238)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40238

Differential Revision: D22160528

Pulled By: anjali411

fbshipit-source-id: 04093e5929318a7acc9c9b502c76d0a8bf15d5e1",60.0,9.0,"aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cuda/CUDABlas.h,aten/src/ATen/native/cuda/Blas.cu,aten/src/ATen/test/cuda_complex_test.cu,test/test_torch.py",5.0,8,2,2.078884917,42.0,19943.0,3.0,2735151.6,3069.0,7416.5,0.0,Feature Addition,0.0,1
pytorch,5ead88c7dcdc7b54a33e9b2b5c22e47277205fba,c7759445e5c08a09a323f9f17ec31ce115f72607,Richard Zou,zou3519@gmail.com,Thu May 27 14:19:33 2021 -0700,1622125173.0,"[functorch] Add vjpvjp tests, fix pytorch/functorch#44

Bug was that TensorWrapper wasn't setting the storage_offset.
Unfortunate.",83.0,7.0,"functorch/functorch/_src/eager_transforms.py,functorch/functorch/csrc/DynamicLayer.cpp,functorch/functorch/csrc/TensorWrapper.cpp,functorch/functorch/csrc/TensorWrapper.h,functorch/test/test_grad.py",5.0,5,1,0.867939717,1.0,1163.0,1.0,0.0,116.0,246.0,0.0,Corrective,1.0,1
pytorch,a3bab37d96a58e8eecfba7b2d20959c278e04951,c78691b4a6e89ebd9794a5ab9b02bfafd6903094,nuka137,n.a.sch137@gmail.com,Wed May 20 16:12:51 2020 -0700,1589991171.0,"[CPU] torch.gather for complex dtypes (#36430)

Summary:
This PR resolves https://github.com/pytorch/pytorch/issues/36340 .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36430

Differential Revision: D21662139

Pulled By: anjali411

fbshipit-source-id: 361d064c1144b368afae3059c19f77abe26080a3",20.0,19.0,"aten/src/ATen/native/cpu/ScatterGatherKernel.cpp,test/test_torch.py",2.0,6,2,0.291818257,41.0,18827.0,2.0,234200.5,2229.0,5515.5,0.0,,0.0,1
pytorch,32acc96f78138d04b2e7d03daf8fe01ee8c1d576,c790fd2bf89f16de25e5918e67b3e5fb6288c4cb,Nikita Vedeneev,nik@quansight.com,Wed May 12 05:46:06 2021 -0700,1620798366.0,"ATen lu_unpack. Required for making `torch.lu_solve` differentiable. (#46913)

Summary:
Backward methods for `torch.lu` and `torch.lu_solve` require the `torch.lu_unpack` method.
However, while `torch.lu` is a Python wrapper over a native function, so its gradient is implemented via `autograd.Function`,
`torch.lu_solve` is a native function, so it cannot access `torch.lu_unpack` as it is implemented in Python.

Hence this PR presents a native (ATen) `lu_unpack` version. It is also possible to update the gradients for `torch.lu` so that backward+JIT is supported (no JIT for `autograd.Function`) with this function.

~~The interface for this method is different from the original `torch.lu_unpack`, so it is decided to keep it hidden.~~

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46913

Reviewed By: albanD

Differential Revision: D28355725

Pulled By: mruberry

fbshipit-source-id: 281260f3b6e93c15b08b2ba66d5a221314b00e78",452.0,141.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.h,aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,test/test_autograd.py,test/test_linalg.py,test/test_namedtuple_return_api.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/functional.py,torch/jit/_builtins.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",17.0,15,4,3.256134435,44.0,56848.0,7.0,86264.70588235294,11930.0,27114.5,0.0,,0.0,1
pytorch,c771d73461449f89e26bc4130d1641340a03e05d,c794ee5cc12192da527bbbcf5c5b9ec33c935cbe,Nikita Shulga,nshulga@fb.com,Tue Sep 06 17:49:29 2022 +0000,1662486569.0,"Reenable TestCppExtensionJIT on M1 (#84552)

Works fine locally, let's see if it'll pass CI

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84552
Approved by: https://github.com/kit1980",1.0,2.0,test/test_cpp_extensions_jit.py,1.0,1,1,0,3.0,887.0,1.0,2770641.0,7089.0,16588.0,0.0,,0.0,1
pytorch,3569a1c6ddb756a28fc3aafe083f1303eb269b73,c79d116a7d3375a5108d64fab4f18cfbaa6ca711,Lara,lahaidar@microsoft.com,Tue Sep 24 00:11:37 2019 -0700,1569283897.0,"Update ONNX Export for Gather and Scatter for Opset 11

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/24790

Reviewed By: hl475

Differential Revision: D17159723

Pulled By: houseroad

fbshipit-source-id: a63bb7c681120de85588dafecd03f04742dde8b7",374.0,7.0,"test/onnx/expect/TestOperators.test_gather_opset11.expect,test/onnx/expect/TestOperators.test_scatter_add.expect,test/onnx/expect/TestOperators.test_scatter_add_opset11.expect,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",8.0,5,2,2.512084617,3.0,4371.0,4.0,758043.8,11648.0,32797.83333,0.0,,0.0,1
pytorch,0e77c0f5de4d9d8ee63fc040de14578ca4eb6537,c7c02724cd65300de7e87b7d03b7197a383f2db0,Jiakai Liu,liujiakai@fb.com,Fri May 03 16:23:11 2019 -0700,1556900591.0,"CMakeLists changes to enable libtorch for Android (#19762)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19762
ghimport-source-id: 287aa7fea4efd38994e14d794123eb2046b91fc0

Differential Revision: D15087653

Pulled By: ljk53

fbshipit-source-id: 4498ff9f7f7903c3e25541184302b811267958e9",50.0,14.0,".jenkins/pytorch/build.sh,CMakeLists.txt,aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,caffe2/CMakeLists.txt,caffe2/utils/CMakeLists.txt,cmake/Codegen.cmake,cmake/Dependencies.cmake,cmake/Summary.cmake,cmake/TorchConfig.cmake.in",10.0,8,4,2.809470112,75.0,3909.0,5.0,1169605.5,8455.0,25149.83333,0.0,,0.0,1
pytorch,97abc21f2bda38e73de2a86da7f43c8126930681,c7c09722ad5ee25c5891f863e5bbd1575ad77970,Jason Ansel,jansel@fb.com,Thu Oct 13 23:18:06 2022 +0000,1665703086.0,"Move TorchDynamo into PyTorch core (#86461)

Context:
https://github.com/pytorch/torchdynamo/issues/1588

This PR moves [TorchDynamo](https://github.com/pytorch/torchdynamo) and TorchInductor into PyTorch core.
- `torchdynamo` becomes `torch._dynamo`
- `torchinductor` becomes `torch._inductor`

This PR was generated by running `copy_to_core.sh` in https://github.com/pytorch/torchdynamo/pull/1538

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86461
Approved by: https://github.com/voznesenskym",85171.0,14.0,".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat,benchmarks/dynamo/README.md,benchmarks/dynamo/__init__.py,benchmarks/dynamo/common.py,benchmarks/dynamo/huggingface.py,benchmarks/dynamo/huggingface_models_list.txt,benchmarks/dynamo/microbenchmarks/__init__.py,benchmarks/dynamo/microbenchmarks/bench_autotune_conv.py,benchmarks/dynamo/microbenchmarks/bench_conv.py,benchmarks/dynamo/microbenchmarks/bench_conv1x1.py,benchmarks/dynamo/microbenchmarks/bench_conv_fusion.py,benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py,benchmarks/dynamo/microbenchmarks/benchmark_helper.py,benchmarks/dynamo/microbenchmarks/inductor_bmm.py,benchmarks/dynamo/microbenchmarks/inductor_mm.py,benchmarks/dynamo/microbenchmarks/matmul_relu.py,benchmarks/dynamo/microbenchmarks/microbench.py,benchmarks/dynamo/microbenchmarks/model.py,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AlbertForMaskedLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AlbertForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AllenaiLongformerBase_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BartForConditionalGeneration_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BertForMaskedLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BertForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BigBird_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BlenderbotSmallForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BlenderbotSmallForConditionalGeneration_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/CamemBert_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForMaskedLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForMaskedLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistillGPT2_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPTNeoForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPTNeoForSequenceClassification_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GoogleFnet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForMaskedLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/M2M100ForConditionalGeneration_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForConditionalGeneration_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MegatronBertForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MegatronBertForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/OPTForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForConditionalGeneration_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForQuestionAnswering_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/TrOCRForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XLNetLMHeadModel_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/YituTechConvBert_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/beit_base_patch16_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/botnet26t_256_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/coat_lite_mini_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cspdarknet53_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/deit_base_distilled_patch16_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dla102_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ese_vovnet19b_dw_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gernet_l_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_senet154_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gmixer_24_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gmlp_s16_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/jx_nest_base_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixer_b16_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mnasnet_100_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv2_100_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilevit_s_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nasnetalarge_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pnasnet5large_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/poolformer_m36_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/repvgg_a2_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net101_26w_4s_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resmlp_12_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnet18_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tnt_s_patch16_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/vit_base_patch16_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/BERT_pytorch_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/alexnet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/dcgan_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fastNLP_Bert_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Albert_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bart_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bert_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Longformer_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/maml_omniglot_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_CycleGAN_and_pix2pix_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_struct_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/speech_transformer_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/squeezenet1_1_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vision_transformer_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt,benchmarks/dynamo/microbenchmarks/operator_inp_utils.py,benchmarks/dynamo/microbenchmarks/operatorbench.py,benchmarks/dynamo/microbenchmarks/profile_conv.py,benchmarks/dynamo/microbenchmarks/utils.py,benchmarks/dynamo/runner.py,benchmarks/dynamo/timm_models.py,benchmarks/dynamo/timm_models_list.txt,benchmarks/dynamo/torchbench.py,benchmarks/dynamo/torchbench_models_list.txt,benchmarks/dynamo/training_loss.py,requirements.txt,setup.py,test/dynamo/__init__.py,test/dynamo/mock_modules/__init__.py,test/dynamo/mock_modules/mock_module1.py,test/dynamo/mock_modules/mock_module2.py,test/dynamo/mock_modules/mock_module3.py,test/dynamo/test_aot_autograd.py,test/dynamo/test_aot_cudagraphs.py,test/dynamo/test_distributed.py,test/dynamo/test_dynamic_shapes.py,test/dynamo/test_export.py,test/dynamo/test_functions.py,test/dynamo/test_global.py,test/dynamo/test_global_declaration.py,test/dynamo/test_minifier.py,test/dynamo/test_misc.py,test/dynamo/test_model_output.py,test/dynamo/test_modules.py,test/dynamo/test_no_fake_tensors.py,test/dynamo/test_nops.py,test/dynamo/test_optimizations.py,test/dynamo/test_optimizers.py,test/dynamo/test_python_autograd.py,test/dynamo/test_recompile_ux.py,test/dynamo/test_replay_record.py,test/dynamo/test_repros.py,test/dynamo/test_skip_non_tensor.py,test/dynamo/test_subgraphs.py,test/dynamo/test_unspec.py,test/dynamo/test_verify_correctness.py,test/functorch/test_aotdispatch.py,test/inductor/__init__.py,test/inductor/cpp/.gitignore,test/inductor/cpp/CMakeLists.txt,test/inductor/cpp/test.sh,test/inductor/cpp/test_cpp_prefix.cpp,test/inductor/opinfo_harness.py,test/inductor/test_torchinductor.py,test/inductor/test_torchinductor_opinfo.py,test/test_dynamic_shapes.py,test/test_proxy_tensor.py,torch/_dynamo/__init__.py,torch/_dynamo/allowed_functions.py,torch/_dynamo/bytecode_analysis.py,torch/_dynamo/bytecode_transformation.py,torch/_dynamo/codegen.py,torch/_dynamo/config.py,torch/_dynamo/convert_frame.py,torch/_dynamo/debug_utils.py,torch/_dynamo/eval_frame.py,torch/_dynamo/exc.py,torch/_dynamo/guards.py,torch/_dynamo/logging.py,torch/_dynamo/mutation_guard.py,torch/_dynamo/optimizations/__init__.py,torch/_dynamo/optimizations/analysis.py,torch/_dynamo/optimizations/backends.py,torch/_dynamo/optimizations/distributed.py,torch/_dynamo/optimizations/inference.py,torch/_dynamo/optimizations/log_args.py,torch/_dynamo/optimizations/normalize.py,torch/_dynamo/optimizations/subgraph.py,torch/_dynamo/optimizations/training.py,torch/_dynamo/output_graph.py,torch/_dynamo/profiler.py,torch/_dynamo/replay_record.py,torch/_dynamo/resume_execution.py,torch/_dynamo/side_effects.py,torch/_dynamo/skipfiles.py,torch/_dynamo/source.py,torch/_dynamo/symbolic_convert.py,torch/_dynamo/testing.py,torch/_dynamo/utils.py,torch/_dynamo/variables/__init__.py,torch/_dynamo/variables/base.py,torch/_dynamo/variables/builder.py,torch/_dynamo/variables/builtin.py,torch/_dynamo/variables/constant.py,torch/_dynamo/variables/dicts.py,torch/_dynamo/variables/functions.py,torch/_dynamo/variables/lists.py,torch/_dynamo/variables/misc.py,torch/_dynamo/variables/nn_module.py,torch/_dynamo/variables/tensor.py,torch/_dynamo/variables/torch.py,torch/_dynamo/variables/user_defined.py,torch/_inductor/__init__.py,torch/_inductor/codecache.py,torch/_inductor/codegen/__init__.py,torch/_inductor/codegen/autotuner.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/cpp_prefix.h,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/triton_conv_delta_x.j2,torch/_inductor/codegen/triton_conv_delta_x_hwc.j2,torch/_inductor/codegen/triton_mm.j2,torch/_inductor/codegen/triton_template.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/compile_fx.py,torch/_inductor/config.py,torch/_inductor/debug.py,torch/_inductor/decomposition.py,torch/_inductor/dependencies.py,torch/_inductor/exc.py,torch/_inductor/graph.py,torch/_inductor/ir.py,torch/_inductor/lowering.py,torch/_inductor/metrics.py,torch/_inductor/overrides.py,torch/_inductor/scheduler.py,torch/_inductor/sizevars.py,torch/_inductor/triton_ops/__init__.py,torch/_inductor/triton_ops/autotune.py,torch/_inductor/triton_ops/batched_matmul.py,torch/_inductor/triton_ops/conv.py,torch/_inductor/triton_ops/conv1x1.py,torch/_inductor/triton_ops/conv_perf_model.py,torch/_inductor/triton_ops/matmul.py,torch/_inductor/triton_ops/mm_perf_model.py,torch/_inductor/triton_ops/utils.py,torch/_inductor/utils.py,torch/_inductor/virtualized.py,torch/csrc/dynamo/eval_frame.c",308.0,25,4,7.321024099,44.0,4956.0,5.0,899557.4285714285,8348.0,19926.0,0.0,,0.0,1
pytorch,b69155f754bb077acf0dc3e32251f0352234ed5c,c7d5e0f53f9ca2148806324cad6b4361e2415eae,anjali411,chourdiaanjali123@gmail.com,Wed Nov 24 17:52:25 2021 -0800,1637776345.0,"OpInfos for torch.atleast_{1d, 2d, 3d} (#67355)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67355

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D32649416

Pulled By: anjali411

fbshipit-source-id: 1b42e86c7124427880fff52fbe490481059da967",52.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.318215298,2.0,15512.0,2.0,111843.5,17307.0,40681.5,0.0,,0.0,1
pytorch,7d192d48d23dfb285ec2bcfc844ade3fb14a22d9,c7d6cec078bcd0b0652ba10d1d55931b27be9f36,lezcano,lezcano-93@hotmail.com,Tue Jun 07 15:51:04 2022 +0000,1654617064.0,"Add linalg.lu_solve

This PR adds `linalg.lu_solve`. While doing so, I found a bug in MAGMA
when calling the batched MAGMA backend with trans=True. We work around
that by solving the system solving two triangular systems.

We also update the heuristics for this function, as they were fairly
updated. We found that cuSolver is king, so luckily we do not need to
rely on the buggy backend from magma for this function.

We added tests testing this function left and right. We also added tests
for the different backends. We also activated the tests for AMD, as
those should work as well.

Fixes https://github.com/pytorch/pytorch/issues/61657

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77634

Approved by: https://github.com/malfet",706.0,456.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/allowlist_for_publicAPI.json,test/test_linalg.py,test/test_meta.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_meta_registrations.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",22.0,17,5,3.286893052,20.0,74564.0,11.0,670818.8636363636,4051.0,9379.0,0.0,Corrective,1.0,1
pytorch,38e5e4a85f18c716ed84d12e6c7d5155ac582b65,c7edcd69683f6e3b08305ed0d4621e148fbfbe17,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Sat Aug 27 01:23:17 2022 +0000,1661563397.0,"Revert ""Don't introduce new overload for SymInt (#83628)""

This reverts commit 9790d90e4b0288796ab44a6b4979db0a67580ba8.

Reverted https://github.com/pytorch/pytorch/pull/83628 on behalf of https://github.com/malfet due to Breaks internal builds, see D39076487",715.0,752.0,".github/ci_commit_pins/xla.txt,aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/FunctionalInverses.cpp,aten/src/ATen/core/NamedRegistrations.cpp,aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h,aten/src/ATen/core/dispatch/OperatorEntry.cpp,aten/src/ATen/core/dynamic_type.cpp,aten/src/ATen/core/dynamic_type.h,aten/src/ATen/core/function_schema.cpp,aten/src/ATen/core/function_schema.h,aten/src/ATen/core/jit_type.h,aten/src/ATen/native/MathBitFallThroughLists.h,aten/src/ATen/native/MetaTensor.cpp,aten/src/ATen/native/SummaryOps.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cudnn/ConvShared.cpp,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/ATen/native/mkldnn/TensorFactories.cpp,aten/src/ATen/native/mps/TensorFactory.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/TensorFactories.cpp,aten/src/ATen/native/sparse/SparseCsrTensor.cpp,aten/src/ATen/native/sparse/SparseTensor.cpp,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/ATen/native/ts_native_functions.yaml,aten/src/ATen/native/vulkan/ops/Factory.cpp,aten/src/ATen/native/vulkan/ops/Shape.cpp,aten/src/ATen/templates/CompositeViewCopyKernels.cpp,aten/src/ATen/test/ExclusivelyOwned_test.cpp,aten/src/ATen/test/MaybeOwned_test.cpp,aten/src/ATen/test/extension_backend_test.cpp,aten/src/ATen/test/math_kernel_test.cpp,functorch/functorch/csrc/BatchRulesFactory.cpp,functorch/functorch/csrc/BatchRulesViews.cpp,test/cpp/lazy/test_lazy_ops.cpp,test/cpp_extensions/open_registration_extension.cpp,test/cpp_extensions/ort_extension.cpp,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/test_decomp.py,test/test_dynamic_shapes.py,test/test_meta.py,test/test_nn.py,test/test_profiler_tree.py,test/test_proxy_tensor.py,tools/autograd/derivatives.yaml,tools/autograd/gen_inplace_or_view_type.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_trace_type.py,tools/autograd/gen_variable_factories.py,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py,tools/test/test_codegen.py,torch/_subclasses/fake_tensor.py,torch/csrc/jit/codegen/cuda/interface.cpp,torch/csrc/jit/frontend/schema_type_parser.cpp,torch/csrc/jit/python/pybind_utils.cpp,torch/csrc/jit/python/pybind_utils.h,torch/csrc/jit/runtime/static/ops.cpp,torch/csrc/lazy/core/ir_builder.h,torch/csrc/lazy/ts_backend/ts_native_functions.cpp,torchgen/api/autograd.py,torchgen/api/cpp.py,torchgen/api/dispatcher.py,torchgen/api/lazy.py,torchgen/api/native.py,torchgen/api/python.py,torchgen/api/structured.py,torchgen/api/translate.py,torchgen/api/types.py,torchgen/api/ufunc.py,torchgen/api/unboxing.py,torchgen/dest/lazy_ir.py,torchgen/dest/register_dispatch_key.py,torchgen/gen.py,torchgen/gen_backend_stubs.py,torchgen/gen_functionalization_type.py,torchgen/model.py,torchgen/static_runtime/generator.py",81.0,49,7,5.426295563,49.0,102425.0,4.0,82967.0,6860.0,16139.0,0.0,Feature Addition,0.0,1
pytorch,25b18bb5d7e36fec9d212264ba30dafa7ac17083,c7f1595b194621abb082b896590ec77f9ef8849f,BowenBao,bowbao@microsoft.com,Thu Feb 04 20:35:27 2021 -0800,1612470927.0,"fix bug (#51222) (#51527)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51527

Fix bug in scatter_add symbolic

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D26203119

Pulled By: SplitInfinity

fbshipit-source-id: e61f024e2daa7bc396fb264b8823a72ebf94ccdb",16.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.672294817,3.0,9711.0,2.0,13.0,8687.0,19534.0,0.0,Corrective,1.0,1
pytorch,e476645024f0b47e4fe1f23900f835cd0211fd1e,c811db50326e4b9617cb13a54dabde043f6f81be,Richard Zou,zou3519@users.noreply.github.com,Wed Jun 08 13:51:18 2022 -0400,1654696278.0,"[functorch] Fix CI (pytorch/functorch#854)

- removed xfails
- removed some decompositions b/c we have support in core",16.0,13.0,"functorch/functorch/_src/eager_transforms.py,functorch/functorch/csrc/DynamicLayer.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,5,1,1.173071233,1.0,7514.0,3.0,0.25,1097.0,1489.5,0.0,Corrective,1.0,1
pytorch,30063347e71bc540737889895ed3212cf18683b3,c835dedce905405839774c985d7469df07cd61f1,chengjinfang,chengjf@cn.fujitsu.com,Wed May 27 20:56:07 2020 -0700,1590612967.0,"Fix the issue that PyTorch doesn't construct bool tensors from non-boâ¦ (#38392)

Summary:
â¦ol values correctly(https://github.com/pytorch/pytorch/issues/37398)

Signed-off-by: chengjinfang <chengjf@cn.fujitsu.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38392

Differential Revision: D21737009

Pulled By: mruberry

fbshipit-source-id: c77d8c940af95f5011fe008b48ea0d16c3f501d1",47.0,8.0,"test/test_torch.py,torch/csrc/utils/python_numbers.h,torch/csrc/utils/python_scalars.h",3.0,4,2,1.158838516,42.0,18958.0,3.0,3168260.333333333,2365.0,5964.5,0.0,Corrective,1.0,1
pytorch,8a6e28ccd3c2f46449c0c68cc6f439f0b1ea14b3,c848a777e827696bae67938642a2303f491f6951,loganthomas,logan.thomas005@gmail.com,Mon Mar 20 20:46:04 2023 +0000,1679345164.0,"DOC: Various typo fixes (#97095)

Various typos found while browsing documentation/source code.

Thank you for a wonderful deep-learning library!
Pull Request resolved: https://github.com/pytorch/pytorch/pull/97095
Approved by: https://github.com/mikaylagawarecki, https://github.com/kit1980",15.0,15.0,"torch/backends/xeon/run_cpu.py,torch/cuda/__init__.py,torch/cuda/_memory_viz.py,torch/cuda/jiterator.py,torch/cuda/memory.py,torch/nn/backends/thnn.py,torch/nn/init.py",7.0,6,1,2.173557262,44.0,4879.0,7.0,19933867.714285716,13512.0,31296.5,0.0,Corrective,1.0,1
pytorch,3f7ab95890ee9e9685cb61a1ce69b2f464badd90,c8a4734b971883cb1602c7190631747ec0df4c77,Adam Paszke,adam.paszke@gmail.com,Thu Sep 29 16:39:14 2016 -0700,1475167154.0,Add RReLU to both nn packages,102.0,26.0,"test/common.py,test/common_nn.py,test/test_legacy_nn.py,test/test_nn.py,tools/cwrap/plugins/StandaloneExtension.py,torch/legacy/nn/RReLU.py,torch/nn/functions/thnn/activation.py,torch/nn/functions/thnn/auto.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py",10.0,11,3,2.697745703,8.0,3633.0,6.0,172735.5,211.0,3401.992183,0.0,Feature Addition,0.0,1
pytorch,66547ca06171812a65afbd82473372aa739a8b2a,c93076495dd50dc434e819661022fe3fd12e34c1,Evpok,evpok.padding@gmail.com,Mon Mar 05 10:39:03 2018 +0100,1520246343.0,add: padding_value to `torch.nn.utils.rnn.pad_sequence` (#5540),9.0,3.0,"test/test_nn.py,torch/nn/utils/rnn.py",2.0,4,2,0.979868757,37.0,6922.0,2.0,566157.5,980.0,6843.672317,0.0,Feature Addition,0.0,1
pytorch,dc3eb6af009e3da69384e98f8f5523d01c06514f,c94446a04d95abb2c2bd2e387f14f1d8a04a7e6e,vfdev,vfdev.5@gmail.com,Fri Dec 17 16:31:42 2021 +0100,1639758702.0,"[functorch] Fixed python code formatting and added flake8 setup (pytorch/functorch#346)

* Fixed python code formatting and added flake8 setup

* Fixes config.yaml

* Added missing setup.cfg

* Removed flake8 job from circle ci and setup GHA

* More flake8 fixes

* Fixed test_conv2d

* Fixed failing flake8",837.0,475.0,"functorch/.github/workflows/lint.yml,functorch/codegen/codegen_outofplacebatching.py,functorch/codegen/gen_plumbing.py,functorch/examples/compilation/eager_fusion.py,functorch/examples/compilation/fuse_module.py,functorch/examples/compilation/linear_train.py,functorch/examples/compilation/simple_function.py,functorch/examples/dp_cifar10/cifar10_transforms.py,functorch/examples/ensembling/parallel_train.py,functorch/examples/lennard_jones/lennard_jones.py,functorch/examples/maml_omniglot/maml-omniglot-higher.py,functorch/examples/maml_omniglot/maml-omniglot-ptonly.py,functorch/examples/maml_omniglot/maml-omniglot-transforms.py,functorch/examples/maml_omniglot/support/omniglot_loaders.py,functorch/examples/maml_regression/evjang.py,functorch/examples/maml_regression/evjang_transforms.py,functorch/examples/maml_regression/evjang_transforms_module.py,functorch/functorch/__init__.py,functorch/functorch/_src/aot_autograd.py,functorch/functorch/_src/compilers.py,functorch/functorch/_src/custom_function.py,functorch/functorch/_src/decompositions.py,functorch/functorch/_src/eager_transforms.py,functorch/functorch/_src/fx_minifier.py,functorch/functorch/_src/make_functional.py,functorch/functorch/_src/memory_efficient_op_authoring.py,functorch/functorch/_src/nnc_compile.py,functorch/functorch/_src/operator_authoring.py,functorch/functorch/_src/python_key.py,functorch/functorch/_src/pytree_hacks.py,functorch/functorch/_src/top_operators_github_usage.py,functorch/functorch/_src/vmap.py,functorch/op_analysis/gen_data.py,functorch/setup.cfg,functorch/test/common_utils.py,functorch/test/discover_coverage.py,functorch/test/functorch_additional_op_db.py,functorch/test/test_compile_cache.py,functorch/test/test_eager_transforms.py,functorch/test/test_minifier.py,functorch/test/test_operator_authoring.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py,functorch/test/xfail_suggester.py",45.0,16,1,4.920598478,1.0,16325.0,30.0,9.465116279069768,668.0,912.5,0.0,Corrective,1.0,1
pytorch,9f89692dcdc7ddd5ce7f5394b1832eb9c95e64f7,c991258b9300efdc24a2f9293d31990ddd39f78d,Jean A. Senellart,jean.senellart@systrangroup.com,Fri Jan 20 21:53:31 2017 +0100,1484949211.0,fix formula for GRU cells,2.0,2.0,torch/nn/modules/rnn.py,1.0,3,1,0,18.0,495.0,1.0,19627.0,369.0,4888.476424,0.0,Corrective,1.0,1
pytorch,b977a3b66d7d56617088e242646e172ff8bbd024,c9af4c2636c7ca27be15f39f16512c2f5f36b7fe,kshitij12345,kshitijkalambarkar@gmail.com,Sat May 29 01:20:30 2021 -0700,1622251230.0,"OpInfo: where (#58349)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58349

Reviewed By: mrshenli

Differential Revision: D28744220

Pulled By: mruberry

fbshipit-source-id: 893a2fb88a48a60df75c7d6e2f58a42ca949daa7",54.0,7.0,"test/test_fx.py,test/test_fx_experimental.py,test/test_jit_fuser_te.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,0.360853768,2.0,13556.0,3.0,239161.5,12561.0,28498.5,0.0,,0.0,1
pytorch,5c641cc14fa0adeb7b83e12605ac8e803686457b,c9bc6c2bc3af1308cff72c95eda55a1b7eff293c,Alican Bozkurt,alicanb@gmail.com,Mon Jan 08 09:23:48 2018 -0500,1515403428.0,Implement Student's t-distribution (#4510),157.0,8.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/studentT.py",4.0,5,3,1.243838807,8.0,1368.0,2.0,287415.0,900.0,6700.672317,0.0,,0.0,1
pytorch,ba1da47e8fa95ca0dd8b2d63430f7eb54fdbbccb,c9c90765c1a4b0eb7649f9a0dcedcf0b9e4ebaae,andreasfloros,77194848+andreasfloros@users.noreply.github.com,Tue Aug 15 05:25:29 2023 +0000,1692077129.0,"grad_mode decorators without paren (#107086)

This PR implements the feature described in #107036 for `no_grad`, `enable_grad` and `inference_mode`.

Users can still use the above as before but they can also use them without parentheses.

For example:

```python
import torch

a = torch.ones(1, requires_grad=True)

def do_something():
    print(2 * a)

with torch.no_grad():
    do_something()  # tensor([2.])

torch.no_grad()(do_something)()  # tensor([2.])

torch.no_grad(do_something)()  # tensor([2.])

do_something()  # tensor([2.], grad_fn=<MulBackward0>)
```

For `inference_mode`, decorating without parenthesis is equivalent to decorating with the default `mode=True`, similiar to how dataclasses behave (https://docs.python.org/3/library/dataclasses.html#module-contents)

Closes #107036

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107086
Approved by: https://github.com/albanD",75.0,21.0,"test/test_autograd.py,torch/autograd/grad_mode.py,torch/utils/_contextlib.py",3.0,4,2,1.35433702,44.0,11838.0,3.0,6618474.0,18608.0,42163.0,0.0,Feature Addition,0.0,1
pytorch,cdbf78fba0cde2e23584fae90e6d51cc3cfa0079,ca0540a7ebeb611d752d3f20fc13f0daba8454a1,Rohan Varma,rvarm1@fb.com,Fri Jun 19 19:23:12 2020 -0700,1592594592.0,"Remove variable shadowing from tensorpipe lambda (#39126)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39126
futureResponseMessage is shadowed in the pipeWrite lambda which
creates some confusion, since it is used in the initial error handling but then
a future of the same name is created when marking the future as completed. This
change removes this by getting rid of the futureResponseMessage capture,
instead capturing the message id. This change also makes it so that we don't
need to copy it into the lambda.
ghstack-source-id: 106211353

Test Plan: CI

Differential Revision: D22127398

fbshipit-source-id: c98a53b5630ce487461e4ca9cd72fbd34788298d",6.0,3.0,torch/csrc/distributed/rpc/tensorpipe_agent.cpp,1.0,4,1,0,1.0,996.0,1.0,120685.0,3010.0,7313.5,0.0,,0.0,1
pytorch,c13dc2cab2d397f41970454ccc9059f7f9048dbe,ca2206d071df17d1e00b3c39c32df1b300079888,Donna Choi,choidong@amazon.com,Fri May 08 21:58:36 2020 -0700,1588975116.0,"Add documentation for FeatureAlphaDropout (#36295)

Summary:
These changes add documentation for FeatureAlphaDropout, based on a need raised in an issue by SsnL (Issue https://github.com/pytorch/pytorch/issues/9886).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36295

Differential Revision: D21478591

Pulled By: zou3519

fbshipit-source-id: a73c40bf1c7e3b1f301dc3347cef7b32e9842320",68.0,1.0,"docs/source/nn.functional.rst,test/test_torch.py,torch/nn/functional.py,torch/nn/modules/dropout.py",4.0,6,3,1.359978838,45.0,22964.0,3.0,6700559.5,1878.0,4859.0,0.0,Feature Addition,0.0,1
pytorch,ba04c326b68f2e803d74e1644f6b7f82f1173953,ca41c5f08db7b578a07dba02cec50d2f154bdefd,vfdev,vfdev.5@gmail.com,Thu Dec 09 20:59:13 2021 +0100,1639083553.0,"[functorch] Added tests for vmap and autocast (pytorch/functorch#327)

Description:
- Added tests for vmap and autocast on CUDA/CPU
  - Tests are failing somehow on CPU
- Removed unused imports in test_vmap.py

Related to pytorch/functorch#305",62.0,6.0,functorch/test/test_vmap.py,1.0,2,1,0,1.0,3384.0,1.0,0.0,614.0,840.0,0.0,Feature Addition,0.0,1
pytorch,c31fccd6782c364a9f3437b9ed9529f2bdb75378,caa0d0c50a25b6287de58e7012c9c26f727245b6,Pieter Noordhuis,pietern@fb.com,Thu May 09 21:07:36 2019 -0700,1557436056.0,"Add c10d::broadcast_coalesced and tests (#20234)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20234

The differences with the existing function _dist_broadcast_coalesced
is that this one works for both CPU and CUDA tensors and that it has a
maximum number of in flight operations.

This should be the final change needed to have only a single version
of DistributedDataParallel that both supports CPU and CUDA models, or
even a mix of both.

See #17757 for more information.

Reviewed By: mrshenli

Differential Revision: D15228099

fbshipit-source-id: a2113ba6b09b68cb5328f49f4c1960031eb43c93",186.0,2.0,"test/test_c10d.py,tools/build_variables.py,torch/CMakeLists.txt,torch/csrc/distributed/c10d/comm.cpp,torch/csrc/distributed/c10d/comm.h,torch/csrc/distributed/c10d/init.cpp",6.0,6,3,1.874265656,3.0,4332.0,3.0,322174.5,8586.0,25433.83333,0.0,Feature Addition,0.0,1
pytorch,76c7652cc524d95ecad3084a447c6805cb914844,caa45c8e33edbc46a25c199c3ff304874fc3405d,Nick Gibson,nickg@fb.com,Wed Apr 08 22:39:24 2020 -0700,1586385564.0,"[TensorExpr] fix warnings (#36167)

Summary:
Fix a bunch of minor warnings in jit/tensorexpr, mostly unused variable & wrong sign comparisons.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/36167

Differential Revision: D20905081

Pulled By: nickgg

fbshipit-source-id: 16fe605a86f08596f64e74e9337c59a2581a4d5a",88.0,111.0,"test/cpp/tensorexpr/padded_buffer.cpp,test/cpp/tensorexpr/test_base.h,test/cpp/tensorexpr/test_expr.cpp,test/cpp/tensorexpr/test_ir_printer.cpp,test/cpp/tensorexpr/test_llvm.cpp,torch/csrc/jit/tensorexpr/buffer.h,torch/csrc/jit/tensorexpr/eval.h,torch/csrc/jit/tensorexpr/expr.h,torch/csrc/jit/tensorexpr/function.cpp,torch/csrc/jit/tensorexpr/function.h,torch/csrc/jit/tensorexpr/ir.h,torch/csrc/jit/tensorexpr/ir_mutator.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/kernel.cpp,torch/csrc/jit/tensorexpr/kernel.h,torch/csrc/jit/tensorexpr/llvm_codegen.cpp,torch/csrc/jit/tensorexpr/loopnest.cpp",18.0,7,2,3.231791736,2.0,10747.0,4.0,428854.3333333333,889.0,2437.0,0.0,Corrective,1.0,1
pytorch,5c189946746599d47479c16f097dcc1beeecd7e0,cab65ea3b94691af4e0ba530fa4aa070bc5c548e,kshitij12345,kshitijkalambarkar@gmail.com,Sun May 30 05:37:12 2021 -0700,1622353032.0,"OpInfo: renorm (#59079)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59079

Reviewed By: ngimel

Differential Revision: D28776789

Pulled By: mruberry

fbshipit-source-id: ca46f2debe918c3de1f3b5bbc9924b7ddfe9442a",23.0,3.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,7187.0,1.0,6172.0,12569.0,28516.0,0.0,,0.0,1
pytorch,3da2e09c9b7297cea9fa92d1f691f62433964737,cac3cd1433a0d29f3a6ee680d0636130f2202cb2,Mikayla Gawarecki,mikaylagawarecki@gmail.com,Wed Nov 17 17:10:37 2021 -0800,1637169037.0,"add torch.diff support for n greater than 1 (#67260)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67260

Addressing 54853

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D31930294

Pulled By: mikaylagawarecki

fbshipit-source-id: 97c7a27e9200c6688242680ff96b73dfff828479",85.0,42.0,"aten/src/ATen/native/ReduceOps.cpp,test/test_torch.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",4.0,8,3,1.629692318,44.0,35819.0,4.0,795866.75,17156.0,40410.0,0.0,Feature Addition,0.0,1
pytorch,6400d27bbbbd7fb4d6d66c8d493fa6fa91d62998,cac553cf34805d5fd26e0e412ce6ce0f887d1ea6,Yi Wang,wayi@fb.com,Sat Nov 21 17:18:10 2020 -0800,1605979090.0,"[Gradient Compression] clang-format test_c10d.py (#48349)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/48349

Apply clang-format only.

Original PR issue: Investigate Applying PowerSGD to Communication Hook for Gradient Compression #47202
ghstack-source-id: 117305263

Test Plan: N/A

Reviewed By: pritamdamania87

Differential Revision: D25138833

fbshipit-source-id: 4ff112b579c0c5b8146495ebd2976d5faead2c1b",622.0,341.0,test/distributed/test_c10d.py,1.0,2,1,0,2.0,4243.0,1.0,184.0,6983.0,15795.5,0.0,,0.0,1
pytorch,6ca8272d46a67b56b1cb65bb551bcbf5709f005b,cada2cd3aec91cf84013831e059ec6fda1f0d5e2,BowenBao,bowbao@microsoft.com,Fri Apr 22 22:08:22 2022 -0700,1650665302.0,"[ONNX] Support per channel quantization

Extending the support for quantization with per channel quantization.
An extra attribute `axis` can be found for per channel quantized tensors,
most commonly in quantized weight of Convolution or Linear module.
The PR adds support to correctly parse the `axis` attribute, and map to
ONNX representation in `QuantizeLinear` and `DequantizeLinear`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76002

Approved by: https://github.com/garymm",403.0,114.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset14.py,torch/onnx/symbolic_opset9.py",7.0,8,2,2.017880404,4.0,17705.0,6.0,861337.8571428572,2564.0,6060.5,0.0,Corrective,0.0,1
pytorch,0e691f89985c35b29f32e88566c90d9774309742,caecbffe62c2ba5e6037c545ee7511c0ed2c3942,peterjc123,peter_jiachen@163.com,Sun Sep 17 03:58:22 2017 +0800,1505620702.0,Improve Windows Compatibility(for lib/THCUNN) (#2443),747.0,731.0,"torch/lib/THCUNN/CMakeLists.txt,torch/lib/THCUNN/ClassNLLCriterion.cu,torch/lib/THCUNN/IndexLinear.cu,torch/lib/THCUNN/LogSigmoid.cu,torch/lib/THCUNN/LookupTable.cu,torch/lib/THCUNN/LookupTableBag.cu,torch/lib/THCUNN/SpatialClassNLLCriterion.cu,torch/lib/THCUNN/SpatialDilatedMaxPooling.cu,torch/lib/THCUNN/SpatialMaxUnpooling.cu,torch/lib/THCUNN/SpatialSubSampling.cu,torch/lib/THCUNN/SpatialUpSamplingNearest.cu,torch/lib/THCUNN/THCUNN.h,torch/lib/THCUNN/VolumetricMaxUnpooling.cu,torch/lib/THCUNN/VolumetricUpSamplingNearest.cu,torch/lib/THCUNN/generic/ClassNLLCriterion.cu,torch/lib/THCUNN/generic/FeatureLPPooling.cu,torch/lib/THCUNN/generic/FusedRNNKernel.cu,torch/lib/THCUNN/generic/GatedLinearUnit.cu,torch/lib/THCUNN/generic/IndexLinear.cu,torch/lib/THCUNN/generic/LookupTable.cu,torch/lib/THCUNN/generic/LookupTableBag.cu,torch/lib/THCUNN/generic/PReLU.cu,torch/lib/THCUNN/generic/SoftMax.cu,torch/lib/THCUNN/generic/SparseLinear.cu,torch/lib/THCUNN/generic/SpatialAdaptiveAveragePooling.cu,torch/lib/THCUNN/generic/SpatialAdaptiveMaxPooling.cu,torch/lib/THCUNN/generic/SpatialAveragePooling.cu,torch/lib/THCUNN/generic/SpatialClassNLLCriterion.cu,torch/lib/THCUNN/generic/SpatialConvolutionLocal.cu,torch/lib/THCUNN/generic/SpatialConvolutionMM.cu,torch/lib/THCUNN/generic/SpatialDepthWiseConvolution.cu,torch/lib/THCUNN/generic/SpatialDilatedConvolution.cu,torch/lib/THCUNN/generic/SpatialDilatedMaxPooling.cu,torch/lib/THCUNN/generic/SpatialFractionalMaxPooling.cu,torch/lib/THCUNN/generic/SpatialFullConvolution.cu,torch/lib/THCUNN/generic/SpatialFullDilatedConvolution.cu,torch/lib/THCUNN/generic/SpatialMaxUnpooling.cu,torch/lib/THCUNN/generic/SpatialSubSampling.cu,torch/lib/THCUNN/generic/SpatialUpSamplingNearest.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THCUNN/generic/TemporalConvolution.cu,torch/lib/THCUNN/generic/TemporalRowConvolution.cu,torch/lib/THCUNN/generic/VolumetricConvolution.cu,torch/lib/THCUNN/generic/VolumetricDilatedConvolution.cu,torch/lib/THCUNN/generic/VolumetricDilatedMaxPooling.cu,torch/lib/THCUNN/generic/VolumetricFractionalMaxPooling.cu,torch/lib/THCUNN/generic/VolumetricFullDilatedConvolution.cu,torch/lib/THCUNN/generic/VolumetricUpSamplingNearest.cu",48.0,4,1,4.906315189,37.0,13425.0,1.0,282011.0,1737.0,22447.55562,0.0,Perfective,0.0,1
pytorch,849dcb8b69eec9736a7e4c64eacd4307cd406421,cb0cee4a3dfe71877337fec688fdfc96f0293730,Ksenija Stanojevic,KsenijaS@users.noreply.github.com,Thu Apr 01 04:11:25 2021 -0700,1617250285.0,"[ONNX] Replace decomposeLinear pre process pass with a symbolic (#53077) (#54866)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54866

Replace decomposeLinear pre process pass with a symbolic

Test Plan: Imported from OSS

Reviewed By: nikithamalgifb

Differential Revision: D27408981

Pulled By: SplitInfinity

fbshipit-source-id: d2d76cab3383122a60df1f356742a33db56adc71",62.0,88.0,"test/jit/test_onnx_export.py,test/onnx/expect/TestOperators.test_linear.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp,torch/onnx/symbolic_opset9.py",5.0,10,2,2.15751341,3.0,11665.0,3.0,2306583.6,10293.0,22771.5,0.0,,0.0,1
pytorch,a2d4d9eca6bfda3283bf2534260c0a8ba201b9e8,cb26661fe4faf26386703180a9045e6ac6d157df,Mike Ruberry,mruberry@devfair044.maas,Wed Jun 24 06:21:31 2020 -0700,1592979691.0,"Throws runtime error when torch.full would infer a float dtype from a bool or integral fill value (#40364)

Summary:
BC-breaking NOTE:

In PyTorch 1.6 bool and integral fill values given to torch.full must set the dtype our out keyword arguments. In prior versions of PyTorch these fill values would return float tensors by default, but in PyTorch 1.7 they will return a bool or long tensor, respectively. The documentation for torch.full has been updated to reflect this.

PR NOTE:

This PR causes torch.full to throw a runtime error when it would have inferred a float dtype by being given a boolean or integer value. A versioned symbol for torch.full is added to preserve the behavior of already serialized Torchscript programs. Existing tests for this behavior being deprecated have been updated to reflect it now being unsupported, and a couple new tests have been added to validate the versioned symbol behavior. The documentation of torch.full has also been updated to reflect this change.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/40364

Differential Revision: D22176640

Pulled By: mruberry

fbshipit-source-id: b20158ebbcb4f6bf269d05a688bcf4f6c853a965",152.0,51.0,"aten/src/ATen/native/TensorFactories.cpp,caffe2/serialize/inline_container.h,test/cpp/api/autograd.cpp,test/distributed/test_c10d.py,test/jit/test_save_load.py,test/jit/test_tracer.py,test/onnx/test_onnx_opset.py,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_autograd.py,test/test_multiprocessing.py,test/test_namedtensor.py,test/test_torch.py,torch/_torch_docs.py,torch/csrc/jit/frontend/builtin_functions.cpp,torch/csrc/jit/frontend/versioned_symbols.cpp,torch/testing/_internal/common_utils.py",19.0,18,4,2.976584349,46.0,52519.0,14.0,1283476.0,3156.0,7678.0,0.0,Feature Addition,0.0,1
pytorch,4cc163f8ec5884a60df20871fb5c9acaa6d6fb4e,cb285080b0f82e8133a2e8a35c9ecdb182458d09,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Dec 04 10:21:55 2020 -0800,1607077315.0,"Added computing matrix condition numbers (linalg.cond) (#45832)

Summary:
This PR adds `torch.linalg.cond` for NumPy compatibility.

Ref https://github.com/pytorch/pytorch/issues/42666.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45832

Reviewed By: ngimel

Differential Revision: D25183690

Pulled By: mruberry

fbshipit-source-id: a727959bfec2bc2dc36df59d9ef79c0534b68194",381.0,1.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,test/test_jit.py,test/test_linalg.py,torch/linalg/__init__.py,torch/overrides.py",6.0,7,3,1.841261513,18.0,34945.0,3.0,81881.16666666667,7199.0,16288.0,0.0,Feature Addition,0.0,1
pytorch,73f1e2d1dcea810d3655d2a225bb39edf83025a0,cb2b5f06c9a58369ccc3c196313618a020f5fcb2,Nikita Shulga,nshulga@fb.com,Wed Jul 28 05:12:37 2021 -0700,1627449157.0,"Revert D29816592: [pytorch][PR] [fix] polygamma n>=1

Test Plan: revert-hammer

Differential Revision:
D29816592 (https://github.com/pytorch/pytorch/commit/b73d7597089c6d27f3b3894e11d9266bd427d1e6)

Original commit changeset: 2c020a6e4c32

fbshipit-source-id: 310c93ade300966366ef04f206a5908fb27745db",105.0,17.0,"aten/src/ATen/native/Math.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/cuda/UnaryGammaKernels.cu,torch/testing/_internal/common_methods_invocations.py",5.0,9,2,2.077346345,8.0,11527.0,2.0,106123.8,14208.0,32501.0,0.0,Corrective,1.0,1
pytorch,7a80fc2ce7568212b4620ef014f9a70f2efc0772,cb37e7a080e17edc6d417a0a3d731fe526cfc801,Peter Bell,peterbell10@live.co.uk,Fri Apr 22 16:34:59 2022 +0100,1650645299.0,"Remove F.pad python implementation

Pull Request resolved: https://github.com/pytorch/pytorch/pull/73433

Approved by: https://github.com/albanD, https://github.com/jbschlosser",140.0,278.0,"test/onnx/expect/TestOperators.test_pad.expect,test/test_fx.py,test/test_jit.py,test/test_nn.py,torch/nn/functional.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",7.0,6,2,1.116036109,48.0,52163.0,7.0,290394.71428571426,2548.0,6008.0,0.0,,0.0,1
pytorch,9bf5e40dfa66a78092a1c0dc953aed05406e1bf4,cb4f6c31482a775d151efd579c0e3c872aee7456,James Reed,jamesreed@fb.com,Tue Dec 19 04:52:36 2017 -0800,1513659156.0,"conv_tbc (#3730)

attempt to rebase

skip conv_tbc in preprocess_nn_functions

Add conv_tbc symbolic

Fix backward issue with dBias

ConvTBC nn wrapper and unit test",130.0,1.0,"aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,tools/autograd/derivatives.yaml,torch/nn/functional.py,torch/onnx/symbolic.py",6.0,10,4,1.320231901,37.0,9047.0,5.0,74919.83333333333,2217.0,24279.85823,0.0,Corrective,1.0,1
pytorch,f926b0a4dc34527eefb300db42962c8ac79b8f00,cb6278899071e31fd5395f2d574357e56a760876,Samantha Andow,samdow@fb.com,Mon Jul 18 18:44:14 2022 -0400,1658169854.0,"[functorch] Allow batch norm with all variations of batching when training=False (pytorch/functorch#958)

* allow batch norm with all variations of batching when training=False

* make running mean/var always call contiguous",57.0,28.0,"functorch/functorch/csrc/BatchRulesNorm.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,1.884264518,1.0,6770.0,4.0,0.5,1173.0,1576.5,0.0,,0.0,1
pytorch,6e11435c4192d90655baffa1225285bc4d18da8b,cbabd8f9f8e850051fb7a059ad7f8c8d743b7f70,Thiago Crepaldi,thiago.crepaldi@microsoft.com,Fri Apr 08 13:47:09 2022 +0000,1649425629.0,"[ONNX] Raise exception for mixed precision input for BatchNormalization

Fixes #72494

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74875
Approved by: https://github.com/garymm",29.0,2.0,"torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset14.py,torch/onnx/symbolic_opset9.py",3.0,2,1,1.491860346,4.0,4960.0,3.0,801681.3333333334,2104.0,5073.0,0.0,Corrective,1.0,1
pytorch,f75ab857b89e334ef43ce19ace35504f31dc8acd,cbb9f08b7107b99da71888887edb88f891a675e9,Kai Arulkumaran,Kaixhin@users.noreply.github.com,Fri Apr 28 21:16:40 2017 +0100,1493414200.0,"Add new init methods gain, eye and dirac (#1172)",267.0,51.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/init.py",3.0,5,3,1.014903454,28.0,4002.0,1.0,12514.0,678.0,8671.817468,0.0,Feature Addition,0.0,1
pytorch,1e4af2b9693f72cab501d4f6ccd223e663760a10,cbcb2b5ad767622cf5ec04263018609bde3c974a,Shen Li,cs.shenli@gmail.com,Wed Jun 12 14:05:12 2019 -0700,1560348312.0,"Delete DDP hooks in Reducer destructor (#21591)

Summary:
Closes https://github.com/pytorch/pytorch/issues/21344

DDP assigns the original module to the first module replica instead of creating a new one. Then, it creates a new Reducer to add post hooks to sync gradients. However, because every reconstructed DDP instance wraps the same original module, all their reducers will add hooks to the same set of variables. This PR deletes DDP hooks from variables when destructing Reducer, trying to make DDP failure recoverable.

pietern kuttas and I discussed the following solutions:

#### Solution 1

Keep `add_post_hook` API intact, and do a `dynamic_cast` in `del_post_hook` to check hook type. If the type matches Reducer's hook, delete it. As pietern mentioned, this will not work if we create multiple DDP instances from the same original model.

#### Solution 2

Use a counter to generate a unique key for every hook in `Function`, and keep them in a map. return the key to the caller of `add_post_hook`, and ask the caller to provide key if it needs to delete the hook.

Con: this would add extra overhead to `add_post_hook` and every `Function` object.

#### Solution 3 [Current implementation]

kuttas suggests that, instead of generating a unique key, directly using the address of the pointer would be better. In order to avoid messing up dereferencing, let `add_post_hook` to return a `uintptr_t`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21591

Differential Revision: D15745706

Pulled By: mrshenli

fbshipit-source-id: e56d2d48de0c65f6667790ab16337eac7f7d8b76",105.0,6.0,"test/test_c10d.py,torch/csrc/autograd/function.h,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h",4.0,6,2,1.518151537,36.0,3893.0,4.0,2619844.0,9357.0,27146.83333,0.0,Feature Addition,0.0,1
pytorch,4428218945e797cfc71a93dbb2d165535ea5a85b,cbdb694f158b8471d71822873c3ac130203cc218,Kulin Seth,kulinseth@gmail.com,Sat May 21 14:33:56 2022 +0000,1653143636.0,"MPS: Fix the memory growing issue and BERT_pytorch network crash fix. (#78006)

Fixes #77753

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78006
Approved by: https://github.com/albanD",59.0,31.0,"aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm",3.0,6,1,1.351644115,1.0,990.0,3.0,289790.6666666667,3523.0,8348.0,0.0,Corrective,1.0,1
pytorch,7da3c938cf7304c8c1a2503f5a230df55e478598,cc03e3a8922d286931bf5468e6dea9e82ff95625,Oleg Khabinov,khabinov@fb.com,Fri Sep 15 18:38:05 2023 +0000,1694803085.0,"[AOTInductor] Do not hardcode directory with .cubin files (#109151)

Reviewed By: frank-wei, chenyang78

Differential Revision: D49081883

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109151
Approved by: https://github.com/chenyang78",57.0,22.0,"torch/_inductor/codegen/aot_runtime/interface.cpp,torch/_inductor/codegen/wrapper.py,torch/csrc/inductor/aot_runtime/interface.h,torch/csrc/inductor/aot_runtime/model.h,torch/csrc/inductor/aot_runtime/model_container.h",5.0,7,1,2.140769324,1.0,2767.0,3.0,106525.2,19746.0,45044.5,0.0,,0.0,1
pytorch,695d98b0bce39d9ddf8f3ef85f7814c21ebdb192,cc0701e5b36874387fbcca1ebb948af0874f3ca0,Bert Maher,bertrand@fb.com,Tue Mar 21 02:48:13 2023 +0000,1679366893.0,"[inductor] Move fx-fusion tests to a separate file (#97028)

They're sort of independent of the rest of inductor, and this makes
them a bit easier to find and marginally faster to run.

Differential Revision: [D44168337](https://our.internmc.facebook.com/intern/diff/D44168337/)

**NOTE FOR REVIEWERS**: This PR has internal Meta-specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D44168337/)!

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97028
Approved by: https://github.com/jansel",155.0,147.0,"test/inductor/test_fx_fusion.py,test/inductor/test_torchinductor.py",2.0,2,1,0.999715251,1.0,7889.0,1.0,226.0,13530.0,31316.5,0.0,Non Functional,0.0,1
pytorch,28bf2f80cf4de8edf0d7f95118563e04774be75b,cc2aad2ef2f6da367d29c46fb0fe1d767daed9ad,BowenBao,bowbao@microsoft.com,Tue Feb 22 18:20:21 2022 -0800,1645554021.0,"[ONNX] Add symbolic for torch.addcmul (#72126)

* Add addcmul op

* Remove required_grad

Pull Request resolved: https://github.com/pytorch/pytorch/pull/73101",16.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.954434003,4.0,14284.0,1.0,340382.0,904.0,2190.5,0.0,Feature Addition,0.0,1
pytorch,f23ff680a434e05e77b23d5d146d9654fa00e4a1,cc32a1e655caf090749bcd584ec19b35057f23a2,Richard Zou,zou3519@users.noreply.github.com,Wed Mar 16 14:21:31 2022 -0400,1647440491.0,[functorch] Add tests for vmapjvpall batch rule coverage (pytorch/functorch#602),211.0,7.0,"functorch/test/discover_coverage.py,functorch/test/test_ops.py",2.0,2,1,0.910460104,1.0,1733.0,2.0,2.5,895.0,1247.0,0.0,Feature Addition,0.0,1
pytorch,24476090df4dbfe898f5040d2cd549e9dc382010,cc62ee229ee8d1ffc3a5955edbdad79d2a49410c,Adam Paszke,adam.paszke@gmail.com,Wed Aug 24 15:38:52 2016 -0700,1472053132.0,Fix torch tests,4.0,4.0,test/test_torch.py,1.0,1,1,0,4.0,2243.0,1.0,84496.0,133.0,2245.028571,0.0,Corrective,1.0,1
pytorch,065fdbd500e024526f879fe6083c7bd80892f5f8,cc6b046f4833561a03625def4d0a71b5dd2dd7b0,li-roy,8813817+li-roy@users.noreply.github.com,Wed Jun 20 16:53:06 2018 -0700,1529513586.0,"Implement flatten function (#8578)

* Implement flatten function

* address comments

* allow start_dim=end_dim

* undo submodule change",97.0,5.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,torch/_tensor_docs.py,torch/_torch_docs.py",7.0,9,4,2.274897356,40.0,19392.0,6.0,285520.0,2743.0,25238.85823,0.0,Feature Addition,0.0,1
pytorch,48436ac124c0ec10cf9898ba4ce0595c121184e4,cc70a33e747ef38c5242476e34af63086f5600aa,Will Feng,willfeng@fb.com,Wed Jan 03 22:21:23 2018 -0500,1515018083.0,Windows fix for #4312,10.0,6.0,"aten/src/TH/vector/AVX2.h,aten/src/TH/vector/avx_mathfun.h",2.0,4,1,0.337290067,2.0,731.0,1.0,17856.0,403.0,2222.5,0.0,Corrective,1.0,1
pytorch,645a3e9a92335d76edd1bfd96e895483a0b95071,cc7a28d7271ad658aacfe034789b182e8a2ebdc2,Iurii Zdebskyi,iuriiz@devfair004.maas,Fri Mar 19 16:26:08 2021 -0700,1616171168.0,"Refactor Unary Ops tests (#49712)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49712

Test Plan: Imported from OSS

Reviewed By: zou3519

Differential Revision: D25673712

Pulled By: izdeby

fbshipit-source-id: 4420d5d129026195097d914e410b75b144bea795",159.0,118.0,"aten/src/ATen/native/cuda/ForeachUnaryOp.cu,test/test_foreach.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.159082871,2.0,5549.0,3.0,521540.3333333333,9920.0,21976.0,0.0,Perfective,0.0,1
pytorch,cc70a33e747ef38c5242476e34af63086f5600aa,cc9dc3f3434a7417526ab965a3ab77a9a28023e2,Tongzhou Wang,SsnL@users.noreply.github.com,Thu Jan 04 03:45:05 2018 -0500,1515037505.0,add lock for SynchronizedSeedDataset; add additional os level close stderr for tests that launch failing process (#4463),8.0,4.0,test/test_dataloader.py,1.0,1,1,0,33.0,520.0,1.0,121696.0,2240.0,24318.85823,0.0,Feature Addition,0.0,1
pytorch,7cdd39b39316173487c0c4cfbb60aef0cb645757,cca909645f9c3528469381e4236a468667258a1e,CaoE,e.cao@intel.com,Thu Sep 29 01:16:16 2022 +0000,1664414176.0,"Add bfloat16 support for lerp on CPU (#84327)

### Description
Add bfloat16 support for lerp on CPU

### Testing
single core:
<html>
<body>
<!--StartFragment-->

op | shape |fp32 forward/ms|bf16 forward/s|fb32 backward/s| bf16 backward/s
-- | -- | -- | -- | -- | --
lerp (tensor) | [10, 128, 10, 124] | 0.005489 | 0.000613 | 0.006658 | 0.003385
Â  | [10, 128, 20, 124] | 0.011057 | 0.001204 | 0.016032 | 0.007869
Â  | [10, 128, 30, 124] | 0.016691 | 0.001954 | 0.025549 | 0.012823
Â  | Â  | Â  | Â  | Â  | Â 
lerp (scalar) | [10, 128, 10, 124] | 0.001096 | 0.000507 | 0.002024 | 0.001479
Â  | [10, 128, 20, 124] | 0.00247 | 0.000997 | 0.005468 | 0.002907
Â  | [10, 128, 30, 124] | 0.004178 | 0.001513 | 0.009775 | 0.004859

<!--EndFragment-->
</body>
</html>

single socket (28cores):
<html>
<body>
<!--StartFragment-->

op | shape | fp32 forward/s| bf16 forward/s| fb32backward/s| bf16 backward/s
-- | -- | -- | -- | -- | --
lerp (tensor) | [10, 128, 10, 124] | 0.000236 | 3.93E-05 | 0.000494 | 0.000235
Â  | [10, 128, 20, 124] | 0.000525 | 7.39E-05 | 0.002485 | 0.000638
Â  | [10, 128, 30, 124] | 0.000801 | 0.000121 | 0.004235 | 0.001529
Â  | Â  | Â  | Â  | Â  | Â 
lerp (scalar) | [10, 128, 10, 124] | 5.90E-05 | 3.32E-05 | 0.000129 | 0.000116
Â  | [10, 128, 20, 124] | 0.000155 | 5.87E-05 | 0.000368 | 0.000206
Â  | [10, 128, 30, 124] | 0.000324 | 9.04E-05 | 0.001322 | 0.000313

<!--EndFragment-->
</body>
</html>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84327
Approved by: https://github.com/frank-wei",80.0,4.0,"aten/src/ATen/native/cpu/LerpKernel.cpp,test/test_binary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,0.881117707,7.0,22485.0,3.0,3336910.6666666665,7810.0,18337.5,0.0,Feature Addition,0.0,1
pytorch,c379d6283aa5394ad91cf273c98d53e7fa4304c4,cce2c52b0b932757d17086cfb5229537f7e1fb54,kshitij12345,kshitijkalambarkar@gmail.com,Wed Aug 09 03:39:33 2023 +0000,1691552373.0,"[pt2] support vmap (#101707)

Teach dynamo about `vmap`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/101707
Approved by: https://github.com/zou3519",1064.0,333.0,"aten/src/ATen/functorch/BatchRulesHelper.cpp,aten/src/ATen/functorch/BatchRulesViews.cpp,aten/src/ATen/functorch/BatchedTensorImpl.cpp,aten/src/ATen/functorch/BatchedTensorImpl.h,functorch/experimental/__init__.py,test/dynamo/test_dynamic_shapes.py,test/dynamo/test_higher_order_ops.py,test/functorch/test_vmap.py,torch/_dynamo/allowed_functions.py,torch/_dynamo/skipfiles.py,torch/_dynamo/variables/higher_order_ops.py,torch/_dynamo/variables/torch.py,torch/_functorch/apis.py,torch/_functorch/autograd_function.py,torch/_functorch/deprecated.py,torch/_functorch/eager_transforms.py,torch/_functorch/utils.py,torch/_functorch/vmap.py,torch/func/__init__.py",19.0,14,4,2.343943249,1.0,14280.0,16.0,4440903.263157895,18449.0,41815.0,0.0,,0.0,1
pytorch,6134ac17baa95bd1b24f008930a1a5496bbce9a7,cce5982c4ceb77a0797e8bd4c717ebfec0681eab,Iurii Zdebskyi,iuriiz@devfair004.maas,Tue Sep 08 02:55:40 2020 -0700,1599533740.0,"Add unary ops: exp and sqrt (#42537)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42537

[First PR: Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar)](https://github.com/pytorch/pytorch/pull/41554).

**Motivation**
[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)
Current PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.
As an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).
In order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and itâs FusedAdam optimizer.

**Current API restrictions**
- List can't be empty (will fixed in upcoming PRs).
- All tensors in the list must have the same dtype, device and size.

**Broadcasting**
At this point we don't support broadcasting.

**What is 'Fast' and 'Slow' route**
In particular cases, we cant process an op with a fast list CUDA kernel. Still, we can do with a regular for-loop where the op will be applied to each tensor individually through the dispatch mechanisms. There are a few checks that decide whether the op will be performed via a 'fast' or 'slow' path.
To go the fast route,
- All tensors must have strided layout
- All tensors must be dense and not have overlapping memory
- The resulting tensor type must be the same.

----------------
**In this PR**
Adding APIs:
```
torch._foreach_exp(TensorList tl1)
torch._foreach_exp_(TensorList tl1)
torch._foreach_sqrt(TensorList tl1)
torch._foreach_sqrt_(TensorList tl1)
```

**Tests**
Tested via unit tests

**TODO**
1. Properly handle empty lists
2. Properly handle bool tensors

**Plan for the next PRs**
1. APIs
- Pointwise Ops

2. Complete tasks from TODO
3. Rewrite PyTorch optimizers to use for-each operators for performance gains.

Test Plan: Imported from OSS

Reviewed By: cpuhrsch

Differential Revision: D23331889

Pulled By: izdeby

fbshipit-source-id: 8b04673b8412957472ed56361954ca3884eb9376",604.0,19.0,"aten/src/ATen/native/ForeachOpsKernels.cpp,aten/src/ATen/native/ForeachUtils.h,aten/src/ATen/native/cuda/ForeachFunctors.cuh,aten/src/ATen/native/cuda/ForeachPointwiseOp.cu,aten/src/ATen/native/cuda/ForeachUnaryOp.cu,aten/src/ATen/native/native_functions.yaml,test/test_foreach.py,tools/codegen/model.py",8.0,8,3,2.440440493,12.0,9153.0,2.0,104992.66666666669,4914.0,11363.0,0.0,Corrective,1.0,1
pytorch,d8b2e5d0916a85e603c9bda6def5cd20cbe94fa6,ccf4dc15250534e6906aaca696f94b430a4da443,Richard Zou,zou3519@users.noreply.github.com,Mon Dec 18 17:28:53 2017 -0500,1513618133.0,"Add reduce arg to BCELoss (#4231)

* Add reduce arg to BCELoss

* Fix test precision

* reduce keyword for BCELoss in derivatives.yaml",188.0,29.0,"aten/src/ATen/nn.yaml,aten/src/THCUNN/BCECriterion.cu,aten/src/THCUNN/generic/BCECriterion.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/BCECriterion.c,aten/src/THNN/generic/THNN.h,test/common_nn.py,test/test_nn.py,tools/autograd/derivatives.yaml,torch/nn/functional.py,torch/nn/modules/loss.py",11.0,13,4,2.784644328,37.0,13234.0,2.0,52259.36363636364,2213.0,24261.35823,0.0,Corrective,1.0,1
pytorch,1486d880b0be575bc1ebdf23204b1553b2fb1b2b,cd0929aa5e5ac52a358fc21c133e4bebca61bdd5,Sam Gross,sgross@fb.com,Wed Sep 07 22:51:44 2016 -0700,1473288704.0,"Use chainer-style constructor for Conv2d

 * Conv2d, MaxPool2d, and AvgPool2d have one argument for each of ksize,
   stride, and pad. This argument can be either a single number or a
   tuple of (h, w)",99.0,67.0,"test/common_nn.py,test/test_legacy_nn.py,test/test_nn.py,torch/nn/modules/conv.py,torch/nn/modules/pooling.py,torch/nn/modules/utils.py",6.0,4,2,2.395416178,4.0,1876.0,2.0,286844.2,23.0,334.75,0.0,,0.0,1
pytorch,1018b238acf2ecfc836484f3153c2864e9f4e963,cd3bbc9dfdb3dcab2d3fac4fb1819f84e982c107,Martin Raison,raison@fb.com,Wed Mar 22 16:25:48 2017 -0700,1490199948.0,"more operations and optimizations (hspmm, reorder, ...)",380.0,170.0,"test/test_sparse.py,torch/csrc/Module.cpp,torch/csrc/ModuleSparse.cpp,torch/csrc/generic/methods/SparseTensor.cwrap,torch/lib/THC/generic/THCStorage.cu,torch/lib/THCS/THCSTensor.cu,torch/lib/THCS/generic/THCSTensor.c,torch/lib/THCS/generic/THCSTensor.cu,torch/lib/THCS/generic/THCSTensor.h,torch/lib/THCS/generic/THCSTensorMath.cu,torch/lib/THCS/generic/THCSTensorMath.h,torch/lib/THS/generic/THSTensorMath.c,torch/lib/THS/generic/THSTensorMath.h,torch/optim/adagrad.py",14.0,13,2,2.39076585,29.0,3843.0,7.0,517.8571428571429,335.0,15028.107,0.0,,0.0,1
pytorch,04fce5eca65bc4182bb2da85cacbd124127978a9,cd3e067e4654daa0edb5f4c9db99ffce3b38c64b,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Fri Jul 13 02:24:10 2018 -0700,1531448650.0,"Add reversed(torch.Tensor) (#9216)

Summary:
Closes https://github.com/pytorch/pytorch/issues/3376
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9216

Differential Revision: D8753933

Pulled By: soumith

fbshipit-source-id: 5dac9b8b11ff34a205b6478db99b02fda8bd9cce",17.0,0.0,"test/test_torch.py,torch/tensor.py",2.0,2,2,0.977417818,40.0,8445.0,2.0,193876.5,2921.0,6842.333333,0.0,Feature Addition,0.0,1
pytorch,c65f332da47eb9bc76aefc50122cae2630fff2cc,cd51d2a3ecc8ac579bee910f6bafe41a4c41ca80,ankitaS11,ankitalrm@gmail.com,Tue Nov 02 03:26:46 2021 -0700,1635823606.0,"Adding OpInfo for `logical_or`, `logical_and`, `logical_xor` (#67178)

Summary:
This PR addresses https://github.com/pytorch/pytorch/issues/54261.

This adds OpInfos for binary logical element wise operators. This is my first PR in OpInfos to PyTorch, looking forward to suggestions and any feedback.

cc: mruberry krshrimali

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67178

Reviewed By: jbschlosser

Differential Revision: D32057889

Pulled By: mruberry

fbshipit-source-id: 7e670260af6b478dba9d6e8d77de4df1b6d0b5d1",27.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,12335.0,1.0,15338.0,16759.0,39293.0,0.0,Feature Addition,0.0,1
pytorch,126a1cc398de64af4139f0322ff17b3b8c9d2411,cd82b2b869b087621785d92577974a35d54569b5,Adam Paszke,adam.paszke@gmail.com,Tue Dec 13 21:06:35 2016 +0100,1481663195.0,Implement comparison and logical operators for tensors,231.0,81.0,"test/common.py,test/common_nn.py,test/test_legacy_nn.py,test/test_nn.py,test/test_torch.py,torch/autograd/variable.py,torch/backends/cudnn/rnn.py,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/Bilinear.py,torch/legacy/nn/Concat.py,torch/legacy/nn/ConcatTable.py,torch/legacy/nn/CosineDistance.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/FlattenTable.py,torch/legacy/nn/JoinTable.py,torch/legacy/nn/Max.py,torch/legacy/nn/Min.py,torch/legacy/nn/SelectTable.py,torch/legacy/nn/SpatialConvolution.py,torch/legacy/nn/SpatialConvolutionLocal.py,torch/legacy/nn/SpatialFullConvolution.py,torch/legacy/nn/VolumetricConvolution.py,torch/nn/functions/linear.py,torch/tensor.py",24.0,9,2,3.607323446,19.0,9317.0,1.0,6702.0,339.0,2481.012243,0.0,,0.0,1
pytorch,4dc9795ebf75f65fa08896377bca2a04c885e8fa,cd89bf77c8ecf650e4e415a9d03b117f040e3e56,Jiong Gong,jiong.gong@intel.com,Thu Aug 15 03:13:49 2024 -0700,1723691629.0,"[inductor][cpp][gemm] easy: adjust indentation of template, var renaming etc. (#133312)

Indent the template instructions separately from the generated code, for readability. Also, renaming M0,N0,K0 to Mr,Nr,Kr (""r"" meaning ""register"") to consistent naming.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/133312
Approved by: https://github.com/Skylion007, https://github.com/leslie-fang-intel
ghstack dependencies: #132729, #132730",120.0,120.0,"torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_micro_gemm.py,torch/_inductor/codegen/cpp_prefix.h",3.0,3,1,1.13039614,2.0,2582.0,3.0,278094.6666666667,32872.0,83168.5,0.0,,0.0,1
pytorch,e7a3bbce89631942814ae4133a8da63f558ac9c1,cdd5d16489fa4563b16ddd3ef338dbeb37063d80,Masaki Kozuki,mkozuki@nvidia.com,Fri Nov 05 18:19:00 2021 -0700,1636136340.0,"[Foreach] Implement L1&L2 norm (#62646)

Summary:
Implement L1 & L2 norm in fast path with the reference of [nvidia/apex](https://github.com/NVIDIA/apex/blob/master/csrc/multi_tensor_l2norm_kernel.cu).
When `ord` is neither 1 nor 2, then slow path is chosen.

Related: https://github.com/pytorch/pytorch/issues/58833

cc ptrblck mcarilli ngimel

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62646

Reviewed By: malfet

Differential Revision: D32173421

Pulled By: ngimel

fbshipit-source-id: 14b7544601658a979b83509df351e1848ded7675",233.0,3.0,"aten/src/ATen/native/ForeachOpsKernels.cpp,aten/src/ATen/native/cuda/ForeachReduceOp.cu,aten/src/ATen/native/cuda/MultiTensorApply.cuh,aten/src/ATen/native/cuda/block_reduce.cuh,aten/src/ATen/native/native_functions.yaml,test/test_foreach.py,torch/testing/_internal/common_methods_invocations.py",7.0,9,3,1.344458673,12.0,24494.0,6.0,6080434.833333333,16898.0,39697.5,0.0,,0.0,1
pytorch,e2e9d1572617a151ba04e086ce8baa171696fa2a,cddd0db241a3b8df930284fd29523da9d28b1f2c,Nikita Shulga,nikita.shulga@gmail.com,Wed Sep 20 23:37:55 2023 -0700,1695253075.0,"Add `finfo` properties for float8 dtypes (#109744)

Add float8 finfo checks to `test_type_info.py`
Fixes https://github.com/pytorch/pytorch/issues/109737
Pull Request resolved: https://github.com/pytorch/pytorch/pull/109744
Approved by: https://github.com/drisspg",79.0,11.0,"aten/src/ATen/Dispatch.h,test/test_type_info.py,torch/csrc/TypeInfo.cpp",3.0,6,3,1.367381018,8.0,1061.0,3.0,1151632.6666666667,19940.0,45509.0,0.0,Corrective,1.0,1
pytorch,5d5990fc49260cfd717df629017720109c89f1d4,cddeceb6b6a64ad5dae7b19532fc5f7bfbe704fa,Shunting Zhang,shunting@fb.com,Thu Sep 14 18:34:28 2023 -0700,1694716468.0,"[inductor] scale down RBLOCK for occupancy (#109275)

For large reduction (with large xnumel and rnumel), we potentially need run large number of thread blocks. Occupancy matters here since with larger occupancy we can run more blocks on each SM and we may need less number of waves to run the entire kernel on the GPU.  Number of registers used by each thread can limit the occupancy. For A100, it's safe to say that register usage does not limit occupancy only if each thread use <= 32 registers. This PR leverage this observation and reduce RBLOCK (thus reduce registers used by each thread) if thread usage limit occupancy for large reduction.

The scenario mentioned can happen for the softmax kernel used in transformers. Here are some results get from devgpu:
- PLBartForCausalLM we improve from 1.88x (58.7ms) to 2.00x (55.82ms)
- TrOCRForCausalLM we improve from 1.45x (92.9ms) to 1.51x (89.12ms)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/109275
Approved by: https://github.com/jansel",79.0,12.0,"torch/_inductor/config.py,torch/_inductor/triton_heuristics.py",2.0,2,1,0.209059806,1.0,1587.0,2.0,277562.0,19810.0,45208.0,0.0,Perfective,0.0,1
pytorch,fb18c29486a47d6a2cad853c05f4617384d3b05f,cdf4a80cc111b210f9ab9448da5aeea2007a0171,Kshiteej K,kshitijkalambarkar@gmail.com,Wed Dec 14 20:35:58 2022 +0000,1671050158.0,"replace skipIf with xfailif (#90368)

Replace skips with xfails.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90368
Approved by: https://github.com/zou3519",6.0,5.0,test/functorch/test_ops.py,1.0,2,1,0,1.0,1891.0,1.0,15305.0,10552.0,24093.5,0.0,,0.0,1
pytorch,7d78a6fcdd0c3c32746e357dc3b183343cb6ff04,cdf5e2ae86b61a02b4cadaec542de3c8e85d472d,Guilherme Leobas,guilhermeleobas@gmail.com,Fri Sep 11 17:13:43 2020 -0700,1599844423.0,"add typing annotations for a few torch.utils.* modules (#43806)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/43431. Depends on [gh-43862](https://github.com/pytorch/pytorch/pull/43862) (EDIT: now merged)

Modules:
- torch.utils.mkldnn
- torch.utils.mobile_optimizer
- torch.utils.bundled_inputs

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43806

Reviewed By: gmagogsfm

Differential Revision: D23635151

Pulled By: SplitInfinity

fbshipit-source-id: a85b75a7927dde6cc55bcb361f8ff601ffb0b2a1",39.0,25.0,"mypy.ini,tools/pyi/gen_pyi.py,torch/_C/__init__.pyi.in,torch/_C/_nn.pyi.in,torch/jit/_serialization.py,torch/utils/bundled_inputs.py,torch/utils/tensorboard/writer.py",7.0,7,2,2.195552362,5.0,3205.0,5.0,3105441.571428572,5040.0,11551.5,0.0,Corrective,1.0,1
pytorch,903313741d0d8672ed039da1dac0f911c96b319f,cdf866da96d4634220692a701baccd2022cbdeeb,vfdev,vfdev.5@gmail.com,Tue Sep 28 16:27:29 2021 +0200,1632846449.0,"[functorch] Added cross batching rule (pytorch/functorch#144)

* Added cross batching rule
Description:
- Added cross batching rule
- Updated tests
- Fixed issue in test/common_utils.py with repeated number 3 in dims. Set it to 4 and 2 to avoid special value for cross op

* Covered batch_size=3 corner case

* Added batch_size = 3 specific test for cross op",57.0,9.0,"functorch/functorch/csrc/BatchRulesModules.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,4,1,1.384536096,1.0,4326.0,4.0,0.25,389.0,566.5,0.0,Corrective,1.0,1
pytorch,a24b17248f307d3482f3a0e5496c53f35b355c34,ce05b7a3244ae7a61e989c9cd4eabf6d668ecbb0,Rohan Varma,rvarm1@fb.com,Sun Apr 18 21:08:45 2021 -0700,1618780125.0,"[c10d] Remove deprecated use of torch.LongTensor, torch.ByteTensor (#55861)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55861

APIs such as torch.LongTensor and torch.ByteTensor are deprecated and
the recommended API is torch.tensor(args, dtype=...). Use this API in
distributed_c10d.
ghstack-source-id: 126777875

Test Plan: CI

Reviewed By: pbelevich

Differential Revision: D27726600

fbshipit-source-id: 07eb8168d93697593589002c93c3903ce29431ef",17.0,10.0,torch/distributed/distributed_c10d.py,1.0,2,1,0,2.0,2670.0,1.0,351908.0,10966.0,24185.0,0.0,,0.0,1
pytorch,3078233e9a82e9c98f932a6e36f960518558d9f0,ce0fd095a826f7a547f3d37a5c6b7f26700c9e15,Edvard Ghazaryan,edvardg@fb.com,Sat Mar 13 01:49:56 2021 -0800,1615600196.0,"Implemented embedding_bag for SR (#52429)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52429

Implemented embedding_bag for supporting out version in SR

Befor:Milliseconds per iter: 1.15443. Iters per second: 866.226

 After:  Milliseconds per iter: 1.14791. Iters per second: 871.149

Test Plan:
buck test caffe2/test:nn
buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest

Reviewed By: hlu1

Differential Revision: D26089498

fbshipit-source-id: c9ba7068d5aa696c8f37a4846d8e80c6379538d2",470.0,203.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/EmbeddingBag.h,benchmarks/static_runtime/test_scripts.h,benchmarks/static_runtime/test_static_runtime.cc,torch/csrc/jit/runtime/static/ops.cpp",5.0,11,3,1.501694209,7.0,2525.0,4.0,540384.75,9759.0,21577.0,0.0,,0.0,1
pytorch,ec2b0fb5f8954752f49ace82038aab54aa98ccb2,ce8a5876723e5b4a4daeaeeca0df74d6d821cb9a,Richard Zou,zou3519@gmail.com,Thu Apr 07 23:33:51 2022 -0700,1649374431.0,[functorch] silu batch rule,13.0,0.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/test/test_vmap.py",2.0,4,1,0.391243564,1.0,4617.0,2.0,1.0,947.0,1304.0,0.0,,0.0,1
pytorch,79ac2120ba7bbc6a14ea9464af3530bc6fc0c8ec,ce92cf9bd14b3f829a2d1472ee0c1ae807b6c7ad,Pieter Noordhuis,pietern@fb.com,Fri Apr 05 16:04:43 2019 -0700,1554480283.0,"Add tests for reducer class (#18845)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18845

This adds a few CPU only test cases for the reducer class.

Reviewed By: mrshenli

Differential Revision: D14768432

fbshipit-source-id: c008a52206826304e634a95bc14167ed94c97662",213.0,66.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h",4.0,5,2,1.387532549,2.0,2835.0,3.0,530257.75,7920.0,23935.33333,0.0,Feature Addition,0.0,1
pytorch,719589c9bf1eec6673a1f593eac0a89d0c19f534,cef776bcd1c67906f91578817e103a044cf3e0a2,Jiong Gong,jiong.gong@intel.com,Wed May 29 01:58:59 2024 -0700,1716947939.0,"[inductor][cpp] GEMM template (infra and fp32) (#124021)

This PR adds the Cpp template infrastructure and the initial FP32 gemm template. See RFC https://github.com/pytorch/pytorch/issues/125683 for more background info.
1. Cpp template infrastructure
Similar template abstractions as the CUTLASS template, i.e., `CppTemplate`, `CppTemplateKernel`, `CppTemplateBuffer`. The MicroGemm micro-kernel abstraction that can be used by Cpp GEMM templates.
2. Initial FP32 gemm template
This involves a GEMM template implementation `CppPackedGemmTemplate` that supports GEMM with constant weight (`B`) requiring `N` to be a multiple of register blocking while allows the static or dynamic sizes for the `M` (batch dim) of `A`. The `B` matrix would be prepacked. This is a typical setting for inference workloads. The template handles the thread decomposition (via `thread_blocking`) and cache blocking (via `cache_blocking`). Then it invokes `CppMicroGemm` which handles register blocking, instruction selection, and other CPU architecture-specific optimizations. A `CppMicroGemmFP32Vec` micro-kernel implementation is provided for fp32 matmuls implemented with ATen vec abstraction.
3. Correctness and performance
The changes have been validated with fp32 inference on the three benchmark suites (torchbench, huggingface and timm_models) with both static shape and dynamic shapes. Since it is an initial implementation, we are still working on further performance improves with follow-up PRs including the optimizations in kernels as well as fusions. The perf gains are only observed from a selective number of models compared to the ATen kernels which are implemented with MKL. The perf gains are more obvious with dynamic shapes since MKL only supports packed gemm for static shapes. Below are details.

Static shapes
| Benchmark | torchbench | huggingface | timm_models |
|------------|-------------|--------------|--------------|
| Multi-threaded (baseline) | 1.47x | 1.36x | 1.91x |
| Multi-threaded (max-autotune) | 1.47x | 1.36x | 1.92x |
| Single-threaded (baseline) | 1.56x | 1.19x | 1.51x |
| Single-threaded (max-autotune) | 1.56x | 1.19x | 1.52x |

Key models being sped up:
drq: 1.14x
soft_act: 1.12
cait_m36_384: 1.18x

Dynamic shapes
| Benchmark | torchbench | huggingface | timm_models |
| --- | --- | --- | --- |
| Multi-threaded (baseline) | 1.43x | 1.28x | 1.85x |
| Multi-threaded (max-autotune) | 1.47x | 1.28x | 1.85x |
| Single-threaded (baseline) | 1.55x | 1.20x | 1.51x |
| Single-threaded (max-autotune) | 1.56x | 1.19x | 1.53x |

Key models being sped up:
BERT_pytorch: 1.22x
pyhpc_turbulent: 1.13x
soft_actor_critic: 1.77x
BlenderbotForCausalLM: 1.09x
cait_m36_384: 1.17x

Differential Revision: [D57585365](https://our.internmc.facebook.com/intern/diff/D57585365)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/124021
Approved by: https://github.com/jansel",1605.0,14.0,"test/inductor/test_cpu_select_algorithm.py,torch/_inductor/codegen/cpp.py,torch/_inductor/codegen/cpp_gemm_template.py,torch/_inductor/codegen/cpp_micro_gemm.py,torch/_inductor/codegen/cpp_prefix.h,torch/_inductor/codegen/cpp_template.py,torch/_inductor/codegen/cpp_template_kernel.py,torch/_inductor/codegen/cpp_utils.py,torch/_inductor/config.py,torch/_inductor/decomposition.py,torch/_inductor/ir.py,torch/_inductor/kernel/mm.py,torch/_inductor/mkldnn_lowerings.py,torch/_inductor/select_algorithm.py,torch/_inductor/utils.py",15.0,6,2,3.094112921,3.0,18937.0,4.0,129437.93333333332,29222.0,71641.5,0.0,Corrective,0.0,1
pytorch,c36b31d5302d31746f3f3bd64ed8d9acd8e36155,cf1b494afd0d0368c22e70e93d91da3d9fe1ddce,Oleg Khabinov,khabinov@fb.com,Thu Oct 05 10:17:05 2023 +0000,1696501025.0,"[AOTInductor] Store loaded kernels in the model (#110554)

Defining kernels as static vars is problematic for subsequent model loading on non-default CUDA devices.

Assuming those kernels were loaded in context of the device #0, so, they are not nullptr anymore, therefore kernels won't work on devices other than the device #0.

This change makes devices remembered at model level in AOT mode.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110554
Approved by: https://github.com/chenyang78, https://github.com/desertfire",100.0,27.0,".ci/pytorch/test.sh,test/inductor/test_aot_inductor.py,torch/_inductor/codegen/wrapper.py,torch/csrc/inductor/aoti_runtime/model.h",4.0,10,3,1.353532872,1.0,4399.0,3.0,37259.0,20412.0,46694.0,0.0,,0.0,1
pytorch,c8a608b197fabb9f572e46df3459a8eeeafa5411,cf2d15bf84d70f40b15435ebc1fdc7c23273eed6,Elias Ellison,eellison@devfair044.h1.fair,Wed Sep 08 01:19:14 2021 -0700,1631063954.0,"Add support for slice, selec twith int, index_select (#63365)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/63365

Test Plan: Imported from OSS

Reviewed By: driazati

Differential Revision: D30738144

Pulled By: eellison

fbshipit-source-id: 7e0c572209bdc6e62ecb4fd1f06f80291de69803",85.0,13.0,"test/test_ops.py,torch/csrc/jit/runtime/symbolic_shape_registry.cpp,torch/testing/_internal/common_methods_invocations.py",3.0,7,2,1.296580561,2.0,11217.0,1.0,3.0,15251.0,34958.0,0.0,Feature Addition,0.0,1
pytorch,75d379c8c017dc102846ad5e7faa04d67821f7d8,cf2de77dd313a596beae7909c64e73f3a46e2f77,Richard Zou,zou3519@gmail.com,Fri Apr 15 17:43:52 2022 -0700,1650044632.0,[functorch] Fix tolerance override,1.0,1.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1536.0,1.0,0.0,973.0,1354.0,0.0,Corrective,1.0,1
pytorch,2b41bf40c5888c5f44fe2b1d20fdb25e9c7502a8,cfa6162e5e5bafcb1e89bd14b64ae336f57fcdbe,Mike Ruberry,mruberry@fb.com,Thu Sep 09 17:02:03 2021 -0700,1631206923.0,"Reverts cat and stack warning when out= is not the expected shape (#64714)

Summary:
These warnings are being thrown too aggressively at the moment. See https://github.com/pytorch/pytorch/issues/64709 for a follow-up to reenable them once internal call sites are reviewed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64714

Reviewed By: ngimel

Differential Revision: D30822965

Pulled By: mruberry

fbshipit-source-id: 3ad7c92d381d42ac6187ed84afab477c579a8f35",34.0,6.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/Shape.cu,torch/testing/_internal/common_methods_invocations.py",3.0,8,2,1.376356784,7.0,12830.0,2.0,140750.0,15319.0,35070.5,0.0,,0.0,1
pytorch,7edd451a4e81080562d8e2daa1e962bf7fbe4a9b,cfc1d929756c948195ae40527db8f789efed2d8a,Thomas Viehmann,tv.github@beamnet.de,Sun May 13 03:39:37 2018 +0200,1526182777.0,"Implement ellipses ('...') and diagonals (e.g. 'ii->i') in einsum. (#7173)

This brings the two most important missing numpy einsum features
to toch.einsum.",200.0,89.0,"aten/src/ATen/native/Linear.cpp,test/test_torch.py,torch/_torch_docs.py",3.0,6,3,0.697951825,39.0,12995.0,3.0,455984.3333333333,642.0,3606.5,0.0,Feature Addition,0.0,1
pytorch,39829c16709271f4573e593a81d6c1edd62614e1,cfd94c481ef3aa0b77a64b7d55ee07f54c166e8e,Tongzhou Wang,SsnL@users.noreply.github.com,Mon Mar 26 18:09:38 2018 -0400,1522087778.0,"Add precision matrix to MultivariateNormal (#5998)

Also changed some .contiguous().view(*) to .reshape(*).",78.0,27.0,"test/test_distributions.py,torch/distributions/multivariate_normal.py",2.0,3,2,0.822404226,8.0,3533.0,1.0,103506.0,531.0,2462.5,0.0,Feature Addition,0.0,1
pytorch,ccb1de3595fad0d8dc1f9269130dede16547fb77,cff84871ce5fd78fbb8b59a2adf3e1fdae3f257b,Khushi Agrawal,khushiagrawal411@gmail.com,Mon Nov 27 14:45:44 2023 +0000,1701096344.0,"[reland][opinfo][fix] conv3d & fix conv{1, 2}d for neg dilation|groups & add ErrorInputs for conv ops (#114589)

Previous PR: #113885

Pull Request resolved: https://github.com/pytorch/pytorch/pull/114589
Approved by: https://github.com/lezcano",231.0,26.0,"aten/src/ATen/native/Convolution.cpp,test/functorch/test_ops.py,test/test_mps.py,torch/testing/_internal/common_methods_invocations.py",4.0,9,3,0.367691789,10.0,37872.0,1.0,25964.0,22407.0,51100.0,0.0,Corrective,1.0,1
pytorch,5fdcc20d8d96a6b42387f57c2ce331516ad94228,cffad597ea620600472c509b642a9ad03a56fdbe,Xiang Gao,qasdfgtyuiop@gmail.com,Sat Nov 27 02:57:44 2021 -0800,1637981864.0,"Tune test_reference_numerics_normal (#68019)

Summary:
Fixes #{issue number}

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68019

Reviewed By: albanD

Differential Revision: D32482535

Pulled By: mruberry

fbshipit-source-id: 48300a5c6a4484fb81789f9049d3f08272d9f31c",96.0,40.0,"test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.873981048,2.0,15373.0,2.0,740844.5,17320.0,40697.0,0.0,Corrective,1.0,1
pytorch,2ea3c24c06940595c6f8ed15b26ad3cd8a4d335f,d01302431c41691ee29ccf2b710cc5738128993e,Jeffrey Wan,jw3468@fb.com,Fri Apr 23 02:42:20 2021 -0700,1619145740.0,"Enable fast gradcheck for real inputs and outputs (#55237)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/55237

In this PR, we reenable fast-gradcheck and resolve misc issues that arise:
Before landing this PR, land #55182 so that slow tests are still being run periodically.

Bolded indicates the issue is handled in this PR, otherwise it is handled in a previous PR.

**Non-determinism issues**:
- ops that do not have deterministic implementation (as documented https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms)
  - test_pad_cuda (replication_pad2d) (test_nn)
  - interpolate (test_nn)
  - cummin, cummax (scatter_add_cuda_kernel) (test_ops)
  - test_fn_gradgrad_prod_cpu_float64 (test_ops)

Randomness:
  - RRelu (new module tests) - we fix by using our own generator as to avoid messing with user RNG state (handled in #54480)

Numerical precision issues:
- jacobian mismatch: test_gelu (test_nn, float32, not able to replicate locally) - we fixed this by disabling for float32 (handled in previous  PR)
- cholesky_solve (test_linalg): #56235 handled in previous PR
- **cumprod** (test_ops) - #56275 disabled fast gradcheck

Not yet replicated:
 - test_relaxed_one_hot_categorical_2d (test_distributions)

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D27920906

fbshipit-source-id: 894dd7bf20b74f1a91a5bc24fe56794b4ee24656",85.0,35.0,"test/test_nn.py,test/test_ops.py,torch/autograd/gradcheck.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",5.0,5,2,2.097534764,44.0,26467.0,5.0,183769.8,11181.0,24719.0,0.0,Corrective,1.0,1
pytorch,b154761547f02001c871f735f847cf90c813c523,d017e1798f210e987743963326a7bebf60b463d6,Alican Bozkurt,alicanb@gmail.com,Thu Jul 12 15:10:02 2018 -0700,1531408202.0,"add erfc

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/9366

Differential Revision: D8816768

Pulled By: soumith

fbshipit-source-id: 7d709f932cf156a2e7ec71c710837beb7f647d66",115.0,1.0,"aten/doc/Functions.h,aten/doc/Tensor.h,aten/doc/Type.h,aten/src/ATen/Declarations.cwrap,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h,aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.cpp,aten/src/THC/THCNumerics.cuh,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathPointwise.h,docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_cuda.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py",28.0,19,5,4.027807569,43.0,42162.0,16.0,1195163.857142857,2910.0,6815.333333,0.0,Feature Addition,0.0,1
pytorch,67b104af02584ef60b8d26a0392947367767fbf1,d02f085f704476fb409c8e2feed7398d631577ee,Richard Zou,zou3519@users.noreply.github.com,Wed Jul 13 17:07:20 2022 -0400,1657732040.0,[functorch] Align functorch's flake8 config with pytorch's (pytorch/functorch#963),40.0,35.0,"functorch/.flake8,functorch/codegen/gen.py,functorch/examples/compilation/fuse_module.py,functorch/functorch/_src/partitioners.py,functorch/test/common_utils.py,functorch/test/test_eager_transforms.py,functorch/test/test_functionalize.py,functorch/test/test_memory_efficient_fusion.py,functorch/test/test_minifier.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",11.0,7,1,3.107349025,1.0,10039.0,8.0,0.4,1161.0,1563.0,0.0,,0.0,1
pytorch,b28a8348139f4f35b69972e281e463e34d2735c4,d0435604a5ae24e9d6504f7835a8bc7b93b1d9e0,Supriya Rao,supriyar@fb.com,Thu Feb 13 20:12:22 2020 -0800,1581624742.0,"[quant] Add a quantized batch_norm operator (#33080)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33080

Quantized batch norm for cases where batch norm cannot be fused with conv.
AVX2 implementation is from Caffe2.

Test Plan:
python test/test_quantized.py TestQuantizedOps.test_batch_norm

Imported from OSS

Differential Revision: D19861927

fbshipit-source-id: bd8cd101fc063cb6358132ab7c651a160999293c",276.0,0.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py",5.0,8,2,1.474300155,10.0,9955.0,2.0,115325.0,14733.0,39625.33333,0.0,Feature Addition,0.0,1
pytorch,b4462511fdc4f44a3620fc8cc25dd9c6f73816c0,d043f830198b1e222f69a41e97e96c200f146636,Tongzhou Wang,ssnl@users.noreply.github.com,Tue Aug 14 18:35:36 2018 -0700,1534271736.0,"Add tests for Tensor.* nn.* F.* docs (#10311)

Summary:
Test only for existence for now. I had to skip a lot of them so there a FIXME in the test.

Also I'm not testing torch.* because of namespace issue.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10311

Differential Revision: D9196341

Pulled By: SsnL

fbshipit-source-id: 9c2ca1ffe660bc1cc664474993f8a21198525ccc",241.0,89.0,"aten/src/ATen/native/native_functions.yaml,test/test_torch.py,torch/_tensor_docs.py,torch/nn/functional.py,torch/tensor.py",5.0,7,3,1.347104746,43.0,16084.0,4.0,573642.4,3446.0,9266.833333,0.0,Corrective,1.0,1
pytorch,e01279cc2e987b2b00b460770199661f6461d01a,d0662f2f76c7e68b44530247141af252e01a9b7c,Natalia Gimelshein,ngimel@fb.com,Sun Oct 31 04:18:01 2021 -0700,1635653881.0,"Add adaptive_max_pool OpInfo (#67405)

Summary:
Per title

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67405

Reviewed By: mruberry

Differential Revision: D32044712

Pulled By: ngimel

fbshipit-source-id: 4619d134d18359601801c029dd5be3f59b91626d",118.0,25.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,12220.0,1.0,21227.0,16736.0,39209.0,0.0,Feature Addition,0.0,1
pytorch,565365baafd451526a9507c5c58e995a8a23375f,d095055afb6df226ab48615b8a5b2d474f1ffd92,Samantha Andow,samdow@fb.com,Fri Feb 18 18:56:54 2022 -0500,1645210614.0,"[functorch] randperm support (pytorch/functorch#411)

[ghstack-poisoned]",75.0,8.0,"functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/test/test_vmap.py",3.0,4,1,1.256986801,1.0,3687.0,2.0,0.0,808.0,1107.5,0.0,,0.0,1
pytorch,d088359e5a79175f26eb50a9623fa9ab66d141c0,d0a12c5a47a898609ee6520fc42033211bddd019,Jeffrey Wan,jw3468@fb.com,Fri Dec 18 23:44:28 2020 -0800,1608335068.0,"Add sinc operator (#48740)

Summary:
Implements the sinc operator.
See https://numpy.org/doc/stable/reference/generated/numpy.sinc.html

![image](https://user-images.githubusercontent.com/13428986/101653855-cdffa080-3a0d-11eb-8426-ecc81c152ebd.png)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48740

Reviewed By: ezyang

Differential Revision: D25597565

Pulled By: soulitzer

fbshipit-source-id: 6dbcf282ee4eba34930bc9e5c85c0c5e79cf0322",133.0,1.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",14.0,14,4,3.066501722,37.0,35474.0,5.0,153072.92857142858,7648.0,17219.0,0.0,Feature Addition,0.0,1
pytorch,42e7eb0426190e07339f03d4e6afb61b7ff5ae9c,d0a4b2f586e0901c3c65f1f0e0bae15364e28821,Peter Bell,peterbell10@live.co.uk,Fri Oct 04 06:30:02 2019 -0700,1570170602.0,"Choose num_threads in parallel_for based on GRAIN_SIZE (#26963)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/24080, Continuation of https://github.com/pytorch/pytorch/issues/26886

What soumith said in https://github.com/pytorch/pytorch/pull/26886#issuecomment-535760635 seems plausible
> I wonder if it has to do with `#pragma omp parallel num_threads(num_threads)` which has unintended consequences, where even if `num_threads=1`, entering an omp block inside an omp block results in bad behavior.

I know for a fact that gcc's openmp doesn't start the thread pool when given `num_threads(1)` but it seems clang behaves differently.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26963

Differential Revision: D17626981

Pulled By: soumith

fbshipit-source-id: 484ffe6cc172382bb5ff49ce1fceda7eba20a512",7.0,1.0,aten/src/ATen/ParallelOpenMP.h,1.0,3,1,0,1.0,94.0,1.0,627321.0,12023.0,33761.83333,0.0,Corrective,1.0,1
pytorch,38f13447bce8b7dfe925664c2ef7e42659551cb4,d0cabbde74014a4695d07ca464a21ce7d4a148b8,Sam Gross,colesbury@gmail.com,Wed Dec 06 19:08:56 2017 -0500,1512587336.0,"Implement Variable.from_numpy (#4043)

Implements from_numpy using ATen tensors. Variable.from_numpy is a
convenient placeholder for the variant that returns Variables until we
merge Tensor and Variable.

The behavior is slightly changed:

 - from_numpy() on an empty array now returns an empty tensor instead of
   throwing an exception. The shape may not be preserved.
 - CharTensor(ndarray) used to throw an exception. It now copies the
   ndarray. Copying is implemented via ATen toType.",262.0,266.0,"setup.py,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/autograd/variable.py,torch/csrc/DynamicTypes.cpp,torch/csrc/DynamicTypes.h,torch/csrc/Exceptions.cpp,torch/csrc/Exceptions.h,torch/csrc/Module.cpp,torch/csrc/Tensor.cpp,torch/csrc/autograd/python_variable_numpy.cpp,torch/csrc/autograd/python_variable_numpy.h,torch/csrc/cuda/Tensor.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/utils/tensor_numpy.cpp,torch/csrc/utils/tensor_numpy.h",16.0,11,3,2.739803136,39.0,9978.0,7.0,851622.1428571428,364.0,1089.905869,0.0,,0.0,1
pytorch,e64fca4b04ec59c8fd28634072db804ec502f9c0,d0cf5f7b658a0565304d011f97acd8a85652679f,Gregory Chanan,gchanan@fb.com,Wed Nov 09 21:11:06 2016 -0800,1478725866.0,"Improving error messages in nn.

Differences from nn equivalent:
1) No changes to VolumetricConvolutionMM, which doesn't exist in cunn.
2) No changes to HardShrink, which doesn't  exist in cunn.
3) LookupTable doesn't verify that all inputs are within range.",485.0,152.0,"SpatialReflectionPadding.cu,SpatialReplicationPadding.cu,VolumetricReplicationPadding.cu,common.h,generic/Abs.cu,generic/AbsCriterion.cu,generic/BCECriterion.cu,generic/BatchNormalization.cu,generic/ClassNLLCriterion.cu,generic/DistKLDivCriterion.cu,generic/ELU.cu,generic/HardTanh.cu,generic/L1Cost.cu,generic/LeakyReLU.cu,generic/LogSigmoid.cu,generic/LogSoftMax.cu,generic/LookupTable.cu,generic/MSECriterion.cu,generic/MarginCriterion.cu,generic/MultiLabelMarginCriterion.cu,generic/MultiMarginCriterion.cu,generic/PReLU.cu,generic/RReLU.cu,generic/Sigmoid.cu,generic/SmoothL1Criterion.cu,generic/SoftMarginCriterion.cu,generic/SoftMax.cu,generic/SoftPlus.cu,generic/SoftShrink.cu,generic/SpatialAdaptiveMaxPooling.cu,generic/SpatialAveragePooling.cu,generic/SpatialClassNLLCriterion.cu,generic/SpatialConvolutionLocal.cu,generic/SpatialConvolutionMM.cu,generic/SpatialDilatedConvolution.cu,generic/SpatialDilatedMaxPooling.cu,generic/SpatialFractionalMaxPooling.cu,generic/SpatialFullConvolution.cu,generic/SpatialMaxUnpooling.cu,generic/SpatialReflectionPadding.cu,generic/SpatialReplicationPadding.cu,generic/SpatialUpSamplingBilinear.cu,generic/SpatialUpSamplingNearest.cu,generic/Sqrt.cu,generic/Square.cu,generic/Tanh.cu,generic/TemporalConvolution.cu,generic/Threshold.cu,generic/VolumetricAveragePooling.cu,generic/VolumetricConvolution.cu,generic/VolumetricDilatedConvolution.cu,generic/VolumetricDilatedMaxPooling.cu,generic/VolumetricFullConvolution.cu,generic/VolumetricMaxUnpooling.cu,generic/VolumetricReplicationPadding.cu",55.0,1,1,4.761951451,11.0,7928.0,45.0,180148.27272727276,85.0,1069.933333,0.0,,0.0,1
pytorch,cab32d9cdf3df1d838d41739fc519f589f834f9d,d0df29ac22789aae9078c767b8296d0f6d98a64b,James Reed,jamesreed@fb.com,Thu Oct 29 07:33:55 2020 -0700,1603956835.0,"[FX] Put inf and nan in globals instead of with an import string (#47035)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/47035

Chillee thought the `from math import inf, nan` string at the top of `.code` was annoying so here's an alternative way to do it by putting those values in `globals` before we `exec`

Test Plan: Imported from OSS

Reviewed By: dzhulgakov

Differential Revision: D24611278

Pulled By: jamesr66a

fbshipit-source-id: c25ef89e649bdd3e79fe91aea945a30fa7106961",3.0,3.0,"torch/fx/graph.py,torch/fx/graph_module.py",2.0,2,1,1,1.0,767.0,2.0,1032650.5,6304.0,14420.5,0.0,,0.0,1
pytorch,75955e4ef8a941d72db20c5098371325bd83ffd1,d0eff8d84657ef90a32bd038f90468742e5457dc,Pearu Peterson,pearu.peterson@gmail.com,Sat Nov 20 04:50:19 2021 -0800,1637383819.0,"Strided masked softmin. (#68463)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68463

Test Plan: Imported from OSS

Reviewed By: dagitses

Differential Revision: D32576497

Pulled By: cpuhrsch

fbshipit-source-id: 286edb2e7a5415df76858c69d0312743437b0fd8",42.0,4.0,"test/test_masked.py,torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.069617416,2.0,14187.0,1.0,15787.0,17241.0,40583.0,0.0,,0.0,1
pytorch,91c4ba79805044a25d93a906fa48440307913b9c,d122b4e4ec4af67b4bc1894203bbc5b9113a3c34,Brandon Amos,bamos@cs.cmu.edu,Tue Apr 04 19:19:03 2017 -0400,1491333543.0,Update btrisolve docs to the newest interface.,2.0,2.0,torch/_torch_docs.py,1.0,1,1,0,15.0,4447.0,1.0,103338.0,591.0,5443.822669,0.0,Non Functional,0.0,1
pytorch,ede82167519c66d14ed89f86bc7b0fa35eabed53,d12a358435afea6b7c43c85dbad9dd34e9877659,Adam Paszke,adam.paszke@gmail.com,Fri Jul 29 20:54:18 2016 -0400,1469825658.0,Add converted nn modules,5535.0,106.0,"test/legacy/nn.py,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/DepthConcat.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/MV.py,torch/legacy/nn/Mean.py,torch/legacy/nn/MultiLabelMarginCriterion.py,torch/legacy/nn/MultiLabelSoftMarginCriterion.py,torch/legacy/nn/MultiMarginCriterion.py,torch/legacy/nn/Narrow.py,torch/legacy/nn/NarrowTable.py,torch/legacy/nn/Normalize.py,torch/legacy/nn/PReLU.py,torch/legacy/nn/Padding.py,torch/legacy/nn/PairwiseDistance.py,torch/legacy/nn/Parallel.py,torch/legacy/nn/ParallelCriterion.py,torch/legacy/nn/ParallelTable.py,torch/legacy/nn/PartialLinear.py,torch/legacy/nn/Power.py,torch/legacy/nn/RReLU.py,torch/legacy/nn/ReLU.py,torch/legacy/nn/ReLU6.py,torch/legacy/nn/Replicate.py,torch/legacy/nn/Reshape.py,torch/legacy/nn/Select.py,torch/legacy/nn/SelectTable.py,torch/legacy/nn/Sigmoid.py,torch/legacy/nn/SmoothL1Criterion.py,torch/legacy/nn/SoftMarginCriterion.py,torch/legacy/nn/SoftMax.py,torch/legacy/nn/SoftMin.py,torch/legacy/nn/SoftPlus.py,torch/legacy/nn/SoftShrink.py,torch/legacy/nn/SoftSign.py,torch/legacy/nn/SpatialAdaptiveMaxPooling.py,torch/legacy/nn/SpatialAveragePooling.py,torch/legacy/nn/SpatialBatchNormalization.py,torch/legacy/nn/SpatialClassNLLCriterion.py,torch/legacy/nn/SpatialContrastiveNormalization.py,torch/legacy/nn/SpatialConvolution.py,torch/legacy/nn/SpatialConvolutionLocal.py,torch/legacy/nn/SpatialConvolutionMap.py,torch/legacy/nn/SpatialCrossMapLRN.py,torch/legacy/nn/SpatialDilatedConvolution.py,torch/legacy/nn/SpatialDivisiveNormalization.py,torch/legacy/nn/SpatialDropout.py,torch/legacy/nn/SpatialFractionalMaxPooling.py,torch/legacy/nn/SpatialFullConvolution.py,torch/legacy/nn/SpatialFullConvolutionMap.py,torch/legacy/nn/SpatialLPPooling.py,torch/legacy/nn/SpatialMaxPooling.py,torch/legacy/nn/SpatialMaxUnpooling.py,torch/legacy/nn/SpatialReflectionPadding.py,torch/legacy/nn/SpatialReplicationPadding.py,torch/legacy/nn/SpatialSoftMax.py,torch/legacy/nn/SpatialSubSampling.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/legacy/nn/SpatialUpSamplingNearest.py,torch/legacy/nn/SpatialZeroPadding.py,torch/legacy/nn/SplitTable.py,torch/legacy/nn/Sqrt.py,torch/legacy/nn/Square.py,torch/legacy/nn/Squeeze.py,torch/legacy/nn/Sum.py,torch/legacy/nn/Tanh.py,torch/legacy/nn/TanhShrink.py,torch/legacy/nn/TemporalConvolution.py,torch/legacy/nn/TemporalMaxPooling.py,torch/legacy/nn/TemporalSubSampling.py,torch/legacy/nn/Threshold.py,torch/legacy/nn/Transpose.py,torch/legacy/nn/Unsqueeze.py,torch/legacy/nn/View.py,torch/legacy/nn/VolumetricAveragePooling.py,torch/legacy/nn/VolumetricBatchNormalization.py,torch/legacy/nn/VolumetricConvolution.py,torch/legacy/nn/VolumetricDropout.py,torch/legacy/nn/VolumetricFullConvolution.py,torch/legacy/nn/VolumetricMaxPooling.py,torch/legacy/nn/VolumetricMaxUnpooling.py,torch/legacy/nn/VolumetricReplicationPadding.py,torch/legacy/nn/WeightedEuclidean.py,torch/legacy/nn/WeightedMSECriterion.py,torch/legacy/nn/__init__.py,torch/legacy/nn/utils.py",86.0,5,2,5.815136154,1.0,1816.0,2.0,128679.66666666669,54.0,238.5,0.0,Feature Addition,0.0,1
pytorch,d247912dbf600a55a1e334e6d090b66ebca53c21,d14abe3aff95091549e675f3f8d9d2c2aa35ea12,Vitaly Fedyunin,vitalyf@fb.com,Wed Apr 24 22:31:46 2019 -0700,1556145106.0,"Add torch.from_file function similar to the Storage.from_file, but returning tensor (#18688)

Summary:
Porting `torch.Storage.from_file(filename, shared, size)` function to `torch.from_file(filename, shared, size, dtype=torch.int)`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18688

Differential Revision: D15012644

Pulled By: VitalyFedyunin

fbshipit-source-id: 3f62ca9e414fad3847fe71b785ff97b5bdc2d2cd",69.0,1.0,"aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/gen_python_functions.py,torch/csrc/jit/tracer.cpp,torch/csrc/jit/tracer.h,torch/csrc/utils/python_arg_parser.h",7.0,11,4,2.347221356,41.0,18603.0,6.0,148323.57142857142,8299.0,24830.83333,0.0,Feature Addition,0.0,1
pytorch,804e32a467c87ab907c6196808e85557fae624b0,d16c8238e164c6499714de625eb73422382e5ec1,Povilas Kanapickas,povilas@radix.lt,Tue May 05 01:05:16 2020 -0700,1588640716.0,"[ONNX] Fix numerical errors in softmax when dim is not last dimension (#37326)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/34585.

This PR improves the workaround for the problem of different semantics between ONNX softmax and Pytorch softmax.

In Pytorch the `dim` parameter specifies over which dimension normalize the values.  ONNX on the other hand always coerces the input into a 2D tensor and the `axis` parameter specifies which dimensions represent rows and columns of the resulting tensor. As a result, only when we are normalizing the last dimension (`dim == ndim - 1`) semantics are the same.

Previously this was handled by recognizing the `dim == ndim - 1` case and using `softmax` for that. All other cases used a fallback path of explicit invocations of exp, reducesum and div operators to compute the result. Unfortunately, this results in numeric errors when input values are large: the result of exp will produce infinity on both numerator and denumerator and the division of that will result in NaN.

This can be improved by transposing the input tensor so that we can reuse ONNX softmax.

Similar approach has been applied to `logsoftmax` function in https://github.com/pytorch/pytorch/issues/30433.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37326

Reviewed By: hl475

Differential Revision: D21389712

Pulled By: houseroad

fbshipit-source-id: 554fd1b98231a28984c30c7e7abd3c0643386ff7",59.0,9.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,1,3.0,5737.0,1.0,538313.0,1658.0,4356.0,0.0,Corrective,1.0,1
pytorch,6325b6e44e56f518d423cb46c93cfad892b236d0,d17c22d024467b7185e33c4652b44739f67965be,James Reed,jamesreed@fb.com,Sat Apr 20 02:13:10 2019 -0700,1555726390.0,"Improve embedding_bag add kernel (#19329)

Summary:
This was actually getting pretty poor throughput with respect to memory bandwidth. I used this test to measure the memory bandwidth specifically for the AXPY call: https://gist.github.com/jamesr66a/b27ff9ecbe036eed5ec310c0a3cc53c5

And I got ~8 GB/s before this change, but ~14 GB/s after this change.

This seems to speed up the operator overall by around 1.3x (benchmark: https://gist.github.com/jamesr66a/c533817c334d0be432720ef5e54a4166):

== Before ==

time_per_iter 0.0001298875093460083
GB/s 3.082544287868467

== After ==

time_per_iter 0.00010104801654815674
GB/s 3.9623142905451076

The large difference between the local BW increase and the full-op BW increase likely indicates significant time is being spent elsewhere in the op, so I will investigate that.

EDIT: I updated this PR to include a call into caffe2/perfkernels. This is the progression:

before

time_per_iter 8.983819484710693e-05
GB/s 4.456723564864611

After no axpy
time_per_iter 7.19951868057251e-05
GB/s 5.56126065872172

AFter perfkernels
time_per_iter 5.6699180603027346e-05
GB/s 7.061548257694262

After perfkernels no grad
time_per_iter 4.388842582702637e-05
GB/s 9.122769670026413
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19329

Reviewed By: dzhulgakov

Differential Revision: D14969630

Pulled By: jamesr66a

fbshipit-source-id: 42d1015772c87bedd119e33c0aa2c8105160a738",211.0,39.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,caffe2/perfkernels/embedding_lookup.h,test/test_nn.py,tools/autograd/derivatives.yaml",6.0,10,4,0.649519533,43.0,15414.0,5.0,1387001.8333333333,8212.0,24657.33333,0.0,Feature Addition,0.0,1
pytorch,264ffd143c3694df6f39a16ca0c5df3ba34a0d32,d1a992a85ebce32b3aaa8efc520c35cbb17d97b0,Richard Zou,zou3519@users.noreply.github.com,Thu Apr 19 22:31:14 2018 -0400,1524177074.0,"Disallow chunks that are <= in torch.chunk (#6761)

Fixes #6759.

Before, `tensor.chunk(0)` would cause a divide by 0.
`tensor.chunk(-1)` would throw an error complaining that ""split_size
needs to be positive"".

This PR changes it so that the error message makes it clear that
`chunks` has to be greater than 0.",12.0,1.0,"aten/src/ATen/native/TensorShape.cpp,test/test_torch.py",2.0,5,2,0.995727452,39.0,7316.0,2.0,908652.0,2584.0,24936.85823,0.0,Corrective,1.0,1
pytorch,95d545e75b4005afe74b1a07ee36a42d609d0ef7,d1fda539b79e7fd1ee912a5d257bf3558e346bf5,Adam Paszke,adam.paszke@gmail.com,Fri Sep 16 01:46:47 2016 -0700,1473990407.0,Fix nn serialization errors,71.0,26.0,"test/common_nn.py,test/test_nn.py,torch/_thnn/thcunn.py,torch/_thnn/thnn.py,torch/_thnn/utils.py,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/serialization.cpp,torch/nn/backends/thnn.py,torch/serialization.py",9.0,7,2,2.686643629,6.0,5012.0,2.0,21631.33333333333,174.0,3746.532937,0.0,Corrective,1.0,1
pytorch,e47b3018b7265734c508973f7b4411c349ac6570,d26ab68485fe32da1b696d0ce3a273a8c43ae207,Adam Paszke,adam.paszke@gmail.com,Thu Apr 19 01:51:35 2018 +0200,1524102695.0,"Sort declarations when generating Python bindings (#6701)

* Sort declarations when generating Python bindings

This helps resolve ambiguities in argument parsing according to
any rules we will need.

For now, this allows us to make scalar operations more conservarive
wrt. argument types, but makes them commutative again.

* Fix inconsistencies between mod with tensor and scalar

* Fix a stupid mistake",122.0,55.0,"aten/src/TH/generic/THTensorMath.c,aten/src/THC/THCNumerics.cuh,aten/src/THC/THCTensorMathPairwise.cu,aten/src/THC/THCTensorMathPointwise.cuh,test/test_torch.py,tools/autograd/gen_python_functions.py",6.0,8,3,2.237981544,39.0,14074.0,4.0,643298.5,50.0,104.0,0.0,Corrective,1.0,1
pytorch,42b4a7132e7c6f1df963b473d1583e4791fb1808,d28639a08054b6662693d1c81c734dd69938592c,Paul Shao,pshao@fb.com,Tue Aug 11 15:40:21 2020 -0700,1597160421.0,"Optimization with Backward Implementation of Learnable Fake Quantize Per Channel Kernel (CPU and GPU) (#42810)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42810

In this diff, the original backward pass implementation is sped up by merging the 3 iterations computing dX, dScale, and dZeroPoint separately. In this case, a native loop is directly used on a byte-wise level (referenced by `strides`). In addition, vectorization is used such that scale and zero point are expanded to share the same shape and the element-wise corresponding values to X along the channel axis.

In the benchmark test on the operators, for an input of shape `3x3x256x256`, we have observed the following improvement in performance:
**Speedup from python operator**: ~10x
**Speedup from original learnable kernel**: ~5.4x
**Speedup from non-backprop kernel**: ~1.8x

Test Plan:
To assert correctness of the new kernel, on a devvm, enter the command

`buck test //caffe2/test:quantization -- learnable_backward_per_channel`

To benchmark the operators, on a devvm, enter the command
1. Set the kernel size to 3x3x256x256 or a reasonable input size.
2. Run `buck test //caffe2/benchmarks/operator_benchmark/pt:quantization_test`
3. The relevant outputs for CPU are as follows:

```
# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cpu_op_typepy_module
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: py_module
Backward Execution Time (us) : 989024.686

# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cpu_op_typelearnable_kernel
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: learnable_kernel
Backward Execution Time (us) : 95654.079

# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cpu_op_typeoriginal_kernel
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: original_kernel
Backward Execution Time (us) : 176948.970
```
4. The relevant outputs for GPU are as follows:
The relevant outputs are as follows

**Pre-optimization**:

```
# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typepy_module
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: py_module
Backward Execution Time (us) : 6795.173

# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typelearnable_kernel
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: learnable_kernel
Backward Execution Time (us) : 4321.351

# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typeoriginal_kernel
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: original_kernel
Backward Execution Time (us) : 1052.066
```

**Post-optimization**:
```
# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typepy_module
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: py_module
Backward Execution Time (us) : 6737.106

# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typelearnable_kernel
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: learnable_kernel
Backward Execution Time (us) : 2112.484

# Benchmarking PyTorch: FakeQuantizePerChannelOpBenchmark
# Mode: Eager
# Name: FakeQuantizePerChannelOpBenchmark_N3_C3_H256_W256_cuda_op_typeoriginal_kernel
# Input: N: 3, C: 3, H: 256, W: 256, device: cpu, op_type: original_kernel
Backward Execution Time (us) : 1078.79

Reviewed By: vkuzo

Differential Revision: D22946853

fbshipit-source-id: 1a01284641480282b3f57907cc7908d68c68decd",110.0,176.0,"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu,aten/src/ATen/native/quantized/fake_quant_affine.h,aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp,benchmarks/operator_benchmark/pt/quantization_test.py,test/quantization/test_workflow_module.py",6.0,13,3,1.926957407,2.0,4981.0,5.0,578730.6666666666,4213.0,9869.0,0.0,Corrective,0.0,1
pytorch,07932e27356c32df8a3c17361e4779c635d2d8ce,d2917f705ac42da6f4b3a70f3033ea0f3cd8feb1,"Gao, Xiang",qasdfgtyuiop@gmail.com,Thu Dec 09 15:32:13 2021 -0800,1639063933.0,"Fix errors in `common_utils.py` (#69578)

Summary:
This fixes the following error:
```python
Traceback (most recent call last):
  File ""/home/gaoxiang/pytorch-ucc2/test/distributed/test_distributed_spawn.py"", line 40, in <module>
    run_tests()
  File ""/home/gaoxiang/.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py"", line 618, in run_tests
    ['--import-slow-tests'] if IMPORT_SLOW_TESTS else List[str]([]))
  File ""/usr/lib/python3.9/typing.py"", line 680, in __call__
    raise TypeError(f""Type {self._name} cannot be instantiated; ""
TypeError: Type List cannot be instantiated; use list() instead
Traceback (most recent call last):
  File ""/home/gaoxiang/pytorch-ucc2/test/run_test.py"", line 1058, in <module>
    main()
  File ""/home/gaoxiang/pytorch-ucc2/test/run_test.py"", line 1036, in main
    raise RuntimeError(err_message)
RuntimeError: distributed/test_distributed_spawn failed!
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/69578

Reviewed By: mrshenli

Differential Revision: D32963113

Pulled By: malfet

fbshipit-source-id: b064e230c5e572e890b4ac66ebdda2707b8c12d7",5.0,2.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,3130.0,1.0,578476.0,17637.0,41507.5,0.0,Corrective,1.0,1
pytorch,9705d60a2f7dabe0adcc13598b0e5268678f9d00,d2eb08d17bcb16bfe07a88d9de9fe1fb9c0d6488,neginraoof,neginmr@utexas.edu,Wed Oct 23 00:07:39 2019 -0700,1571789259.0,"Fix tracing slice/select with dynamic inputs (#26549)

Summary:
Fix Slice/Select trace arguments. This PR stashes arguments to functions in order to avoid tracing them as constants.
This PR depends on a fix for select op in PR:
https://github.com/pytorch/pytorch/pull/25273
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26549

Reviewed By: hl475

Differential Revision: D17623851

Pulled By: houseroad

fbshipit-source-id: ae314004266688d2c25c5bada2dcedbfc4f39c5b",362.0,250.0,"test/onnx/expect/TestOperators.test_arange_dynamic.expect,test/onnx/expect/TestOperators.test_dyn_arange.expect,test/onnx/expect/TestOperators.test_full.expect,test/onnx/expect/TestOperators.test_slice_dynamic.expect,test/onnx/expect/TestOperators.test_upsample_nearest.expect,test/onnx/expect/TestOperators.test_view_flatten.expect,test/onnx/test_onnx_opset.py,test/onnx/test_operators.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_jit.py,torch/csrc/autograd/python_variable_indexing.cpp,torch/onnx/symbolic_opset9.py",13.0,7,2,2.936537232,14.0,27483.0,7.0,886847.8333333334,12465.0,34747.83333,0.0,Corrective,1.0,1
pytorch,b5dc36f278fc296e8b9cd4f4f826f4840907b60e,d2ef49384ec83d4c336f63d985688a9dfc198a83,Adam Paszke,adam.paszke@gmail.com,Sat Dec 31 15:32:00 2016 +0100,1483198320.0,Add custom docs stylesheet (#387),94.0,0.0,"docs/source/_static/css/pytorch_theme.css,docs/source/_static/img/pytorch-logo-dark.svg,docs/source/conf.py",3.0,5,1,1.286835791,6.0,180.0,1.0,37487.0,276.0,6359.224559,0.0,Feature Addition,0.0,1
pytorch,d50dd47ccd8256b4375926ff44b288d97f673947,d2f26a450ed16e9a8742c09022c4d515deb32a9b,Pieter Noordhuis,pietern@fb.com,Mon Nov 05 21:49:21 2018 -0800,1541454561.0,"Add new style allreduce support in c10d/gloo (#13426)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13426

This replaces the existing allreduce implementation with the new style collective call in the gloo backend. This is the first one to include both a CPU and a CUDA path. The CUDA path copies CUDA tensors to CPU tensors and then runs the CPU allreduce implementation. This is not much different from the current situation in the case where there is a single input tensor per call (which is the case when called from DistributedDataParallel).

Reviewed By: teng-li

Differential Revision: D12855689

fbshipit-source-id: 574281d762dd29149fa7f634fb71f8f6a9787598",242.0,60.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.848316531,3.0,2477.0,1.0,3.0,5153.0,15400.33333,0.0,Feature Addition,0.0,1
pytorch,924b001b71555cfd58b31249a2eb7963627f2fc8,d307601365c3b848072b8b8381208aedc1a0aca5,Heitor Schueroff,heitorschueroff@fb.com,Mon Dec 07 19:46:58 2020 -0800,1607370418.0,"Revert D24923679: Fixed einsum compatibility/performance issues (#46398)

Test Plan: revert-hammer

Differential Revision:
D24923679 (https://github.com/pytorch/pytorch/commit/ea2a568cca71aaf690051782c225ca9dd2e5e1f9)

Original commit changeset: 47e48822cd67

fbshipit-source-id: 52f17b66a4aa075d0159bdf1c98616e6098091b8",348.0,543.0,"aten/src/ATen/native/Linear.cpp,test/test_linalg.py,torch/functional.py",3.0,6,3,1.421691038,30.0,8013.0,1.0,99959.0,7263.0,16392.5,0.0,Corrective,1.0,1
pytorch,83cfaf1a126b8a681dca2fa76d67af2eca25e714,d312aeb6acb528c7caf1daa173307e6349afee42,Jeffrey Wan,jw3468@fb.com,Fri Apr 16 22:01:42 2021 -0700,1618610502.0,"Implement faster gradcheck but not enabled for most things (#54480)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54480

This PR shouldn't really change the behavior of gradcheck for most ops. However, the changes in test_autograd allow us to run basic checks for both fast and slow (instead of previously just slow). All it should be doing is wrapping the preexisting tests we introduced in prior PRs in a function which takes `fast_mode` as a param. We then call this function twice, once with `fast_mode=True` and once with `fast_mode=False`.

Plan for rollout:
 - This PR should only land the code (and runs some basic checks as described above).
   - This should help us verify that a) slow is still working as expected b) basic functionality of fast works
   - After we land this, but before we run the next PR in the stack, we should land https://github.com/pytorch/pytorch/pull/55182. This is to ensure that there is no gap where the slow tests aren't running.
 - The next PR is responsible for enabling the fast_mode=True flag on all tests (where the function has real inputs/outputs), and selectively disabling for the cases the fail.
 - Finally in a later PR, we reenable fast-gradcheck for functions w/ complex inputs/outputs

TODOs and open questions (not necessarily blocking this PR):
 - ~How do we think about atol/rtol~ (scale atol, keep rtol as-is)
 - ~reenable fast-gradcheck for complex numbers~
 - ~when inputs are uncoalesced we don't truly test this case because we coalesce the inputs before calling function. Revisit this when https://github.com/pytorch/pytorch/pull/52874/files is landed~

### Developer Experience
Sample output when jacobian mismatch occurs:
```
Traceback (most recent call last):
  File ""/home/s/local/pytorch4/test/test_autograd.py"", line 4220, in test_gradcheck_jacobian_mismatch
    check(fast_mode=True)
  File ""/home/s/local/pytorch4/test/test_autograd.py"", line 4196, in check
    gradcheck(fn, (x,), fast_mode=fast_mode)
  File ""/home/s/local/pytorch4/torch/testing/_internal/common_utils.py"", line 2067, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
  File ""/home/s/local/pytorch4/torch/autograd/gradcheck.py"", line 1020, in gradcheck
    if not fast_gradcheck(fail_test, seeded_func, func_out, tupled_inputs, outputs, eps, rtol,
  File ""/home/s/local/pytorch4/torch/autograd/gradcheck.py"", line 915, in fast_gradcheck
    return fail_test(get_notallclose_msg(a, n, i, j, prefix) + jacobians_str)
  File ""/home/s/local/pytorch4/torch/autograd/gradcheck.py"", line 996, in fail_test
    raise RuntimeError(msg)
RuntimeError: Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(0.9195)
analytical:tensor(0.9389)

The above quantities relating the numerical and analytical jacobians are computed
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[1.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 1.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 1.0000]])
Analytical:
tensor([[1.0100, 0.0100, 0.0100, 0.0100],
        [0.0100, 1.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 1.0100, 0.0100],
        [0.0100, 0.0100, 0.0100, 1.0100]])

The max per-element difference (slow mode) is: 0.010000000000054632.
```
Additionally, if the per-element difference is small i.e., `allclose(analytical_slow, numerical_slow, rtol, atol) is True` we follow up with this message:
```
Fast gradcheck failed but element-wise differences are small. This means that the
test might've passed in slow_mode!

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.

If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `fast_mode=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test
  to have `gradcheck_fast_mode=False`
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_fast_mode=False`
```

Test Plan: Imported from OSS

Reviewed By: walterddr, ejguan

Differential Revision: D27825160

Pulled By: soulitzer

fbshipit-source-id: 1fe60569d8b697c213b0d262a832622a4e9cf0c7",587.0,277.0,"test/test_autograd.py,torch/autograd/gradcheck.py",2.0,3,2,0.999242279,42.0,9420.0,2.0,146204.5,10945.0,24154.0,0.0,Corrective,1.0,1
pytorch,6ce6939be9b01965cc830f5d819010ac97bf2a1f,d33623f7c12b16fd49f74594e0e89ceecb817be9,Max Balandat,balandat@fb.com,Tue Aug 20 16:20:34 2019 -0700,1566318034.0,"Make SobolEngine use random seed if not specified (#24884)

Summary:
Addresses https://github.com/pytorch/pytorch/issues/24881. Makes behavior consistent with the rest of the random functions.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24884

Test Plan: Unit tests

Reviewed By: sdsingh

Differential Revision: D16912036

Pulled By: Balandat

fbshipit-source-id: eff00cca989926a5d9e20d8846a8674f7cd270cb",8.0,1.0,"test/test_torch.py,torch/quasirandom.py",2.0,2,2,0.99107606,40.0,13238.0,2.0,6385985.5,10757.0,30510.33333,0.0,Feature Addition,0.0,1
pytorch,3e2f309177b4982eac7ac5f0842fba773285c954,d3552a53d15d7704e00b9c2e3aa2df2747e5fa9d,Richard Zou,zou3519@users.noreply.github.com,Fri Mar 04 18:14:04 2022 -0500,1646417644.0,[functorch] Add way to override op precision; manually override conv_transpose precision (pytorch/functorch#560),78.0,13.0,"functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,2,1,1.546861673,1.0,5231.0,3.0,0.3333333333333333,851.0,1188.5,0.0,Feature Addition,0.0,1
pytorch,52058204d6759d435988db10247ccd3383657634,d35f365ad578110e10afce56574e2d00bf59e727,Adam Paszke,adam.paszke@gmail.com,Fri Aug 17 18:07:31 2018 -0700,1534529251.0,"Remove all cuDNN specific inputs to RNN functions (#10581)

Summary:
This is still not the final PR, but it removes all blockers for actually using the RNN functions directly in the JIT. Next patch should be final, and will actually remove the symbolic_override code, and change it to proper symbolics for those ATen functions. Turns out the symbolic code can be also cleaned up a bit, and I'll do that too.

zdevito ezyang
colesbury (for minor DispatchStub.h) changes

There was no way to handle those in the JIT for now, and they turned
out to be completely unnecessary. It should make the Python and C++
module code much simpler too, since all the logic is now centralized
in the native functions.

The downside is that RNN modules no longer own their dropout buffers,
which are shared per-device instead (with appropriate locking and
synchronization). This might appear as a perf regression at first, but
in reality it's highly unlikely that anyone will want to run cuDNN RNNs
on the same GPU in parallel.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10581

Reviewed By: colesbury

Differential Revision: D9365541

Pulled By: apaszke

fbshipit-source-id: 3ef8677ee5481bae60c74a9117a2508665b476b5",531.0,161.0,"aten/src/ATen/cuda/CUDAEvent.cpp,aten/src/ATen/cuda/CUDAEvent.h,aten/src/ATen/cuda/CUDAStream.cpp,aten/src/ATen/cuda/CUDAStream.h,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/RNN.h,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/native_functions.yaml,torch/nn/_functions/rnn.py,torch/nn/modules/rnn.py,torch/onnx/symbolic.py",12.0,11,2,2.612676134,39.0,6306.0,4.0,626806.2222222222,3536.0,9598.333333,0.0,,1.0,1
pytorch,d62c3ea35447b16301dfab2383d6d09e8870bf86,d36ce61a5ee0bf46d053676d0ff3893fe68e6f0e,Jeff Daily,jeff.daily@amd.com,Tue Jun 29 01:13:47 2021 -0700,1624929227.0,"use explicitly non-returning GPU atomics (#60607)

Summary:
Enables an important performance optimization for ROCm, in light of the discussion in https://github.com/pytorch/pytorch/issues/41028.

CC jithunnair-amd sunway513

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60607

Reviewed By: jbschlosser

Differential Revision: D29409894

Pulled By: ngimel

fbshipit-source-id: effca258a0f37eaefa35674a7fd19459ca7dc95b",147.0,97.0,"aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu,aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu,aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu,aten/src/ATen/native/cuda/AveragePool3d.cu,aten/src/ATen/native/cuda/DilatedMaxPool3d.cu,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/cuda/FractionalMaxPool2d.cu,aten/src/ATen/native/cuda/FractionalMaxPool3d.cu,aten/src/ATen/native/cuda/Indexing.cu,aten/src/ATen/native/cuda/KernelUtils.cuh,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/ReflectionPad.cu,aten/src/ATen/native/cuda/ReplicationPadding.cu,aten/src/ATen/native/cuda/ScatterGatherKernel.cu,aten/src/ATen/native/cuda/Sorting.cu,aten/src/ATen/native/cuda/SortingRadixSelect.cuh,aten/src/ATen/native/cuda/SummaryOps.cu,aten/src/ATen/native/cuda/UpSample.cuh,aten/src/ATen/native/cuda/UpSampleLinear1d.cu,aten/src/THC/THCAtomics.cuh,caffe2/operators/accuracy_op.cu,caffe2/operators/batch_gather_ops.cu,caffe2/operators/channelwise_conv3d_op_cudnn.cu,caffe2/operators/deform_conv_op.cu,caffe2/operators/depthwise_3x3_conv_op_cudnn.cu,caffe2/operators/multi_class_accuracy_op.cu,caffe2/operators/pad_op_gpu.cu,caffe2/operators/resize_3d_op.cu,caffe2/operators/resize_op.cu,caffe2/operators/roi_align_gradient_op.cu,caffe2/operators/roi_align_rotated_gradient_op.cu,caffe2/operators/roi_pool_op.cu,caffe2/operators/segment_reduction_op_gpu.cu,caffe2/operators/sparse_to_dense_op.cu,caffe2/operators/top_k_radix_selection.cuh,caffe2/operators/upsample_op.cu,caffe2/operators/utility_ops.cu,caffe2/sgd/adagrad_fused_op_gpu.cuh,caffe2/utils/GpuAtomics.cuh,caffe2/utils/math_gpu.cu,torch/utils/hipify/cuda_to_hip_mappings.py",42.0,13,3,4.839948927,15.0,28572.0,21.0,9100718.365853658,13440.0,30454.0,0.0,,0.0,1
pytorch,5b0dfd0f8aff50e2fce8f2f1fe6f2ef0594a9e25,d37636901ed1c65c1f8b68e36e37e59eb503c554,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Mon Aug 30 19:16:23 2021 -0700,1630350983.0,"[Doc] `make_tensor` to `torch.testing` module (#63925)

Summary:
This PR aims to add `make_tensor` to the `torch.testing` module in PyTorch docs.

TODOs:

* [x] Add examples

cc: pmeier mruberry brianjo

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63925

Reviewed By: ngimel

Differential Revision: D30633487

Pulled By: mruberry

fbshipit-source-id: 8e5a1f880c6ece5925b4039fee8122bd739538af",213.0,140.0,"docs/source/testing.rst,test/test_autograd.py,test/test_binary_ufuncs.py,test/test_buffer_protocol.py,test/test_foreach.py,test/test_indexing.py,test/test_jit.py,test/test_linalg.py,test/test_ops.py,test/test_reductions.py,test/test_shape_ops.py,test/test_sort_and_select.py,test/test_sparse.py,test/test_sparse_csr.py,test/test_tensor_creation_ops.py,test/test_testing.py,test/test_torch.py,test/test_unary_ufuncs.py,test/test_view_ops.py,torch/testing/__init__.py,torch/testing/_creation.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_modules.py,torch/testing/_internal/common_utils.py",24.0,6,3,2.659439936,47.0,77765.0,11.0,1516181.3043478262,15058.0,34486.5,0.0,Feature Addition,0.0,1
pytorch,7f8b02f5b7278e7a5d37fecc04ebeaafe70571bd,d38a71d579dc2f03d7a35132ccb168e697c1838e,Emilio Castillo,ecastill@preferred.jp,Mon Oct 19 20:09:16 2020 -0700,1603138156.0,"`torch.nn.modules.LazyModuleMixin` and `torch.nn.LazyLinear` (Shape Inference II) (#44538)

Summary:
Retake on https://github.com/pytorch/pytorch/issues/40493 after all the feedback from albanD

This PR implements the generic Lazy mechanism and a sample `LazyLinear` layer with the `UninitializedParameter`.

The main differences with the previous PR are two;
Now `torch.nn.Module` remains untouched.
We don't require an explicit initialization or a dummy forward pass before starting the training or inference of the actual module. Making this much simpler to use from the user side.

As we discussed offline, there was the suggestion of not using a mixin, but changing the `__class__` attribute of `LazyLinear` to become `Linear` once it's completely initialized. While this can be useful, by the time being we need `LazyLinear` to be a `torch.nn.Module` subclass since there are many checks that rely on the modules being instances of `torch.nn.Module`.
This can cause problems when we create complex modules such as
```
class MyNetwork(torch.nn.Module):
    def __init__(self):
        super(MyNetwork, self).__init__()
        self.conv = torch.nn.Conv2d(20, 4, 2)
        self.linear = torch.nn.LazyLinear(10)
    def forward(self, x):
        y = self.conv(x).clamp(min=0)
        return self.linear(y)
```
Here, when the __setattr__ function is called at the time LazyLinear is registered, it won't be added to the child modules of `MyNetwork`, so we have to manually do it later, but currently there is no way to do such thing as we can't access the parent module from LazyLinear once it becomes the Linear module. (We can add a workaround to this if needed).

TODO:

Add convolutions once the design is OK
Fix docstrings

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44538

Reviewed By: ngimel

Differential Revision: D24162854

Pulled By: albanD

fbshipit-source-id: 6d58dfe5d43bfb05b6ee506e266db3cf4b885f0c",662.0,12.0,"docs/source/nn.rst,test/distributed/test_c10d.py,test/distributed/test_data_parallel.py,test/test_nn.py,torch/jit/_recursive.py,torch/nn/__init__.py,torch/nn/modules/__init__.py,torch/nn/modules/lazy.py,torch/nn/modules/linear.py,torch/nn/modules/module.py,torch/nn/parallel/distributed.py,torch/nn/parameter.py,torch/nn/parameter.pyi,torch/nn/utils/spectral_norm.py,torch/nn/utils/weight_norm.py",15.0,10,3,2.469383763,44.0,22498.0,12.0,5949642.071428572,6075.0,13964.5,0.0,Corrective,1.0,1
pytorch,1fee7cd626570803a0dcbc841efa44c385af274c,d38cf0e1e91c2ffed6c4ce41defe2932e5a8b693,gchanan,gregchanan@gmail.com,Sat Jan 20 03:28:37 2018 -0500,1516418917.0,"Allow assertEqual checks with mixed Tensors, Variables, numbers. (#4754)

Currently, a Variable can only be compared with a Variable, but a Tensor
can be compared with Tensors or numbers.  Relax this constraint so Variables
behave identically to Tensors.",12.0,4.0,"test/common.py,test/test_torch.py",2.0,1,1,1,38.0,5826.0,2.0,509691.5,2292.0,24389.35823,0.0,,0.0,1
pytorch,2be8bd1880fbc433c063b3d9d1eb718e15a155cd,d38fccc586ba6e6c7f3811d7255c17d0ea34d54c,Adam CÃ©cile,acecile@le-vert.net,Mon Oct 30 22:36:35 2017 +0100,1509402995.0,Debian/Ubuntu comes with GCC 4.9.2 and it does require -D_FORCE_INLINES (#3380),6.0,6.0,"torch/lib/THC/CMakeLists.txt,torch/lib/THCS/CMakeLists.txt,torch/lib/THCUNN/CMakeLists.txt",3.0,5,1,1.584962501,37.0,645.0,1.0,349513.0,2025.0,23890.85823,0.0,,0.0,1
pytorch,60343a82e9238a09e60ddc980409409b08234082,d396c7332a852c55f00062b20f49d21b8a1a6d77,Lara,lahaidar@microsoft.com,Wed Sep 25 12:41:45 2019 -0700,1569415305.0,"Update ONNX Export for Interpolate in Opset 11 (#26778)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26778

- Add support for linear and cubic interpolate in opset 11.
- Add support for 1d and 3d interpolate in nearest mode for opset 7 and 8.
- Add tests for all cases of interpolate in ORT tests (nearest/linear/cubic, 1d/2d/3d, upsample/downsample).
Original PR resolved: https://github.com/pytorch/pytorch/pull/24805

Reviewed By: hl475

Differential Revision: D17564911

Pulled By: houseroad

fbshipit-source-id: 591e1f5b361854ace322eca1590f8f84d29c1a5d",154.0,90.0,"test/onnx/expect/TestOperators.test_upsample_nearest.expect,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/nn/functional.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",8.0,6,2,2.626486839,34.0,7807.0,1.0,38795.0,11717.0,32916.33333,0.0,Feature Addition,0.0,1
pytorch,e598ba2ef3c5f28d1395feceec863324acb0f094,d39790340db916e128b2b637cd12f4616fddb87d,BowenBao,bowbao@microsoft.com,Fri Oct 01 04:05:46 2021 -0700,1633061146.0,"[ONNX] Enable export of __xor_ (#64042) (#64581)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64581

* Enbale xor

* Update test_pytorch_onnx_onnxruntime.py

* Update symbolic_opset9.py

* Update symbolic_opset9.py

* Update test_pytorch_onnx_onnxruntime.py

* Update symbolic_opset9.py

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D30919598

Pulled By: malfet

fbshipit-source-id: 044e55d0697da0050f26a6ceccd1517493d7e8a6",41.0,10.0,"test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.286458932,3.0,15786.0,3.0,200168.0,15863.0,36649.0,0.0,,0.0,1
pytorch,cbe5ab11093355e5e8d0d09878d10de094cdf22f,d39ab0312ad79dde7d0b5b8c00ea53bea348524f,Vitaly Fedyunin,vitalyf@fb.com,Tue Oct 15 19:54:18 2019 -0700,1571169258.0,"Add memory_format support `to` and `type` operators (#27107)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27107

Adds memory_format keyword argument (positional for cpp).

'Preserve' behavior now follows next rules:
1) If tensor is non-overlapping and dense - output tensor will have the same strides as input tensor.
2) If not (1) and tensor is stored in the channels last format, output tensor going to have channels last format.
3) Output tensor is going to be contiguous in all other cases.

 ---
Dense tensor is the tensor that store values in a contiguous block of memory.
Non-overlapping tensor is the tensor in which elements occupy individual non-repetitive memory.

Test Plan: Imported from OSS

Differential Revision: D17931062

Pulled By: VitalyFedyunin

fbshipit-source-id: 2c5dd3dd05bf58a9a29f25562cd45190b009c3f9",139.0,96.0,"aten/src/ATen/native/TensorConversions.cpp,aten/src/ATen/native/native_functions.yaml,test/test_torch.py,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/autograd/utils/python_arg_parsing.h,torch/onnx/symbolic_opset9.py",6.0,13,4,2.246235544,42.0,23788.0,6.0,5334146.0,12297.0,34364.33333,0.0,Feature Addition,0.0,1
pytorch,7e6312a5df6463481ca969e84743076ff8748cfd,d3bbb281f37e6c5a8db8799d584dcb0eb77494a7,kshitij12345,kshitijkalambarkar@gmail.com,Wed Jan 26 17:22:56 2022 -0800,1643217776.0,"[numpy] add decimals argument to round (#66195)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/65908

Added a new overload instead of updating the current signature. (Had issues with JIT and **maybe** it would have been FC breaking)

TODO:

* [x] Don't compute `std::pow(10, decimals)` for each element.
* [x] Update docs (https://docs-preview.pytorch.org/66195/generated/torch.round.html?highlight=round#torch.round)
* [x] Add tests
* ~~Should we try to make it composite?~~
* ~~Should we add specialized test with more values of `decimals` outside of OpInfo with larger range of values in input tensor?~~

cc mruberry rgommers

Pull Request resolved: https://github.com/pytorch/pytorch/pull/66195

Reviewed By: anjali411

Differential Revision: D31821385

Pulled By: mruberry

fbshipit-source-id: 9a03fcb809440f0c83530108284e69c345e1850f
(cherry picked from commit 50b67c696880b8dcfc42796956b4780b83bf7a7e)",194.0,20.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryFractionKernels.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",10.0,12,3,2.807131216,35.0,48876.0,9.0,2182562.5,210.0,415.5,0.0,Corrective,1.0,1
pytorch,073dfd8b887e619b68a2a92e69e220ec82b33323,d40a7bf9eb25aacf1d4568d8b076c5b6b4fab6d0,Sam Gross,colesbury@gmail.com,Fri Nov 18 18:58:09 2016 -0500,1479495489.0,Fix Scatter.backward() (#232),24.0,16.0,"test/test_nn.py,torch/nn/parallel/functions.py",2.0,4,2,0.384311544,14.0,1144.0,2.0,162793.5,72.0,53.81012321,0.0,Corrective,1.0,1
pytorch,28890b20461453d21564142295e727dec58044c1,d41b6c7daa11a93b6d8d17e7c814a9f37c9de825,Sam Gross,colesbury@gmail.com,Wed Dec 13 20:40:34 2017 -0500,1513197634.0,"Implement remaining random methods through ATen (#4137)

* Implement remaining random methods through ATen

* Change test_bernoulli on Tensor to avoid broadcasting

The new ATen-dispatched bernoulli_ supports broadcasting. The old
Tensor.bernoulli_ bindings instead require the tensors to have the same
number of elements. I haven't change the old code because it will be
deleted soon.",82.0,65.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/TH/generic/THTensorRandom.c,aten/src/TH/generic/THTensorRandom.h,test/test_torch.py,tools/autograd/derivatives.yaml,torch/autograd/variable.py",10.0,11,4,2.961948162,38.0,12509.0,7.0,605954.5,45.0,92.0,0.0,Preventative,1.0,1
pytorch,e055ffbdc7a09abd58802a116b0d885cd6e31b95,d467a068c22087a94bd0388845acfc4f291d8651,Adam Paszke,adam.paszke@gmail.com,Fri Aug 19 21:23:07 2016 -0700,1471641787.0,Add tests for new modules,921.0,587.0,".travis.yml,test/common.py,test/common_nn.py,test/test_autograd.py,test/test_legacy_nn.py,test/test_nn.py",6.0,1,1,1.676545034,5.0,1607.0,1.0,2254.0,124.0,2211.528571,0.0,Feature Addition,0.0,1
pytorch,80d8a2a2376bca1b9c33527310f9f33e588da82d,d47f715d29d05e28b94c280f15dce097ef3dc7cb,Antoni Viros,aviros@ibm.com,Fri Dec 01 20:15:35 2023 +0000,1701461735.0,"Expose Flash attn to autograd (#114378)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/114378
Approved by: https://github.com/drisspg",174.0,23.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/transformers/cuda/attention.cu,aten/src/ATen/native/transformers/cuda/attention_backward.cu,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,tools/autograd/derivatives.yaml,torch/_inductor/lowering.py,torch/_meta_registrations.py,torch/_subclasses/fake_utils.py,torch/testing/_internal/common_methods_invocations.py",12.0,17,4,2.279183916,21.0,63193.0,8.0,787795.0833333334,22623.0,51444.0,0.0,,0.0,1
pytorch,9ad91913233b363f00ae018ac6d44252d5153e13,d4832f1e7b553e8b6a61ab666517506b051d8f71,Orion Reblitz-Richardson,orionr@gmail.com,Mon Aug 20 16:57:36 2018 -0700,1534784256.0,"More fixes for hidden visibility (#10624)

Summary:
Some more `ATEN_API` additions for hidden visibility.

Running CI tests to see what fails to link.

cc Yangqing mingzhe09088 ezyang
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10624

Reviewed By: mingzhe09088

Differential Revision: D9392728

Pulled By: orionr

fbshipit-source-id: e0f0861496b12c9a4e40c10b6e0c9e0df18e8726",65.0,81.0,"caffe2/core/common.h,caffe2/core/graph.h,caffe2/core/net_simple.h,caffe2/core/operator.h,caffe2/core/operator_gradient.h,caffe2/core/stats.h,caffe2/core/transform.h,caffe2/onnx/onnx_exporter.h,caffe2/operators/generate_proposals_op.h,caffe2/opt/backend_cutting.h,caffe2/opt/converter.h,caffe2/opt/device.h,caffe2/opt/fusion.h,caffe2/opt/mobile.h,caffe2/opt/onnx_convert.h,caffe2/opt/onnxifi_transformer.h,caffe2/opt/optimize_ideep.h,caffe2/opt/optimizer.h,caffe2/opt/passes.h,caffe2/opt/sink.h,caffe2/transforms/common_subexpression_elimination.h,caffe2/transforms/conv_to_nnpack_transform.h,caffe2/transforms/pattern_net_transform.h,caffe2/transforms/single_op_transform.h",24.0,6,1,4.02941462,15.0,3429.0,15.0,6632065.291666667,3553.0,9679.333333,0.0,Corrective,1.0,1
pytorch,e21e4bf3e833c374f30a0e305e8eb5fc9268a66c,d48afd41f9f86f23dab0ed3cd0cdaa2f2cd3a523,Dmitry Ulyanov,dmitry.ulyanov.msu@gmail.com,Wed Apr 12 13:58:28 2017 +0300,1492005508.0,"Add print string for MaxPool3d, change for MaxPool2d (#1115)",8.0,0.0,torch/nn/modules/pooling.py,1.0,3,1,0,27.0,844.0,1.0,46664.0,534.0,3938.995885,0.0,Feature Addition,0.0,1
pytorch,4466ba8f30c60e295dc927cd639396b6dd01e311,d4ae7896554d156732de34c3d3600050f9cb18ec,Richard Zou,zou3519@gmail.com,Thu Nov 11 15:20:08 2021 -0800,1636644008.0,"OpInfos for new_blah functions and some _like functions (#67357)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67357

This PR adds OpInfos for:
- new_ones, new_zeros, new_full, new_empty
- rand_like, randint_like

I forgot to add the _like functions in a previous PR, so here they are.

Test Plan: - wait for tests

Reviewed By: mruberry

Differential Revision: D31969533

Pulled By: zou3519

fbshipit-source-id: 236d70d66e82f1d6f8e5254b55ca2a37b54c9494",141.0,1.0,"test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.312564862,2.0,18314.0,3.0,621841.0,17014.0,40014.5,0.0,Feature Addition,0.0,1
pytorch,ae0c04c773d9fe784d04d452cd289b200e906670,d4c0538be27ac6f537bf1d82559d805baef5101d,gchanan,gregchanan@gmail.com,Fri Mar 09 20:41:14 2018 -0500,1520628074.0,Add device to Tensor.new_tensor. (#5669),7.0,3.0,"test/test_torch.py,torch/csrc/utils/tensor_new.cpp",2.0,4,2,0.881290899,38.0,6241.0,1.0,705.0,599.0,1842.405869,0.0,Feature Addition,0.0,1
pytorch,956d946c25ece0051931a023f45d4ee7d39997f1,d4c9a3782bfe786b00851b05cc550aa32915deb9,Soumith Chintala,soumith@gmail.com,Sun Jan 29 23:38:48 2017 +0500,1485733128.0,"billinear -> bilinear, docs for upsampling, improved docs for Unpooling, pep8 tests fix (#617)

* billinear -> bilinear, docs for upsampling, improved docs for Unpooling, pep8 tests fix",183.0,19.0,"docs/source/nn.rst,test/optim/test.py,test/test_autograd.py,test/test_nn.py,test/test_optim.py,torch/legacy/nn/ClassSimplexCriterion.py,torch/nn/_functions/thnn/upsampling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py,torch/nn/modules/upsampling.py",11.0,11,3,2.140808746,24.0,5314.0,2.0,155000.18181818182,158.0,629.3702739,0.0,Corrective,1.0,1
pytorch,de58a27769fbb0587dfbfb55cbbc7c5b39d30c65,d4d0ab71b3594ea8ccb111dce2362bda89af8823,Philip Meier,github.pmeier@posteo.de,Thu Jan 27 07:35:24 2022 -0800,1643268924.0,"use `torch.testing.assert_equal` in `TestCase.assertEqual` (#67796)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67796

Supersedes #58981.

cc mruberry

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D33542994

Pulled By: mruberry

fbshipit-source-id: 527099f5fdc154fd95ee48cd19f0a85eeec43443
(cherry picked from commit 1a58915e2cfde5c48ad77198a917872a03fd1b72)",387.0,679.0,"test/quantization/core/test_quantized_tensor.py,test/test_foreach.py,test/test_fx.py,test/test_namedtensor.py,test/test_testing.py,torch/testing/_comparison.py,torch/testing/_core.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",9.0,6,2,1.787458743,3.0,29959.0,8.0,3726410.888888889,239.0,486.0,0.0,,0.0,1
pytorch,8307f21bf6a42818e2f7b44371beb85899c182ec,d4d8698581a213aa0babccf6c06faf2210db1ff4,Maciej Kula,maciejkula@users.noreply.github.com,Sat Dec 16 13:09:02 2017 +0000,1513429742.0,Fix repeat non owning (#4084),48.0,18.0,"test/test_torch.py,torch/tensor.py",2.0,2,2,0.997348798,38.0,5544.0,2.0,425948.5,847.0,6600.172317,0.0,Corrective,1.0,1
pytorch,5b3e7638ca62bae34b87c79c7e1d2bdcd0f162a4,d4ddb477191316d555c61e4fa30ca494a2c53319,kshitij12345,kshitijkalambarkar@gmail.com,Fri Apr 30 12:49:56 2021 -0700,1619786996.0,"[special] Add `xlog1py` (#55138)

Summary:
Reference : https://github.com/pytorch/pytorch/issues/50345

* [x] Check Rendered Document (https://12494173-65600975-gh.circle-artifacts.com/0/docs/special.html#torch.special.xlog1py)
* [x] Tests in Binary Ufunc
* [x] OpInfo
* [x] Structured Kernel

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55138

Reviewed By: ngimel

Differential Revision: D27961461

Pulled By: mruberry

fbshipit-source-id: 30a8f41970a829bf50254aadf5615e8ce4148c7e",306.0,51.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_binary_ufuncs.py,tools/autograd/derivatives.yaml,torch/_torch_docs.py,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",14.0,20,5,2.883163412,33.0,36251.0,11.0,835266.4285714285,11486.0,25963.0,0.0,Feature Addition,0.0,1
pytorch,0f0ef4fe644e6f772d3e4a76a1c357b260f9c55a,d4ff344fae461595ff8bf32788964f5d80515830,BowenBao,bowbao@microsoft.com,Fri Oct 01 04:05:46 2021 -0700,1633061146.0,"[ONNX] Fix remainder export (#64230) (#64578)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/64578

* Fix remainder export for edge case when input is negative. New export relies on true_divide export.
* Simplified true_divide export. Cleaned up redundant code which is handled by scalar type analysis pass. Removed dependency on `onnx::Where`, thus supports opset 7 & 8.

Fixes #60179

Test Plan: Imported from OSS

Reviewed By: jansel

Differential Revision: D30919601

Pulled By: malfet

fbshipit-source-id: 0f78621c0ac3bdb6bf4225e049ba5f470dc8ab12

Co-authored-by: BowenBao <bowbao@microsoft.com>",58.0,55.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset7.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",6.0,4,2,2.133391152,3.0,15436.0,5.0,4263445.5,15860.0,36645.0,0.0,Corrective,1.0,1
pytorch,8327982904d7b2ffb2096314ba9de02ce495b806,d5038309a150d028ffa445ed5e05d07ca5e59e0b,gchanan,gregchanan@gmail.com,Tue Feb 27 19:51:11 2018 -0500,1519761071.0,"Remove WITH_SCALARS, as it's enabled by default now. (#5437)",68.0,198.0,"setup.py,test/test_autograd.py,test/test_distributions.py,test/test_indexing.py,test/test_nn.py,test/test_torch.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/VariableType.cpp,torch/csrc/Module.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/variable.h,torch/distributions/distribution.py,torch/distributions/utils.py",14.0,8,3,2.951884989,40.0,23983.0,9.0,446758.8571428572,561.0,1686.405869,0.0,,0.0,1
pytorch,11598da229a58ea9e36a6fcde2cf6ab6c73c14f4,d54cf2aa2710023ccedc2b7a89e8a3a45dc01e78,Hao Lu,hlu@fb.com,Tue Dec 22 06:06:57 2020 -0800,1608617217.0,"[pt][ATen] Optimize bmm (#49506)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/49506

- Get rid of expensive stuff like `TensorArg`, `checkBackend`, `checkSize`, and `TensorAccessor`.
- Add `checkDim` that does not require creating a `TensorArg` which incurs a refcount bump
- Avoid unnecessary calls to `torch.select`, which goes through the dispatcher in the cases we care about, with mat1 and mat2 not permuted or permuted with dims = [0, 2, 1]. The pt version of bmm supports crazy cases like when the inputs are permuted with dims = [1, 2, 0], which is uncommon in SparseNNs.

Test Plan:
Unit test:
```
buck test //caffe2/test:linalg
```

Benchmark with the adindexer model:
```
Before:
I1216 14:02:24.155516 2595800 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0847197. Iters per second: 11803.6
After:
I1216 14:02:26.583878 2595939 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.082051. Iters per second: 12187.5
```

Reviewed By: bwasti

Differential Revision: D25577574

fbshipit-source-id: 8aba69b950e7b4d9d1b14ba837931695a908c068",109.0,43.0,"aten/src/ATen/TensorUtils.cpp,aten/src/ATen/TensorUtils.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/mkl/LinearAlgebra.cpp,test/test_linalg.py",5.0,6,2,1.678845567,8.0,8820.0,5.0,3443474.0,7708.0,17367.0,0.0,Feature Addition,1.0,1
pytorch,f07ac6a00498f4671b7fc0f74f9750ef3fb803cd,d5748d9a1afa85d9e088306234e0b51155ad6f7b,Iurii Zdebskyi,iuriiz@devfair004.maas,Fri Sep 25 19:54:21 2020 -0700,1601063661.0,"Enable binary ops with Scalar Lists with for foreach APIs (#45298)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/45298

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D23931986

Pulled By: izdeby

fbshipit-source-id: 281267cd6f90d57a169af89f9f10b0f4fcab47e3",837.0,115.0,"aten/src/ATen/native/ForeachOpsKernels.cpp,aten/src/ATen/native/ForeachUtils.h,aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu,aten/src/ATen/native/cuda/ForeachFunctors.cuh,aten/src/ATen/native/cuda/MultiTensorApply.cuh,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py,test/test_foreach.py,tools/autograd/gen_python_functions.py,tools/autograd/templates/python_torch_functions.cpp,tools/codegen/model.py,tools/pyi/gen_pyi.py,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h",14.0,15,4,2.149619096,16.0,15146.0,2.0,83287.85714285714,5454.0,12825.5,0.0,,0.0,1
pytorch,7c29ca7f2b49a91dbfc6ee35bf6da55616bd2960,d5988c5eca0221e9ef58918e4f0b504940cb926a,Philip Meier,github.pmeier@posteo.de,Fri Jun 18 14:22:22 2021 -0700,1624026142.0,"remove unused `type: ignore` directives (#60006)

Summary:
During development it is common practice to put `type: ignore` comments on lines that are correct, but `mypy` doesn't recognize this. This often stems from the fact, that the used `mypy` version wasn't able to handle the used pattern.

With every new release `mypy` gets better at handling complex code. In addition to fix all the previously accepted but now failing patterns, we should also revisit all `type: ignore` comments to see if they are still needed or not. Fortunately, we don't need to do it manually: by adding `warn_unused_ignores = True` to the configuration, `mypy` will error out in case it encounters an `type: ignore` that is no longer needed.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60006

Reviewed By: jbschlosser, malfet

Differential Revision: D29133237

Pulled By: albanD

fbshipit-source-id: 41e82edc5cd5affa7ccedad044b59b94dad4425a",108.0,93.0,".github/scripts/lint_native_functions.py,caffe2/contrib/aten/gen_op.py,caffe2/distributed/file_store_handler_op_test.py,caffe2/distributed/redis_store_handler_op_test.py,mypy-strict.ini,mypy.ini,test/test_futures.py,test/test_utils.py,tools/pyi/gen_pyi.py,tools/render_junit.py,torch/distributed/_sharded_tensor/api.py,torch/distributed/distributed_c10d.py,torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py,torch/distributed/elastic/utils/data/elastic_distributed_sampler.py,torch/distributed/launcher/api.py,torch/distributed/nn/api/remote_module.py,torch/distributed/rendezvous.py,torch/distributions/utils.py,torch/fx/experimental/normalize.py,torch/fx/passes/net_min_base.py,torch/fx/passes/split_module.py,torch/jit/_monkeytype_config.py,torch/jit/mobile/__init__.py,torch/nn/quantized/modules/conv.py,torch/nn/utils/parametrizations.py,torch/package/package_importer.py,torch/quantization/fx/convert.py,torch/quantization/fx/prepare.py,torch/testing/_internal/common_distributed.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py,torch/testing/_internal/dist_utils.py,torch/utils/data/dataloader.py,torch/utils/data/dataset.py,torch/utils/data/distributed.py,torch/utils/model_dump/__init__.py,torch/utils/tensorboard/writer.py",37.0,38,5,4.532516904,44.0,26575.0,33.0,3244555.5945945946,13140.0,29738.5,0.0,Corrective,1.0,1
pytorch,6eaf96961d6d8b0c93b883bbb4d19c9bb4cd2e4e,d59fb7a2f6d531811800c6667dd9275b28febee2,Ilqar Ramazanli,iramazanli@fb.com,Sat Mar 27 15:20:07 2021 -0700,1616858407.0,"Add complex autograd support for `torch.unfold` (#52999)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/51875

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52999

Reviewed By: H-Huang

Differential Revision: D26735206

Pulled By: iramazanli

fbshipit-source-id: ee134461e97079722a79f89737a7f0d2b620c2c8",48.0,26.0,"test/test_autograd.py,tools/autograd/gen_variable_type.py,torch/testing/_internal/common_methods_invocations.py",3.0,6,3,0.35742889,42.0,13681.0,3.0,79200.66666666667,10125.0,22402.5,0.0,Corrective,1.0,1
pytorch,e8ec4110f6b65a30d2a423d458c621c44c803a60,d5a0f97ea7859ae3fd36b4fb5e5716a1c57525f0,Gregory Chanan,gchanan@fb.com,Wed Jun 07 17:13:29 2017 -0700,1496855609.0,"Renamed masked_copy to masked_scatter in test, fix use of break/continue.",6.0,6.0,test/test_torch.py,1.0,1,1,0,31.0,3679.0,1.0,0.0,912.0,11498.94394,0.0,Corrective,1.0,1
pytorch,ddb1f293b688f5769b98f09c92eca7ddcab749e4,d5a44f9f12e4ad681def9f88ee18c0586629f2d8,Sam Estep,sestep@fb.com,Mon Jun 28 22:37:58 2021 -0700,1624919878.0,"Use expecttest from PyPI (#60658)

Summary:
This PR removes `torch/testing/_internal/expecttest.py` in favor of https://github.com/ezyang/expecttest. See also https://github.com/ezyang/ghstack/pull/71.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60658

Test Plan: CI.

Reviewed By: ezyang

Differential Revision: D29430763

Pulled By: samestep

fbshipit-source-id: b7cdc7ba37330176149fd465312118e2254ae92e",7.0,454.0,".circleci/docker/common/install_conda.sh,.github/workflows/lint.yml,.jenkins/pytorch/macos-test.sh,.jenkins/pytorch/win-test-helpers/build_pytorch.bat,.jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat,mypy.ini,test/benchmark_utils/test_benchmark_utils.py,test/run_test.py,test/test_expecttest.py,torch/testing/_internal/common_utils.py,torch/testing/_internal/expecttest.py",11.0,13,5,1.177959901,9.0,6458.0,9.0,1561864.3636363635,13438.0,30432.5,0.0,,0.0,1
pytorch,7ff9e0eb6c61855ba9721b3fdfa28c20c527cf4b,d5a7e304fa858136eeffa1ebe35e22ed886bf256,SsnL,tongzhou.wang.1994@gmail.com,Mon Sep 18 23:44:39 2017 -0700,1505778279.0,added volumetric adaptive max pooling,870.0,5.0,"docs/source/nn.rst,test/test_nn.py,torch/lib/THCUNN/VolumetricAdaptiveMaxPooling.cu,torch/lib/THCUNN/generic/THCUNN.h,torch/lib/THCUNN/generic/VolumetricAdaptiveMaxPooling.cu,torch/lib/THNN/generic/THNN.h,torch/lib/THNN/generic/VolumetricAdaptiveMaxPooling.c,torch/lib/THNN/init.c,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py",13.0,13,3,2.568711244,38.0,12499.0,4.0,299156.6,1873.0,24943.55562,0.0,Feature Addition,0.0,1
pytorch,d1f3d85fd80de2166114af1b5e16070a6d33a898,d5bfdd3dac33dfa84e2a511fa79c4ad4e0e6b822,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Wed Sep 01 15:48:25 2021 -0700,1630511305.0,"OpInfo for `nn.functional.layer_norm` (#63276)

Summary:
Please see https://github.com/facebookresearch/functorch/issues/78 and https://github.com/pytorch/pytorch/issues/54261.

Note:

* This PR also adds a reference test inspired by existing tests in `test_nn.py`.

cc: mruberry zou3519

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63276

Reviewed By: ejguan

Differential Revision: D30452483

Pulled By: zou3519

fbshipit-source-id: 2578d01ca34e031668a41bd284db60c31ae1fba8",65.0,26.0,"test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.863120569,43.0,28289.0,1.0,82809.0,15130.0,34668.0,0.0,Feature Addition,0.0,1
pytorch,bdfef2975c5accb86d60134e197575d734e45c47,d5e45b227848990d74f8a48c0c95eca97d5f971d,Sam Gross,colesbury@gmail.com,Thu Jan 12 20:07:11 2017 -0500,1484251631.0,Add AvgPool1d which just uses AvgPool2d implementation (#439),202.0,67.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py",6.0,8,3,1.714926728,21.0,3024.0,5.0,642990.5,123.0,821.6565415,0.0,Feature Addition,0.0,1
pytorch,ecf3ca00d85567a81b8bee29982803945a581a4c,d5ed57569b35aaa56850259a988130c0625addf2,Jane Xu,janeyx@fb.com,Tue Feb 23 22:15:28 2021 -0800,1614118528.0,"Move cuda9 and cuda11.2 CI jobs to a scheduled workflow (#52693)

Summary:
Moving master only resource-interactive CI jobs to a less regular basis.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/52693

Reviewed By: malfet, seemethere

Differential Revision: D26615060

Pulled By: janeyx99

fbshipit-source-id: def46a7890ea46c655ef2ee0f7c548171464cb48",193.0,159.0,".circleci/cimodel/data/pytorch_build_data.py,.circleci/cimodel/data/simple/docker_definitions.py,.circleci/cimodel/data/windows_build_definitions.py,.circleci/config.yml,.circleci/generate_config_yml.py,.circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml",6.0,6,1,1.308156696,3.0,9957.0,4.0,1104607.0,9139.0,20404.5,0.0,,0.0,1
pytorch,0876bab8b782ef5e91b4830001a87ed8065ba43c,d6050582122c09e52e51b518f5af9be04e12e021,Sam Gross,colesbury@gmail.com,Mon Dec 18 20:46:13 2017 -0500,1513629973.0,"Replace Variable.volatile with torch.no_grad() (#3970)

This removes volatile from Variable. The functionality is mostly
replaced by a global (thread-local) flag, which is controlled by
torch.set_grad_enabled() and the context manager torch.no_grad().

In C++, the flag is exposed through GradMode::is_enabled() and GradMode::set_enabled()

Fixes #3627",552.0,528.0,"aten/src/ATen/Declarations.cwrap,setup.py,test/test_autograd.py,test/test_jit.py,test/test_multiprocessing.py,test/test_nn.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/VariableType.cpp,torch/__init__.py,torch/autograd/__init__.py,torch/autograd/function.py,torch/autograd/grad_mode.py,torch/autograd/gradcheck.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/autograd/autograd.h,torch/csrc/autograd/engine.cpp,torch/csrc/autograd/engine.h,torch/csrc/autograd/function.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/functions/accumulate_grad.cpp,torch/csrc/autograd/functions/special.cpp,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/functions/utils.cpp,torch/csrc/autograd/functions/utils.h,torch/csrc/autograd/grad_mode.cpp,torch/csrc/autograd/grad_mode.h,torch/csrc/autograd/init.cpp,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_engine.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/autograd/saved_variable.cpp,torch/csrc/autograd/saved_variable.h,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/jit/interpreter.cpp,torch/csrc/jit/interpreter_autograd_function.cpp,torch/csrc/jit/python_arg_flatten.cpp,torch/csrc/jit/python_arg_flatten.h,torch/csrc/jit/python_compiled_function.cpp,torch/csrc/jit/variable_flags.cpp,torch/csrc/jit/variable_flags.h,torch/jit/__init__.py,torch/nn/_functions/thnn/activation.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/parallel/distributed.py,torch/nn/parallel/parallel_apply.py,torch/nn/parameter.py,torch/optim/optimizer.py",52.0,20,4,4.606205669,40.0,26421.0,28.0,1202740.387755102,376.0,1174.405869,0.0,Corrective,1.0,1
pytorch,9dc6d42c189ab38e391a573f0654a2473018ed58,d63db52349ae3cffd6f762c9027e7363a6271d27,Kulin Seth,kulinseth@gmail.com,Sat May 28 14:41:56 2022 +0000,1653748916.0,"MPS: Fixes the as_strided_mps implementation for contiguous view operations (#78440)

Fixes https://github.com/pytorch/pytorch/issues/78107; https://github.com/pytorch/pytorch/issues/77750

Pull Request resolved: https://github.com/pytorch/pytorch/pull/78440
Approved by: https://github.com/malfet",180.0,85.0,"aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/mps/operations/Indexing.mm,test/test_mps.py",6.0,7,2,1.848108399,1.0,6126.0,3.0,338797.3333333333,3725.0,8787.0,0.0,Corrective,1.0,1
pytorch,50a63ee6fd826669fe47c24585064718b5161e1f,d6a8d28d6529a4f0b80a8c046ca9c36ca6c8b347,Zachary DeVito,zdevito@gmail.com,Mon Nov 06 22:46:15 2017 -0800,1510008375.0,"Simplify ATen Build (#3496)

* THS build change

* merge THCS into ATen build

* THCUNN build change over

* update THNN build

* move THC build to ATen, as well as some of the accumulated top level config from other TH* libraries

* TH library build merged into ATen, and warnings fixes.

* fix magma support checking

* check cuda early

* fall back to GCC atomics if C11 atomics have issues.

* fix install name

* disable openmp in files that also include stdatomic.h

* make sure LAPACK is visible to TH build file.",910.0,1397.0,"aten/CMakeLists.txt,aten/cmake/FindMAGMA.cmake,aten/cmake/select_compute_arch.cmake,aten/src/ATen/ATenConfig.cmake.in,aten/src/ATen/CMakeLists.txt,aten/src/TH/CMakeLists.txt,aten/src/TH/vector/AVX2.c,aten/src/THC/CMakeLists.txt,aten/src/THC/THCCachingHostAllocator.cpp,aten/src/THC/generic/THCTensorMath.cu,aten/src/THCS/CMakeLists.txt,aten/src/THCS/cmake/FindMAGMA.cmake,aten/src/THCS/cmake/select_compute_arch.cmake,aten/src/THCUNN/CMakeLists.txt,aten/src/THCUNN/LookupTableBag.cu,aten/src/THCUNN/generic/LogSoftMax.cu,aten/src/THCUNN/generic/SoftMax.cu,aten/src/THCUNN/generic/SpatialCrossMapLRN.cu,aten/src/THCUNN/generic/Threshold.cu,aten/src/THNN/CMakeLists.txt,aten/src/THS/CMakeLists.txt",21.0,14,1,3.411216099,2.0,3237.0,5.0,324499.1052631579,2075.0,24009.85823,0.0,Corrective,1.0,1
pytorch,939877bf4b5f37307eebd0cb035d65212b9449f6,d6c53328f9c9f0b569f1d32e4df06fdf5b573c66,Peter Goldsborough,psag@fb.com,Fri Dec 07 20:22:49 2018 -0800,1544214169.0,"Large scale fix of python-related files in torch/csrc/

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/14515

Differential Revision: D13247966

Pulled By: goldsborough

fbshipit-source-id: 7a127c508fc576a7a92626dd6b729f660162d628",465.0,462.0,".clang-tidy,tools/run-clang-tidy-in-ci.sh,torch/csrc/DataLoader.cpp,torch/csrc/Device.cpp,torch/csrc/Device.h,torch/csrc/Dtype.cpp,torch/csrc/Exceptions.h,torch/csrc/Generator.cpp,torch/csrc/Layout.cpp,torch/csrc/Module.cpp,torch/csrc/PtrWrapper.cpp,torch/csrc/Size.cpp,torch/csrc/Storage.cpp,torch/csrc/THP.h,torch/csrc/TypeInfo.cpp,torch/csrc/autograd/python_anomaly_mode.h,torch/csrc/autograd/python_cpp_function.cpp,torch/csrc/autograd/python_engine.cpp,torch/csrc/autograd/python_engine.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_function.h,torch/csrc/autograd/python_hook.h,torch/csrc/autograd/python_legacy_variable.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/byte_order.cpp,torch/csrc/byte_order.h,torch/csrc/copy_utils.h,torch/csrc/cuda/Module.cpp,torch/csrc/cuda/Storage.cpp,torch/csrc/cuda/Tensor.cpp,torch/csrc/distributed/c10d/init.cpp,torch/csrc/generic/Storage.cpp,torch/csrc/jit/batched/BatchTensor.h,torch/csrc/jit/constants.cpp,torch/csrc/jit/init.cpp,torch/csrc/jit/interpreter.h,torch/csrc/jit/passes/alias_analysis.cpp,torch/csrc/jit/passes/alias_analysis.h,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/to_batch.cpp,torch/csrc/jit/python_arg_flatten.h,torch/csrc/jit/python_interpreter.cpp,torch/csrc/jit/python_ir.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/script/compiler.h,torch/csrc/jit/script/init.cpp,torch/csrc/jit/script/python_tree_views.cpp,torch/csrc/serialization.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils.cpp,torch/csrc/utils/invalid_arguments.cpp,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/tensor_dtypes.cpp,torch/csrc/utils/tensor_layouts.cpp",57.0,15,2,4.72758824,43.0,14974.0,46.0,5648163.596491228,5912.0,17866.33333,0.0,Corrective,1.0,1
pytorch,bddba1e336571c5bf324ea4848070dd2fef497eb,d6feb6141f4a4b4cfe324cd59665d5d62018dae8,Kimish Patel,kimishpatel@fb.com,Thu Jul 09 23:20:22 2020 -0700,1594336822.0,"[Vec256][neon] Add neon backend for vec256 (#39341)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39341

This PR introduces neon backend for vec256 class for float datatype.
For now only aarch64 is enabled due to few issues with enabling in
aarch32 bit.

Test Plan:
vec256_test

Imported from OSS

Differential Revision: D21822399

fbshipit-source-id: 3851c4336d93d1c359c85b38cf19904f82bc7b8d",1367.0,5.0,"CMakeLists.txt,aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/cpu/vec256/intrinsics.h,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_float_neon.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/vec256_test.cpp,caffe2/CMakeLists.txt,cmake/Dependencies.cmake,scripts/build_android.sh",13.0,9,4,1.315842963,75.0,7428.0,6.0,1885216.6363636365,3506.0,8317.5,0.0,Feature Addition,0.0,1
pytorch,e4eee7c2cf43f4edba7a14687ad59d3ed61d9833,d707dae013778aa5fef01c787c903b0edce90add,Ailing,ailzhang@users.noreply.github.com,Wed Mar 21 20:55:06 2018 -0400,1521665706.0,"Add half test in test_nn for auto generated tests. (#5362)

* add half and double test in NewTestModule

* add half/double/float tests in NewCriterionTest

* resolve merge conflict with master",83.0,2.0,"test/common.py,test/test_nn.py",2.0,1,1,0.322756959,38.0,7226.0,2.0,52444.5,2493.0,24839.35823,0.0,Feature Addition,0.0,1
pytorch,6162a043640cf01695cf568edd0be047d56477ff,d710c95cc01486b7f2922799dd033da9893b0e21,lezcano,lezcano-93@hotmail.com,Fri Sep 16 15:27:25 2022 +0000,1663342045.0,"Implement forward AD for scatter_reduce (#85000)

I left the case `reduction=""prod""` for future work as it's a bit of a pain.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85000
Approved by: https://github.com/soulitzer",83.0,28.0,"functorch/test/test_ops.py,functorch/test/test_vmap.py,tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",6.0,9,3,2.072404388,18.0,34272.0,5.0,138076.5,7401.0,17379.0,0.0,,0.0,1
pytorch,0013611c8129c4481852e25a7b0813d631399d9f,d7317d8a1172dce9871803ff4b2d92d1acf869d6,Adnan Akhundov,aakhundov@meta.com,Fri Oct 13 19:11:24 2023 -0700,1697224284.0,"Fix size_hint call sites failing on unbacked SymInts (#110520)

Summary: Unbacked SymInts can't get a `sizevars.size_hint` due to being data-dependent. #109893 has added a new `fallback` parameter to `sizevars.size_hint` to specify the fallback value in cases like unbacked SymInt. In this PR we add more of those.

Test Plan: CI

Reviewers:

Subscribers:

Tasks:

Tags:

Pull Request resolved: https://github.com/pytorch/pytorch/pull/110520
Approved by: https://github.com/jansel, https://github.com/ezyang",230.0,44.0,"test/inductor/test_unbacked_symints.py,torch/_inductor/autotune_process.py,torch/_inductor/config.py,torch/_inductor/dependencies.py,torch/_inductor/ir.py,torch/_inductor/kernel/mm_common.py,torch/_inductor/lowering.py,torch/_inductor/scheduler.py,torch/_inductor/select_algorithm.py,torch/_inductor/sizevars.py",10.0,5,2,2.943365923,2.0,16616.0,7.0,696902.4444444445,20777.0,47499.0,0.0,Corrective,1.0,1
pytorch,825f4714f92818975c5a900bba45297299a4fa8e,d7a1152ee9948e21f337264460c5f6d12e4ed67f,Zhang Zhi,850734033@qq.com,Sun Sep 08 05:46:24 2019 -0700,1567921584.0,"Fix error message stack overflow (#25146)

Summary:
When the given input size is larger than expected, `weight_sizes` is `k`-length but only has `weight_dim` numbers. And it causes the confusing error message:
```
RuntimeError: Expected 4-dimensional input for 4-dimensional
weight 256 5 3 3 3987964488216321853 94670871813000,
but got 6-dimensional input of size [1, 61, 1, 5, 64, 64] instead
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25146

Differential Revision: D17233651

Pulled By: soumith

fbshipit-source-id: c6ddfa45e854f9b95ca253052f8bc358e35fd9d4",2.0,2.0,aten/src/ATen/native/Convolution.cpp,1.0,4,1,0,5.0,960.0,1.0,965674.0,11228.0,31717.33333,0.0,Corrective,1.0,1
pytorch,2f47e953f786451afaf50f045de3e7ebf6f72733,d7c9f96e43e1399599eb599bfa5b5379a89060db,Sebastian Messmer,messmer@fb.com,Thu Jul 02 02:24:23 2020 -0700,1593656663.0,"Optimize perf for calling ops with custom classes (#38257)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38257

It seems we're doing a runtime type check for custom classes on each operator call if the operator has custom class arguments.
This does not have an effect on operators without custom class arguments, but this is a problem for operators with custom class arguments,
for example operators taking a at::native::xnnpack::Conv2dOpContext argument.

The long term solution would be to move those checks to op registration time instead of doing them at call time,
but as an intermediate fix, we can at least make the check fast by

- Using ska::flat_hash_map instead of std::unordered_map
- Using std::type_index instead of std::string (i.e. avoid calling std::hash on a std::string)
ghstack-source-id: 106805209

Test Plan: waitforsandcastle

Reviewed By: ezyang

Differential Revision: D21507226

fbshipit-source-id: bd120d5574734be843c197673ea4222599fee7cb",12.0,9.0,"aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h,aten/src/ATen/core/ivalue.cpp,aten/src/ATen/core/ivalue.h,torch/custom_class.h",4.0,7,2,1.956066972,2.0,2148.0,3.0,2223578.25,3333.0,7972.5,0.0,Corrective,1.0,1
pytorch,76ca037069321943d7fdd0a700d7e6a53f0a732e,d7cb78478fe0bacb24e10c693833b133b2a50eac,gchanan,gregchanan@gmail.com,Mon Apr 16 17:49:00 2018 -0400,1523900940.0,"Split set_default_tensor_type(dtype) into set_default_dtype(dtype). (#6599)

* Split set_default_tensor_type(dtype) into set_default_dtype(dtype).

* Fix flake8.

The difference between this one and set_default_tensor_type is that it only sets scalar type what determines the type + device of a tensor returned from a factory function with defaults is the default tensor type + the current device (if the default tensor type is cuda). This just changes the scalar type of the default tensor type.

We do eventually want to deprecate set_default_tensor_type; it is not clear how to do that in a sensible and backwards compatible way.",67.0,20.0,"docs/source/torch.rst,test/test_torch.py,torch/__init__.py,torch/csrc/Module.cpp,torch/csrc/tensor/python_tensor.cpp,torch/csrc/tensor/python_tensor.h",6.0,6,3,2.105647116,40.0,8108.0,4.0,532392.0,583.0,3469.5,0.0,Corrective,1.0,1
pytorch,0abba43aa31ab2207afdb995d44605d3e2f4e3ef,d7d266f51e2b525207a37364c70273531e65b848,Richard Zou,zou3519@gmail.com,Wed Apr 28 18:04:58 2021 -0700,1619633098.0,[functorch] Grab bag of batch rules,181.0,115.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/functorch/csrc/BatchRulesViews.cpp,functorch/functorch/csrc/BatchingRegistrations.cpp,functorch/test/test_vmap.py",4.0,4,1,1.638559241,1.0,4444.0,3.0,0.0,28.0,104.5,0.0,,0.0,1
pytorch,92f470da08b3f4fc29ff1cca8dc637ccce2c863a,d7d399f3dfc780f3e49bcffe45694fb04e5db637,Heitor Schueroff,heitorschueroff@fb.com,Tue Aug 03 23:03:27 2021 -0700,1628031807.0,"Exposes _aminmax as aminmax and makes it structured (#62401)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62401

This PR exposes the `torch._aminmax` operator as `torch.aminmax`.

**TODO**

- [x] add examples to documentation
- [x] add minmax to rst docs

fixes https://github.com/pytorch/pytorch/issues/62164

Test Plan: Imported from OSS

Reviewed By: soulitzer

Differential Revision: D30072246

Pulled By: heitorschueroff

fbshipit-source-id: 557d30af7c28ca6c238c59122367104036429ecd",223.0,80.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/ReduceAllOps.cpp,aten/src/ATen/native/ReduceAllOps.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOps.h,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorCompare.h,aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_namedtuple_return_api.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",18.0,13,4,3.156292925,36.0,42971.0,10.0,2656106.388888889,14388.0,32921.5,0.0,Corrective,1.0,1
pytorch,e6b4d77c3e19be879e890681ce9ba6106b79f4c6,d7db6a7b02843b24e4dafe8f26f3901f96aa3703,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Apr 29 06:49:16 2022 +0000,1651214956.0,"Sparse CSR: Add backward for torch.sparse.sampled_addmm

Pull Request resolved: https://github.com/pytorch/pytorch/pull/68084

Approved by: https://github.com/cpuhrsch",154.0,6.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/sparse/SparseCsrTensor.cpp,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp,aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp,test/test_sparse_csr.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/testing/_internal/common_methods_invocations.py",8.0,12,4,2.153644061,19.0,39076.0,7.0,705006.0,2752.0,6602.5,0.0,Feature Addition,0.0,1
pytorch,de3a4eb583f1a63a45333359f225628c3b0bbb7e,d7ddae8e4fe66fa1330317673438d1eb5aa99ca4,Nikita Vedeneev,nik@quansight.com,Tue Jul 27 17:05:23 2021 -0700,1627405523.0,"det_backward: correct, more robust and with complex support [clone] (#61905)

Summary:
Clone of https://github.com/pytorch/pytorch/pull/58195 to ease the import. Done by request from anjali411

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61905

Reviewed By: albanD

Differential Revision: D29937920

Pulled By: anjali411

fbshipit-source-id: 025892a8e6147790825b20458986730ad8c5bb0f",370.0,144.0,"aten/src/ATen/cuda/CUDASolver.cpp,aten/src/ATen/cuda/CUDASolver.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,test/test_namedtuple_return_api.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_core.py,torch/testing/_internal/common_methods_invocations.py",17.0,14,4,3.175230108,16.0,40960.0,7.0,485424.70588235295,14177.0,32423.0,0.0,Corrective,0.0,1
pytorch,de334e6a2f2a24e818a9e006833bef4c2bb93a67,d7ea0fe75a3a0ff144c10aafa3c0c2e549cb9b47,Kshiteej K,kshitijkalambarkar@gmail.com,Thu Feb 11 03:38:36 2021 -0800,1613014716.0,"[testing] Add OpInfo for rad2deg and deg2rad (#51283)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50006

We should probably add aliases for these operators to be consistent with NumPy names i.e. `np.degrees` and `np.radians`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51283

Reviewed By: ngimel

Differential Revision: D26171163

Pulled By: mruberry

fbshipit-source-id: 1869604ed400820d95f6ff50a0e3cba1de1ffa84",26.0,6.0,"test/test_torch.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.668564443,43.0,12579.0,2.0,36074.333333333336,8849.0,19886.5,0.0,Feature Addition,0.0,1
pytorch,20460b0c0591759c73901d449faaa5e679015075,d7eb5836bb18e4bce9d39800e56df88d96a80e5e,Pritam Damania,pritam.damania@fb.com,Tue Jun 15 17:48:08 2021 -0700,1623779288.0,"Add RRef support to ShardedTensor. (#59776)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59776

Overall design: https://github.com/pytorch/pytorch/issues/55207.

In this PR, I've added support to ShardedTensor such that it also creates RRefs
pointing to the remote shards if the RPC framework is initialized.

As a result, this provides more flexiblity for ShardedTensor such that users
can use collectives with local shards or use the RPC framework to interact with
remote shards.
ghstack-source-id: 131381914

Test Plan:
1) unit tests
2) waitforbuildbot

Reviewed By: SciPioneer

Differential Revision: D29020844

fbshipit-source-id: acb308d0029a5e486c464d93189b5de1ba680c85",392.0,19.0,"test/distributed/_sharded_tensor/test_sharded_tensor.py,torch/distributed/_sharded_tensor/api.py",2.0,6,2,0.932207469,1.0,806.0,1.0,473297.0,13018.0,29492.0,0.0,Feature Addition,0.0,1
pytorch,e67c2bc5674dcc42226b9ff603e6c365c868a7b5,d7ee3e0bd0b6084eb151b332b8be4641d7da2826,peterjc123,peter_jiachen@163.com,Fri Sep 29 15:58:28 2017 +0800,1506700708.0,Fix the memory leak for multiple workers (#2897),4.0,0.0,torch/lib/TH/THAllocator.c,1.0,3,1,0,37.0,558.0,1.0,1077643.0,1857.0,24924.05562,0.0,Corrective,1.0,1
pytorch,856e8cf0288fe3c1701d11fae61b214c08635b9d,d7f7c290e3d76a1e3019166644baf78de0d95a31,Xiang Gao,qasdfgtyuiop@gmail.com,Sat Apr 25 14:40:50 2020 -0700,1587825650.0,"addmv migration [resubmit] (#37236)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/37236

Differential Revision: D21232988

Pulled By: anjali411

fbshipit-source-id: ac6c0ee018aef3c841b039d76e6e1fbb3cd0292d",389.0,444.0,"BUILD.bazel,aten/src/ATen/Config.h.in,aten/src/ATen/Declarations.cwrap,aten/src/ATen/NamedTensorUtils.cpp,aten/src/ATen/NamedTensorUtils.h,aten/src/ATen/native/Blas.cpp,aten/src/ATen/native/BlasKernel.cpp,aten/src/ATen/native/BlasWrappersCPU.cpp,aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp,aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp,aten/src/ATen/native/cuda/Blas.cu,aten/src/ATen/native/native_functions.yaml,aten/src/TH/THBlasUtils.h,aten/src/TH/generic/THBlas.cpp,aten/src/TH/generic/THBlas.h,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorMath.h,aten/src/THC/THCBlas.cu,aten/src/THC/THCBlas.h,aten/src/THC/generic/THCTensorMathBlas.cu,aten/src/THC/generic/THCTensorMathBlas.h,aten/src/THCUNN/generic/SpatialConvolutionMM.cu,test/test_torch.py",23.0,12,2,3.525866333,43.0,33869.0,5.0,149009.21739130435,1399.0,3746.0,0.0,Feature Addition,0.0,1
pytorch,44764f131b040a41a6dcf1304bb635c574bf5a3b,d7fc864f0da461512fb7b972f04e24e296bd266d,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Tue Jun 14 13:06:16 2022 +0000,1655211976.0,"Revert ""[primTorch] refs: lerp (#78473)""

This reverts commit a9f6a35a33308f3be2413cc5c866baec5cfe3ba1.

Reverted https://github.com/pytorch/pytorch/pull/78473 on behalf of https://github.com/malfet due to Seems to broke Mac tests, see https://hud.pytorch.org/pytorch/pytorch/commit/a9f6a35a33308f3be2413cc5c866baec5cfe3ba1",41.0,128.0,"aten/src/ATen/native/cpu/LerpKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/zmath.h,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,2,1.82852788,5.0,24505.0,1.0,30119.0,4321.0,10381.5,0.0,,0.0,1
pytorch,5d4452937d9094e4198cc0932ca18e40fbf47492,d810e738b92734b2966560f1c020bfc23779cbd2,Richard Zou,zou3519@gmail.com,Thu Oct 14 16:11:42 2021 -0700,1634227902.0,"OpInfo for `*_like` functions (#65941)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/65941

OpInfos for: empty_like, zeros_like, ones_like, full_like, randn_like

Test Plan: - run tests

Reviewed By: dagitses

Differential Revision: D31452625

Pulled By: zou3519

fbshipit-source-id: 5e6c45918694853f9252488d62bb7f4ccfa1f1e4",104.0,0.0,"mypy.ini,test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,0.78230453,5.0,16736.0,2.0,2147891.25,16250.0,37576.0,0.0,,0.0,1
pytorch,e7b5a23607ce55c6cdfb7b4cdd6235e9faeadc4a,d83cc92948d01de05bb10ab1feb32087e2ffb7eb,Spandan Tiwari,sptiwari@microsoft.com,Mon Aug 10 18:40:13 2020 -0700,1597084813.0,"[ONNX] Add support for scalar src in torch.scatter ONNX export. (#42765)

Summary:
`torch.scatter` supports two overloads â one where `src` input tensor is same size as the `index` tensor input, and second, where `src` is a scalar. Currrently, ONNX exporter only supports the first overload. This PR adds export support for the second overload of `torch.scatter`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42765

Reviewed By: hl475

Differential Revision: D23025189

Pulled By: houseroad

fbshipit-source-id: 5c2a3f3ce3b2d69661a227df8a8e0ed7c1858dbf",22.0,2.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.534337198,3.0,7249.0,2.0,224555.0,4192.0,9832.5,0.0,Feature Addition,0.0,1
pytorch,69728d7dd9b6a05a503e25759ed589754741ff01,d84dc589c24bb28147aaafddb67dff4fac6ac9a7,Richard Zou,zou3519@gmail.com,Thu Aug 18 13:57:21 2022 -0700,1660831041.0,"[functorch] relax as_strided batching rule (#83597)

Previously there was a constraint that the bdim is required to be at
the front. As I noted in the comment in the code that I wrote years ago,
this is not necessary for correctness, we were just guarding against
potentially incorrect behavior and assumed most people would not vmap
over dimensions other than 0.

Now, the above assumption did not age very well, because we have batch
rules that return a BatchedTensor where the bdim is something other than
0 (e.g. convolution batch rule).

This PR deletes the check for that assumption and adds additional manual
tests that the as_strided batching rule works when one vmaps over a dimension
other than 0.

Automatic tests don't exist because it's a bit hard to get the
test_vmap_exhaustive test runner to replicate the strides of the inputs
faithfully.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83597
Approved by: https://github.com/samdow",25.0,28.0,"functorch/functorch/csrc/LegacyBatchingRegistrations.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",3.0,4,1,1.03949094,2.0,6674.0,3.0,285055.3333333333,6559.0,15189.5,0.0,Corrective,0.0,1
pytorch,f8a4b1a2663ed3f214f3e58cd5dda246f7647775,d859c3c7cc129dc872871981f28c068c3a4ba494,Richard Zou,zou3519@gmail.com,Wed Dec 27 20:13:59 2017 -0800,1514405639.0,Fix creating tensors with np.longlong array,3.0,1.0,"test/test_torch.py,torch/csrc/utils/tensor_numpy.cpp",2.0,4,2,0.811278124,38.0,5532.0,2.0,1006573.0,2224.0,24290.35823,0.0,Corrective,1.0,1
pytorch,b1b26a19cd3456854f9f99ed08b9100e7a19c32a,d85f6d82080d07d4f0c97e63942c0afff7ddc1ce,Samantha Andow,samdow@fb.com,Tue Feb 22 18:02:10 2022 -0500,1645552930.0,"[functorch] random_ support (pytorch/functorch#412)

[ghstack-poisoned]",101.0,16.0,"functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/functorch/csrc/VmapModeRegistrations.cpp,functorch/test/test_vmap.py",3.0,4,1,1.211078708,1.0,3754.0,1.0,1.0,819.0,1126.0,0.0,,0.0,1
pytorch,4a85145bbd985cc46ba5adb23160880d33a1ec3a,d88a116015934229c6bb4e903fb265ef6d0cb170,Eugene Lyapustin,e.lyapustin@vk.team,Fri Apr 08 17:23:34 2022 +0000,1649438614.0,"Fix exporting models to ONNX without allow_tf32 in _convolution call

Fixes #75098
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75099
Approved by: https://github.com/BowenBao",24.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.40217919,4.0,14562.0,2.0,46999.0,2118.0,5094.5,0.0,Corrective,1.0,1
pytorch,77f336600a429312046dd2841c65d3080b4ea404,d88bc38b0c4774a0c9b576944ed5c4401b825b47,kshitij12345,kshitijkalambarkar@gmail.com,Thu Jan 26 05:07:23 2023 +0000,1674709643.0,"[functorch] fix batching rule for dropout (#92975)

Fixes https://github.com/pytorch/pytorch/issues/92283

The repro now works:
```python
import torch
import torch.func
import torch.nn as nn

x = torch.randn(3, device='cuda')
y = torch.randn(1, 3, device='cuda')

def fn(x, y):
    # previously output of dropout used to be incorrect [B, 3] (B=1) and thus `mean(1)` used to fail
    # post the fix output of dropout is [B, 1, 3] and `mean(1)` works.
    return x + nn.functional.dropout(y, 0.3).mean(1)

o = torch.func.vmap(fn, in_dims=(0, None), randomness='different')(x, y)
```

**NOTE**:
`native_dropout_batching_rule(const Tensor& tensor, double p, c10::optional<bool> train)` was called only for CUDA tensor. Hence this issue only affected CUDA tensors and not CPU tensors

Ref:
https://github.com/pytorch/pytorch/blob/a6ac922eabee8fce7a48dedac81e82ac8cfe9a45/aten/src/ATen/functorch/PyTorchOperatorHacks.cpp#L251-L258
Pull Request resolved: https://github.com/pytorch/pytorch/pull/92975
Approved by: https://github.com/Chillee, https://github.com/Skylion007",31.0,3.0,"aten/src/ATen/functorch/BatchRulesRandomness.cpp,test/functorch/test_vmap.py",2.0,6,2,0.997502546,1.0,5566.0,2.0,4660934.5,11702.0,27219.5,0.0,Corrective,1.0,1
pytorch,3b641dc80521bf78da9cbd2a0334cd321d336b08,d8b2e5d0916a85e603c9bda6def5cd20cbe94fa6,Tongzhou Wang,SsnL@users.noreply.github.com,Mon Dec 18 17:28:23 2017 -0500,1513618103.0,"Add python only default init expression; Implement stft, hann/hamming/bartlett window. (#4095)

* implement stft

* addressed comments; implemented window functions; added support for python only default initialization",599.0,78.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/NativeFunctions.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/THC/CMakeLists.txt,aten/src/THC/THCNumerics.cuh,aten/src/THC/THCTensorMath2.cu,aten/src/THC/THCTensorMathPointwise.cuh,aten/src/THC/generic/THCTensorMathPointwise.cu,docs/source/torch.rst,test/test_cuda.py,test/test_torch.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/autograd/variable.py,torch/csrc/generic/methods/TensorMath.cwrap,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/utils/python_arg_parser.h,torch/functional.py",22.0,20,5,3.234595845,38.0,25045.0,13.0,990142.380952381,2212.0,24255.85823,0.0,Feature Addition,0.0,1
pytorch,22d70bc1ecfafc9fc867da9c21bb081e4c02ab45,d8c368bd62455f2c9d0bf036640a87281db0face,Dylan Bespalko,dylan.bespalko@gmail.com,Tue Oct 29 20:35:29 2019 -0700,1572381329.0,"CPU-strided-complex support for compare and pointwise ops (#28735)

Summary:
In-tree changes to pytorch to support complex numbers are being submitted here.
Out-of-tree support for complex numbers is here: [pytorch-cpu-strided-complex extension](https://gitlab.com/pytorch-complex/pytorch-cpu-strided-complex)

These changes optimize complex Vec256 math kernels so that are within 2X real number performance on average.  [Benchmarks are here](https://docs.google.com/spreadsheets/d/17pObcrSTpV4BOOX9FYf1vIX3QUlEgQhLvL1IBEyJyzs/edit#gid=0)

Changes so far:

- [x]  Added complex support for eq, neq, max, and min ops.
   - max/min ops need to compare the absolute value for complex numbers (using zabs).
- [x] Added complex support for is_nonzero and where.
   - where op compares the absolute value for complex numbers (using zabs).
- [x] Added complex support for linear interp and and pointwise ops.
- [x] Added complex support for check_convert and Linspace/Logspace.
   - std::complex does not support ++operator.
   - All compilers from clang, g++, c++ on aarch64, x86 produce the same assembly code when using `+=1' instead of `++`. [example for loop](https://godbolt.org/z/O6NW_p)
- [x] Added complex support for log, log2, log10.
- [x] Optimized Vec256 operators using various logarithmic identities.
  - `asin()`, `acos()`, `atan()` is optimized using a `ln()` identity.
  - `sqrt()` is optimized by splitting the computation into real and imag parts.
  - several `_mm256_mul_pd` are avoided by using `_mm256_xor_pd` ops instead.
- [x] Added complex support for pow.
  - exp is cast to `std::complex<double>`.
  - no special optimization is added when the `exp` is real because the `std::pow()` operator expects a std::complex number.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28735

Differential Revision: D18170691

Pulled By: ezyang

fbshipit-source-id: 6f167398e112cdeab02fcfde8b543cb6629c865a",260.0,87.0,"aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/cpu/vec256/vec256_complex_float.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/RangeFactories.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cpu/CrossKernel.cpp,aten/src/ATen/native/cpu/LerpKernel.cpp,aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp,aten/src/ATen/native/cpu/PowKernel.cpp,aten/src/ATen/native/cpu/TensorCompareKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp",14.0,7,1,2.570625036,7.0,4450.0,10.0,2429411.1428571427,12630.0,35091.83333,0.0,Feature Addition,0.0,1
pytorch,352c07c2f5871b26bfc7d6747870259e75689e14,d8c6b4bad02d19b819b178f8ced35295ff4c3ce9,Richard Zou,zou3519@gmail.com,Tue Apr 19 15:05:36 2022 -0700,1650380736.0,"[functorch] Add some unimportant xfails/skips

Some metadata changed around these OpInfos; this is not a regression.",22.0,0.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1533.0,1.0,0.0,978.0,1359.0,0.0,Feature Addition,0.0,1
pytorch,3365d74df90c02fb43085c888847199ffb5cb0df,d8dab6ffa8a49f3579de0c870a7b5961ed95f288,Tongzhou Wang,tongzhou.wang.1994@gmail.com,Mon Oct 29 23:22:49 2018 -0700,1540855369.0,"Add tensor.to(options) (#13146)

Summary:
ezyang on the template hack
smessmer on SFINAE of the `TensorOptions(Device)`
goldsborough on the C++ API test changes
zdevito on the `jit` codegen changes
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13146

Reviewed By: ezyang

Differential Revision: D12823809

Pulled By: SsnL

fbshipit-source-id: 98d65c401c98fda1c6fa358e4538f86c6495abdc",405.0,101.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/TensorOptions.h,aten/src/ATen/core/Type.h,aten/src/ATen/core/typeid.h,aten/src/ATen/function_wrapper.py,aten/src/ATen/native/TensorConversions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/templates/Tensor.h,test/cpp/api/tensor.cpp,test/cpp/api/tensor_cuda.cpp,test/cpp/api/tensor_options.cpp,test/test_torch.py,tools/autograd/gen_variable_type.py,tools/jit/gen_jit_dispatch.py",15.0,12,3,3.029336803,42.0,19962.0,9.0,520766.3333333333,4984.0,14841.33333,0.0,Feature Addition,0.0,1
pytorch,9735ddd89904be67e4924f6e69ac6df4cf40c083,d8f3c601e4926c0bdb79a2dd6842239bc3bd6e71,Richard Zou,zou3519@gmail.com,Fri Oct 27 15:38:05 2017 -0700,1509118685.0,Add reduce keyword to CrossEntropyLoss,16.0,6.0,"torch/nn/functional.py,torch/nn/modules/loss.py",2.0,3,1,0.994030211,36.0,2208.0,1.0,71310.0,778.0,6476.672317,0.0,Feature Addition,0.0,1
pytorch,ff9558a2ea626b1526ded6848e06e1e809fae53c,d917b4a6a6dc0c16cb42495c80a4fd79b8dd4f4c,Samantha Andow,samdow@fb.com,Tue May 10 13:19:02 2022 -0400,1652188742.0,"[functorch] Fix advanced indexing (pytorch/functorch#777)

* fix advanced indexing

* add comments

* fix test device loc, add nested version

* fix cuda test",104.0,17.0,"functorch/functorch/csrc/BatchRulesScatterOps.cpp,functorch/test/test_vmap.py",2.0,4,1,0.982140067,1.0,4904.0,2.0,0.5,1058.0,1447.5,0.0,Corrective,1.0,1
pytorch,a5a2f576a768f01b14d2742e8fd7a478a2ab01d3,d9273e8b6b42dec1cd5b52779075912bee854130,kshitij12345,kshitijkalambarkar@gmail.com,Sat Oct 01 06:32:19 2022 +0000,1664605939.0,"[functorch] refactor: get_exhaustive_batched_inputs (#85965)

`get_exhaustive_batched_inputs_batch_norm_is_training` and `get_exhaustive_batched_inputs` are same except for a couple of lines.

We move the above functionality into `generate_vmap_inputs` (which is now only function to create batched inputs)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85965
Approved by: https://github.com/zou3519",11.0,37.0,"functorch/test/common_utils.py,functorch/test/test_ops.py",2.0,2,1,0.776555785,2.0,2149.0,2.0,145724.0,7923.0,18781.5,0.0,Perfective,0.0,1
pytorch,7ff16baa7dd25976fb8e2872f379a1f528362d3c,d92b7da7338443402e22e2099981ba25aaba2992,soumith,soumith@fb.com,Fri Sep 30 16:49:30 2016 -0700,1475254170.0,fix documentation to not use forward,40.0,40.0,"torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/linear.py,torch/nn/modules/pooling.py",6.0,3,1,2.062944014,6.0,1421.0,6.0,186478.16666666663,116.0,2870.722339,0.0,Corrective,1.0,1
pytorch,cf8001d2d01f68985a8ff3b141fbc956bf6f4137,d92ef9268d08050b5d55e49bda732072b102c1dd,Natalia Gimelshein,ngimel@fb.com,Thu May 28 00:31:48 2020 -0700,1590625908.0,"Revert D21728402: Simplify precision-specification in tests.

Test Plan: revert-hammer

Differential Revision:
D21728402

Original commit changeset: 85f3daf63f1b

fbshipit-source-id: 4e2a36aca15cd8d842985173395b4e1cac7135d8",367.0,374.0,"test/test_torch.py,torch/testing/_internal/common_utils.py",2.0,4,2,0.038008655,42.0,20507.0,2.0,13614.0,2378.0,5986.0,0.0,,0.0,1
pytorch,2082ccbf599086ed071ef2d5c299717ca7c20167,d951d5b1cdbf43a75636da77b9d3938581be6300,Sam Gross,colesbury@gmail.com,Wed Jan 18 06:08:37 2017 -0500,1484719717.0,Fix tensor.cuda(0) when on non-zero device. (#472),42.0,7.0,"test/test_cuda.py,torch/_utils.py",2.0,2,2,0.975525951,21.0,714.0,2.0,237921.5,357.0,4876.976424,0.0,Corrective,1.0,1
pytorch,b9e00dfbb8d9d341abcc1d8eaf4941a5629f50e9,d95f711501ad41d7e1cd3ed1f64a6632371be969,Gregory Chanan,gchanan@fb.com,Thu May 04 21:27:52 2017 -0700,1493933272.0,Add a keepdim test to torch_test.,29.0,0.0,test/test_torch.py,1.0,1,1,0,29.0,3306.0,1.0,542.0,753.0,9959.560487,0.0,Feature Addition,0.0,1
pytorch,61b863cbdc132a5114a5c93a5822ebabb2325a61,d97c9dd01904ff423554345cd877ebc1e520c21e,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Wed Jun 20 16:23:22 2018 -0400,1529511802.0,"Add a warning in gradcheck if inputs precision < float64 (#8663)

* Solves #8659

This PR adds a warning to alert users about the possibility of a failure in the gradcheck

* Fix lint

* Update gradcheck.py

* Update gradcheck.py

* update error message

* Update warning message to be more descriptive",7.0,0.0,torch/autograd/gradcheck.py,1.0,2,1,0,26.0,289.0,1.0,1122724.0,722.0,3841.0,0.0,Corrective,1.0,1
pytorch,cb15df76ade05efabf4de5fe3f25b44ad7353e32,d9a5668983c05a01c8785837394c42081b19d0a5,Shubham Bhokare,32080845+shubhambhokare1@users.noreply.github.com,Wed Oct 27 20:43:41 2021 -0700,1635367421.0,"[ONNX] Add dim argument to all symbolic (#66093) (#67270)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67270

* Add dim argument to all symbolic

* All symbolic depends on any symbolic

Test Plan: Imported from OSS

Reviewed By: msaroufim

Differential Revision: D31962518

Pulled By: malfet

fbshipit-source-id: f7ee05cf4eff5880fc508154267e060952b5b42d",48.0,4.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.995727452,3.0,13552.0,2.0,217676.5,16627.0,38991.5,0.0,Feature Addition,0.0,1
pytorch,a5454bfc6c1d1b4d6c9c3550dc17fc571f164a7b,d9b25b1a2a03a15a1afd376db654453c53fb6a42,Samantha Andow,samdow@fb.com,Mon May 09 21:40:44 2022 -0400,1652132444.0,[functorch] fix ci (pytorch/functorch#789),32.0,22.0,"functorch/test/test_eager_transforms.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",4.0,2,1,1.049623207,1.0,9123.0,4.0,0.25,1055.0,1445.5,0.0,Corrective,1.0,1
pytorch,f1f64c8d07b166066f6adc18daa7b9224af7d3cf,d9b89a352c4ceeff24878f4f5321e16f059e98c3,Sam Gross,colesbury@gmail.com,Thu Oct 19 19:05:07 2017 -0400,1508439907.0,"Replace StochasticFunctions v2 (#3165)

This removes the StochasticFunctions for bernoulli, multinomial, and
normal and replaces them with classes in the torch.distributions
package. Each distribution supports the differentiable log_prob function
that returns the log of the pdf/pmf of the samples.

The current StochasticFunction implementation has a few problems: it can
be painful to use when there are multiple stochastic outputs which need
to be back-propagated through. It also requires that we store grad_fns
on Variables that have requires_grad=False in order to find stochastic
nodes.",292.0,191.0,"docs/source/distributions.rst,docs/source/index.rst,test/run_test.sh,test/test_autograd.py,test/test_distributions.py,torch/__init__.py,torch/autograd/_functions/stochastic.py,torch/autograd/variable.py,torch/distributions.py",9.0,6,3,2.357307797,40.0,3894.0,2.0,14938.5,299.0,842.4058694,0.0,,0.0,1
pytorch,e3713ad706397a29110096270e06028484bced41,d9c76360b2b915639fa63b0af29fd6faee045557,Shen Li,cs.shenli@gmail.com,Tue Dec 01 22:07:25 2020 -0800,1606860445.0,"Add cuda_ipc channel to TensorPipe (#46791)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/46791

Test Plan: Imported from OSS

Reviewed By: lw

Differential Revision: D25237121

Pulled By: mrshenli

fbshipit-source-id: f1428175b260fb23c4e0e6f92651426f38beaca9",117.0,34.0,"caffe2/CMakeLists.txt,torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h",3.0,5,2,1.128640284,14.0,3082.0,3.0,1221199.0,7079.0,16006.0,0.0,Feature Addition,0.0,1
pytorch,4ccfa3ffeb55909a43b7840e774cc5a125e2e71d,d9dc94406fab5525ab032a1504ce7658f73c550a,BowenBao,bowbao@microsoft.com,Thu Jul 08 23:16:00 2021 -0700,1625786160.0,"[ONNX] Add linspace symbolic (#58854) (#60246)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/60246

* Adds support for linspace op
* Modifies arange symbolic in opset 9 to replicate the same behavior in which dtype is determined (similar to opset 11) as in https://pytorch.org/docs/stable/generated/torch.arange.html
* Enabled some arange unit tests which were disabled for opset 9

Test Plan: Imported from OSS

Reviewed By: zou3519, ZolotukhinM

Differential Revision: D29494911

Pulled By: SplitInfinity

fbshipit-source-id: bddff18a90f8a78121c8ecdd1dafc15c69962d66

Co-authored-by: Shubham Bhokare <shubhambhokare@gmail.com>",125.0,38.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.180458553,3.0,13379.0,2.0,466882.3333333333,13681.0,30926.5,0.0,Feature Addition,0.0,1
pytorch,2c038f20748c588c9e98f3d549036237ab280ff7,da0fad8a7a3721eccdd60460914b62dda95daff9,Sam Gross,colesbury@gmail.com,Fri Jun 30 20:53:26 2017 -0400,1498856006.0,"Use torch.matmul in nn.Linear (#1935)

This takes advantage of the broadcasting behavior of torch.matmul to
support inputs with more than two dimensions. The extra dimensions are
treated like part of the batch dimension, much like nn.Bottle in Lua
Torch.

There are a few related small performance changes:

 * Addmm computes the gradient in column-major for inputs in
   column-major format
 * Variable.mm calls Addmm in-place with the desired output buffer",29.0,46.0,"test/test_nn.py,torch/autograd/_functions/blas.py,torch/autograd/variable.py,torch/nn/_functions/linear.py,torch/nn/backends/thnn.py,torch/nn/functional.py,torch/nn/modules/linear.py",7.0,8,2,2.276760912,30.0,5892.0,4.0,438486.5714285714,1063.0,12590.04911,0.0,Feature Addition,0.0,1
pytorch,ad8d1b2aaaf2ba28c51b1cb38f86311749eff755,da10ccd35f8f0de686ba8b36684a5055a4656e0b,Xiong Wei,xiongw.fnst@cn.fujitsu.com,Mon Mar 15 17:42:19 2021 -0700,1615830139.0,"Implements cpu_kernel_multiple_outputs and torch.frexp (#51097)

Summary:
Close https://github.com/pytorch/pytorch/issues/51108
Related https://github.com/pytorch/pytorch/issues/38349

This PR implements the `cpu_kernel_multiple_outputs` to support returning multiple values in a CPU kernel.
```c++
auto iter = at::TensorIteratorConfig()
  .add_output(out1)
  .add_output(out2)
  .add_input(in1)
  .add_input(in2)
  .build();

at::native::cpu_kernel_multiple_outputs(iter,
  [=](float a, float b) -> std::tuple<float, float> {
    float add = a + b;
    float mul = a * b;
    return std::tuple<float, float>(add, mul);
  }
);
```

The `out1` will equal to `torch.add(in1, in2)`, while the result of `out2` will be `torch.mul(in1, in2)`.
It helps developers implement new torch functions that return two tensors more conveniently, such as NumPy-like functions [divmod](https://numpy.org/doc/1.18/reference/generated/numpy.divmod.html?highlight=divmod#numpy.divmod) and [frexp](https://numpy.org/doc/stable/reference/generated/numpy.frexp.html#numpy.frexp).

This PR adds `torch.frexp` function to exercise the new functionality provided by `cpu_kernel_multiple_outputs`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51097

Reviewed By: albanD

Differential Revision: D26982619

Pulled By: heitorschueroff

fbshipit-source-id: cb61c7f2c79873ab72ab5a61cbdb9203531ad469",369.0,12.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/Loops.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/tensor_iterator_test.cpp,docs/source/tensors.rst,docs/source/torch.rst,test/test_namedtuple_return_api.py,test/test_unary_ufuncs.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",17.0,16,5,3.054792606,37.0,38816.0,12.0,2342473.7647058824,9785.0,21628.0,0.0,Feature Addition,0.0,1
pytorch,62ebad4ff9b7f42e5bca77cb6a5233df385095f1,da11d932bc9236504cac50da82663b92f7bf5561,shubhambhokare1,shubhambhokare1@gmail.com,Mon Sep 14 21:46:55 2020 -0700,1600120015.0,"[ONNX] Update arange op to support out argument (#43777)

Summary:
Update arange op to support out argument

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43777

Reviewed By: albanD

Differential Revision: D23674583

Pulled By: bzinodev

fbshipit-source-id: 6fb65e048c6b1a551569d4d2a33223522d2a960c",76.0,10.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.617252176,3.0,8462.0,3.0,289140.5,5108.0,11662.5,0.0,,0.0,1
pytorch,6584b35db22229d2f5248c75416f16f2ff746064,da5bb373e6ad45963f1966bf564e6bbc15ffe890,Adam Paszke,adam.paszke@gmail.com,Thu Sep 15 15:18:53 2016 -0700,1473952733.0,Type conversions now use auto gpu,161.0,55.0,"test/test_cuda.py,torch/Storage.py,torch/Tensor.py,torch/_utils.py,torch/csrc/generic/StorageMethods.cpp,torch/cuda/__init__.py,torch/cuda/storage.py,torch/cuda/tensor.py",8.0,5,2,2.608226305,7.0,1200.0,2.0,21471.8,170.0,3736.532937,0.0,,0.0,1
pytorch,c64594f5cc7f17f9f0638fe2c2d3d1a583412cec,da70976e661a24133b1d2cb7a98954736e7fe099,BowenBao,bowbao@microsoft.com,Fri Aug 21 05:36:44 2020 -0700,1597988204.0,"[ONNX] Add support for operator `add` between tensor list (#41888)

Summary:
E.g.
```python
outs = []
outs += [torch.randn(3,4)]
outs = outs + [torch.randn(4,5), torch.randn(5,6)]
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41888

Reviewed By: houseroad

Differential Revision: D23172880

Pulled By: bzinodev

fbshipit-source-id: 93865106e3de5908a993e0cfa82f626ba94dab7e",23.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.382437341,3.0,7356.0,1.0,276997.0,4439.0,10359.0,0.0,Feature Addition,0.0,1
pytorch,fc6fcf23f7917a3e7d0326ca74d24f34ad92a4a3,da725830c2d5c3dd60514f1b99bf2949c04176e5,Adam Paszke,adam.paszke@gmail.com,Wed Mar 01 16:36:32 2017 +0100,1488386192.0,Add support for variable length sequences in RNNs (#873),480.0,61.0,"docs/source/nn.rst,test/test_nn.py,torch/backends/cudnn/__init__.py,torch/backends/cudnn/rnn.py,torch/csrc/autograd/functions/init.cpp,torch/nn/_functions/rnn.py,torch/nn/modules/__init__.py,torch/nn/modules/rnn.py,torch/nn/utils/__init__.py,torch/nn/utils/rnn.py",10.0,13,3,2.718437807,23.0,4464.0,1.0,64872.0,189.0,637.1909143,0.0,Feature Addition,0.0,1
pytorch,7b33ef4cffed0dcd5c2506c4db1b2624736a22a3,da894901ef1f965a928bdd2df18fb3cffb0b0af0,gchanan,gregchanan@gmail.com,Thu Mar 01 15:58:16 2018 -0500,1519919896.0,"Deprecate variable factory, use torch.tensor instead (#5476)

* Remove usages of torch.autograd.variable; use torch.tensor instead.

* Deprecate torch.autograd.variable.

* Remove unused sample_scalar.",179.0,180.0,"test/common_nn.py,test/test_autograd.py,test/test_distributions.py,test/test_indexing.py,test/test_nn.py,test/test_torch.py,torch/autograd/__init__.py,torch/distributions/beta.py,torch/distributions/utils.py",9.0,4,2,1.812951794,39.0,19993.0,5.0,348328.6666666667,572.0,1751.405869,0.0,,0.0,1
pytorch,c65a1da90a2a3979086e79ee3d9f2b59b6b5f77b,da8cc355a333e521e38473d8839288eabdc1b7b5,Edward Yang,ezyang@fb.com,Wed May 05 16:03:37 2021 -0700,1620230617.0,"Relax tp_new so that it is OK to call (#57544)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/57544

Instead of removing tp_new from the superclass (which causes
super().__new__ to not work), I now still install tp_new on the
superclass, but verify that you are not trying to directly
construct _TensorBase.

Fixes https://github.com/pytorch/pytorch/issues/57421

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D28189475

Pulled By: ezyang

fbshipit-source-id: 9397a3842a77f5428d182dd62244b42425bca827",30.0,17.0,"test/test_torch.py,torch/csrc/autograd/python_variable.cpp",2.0,4,2,0.878674493,43.0,9097.0,2.0,185610.0,11672.0,26379.0,0.0,Corrective,1.0,1
pytorch,571b930114021fb6319f83960f4cf1e18d6866f1,da8fcc422b189ff94995ca9a7b1a67371f2f6b0d,Richard Zou,zou3519@gmail.com,Mon Aug 08 20:20:23 2022 -0700,1659990023.0,"[functorch] better error when vmap over no Tensor inputs (#83016)

I thought we were testing this already, but I guess not.

Test Plan:
- new test
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83016
Approved by: https://github.com/Chillee",9.0,0.0,"functorch/functorch/_src/vmap.py,functorch/test/test_vmap.py",2.0,4,1,0.764204507,2.0,4850.0,2.0,647047.0,6253.0,14487.5,0.0,,0.0,1
pytorch,96ac0e0340e4620d3889fe2a21483113d0af9d1f,da972afdcdb377e718e67e9b0fd457413090d738,kshitij12345,kshitijkalambarkar@gmail.com,Sun Jun 06 02:12:37 2021 -0700,1622945557.0,"OpInfo: to_sparse (#59445)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59445

Reviewed By: ngimel

Differential Revision: D28920866

Pulled By: mruberry

fbshipit-source-id: ba8d3071d9937096288b69511000eeb007f53434",30.0,4.0,"test/test_fx.py,test/test_fx_experimental.py,test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",4.0,4,2,1.188308429,2.0,12760.0,3.0,31571.25,12773.0,29012.5,0.0,,0.0,1
pytorch,880098a7e34a20628f960daa8eab0eb1ad566c39,daa50f1e9f9c35d241188afaa39db5d13064efe5,Benjamin Rowell,brrowell@gmail.com,Tue Sep 21 12:59:16 2021 -0700,1632229156.0,"Adds keyword only args to gradcheck (#65290)

Summary:
Changes the call signature of gradcheck so that kwargs are kwargs only.

Also modifies return call from gradgradcheck, to reflect these changes.

Fixes https://github.com/pytorch/pytorch/issues/65165

Pull Request resolved: https://github.com/pytorch/pytorch/pull/65290

Reviewed By: soulitzer

Differential Revision: D31061316

Pulled By: albanD

fbshipit-source-id: 3505569a33a497a8be4347bdd425bb2b8e536999",3.0,1.0,torch/autograd/gradcheck.py,1.0,2,1,0,28.0,1407.0,1.0,4814772.0,15609.0,35910.5,0.0,Corrective,1.0,1
pytorch,e7051939fb56137ef171ca1a3b1a108fc4600f26,dab5e2a23ed387046d99f825e0d9a45bd58fccaa,Eddie Yan,eddiey@nvidia.com,Wed Mar 02 03:02:46 2022 -0800,1646190166.0,"[cuDNN v8 API] cuDNN benchmark, convolution bwd / transposed convolution fwd, `bfloat16`, conv-bias-activation fusion (#60755)

Summary:
https://github.com/pytorch/pytorch/issues/58414, https://github.com/pytorch/pytorch/issues/58859, https://github.com/pytorch/pytorch/issues/58858 #58860 https://github.com/pytorch/pytorch/issues/58861

We're currently testing performance with both ""find"" and ""get"" with this PR.

CC zasdfgbnm ptrblck ngimel puririshi98

In addition to the `USE_EXPERIMENTAL_CUDNN_V8_API` build flag, we've added a `CUDNN_V8_API_ENABLED` runtime feature flag.
`USE_EXPERIMENTAL_CUDNN_V8_API=1` will build with v8 API support while keeping all v7 functionality, with v8 usage disabled by default.
`CUDNN_V8_API_ENABLED=1` at runtime on a `USE_EXPERIMENTAL_CUDNN_V8_API=1` build uses the v8 API.
A debug flag `CUDNN_V8_API_DEBUG=1` can be used to verify which API is used when dispatching convolutions.

Note that in v7, `bfloat16` convolutions will dispatch to a native PyTorch implementation, but a fully v8 enabled build will dispatch to cuDNN implementations.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60755

Reviewed By: mruberry

Differential Revision: D34393940

Pulled By: ngimel

fbshipit-source-id: 5c317d3aad63336ea416a51a43cf8b7d27aaca21
(cherry picked from commit 3bfc549ce57cee691f83dc894ac7adb4b7882459)",668.0,95.0,"aten/src/ATen/cudnn/Descriptors.cpp,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/cudnn/Types.cpp,aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/cudnn/ConvShared.cpp,aten/src/ATen/native/cudnn/ConvShared.h,aten/src/ATen/native/cudnn/Conv_v7.cpp,aten/src/ATen/native/cudnn/Conv_v8.cpp,third_party/cudnn_frontend",10.0,7,2,1.085852294,9.0,4582.0,7.0,9363165.8,1106.0,2714.5,0.0,Corrective,1.0,1
pytorch,d2067569e7783e20e73601295f04e7ce8e7acf54,dab5f725431bc1f51cf694c75254f2609570a4b5,Zachary DeVito,zdevito@fb.com,Tue Dec 17 19:55:50 2019 -0800,1576612550.0,"we should have a config-based way to skip flaky tests (#30978)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30978

This particular approach queries our issue tracker for test titles that
match the following format:

```
DISABLED test_async_grad_guard_with_grad (jit.test_async.TestAsync)
```

And then skips the python test for them. There is 1 second timeout so
if the internet flakes we still run the test suite, without disabling any
tests.

This is intended as a quick fix, similar to ninja unland, to get to a green
master. Long term test disables should go into the code.

Test Plan: Imported from OSS

Pulled By: zdevito

Differential Revision: D18890532

fbshipit-source-id: fe9447e59a6d5c9ad345f7c3ff15d63b6d2a09e2",46.0,0.0,"test/common_utils.py,tools/update_disabled_tests.sh",2.0,2,2,0.713146749,1.0,1382.0,2.0,977730.5,13885.0,37921.83333,0.0,Corrective,1.0,1
pytorch,7637b7c966f06f11818af398ac1a4b5fd90b697e,dae7616078414e162863de73ae3abf54930292d7,Zachary DeVito,zdevito@fb.com,Sat Oct 27 01:18:20 2018 -0700,1540603100.0,"Shard all of tests based on how many tests exist. (#13160)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13160

Reduces pytorch_core build from 2 hours to 30 minutes

Reviewed By: soumith, dzhulgakov

Differential Revision: D10524261

fbshipit-source-id: 97270ac73404b5ea4c264cd0e9d8d4b1be79b0e9",190.0,107.0,"test/common_utils.py,test/test_autograd.py,test/test_c10d.py,test/test_cuda.py,test/test_dataloader.py,test/test_distributions.py,test/test_jit.py,test/test_multiprocessing.py,test/test_nccl.py,test/test_nn.py,test/test_optim.py,test/test_sparse.py,test/test_torch.py,test/test_type_info.py,test/test_utils.py",15.0,1,1,2.838891295,45.0,43176.0,8.0,429117.06666666665,4940.0,14687.33333,0.0,,0.0,1
pytorch,87602549112aac84c6f36fa2b2fd18902ec6bef6,db0771b05d81e9ca5e46740b09589a8ff0bc3ec0,BowenBao,bowbao@microsoft.com,Fri Aug 20 19:44:29 2021 -0700,1629488669.0,"[ONNX] Update repeat_interleave for dynamic repeats (#59979) (#62764)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62764

Fixes #58733

- Support dynamic interleave for cases with dynamic repeat values
- Moved repeat_interleave symbolic from opset 11 to opset 13, as sequence as output types for loop outputs is needed for this change

Test Plan: Imported from OSS

Reviewed By: SplitInfinity

Differential Revision: D30375179

Pulled By: msaroufim

fbshipit-source-id: 787f96bf91d124fd0483761088c5f4ae930d96a9

Co-authored-by: Shubham Bhokare <shubhambhokare@gmail.com>",171.0,119.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset13.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.661205392,3.0,14092.0,3.0,1553641.25,14817.0,33934.5,0.0,Corrective,1.0,1
pytorch,d30784be310af10ce46e4323839249ca02765704,db0e121b46a03808f77b718d2bda3bbbffa05d44,Kshiteej K,kshitijkalambarkar@gmail.com,Mon Jul 25 15:05:13 2022 +0000,1658761513.0,"[composite compliance] put, take (#81094)

Reference: #69991

This PR makes `put` CompositeExplicit as it is implemented in terms of `put_` (for which we can't handle Composite Compliance at the implementation level).

Ref (put implementation)
https://github.com/pytorch/pytorch/blob/478081c69851dbb6b7c570cbcc951199a80df0ff/aten/src/ATen/native/TensorAdvancedIndexing.cpp#L619-L621

Also, we update the `take` gradient formula to handle Tensor Subclass .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/81094
Approved by: https://github.com/zou3519",27.0,15.0,"aten/src/ATen/native/native_functions.yaml,functorch/test/test_ops.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",7.0,13,4,2.433934103,20.0,48563.0,4.0,159725.2857142857,5706.0,13309.0,0.0,,0.0,1
pytorch,079b3cc02cfedfcf0cbfc8c27d57beeb30429bd7,db298732c1d014e6ab6c5c49a74a14e882d1fda0,Shen Li,cs.shenli@gmail.com,Tue Oct 22 14:45:03 2019 -0700,1571755503.0,"remove deprecated torch.Tensor in test_c10d.py

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/28315

Test Plan: Imported from OSS

Differential Revision: D18019148

Pulled By: mrshenli

fbshipit-source-id: 9aff891c6df0b1cfa5ff01e7551973a16d512909",97.0,97.0,test/test_c10d.py,1.0,1,1,0,2.0,3217.0,1.0,48775.0,12438.0,34686.83333,0.0,,0.0,1
pytorch,a3d08de3311d6d0af581088f3555fe1f5db6d8df,db53389761fa87af09358bc1afa2157be0468130,gchanan,gregchanan@gmail.com,Tue Mar 27 19:27:23 2018 -0400,1522178843.0,"Add numpy.array-like type inference to torch.tensor. (#5997)

* Add numpy.array-like type inference to torch.tensor.

* Temporary fix for int/double types.

* Treat python floats as the default (scalar) dtype.

* Also make 0-length sequences the default scalar type and add more tests.

* Add type inference to sparse_coo_tensor.

* Fix sparse test.

* Remove allow_variables.

* Check numpy platform bits.

* Address review comments.

* Make suggested changes to constraints.

* More checking windows builds.

* Fix test for windows.",261.0,144.0,"aten/src/ATen/ScalarType.h,test/test_autograd.py,test/test_distributions.py,test/test_indexing.py,test/test_sparse.py,test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h,torch/csrc/utils/tensor_numpy.cpp,torch/csrc/utils/tensor_numpy.h,torch/distributions/beta.py,torch/distributions/constraints.py,torch/distributions/utils.py",14.0,11,4,2.225558541,39.0,15643.0,11.0,1386330.642857143,624.0,1962.405869,0.0,Corrective,1.0,1
pytorch,f99b5f1f233b37b6027f7cb1112b75ce8fe9b142,dbf44dffc94d8cf9c6161cee1fd0b9394e0f84b9,Oguz Ulgen,oulgen@meta.com,Tue Nov 07 01:47:20 2023 -0800,1699321640.0,"[Inductor] Cache generated user defined triton kernels on tensor dtype and non tensor parameters (#112752)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/112752
Approved by: https://github.com/jansel",69.0,14.0,"test/dynamo/test_functions.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py",3.0,5,2,1.383283756,2.0,12067.0,3.0,236105.66666666663,21641.0,49406.0,0.0,,0.0,1
pytorch,c52b3d75243c9b8dc15a172047c0270de9c82c3a,dc1b4ff74ed7c6d47525235f82d50ffb10521dbb,SsnL,SsnL@users.noreply.github.com,Sat Oct 07 04:40:51 2017 -0400,1507351251.0,Fix isContiguousDim (#3011),16.0,0.0,"test/test_nn.py,torch/lib/THC/THCDeviceTensor-inl.cuh",2.0,4,2,0.337290067,37.0,4790.0,1.0,32657.0,1938.0,23755.85823,0.0,Corrective,1.0,1
pytorch,319aee1afb2a70d76c78a5d4fdaf37a1ae6c4578,dc1ecdf8d91fd0e5129aa469d847374d8e1ba26a,Natalia Gimelshein,ngimel@fb.com,Wed Apr 01 06:26:11 2020 -0700,1585722371.0,"Moves torch cpu math tests to device-generic framework (#35658)

Summary:
Per title. Also, replaces reference computation with `math.xx` functions and torch.apply_  with numpy/scipy as appropriate.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35658

Differential Revision: D20744541

Pulled By: ngimel

fbshipit-source-id: a16ea506397c07f09f0d7f1c54fd8017418bd506",247.0,303.0,"test/test_torch.py,torch/testing/_internal/common_device_type.py",2.0,4,2,0.098374283,41.0,17433.0,2.0,20273.0,667.0,1895.0,0.0,,0.0,1
pytorch,8a837f0fe378589f8e9460fded2b09c6dca48d4f,dc209ed963afa10950c9f7ab7d73934683043f92,Pieter Noordhuis,pcnoordhuis@gmail.com,Wed Jun 13 22:27:32 2018 -0700,1528928852.0,"[c10d] Rendezvous skeleton (#8294)

* [c10d] Rendezvous skeleton

The rendezvous function takes an URL and produces a triplet of a store,
a process rank, and the process group size.

For the file and TCP handlers, the rank and size must be specified, but
other handlers may discover these parameters dynamically.

It returns a generator function, such that if a rendezvous handler
supports rerendezvous, you can write:

for store, rank, size in c10d.rendezvous(...):
  pg = c10d.ProcessGroup(store, rank, size)
  while the process group is valid:
    # Do stuff with process group

* Add Python 2 fallback for urlparse library

* Import X as Y

* Relative import seems to fix it

* Spelling

* Gate import on c10d availability",175.0,1.0,"test/test_c10d.py,torch/distributed/c10d/__init__.py,torch/distributed/c10d/rendezvous.py",3.0,4,2,1.183720854,1.0,193.0,2.0,275098.5,1322.0,3750.805292,0.0,Corrective,1.0,1
pytorch,e8ed042043c44098a7167d0949de2940d848db96,dc2e630341ad05eac015c34bb1f4d8c051e79cf7,yanbing-j,yanbing.jiang@intel.com,Fri Apr 15 16:31:52 2022 -0700,1650040312.0,"Optimize PReLU (float32) and enable PReLU BFloat16 support in CPU path (#63634)

Summary:
In this PR, we try to optimize PReLU op in CPU path, and enable BFloat16 support based on the optimized PReLU.

The original implementation uses parallel_for to accelerate operation speed, but vectorization is not used. It can be optimized by using TensorIterator, both including parallelization and vectorization.

The difference between PReLU and other activation function ops, is that PReLU supports a learnable parameter `weight`. When called without arguments, nn.PReLU() uses a single parameter `weight` across all input channels. If called with nn.PReLU(nChannels), a separate `weight` is used for each input channel. So we cannot simply use TensorIterator because `weight` is different for each input channel.

In order to use TensorIterator, `weight` should be broadcasted to `input` shape. And with vectorization and parallel_for, this implementation is much faster than the original one. Another advantage is, don't need to separate `share weights` and `multiple weights` in implementation.

We test the performance between the PReLU implementation of public Pytorch and the optimized PReLU in this PR, including fp32/bf16, forward/backward, share weights/multiple weights configurations. bf16 in public Pytorch directly reuses `Vectorized<scalar_t>` for `BFloat16`.

Share weights:
![image](https://user-images.githubusercontent.com/61222868/130403002-ef271bee-0cae-460b-b796-46853599c210.png)

![image](https://user-images.githubusercontent.com/61222868/130403028-96753102-bea3-44c2-8656-2526469e0627.png)

Multiple weights:
![image](https://user-images.githubusercontent.com/61222868/130403059-a3418eb2-9546-471f-b057-15bc0e46f0d0.png)

![image](https://user-images.githubusercontent.com/61222868/130403070-8c620db9-f354-4ddd-b5d5-4557e10ea77a.png)

cc albanD mruberry jbschlosser walterddr

Pull Request resolved: https://github.com/pytorch/pytorch/pull/63634

Reviewed By: yinghai

Differential Revision: D34031616

Pulled By: frank-wei

fbshipit-source-id: 04e2a0f9e92c658fba7ff56b1010eacb7e8ab44c
(cherry picked from commit ed262b15487557720bb0d498f9f2e8fcdba772d9)",154.0,220.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/Activation.h,aten/src/ATen/native/cpu/Activation.cpp,test/test_nn.py,torch/testing/_internal/common_methods_invocations.py",5.0,9,3,0.934391404,45.0,39796.0,1.0,273.0,2334.0,5489.5,0.0,Perfective,0.0,1
pytorch,52701227737489392e59fe57ded40226bf0811f6,dc40d3f93f849e467b2b56595a01f28e84ac7fa2,anjali411,chourdiaanjali123@gmail.com,Tue Nov 15 19:24:31 2022 +0000,1668540271.0,"Add meta impl for grid_sampler_2d_backward (#88745)

TODO: add an OpInfo

Pull Request resolved: https://github.com/pytorch/pytorch/pull/88745
Approved by: https://github.com/ezyang",66.0,9.0,"test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/inductor/test_torchinductor_opinfo.py,test/test_proxy_tensor.py,torch/_meta_registrations.py,torch/testing/_internal/common_methods_invocations.py",6.0,6,2,1.915861048,7.0,26037.0,5.0,96017.16666666669,9557.0,22247.0,0.0,Feature Addition,0.0,1
pytorch,69d3c00ae10a547829ede00729c3f785613a8336,dc7498c84d8a0090731fd1565baa489d345f7bdb,Jane Wang,janewang@fb.com,Fri Nov 30 00:14:01 2018 -0800,1543536841.0,"add gloo support for reduce on GPU (#14443)

Summary:
as titled
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14443

Reviewed By: pietern

Differential Revision: D13222907

Pulled By: janewangfb

fbshipit-source-id: f418c5d84880196f97089114d02957cf739243f8",146.0,13.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.883585086,3.0,2533.0,2.0,88915.5,5708.0,17298.83333,0.0,Feature Addition,0.0,1
pytorch,9e0005fca33594122f0b7f1522ccc8624551d34a,dc7f8163a1bee98345140e9575fc1d9f1c9e8a0d,Richard Zou,zou3519@gmail.com,Fri Sep 24 14:43:38 2021 -0700,1632494618.0,[functorch] Fix CI; update lagging op db,21.0,6.0,"functorch/test/common_utils.py,functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",4.0,2,1,1.396802669,1.0,4452.0,4.0,0.75,372.0,548.0,0.0,Corrective,1.0,1
pytorch,741accb11e3eed6c2eb8171b200a8adadf1616fb,dcb5eb8d9b0daa277a083a4e5c5f7633a8fed862,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Fri Aug 13 13:33:40 2021 -0700,1628861620.0,"OpInfo for `torch.nn.functional.normalize` (#62635)

Summary:
See https://github.com/facebookresearch/functorch/issues/78 and https://github.com/pytorch/pytorch/issues/54261

cc: mruberry zou3519 Chillee

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62635

Reviewed By: H-Huang

Differential Revision: D30136503

Pulled By: zou3519

fbshipit-source-id: 258c069f30d9c2a51ed27dadf94f3703b9432a4a",29.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,8655.0,1.0,33579.0,14600.0,33521.0,0.0,,0.0,1
pytorch,b62780fc4ffc12690ff3d1b42581f68228a64118,dcc6aed52c3347f1146b2fe92022789195130707,lezcano,lezcano-93@hotmail.com,Thu Jan 27 23:07:54 2022 -0800,1643324874.0,"Implement derivatives for torch.remainder and torch.fmod wrt the second argument and update the docs (#69908)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69908

I also took this chance to clarify a bit the documentation of these
functions.

cc brianjo mruberry

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D33774417

Pulled By: mruberry

fbshipit-source-id: ab4a9014006783d1f87d432ecb959c854374c2d4
(cherry picked from commit f319a75d781bbe12a48ef1ffd21d3874dfee3bfa)",36.0,27.0,"tools/autograd/derivatives.yaml,torch/_torch_docs.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,0.877380043,33.0,30392.0,2.0,16486.0,262.0,536.5,0.0,Non Functional,0.0,1
pytorch,bd8bf4a86ea1b1ed9acbc72c3bf5ca6f7ebb333f,dce525ab6bab5ea308863620f41f36540807298f,Geoffrey Roeder,roeder@cs.toronto.edu,Tue Oct 31 13:04:05 2017 +0000,1509455045.0,"adds sample_n function (#3249)

* adds sample_n function

* fixes style issues

* uses more efficient api calls

* fix bug where transpose applied to 1 dimension",43.0,0.0,"test/test_distributions.py,torch/distributions.py",2.0,2,2,0.933025295,7.0,228.0,1.0,401563.0,2035.0,23898.85823,0.0,Corrective,1.0,1
pytorch,87b5cd2e9ca51d31b201fa915629095f0df26c24,dd1327f801bffaecd80426e5159d70501422d634,Richard Zou,zou3519@users.noreply.github.com,Tue Mar 08 22:46:45 2022 -0500,1646779605.0,"[functorch] Make TestTransformFailure test names deterministic (pytorch/functorch#576)

This fixes parallel pytest which assumes that test names are
deterministic",4.0,2.0,functorch/test/test_vmap.py,1.0,2,1,0,1.0,3740.0,1.0,0.0,865.0,1213.0,0.0,Corrective,1.0,1
pytorch,a49af32a6e31541cb9ad6cbc311d8170d8746e4c,dd2d217a3ee28cfe74cea59038f2117ba675cf06,Richard Zou,zou3519@gmail.com,Tue May 25 21:49:34 2021 -0700,1621979374.0,"[functorch] Added error checking to vjp, also added opinfo tests for vjp

Not surprisingly, vjp has the same problems as grad (but no more
problems). Maybe we can just run vjp tests instead of grad tests in the
future.",99.0,15.0,"functorch/functorch/_src/eager_transforms.py,functorch/test/test_grad.py",2.0,4,1,0.561752608,1.0,308.0,2.0,0.0,111.0,216.5,0.0,Feature Addition,0.0,1
pytorch,63e9fdd92f821978afb588435faa984b43b0e73f,dd313d73384f6743b8b651922c5e307b937c7d88,Philip Meier,github.pmeier@posteo.de,Fri May 20 07:46:41 2022 +0200,1653032801.0,"support TestCase.longMessage in TestCase.assertEqual

Pull Request resolved: https://github.com/pytorch/pytorch/pull/77602

Approved by: https://github.com/mruberry",28.0,3.0,"test/test_testing.py,torch/testing/_internal/common_utils.py",2.0,4,2,0.869137581,4.0,5049.0,2.0,68014.5,3481.0,8282.0,0.0,,0.0,1
pytorch,9d4c2d743ba8b69fbaf151b4eaa52f9082c55491,dd6d04ddf243964700670c38e1a5ef4f50dc34cf,Ozan ÃaÄlayan,ozancag@gmail.com,Thu Nov 09 13:12:29 2017 +0100,1510233149.0,"doc: Normalize all true/false in docstrings to ``True|False`` (#3593)

* doc: Normalize all true/false in docstrings to ``True|False``

This makes them more apparent in the documentation.

* doc: fix flake8",209.0,206.0,"torch/_tensor_docs.py,torch/_torch_docs.py,torch/_utils.py,torch/autograd/__init__.py,torch/autograd/profiler.py,torch/autograd/variable.py,torch/cuda/streams.py,torch/jit/__init__.py,torch/nn/functional.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/instancenorm.py,torch/nn/modules/linear.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/utils/rnn.py,torch/optim/lr_scheduler.py,torch/optim/rmsprop.py,torch/random.py,torch/utils/data/dataloader.py",25.0,10,1,3.707248318,37.0,17087.0,15.0,739142.24,2101.0,24053.85823,0.0,Corrective,1.0,1
pytorch,476d85dd3f8ee48f6affc836d5c7fbd8ccfab200,dd893391d59e18ac771ddc77634385ca4ab7e5d8,Francisco Massa,fvsmassa@gmail.com,Fri Mar 24 19:02:05 2017 +0000,1490382125.0,Add argument to children to yield the name of the modules (#941),101.0,7.0,"test/test_nn.py,torch/nn/modules/module.py",2.0,4,2,0.969857018,25.0,2895.0,2.0,55643.5,520.0,3795.215734,0.0,Feature Addition,0.0,1
pytorch,a2934b38f8e9264fe24651ca179c281f25b33eac,dd8f6ac59784472d499d1594ca4e348f92337861,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Wed Sep 08 16:34:46 2021 -0700,1631118886.0,"Add forward mode differentiation for torch.linalg.cholesky and transpose (#62159)

Summary:
This PR adds forward mode differentiation for `torch.linalg.cholesky`, `torch.linalg.cholesky_ex`, and `transpose` functions.
Complex tests for Cholesky fail because for some reason the gradcheck sends matrices full of zeros to `cholesky_jvp` function.

cc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry heitorschueroff walterddr IvanYashchuk xwang233

Pull Request resolved: https://github.com/pytorch/pytorch/pull/62159

Reviewed By: mrshenli

Differential Revision: D30776829

Pulled By: albanD

fbshipit-source-id: 32e5539ed6423eed8c18cce16271330ab0ea8d5e",25.0,3.0,"tools/autograd/derivatives.yaml,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",4.0,7,2,1.367264961,14.0,16308.0,3.0,839311.0,15274.0,34999.0,0.0,Feature Addition,0.0,1
pytorch,757173a4da43f4d61597cbe60e5da16aadbacd42,dda95e69142df17e638076b2a0b46837951144a5,Taylor Robie,taylorrobie@fb.com,Thu Oct 15 23:30:49 2020 -0700,1602804649.0,"More Timer refinement (#46023)

Summary:
This PR just adds more polish to the benchmark utils:

1) `common.py`, `timer.py`, and `valgrind_wrapper/timer_interface.py` are now MyPy strict compliant. (except for three violations due to external deps.) Compare and Fuzzer will be covered in a future PR.
2) `CallgrindStats` now uses `TaskSpec` rather than accepting the individual fields which brings it closer to `Measurement`.
3) Some `__repr__` logic has been moved into `TaskSpec` (which `Measurement` and `CallgrindStats` use in their own `__repr__`s) for a more unified feel and less horrible f-string hacking, and the repr's have been given a cleanup pass.
4) `Tuple[FunctionCount, ...]` has been formalized as the `FunctionCounts` class, which has a much nicer `__repr__` than just the raw tuple, as well as some convenience methods (`__add__`, `__sub__`, `filter`, `transform`) for easier DIY stat exploration. (I find myself using the latter two a lot now.) My personal experience is that manipulating `FunctionCounts` is massively more pleasant than the raw tuples of `FunctionCount`. (Though it's still possible to get at the raw data if you want.)
5) Better support for multi-line `stmt` and `setup`.
6) Compare now also supports rowwise coloring, which is often the more natural layout for A/B testing.
7) Limited support for `globals` in `collect_callgrind`. This should make it easier to benchmark JIT models. (CC ZolotukhinM)
8) More unit tests, including extensive tests for the Callgrind stats manipulation APIs.
9) Mitigate issue with `MKL_THREADING_LAYER` when run in Jupyter. (https://github.com/pytorch/pytorch/issues/37377)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46023

Test Plan: changes should be covered by existing and new unit tests.

Reviewed By: navahgar, malfet

Differential Revision: D24313911

Pulled By: robieta

fbshipit-source-id: 835d4b5cde336fb7ff0adef3c0fd614d64df0f77",2886.0,513.0,"mypy-strict.ini,test/benchmark_utils/callgrind_artifacts.json,test/benchmark_utils/test_benchmark_utils.py,test/run_test.py,test/test_utils.py,torch/utils/benchmark/__init__.py,torch/utils/benchmark/utils/common.py,torch/utils/benchmark/utils/compare.py,torch/utils/benchmark/utils/timer.py,torch/utils/benchmark/utils/valgrind_wrapper/compat_bindings.py,torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py",11.0,7,2,2.406825432,39.0,3050.0,5.0,1351899.25,6026.0,13879.0,0.0,Feature Addition,0.0,1
pytorch,16d937b0dfa4c980b16f8cac9dc4fe6a3d9a6c65,ddc9bd335b21c9d806449e0a2297605aa697be94,Mike Ruberry,mruberry@fb.com,Fri Oct 29 16:52:24 2021 -0700,1635526344.0,"Adds reference vs. noncontiguous OpInfo test (#67434)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/63341.

This PR adds a new test, `test_noncontigous_samples`, that runs ops forward and backward and compares their outputs and grads between ""normal"" contiguous SampleInputs and noncontiguous SampleInputs. This test should preclude the need for noncontiguous SampleInputs going forward.

The test was added by generalizing the `.numpy()` transform on SampleInputs to support a new `.noncontiguous()` transform and copying forward/backward patterns from other tests in test_ops.py. It also discovered that many SampleInputs were incorrectly reusing tensors, so those have been revised. SampleInputs creating noncontiguous tensors for testing have also been altered to no longer do so.

In addition, this test discovered the following high priority silent correctness issues:

- https://github.com/pytorch/pytorch/issues/67432
- https://github.com/pytorch/pytorch/issues/67517
- https://github.com/pytorch/pytorch/issues/67513
- https://github.com/pytorch/pytorch/issues/67512
- https://github.com/pytorch/pytorch/issues/67470

It also identified the following issues:
- https://github.com/pytorch/pytorch/issues/67539

The pow OpInfo also incorrectly specified that pow supported the bool datatype, and this has been fixed. Its SampleInputs were written in a way that made requests for boolean SampleInputs return type promoting inputs that never actually tried to compute pow in bool.

This PR suggests we should add the following guidance for writing SampleInputs:

- ensure that all SampleInputs are independent of each other (don't reuse tensors)
- ensure that all SampleInput tensors have no grad or backward functions (no autograd history) -- they should be leaves
- prefer keeping sample inputs simple where possible, a good set of handwritten samples that test interesting cases may be better than an exhaustive but hard to read and maintain programmatic enumeration
- keep code readable by using functools.partial and writing simple inline helpers; break up large statements into a more readable series of smaller statements; especially don't write complicated generator expressions with a `for` at the end!

fyi kshitij12345 krshrimali pmeier anjali411 saketh-are zou3519 dagitses

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67434

Reviewed By: ngimel

Differential Revision: D32014557

Pulled By: mruberry

fbshipit-source-id: b17e19adc1d41e24441f0765af13d381fef5e3c1",472.0,280.0,"test/test_ops.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",3.0,4,2,0.667230072,2.0,16082.0,3.0,1027818.0,16712.0,39147.0,0.0,Corrective,1.0,1
pytorch,4ac848cf772184b1f8864b665e39957d29c39cf0,ddcf9c050b015a6a68099022b9885bcc7f7c1c52,Oguz Ulgen,oulgen@meta.com,Tue Oct 24 16:29:11 2023 -0700,1698164951.0,"[Inductor] Support calling user defined kernels with different type of arguments (#111939)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/111939
Approved by: https://github.com/jansel, https://github.com/zou3519
ghstack dependencies: #111770, #111808",45.0,5.0,"test/dynamo/test_functions.py,torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py",3.0,5,2,1.368060376,2.0,11313.0,3.0,36087.66666666666,21079.0,48091.5,0.0,,0.0,1
pytorch,89d56ae435373e54932566806c8432634e6a5cfa,de11fe0c8381bc8fa4182f247226d1f606d79678,Wei Yang,weiyang@fb.com,Fri Sep 21 23:21:32 2018 -0700,1537572092.0,"migrate PReLU to ATen (#11758)

Summary:
- fixes https://github.com/pytorch/pytorch/issues/10723
- migrate PReLU to ATen and deprecate legacy PReLU
- performance:

CPU with weight.numel() = 1
```
>>> m = nn.PReLU()
>>> x = torch.randn(100, 100, 100, requires_grad=True)
>>> %timeit -r 100 y = m(x)
100 loops, best of 100: 9.43 ms per loop

>>> y = m(x).sum()
>>> %timeit -r 100 y.backward(retain_graph=True)
10 loops, best of 100: 24.4 ms per loop

>>> m = nn.PReLU()
>>> x = torch.randn(100, 100, 100, requires_grad=True)
>>> %timeit -r 100 y = m(x)
1000 loops, best of 100: 695 Âµs per loop

>>> y = m(x).sum()
>>> %timeit -r 100 y.backward(retain_graph=True)
100 loops, best of 100: 2.47 ms per loop
```

CPU with weight.numel() = channels
```
>>> m = nn.PReLU(100)
>>> x = torch.randn(100, 100, 100, requires_grad=True)
>>> %timeit -r 100 y = m(x)
1000 loops, best of 100: 603 Âµs per loop

>>> y = m(x).sum()
>>> %timeit -r 100 y.backward(retain_graph=True)
100 loops, best of 100: 13.3 ms per loop

>>> m = nn.PReLU(100)
>>> x = torch.randn(100, 100, 100, requires_grad=True)
>>> %timeit -r 100 y = m(x)
1000 loops, best of 100: 655 Âµs per loop

>>> y = m(x).sum()
>>> %timeit -r 100 y.backward(retain_graph=True)
100 loops, best of 100: 2.45 ms per loop
```

CUDA with weight.numel() = 1
```
>>> m = nn.PReLU().cuda()
>>> x = torch.randn(100, 100, 100, requires_grad=True).cuda()
>>> %timeit -r 100 torch.cuda.synchronize(); y = m(x); torch.cuda.synchronize();
10000 loops, best of 100: 187 Âµs per loop

>>> y = m(x).sum()
>>> %timeit -r 100 torch.cuda.synchronize(); y.backward(retain_graph=True); torch.cuda.synchronize();
100 loops, best of 100: 2.01 ms per loop

>>> m = nn.PReLU().cuda()
>>> x = torch.randn(100, 100, 100, requires_grad=True).cuda()
>>> %timeit -r 100 torch.cuda.synchronize(); y = m(x); torch.cuda.synchronize();
1000 loops, best of 100: 195 Âµs per loop

>>> y = m(x).sum()
>>> %timeit -r 100 torch.cuda.synchronize(); y.backward(retain_graph=True); torch.cuda.synchronize();
100 loops, best of 100: 2.28 ms per loop
```

CUDA with weight.numel() = channel
```
>>> m = nn.PReLU(100).cuda()
>>> x = torch.randn(100, 100, 100, requires_grad=True).cuda()
>>> %timeit -r 100 torch.cuda.synchronize(); y = m(x); torch.cuda.synchronize();
1000 loops, best of 100: 174 Âµs per loop

>>> y = m(x).sum()
>>> %timeit -r 100 torch.cuda.synchronize(); y.backward(retain_graph=True); torch.cuda.synchronize();
100 loops, best of 100: 2.27 ms per loop

>>> m = nn.PReLU(100).cuda()
>>> x = torch.randn(100, 100, 100, requires_grad=True).cuda()
>>> %timeit -r 100 torch.cuda.synchronize(); y = m(x); torch.cuda.synchronize();
10000 loops, best of 100: 181 Âµs per loop

>>> y = m(x).sum()
>>> %timeit -r 100 torch.cuda.synchronize(); y.backward(retain_graph=True); torch.cuda.synchronize();
100 loops, best of 100: 2.26 ms per loop
```

The huge performance regression in CPU when weight.numel() = 1 is addressed by replacing at::CPU_tensor_apply* with parallelized kernels.

ezyang SsnL zou3519  soumith
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11758

Differential Revision: D9995799

Pulled By: weiyangfb

fbshipit-source-id: d289937c78075f46a54dafbde92fab0cc4b5b86e",550.0,556.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/cuda/Activation.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/nn.yaml,aten/src/THCUNN/CMakeLists.txt,aten/src/THCUNN/PReLU.cu,aten/src/THCUNN/generic/PReLU.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/PReLU.c,aten/src/THNN/generic/THNN.h,aten/src/THNN/init.cpp,test/test_nn.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/nn/functional.py,torch/nn/modules/activation.py",20.0,17,4,3.018228801,44.0,34068.0,15.0,1730156.6,4274.0,12072.33333,0.0,Corrective,1.0,1
pytorch,d8f3c601e4926c0bdb79a2dd6842239bc3bd6e71,de1f4e69ddb328aa34d1f8736fd7694664537805,SsnL,SsnL@users.noreply.github.com,Fri Oct 27 19:54:02 2017 -0400,1509134042.0,raw text (#3327),69.0,72.0,"torch/nn/functional.py,torch/nn/modules/activation.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/module.py,torch/nn/modules/pooling.py,torch/nn/modules/upsampling.py,torch/nn/parallel/data_parallel.py,torch/nn/parallel/distributed.py,torch/nn/parallel/scatter_gather.py,torch/nn/parameter.py,torch/nn/utils/clip_grad.py,torch/nn/utils/convert_parameters.py,torch/nn/utils/rnn.py,torch/nn/utils/weight_norm.py",15.0,5,1,3.158948874,37.0,5838.0,2.0,75806.0,2016.0,23880.35823,0.0,,0.0,1
pytorch,8c852de54d8b07c1fa546cbd400c093c4b2082d6,de40c8e495ffc7b4042ac544ba38759c4dcb5d50,Mike Ruberry,mruberry@devfair044.maas,Sun Jun 06 21:51:26 2021 -0700,1623016286.0,"Adds remaining OpInfos and removes redundant test generators (#55558)

Summary:
Per title.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/55558

Reviewed By: ngimel

Differential Revision: D28922522

Pulled By: mruberry

fbshipit-source-id: 89cefd93788bc8aa0683f4583cf5caa81aa2dc93",211.0,1421.0,"test/run_test.py,test/test_autograd.py,test/test_cuda.py,test/test_jit.py,test/test_linalg.py,test/test_op_aliases.py,test/test_ops.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",9.0,4,2,2.340703505,47.0,55227.0,8.0,559117.2222222222,12777.0,29029.0,0.0,Feature Addition,0.0,1
pytorch,0d19b81a65461e4386e870623bca74a8e0aaa7b7,de42542351ad933ada59a4a8cf3b247d75d52917,Du Phan,fehiepsi@gmail.com,Sat Mar 31 14:39:33 2018 +0900,1522507173.0,Make precision matrix computation in mvn stable (#6128),6.0,3.0,"torch/distributions/constraints.py,torch/distributions/multivariate_normal.py",2.0,2,1,0.918295834,4.0,445.0,1.0,76183.0,1023.0,6941.672317,0.0,,0.0,1
pytorch,cdbd39ba5714d37398c261a283ede26754e1025e,de949a0e59f458a77c213d270106a0c9fea6c484,Mike Ruberry,mruberry@devfair044.h1.fair,Mon Apr 18 21:55:32 2022 +0000,1650318932.0,"Various OpInfo architecture improvements

This PR makes the following improvements:

- moves the custom skip list for test_normalize_operator_exhaustive in test_fx_experimental to use the typical OpInfo skip architecture. The skips were updated to xfails, and that identified some operators which were no longer failing the test
- redundant tests with OpInfo-based testing in test_jit.py were removed
- test_dtypes was improved so its error messages are clear and it makes test_nondifferentiable redundant; the latter test has been removed
- OpInfo.supports_complex_autograd() is removed in favor of a more accurate and general test for whether the particular dtype is in the backward dtypes of the operator
- gradchecks have been improved to verify that an operator doesn't support grad if it claims not to
- gradchecks have been improved to test the gradient of all input tensors that require gradient
- the concept of ""default test dtypes"" has been removed
- excessive and mostly redundant out testing for elementwise unary operators has been removed
- metadata for whether an op supports nuanced ""safe casting"" to out behavior has been removed from OpInfos
- numerous skips have been converted to xfails
- numerous OpInfos have had their metadata fixed based on the new checks
- jit-specific utilities in common_methods_invocations.py have been moved to jit_programming_utils.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75951
Approved by: https://github.com/ngimel",795.0,1056.0,"test/test_binary_ufuncs.py,test/test_fx_experimental.py,test/test_jit.py,test/test_ops.py,test/test_ops_gradients.py,test/test_ops_jit.py,test/test_reductions.py,test/test_spectral_ops.py,test/test_testing.py,test/test_unary_ufuncs.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_metaprogramming_utils.py",13.0,4,2,1.816064943,17.0,50245.0,11.0,1700474.3846153845,2390.0,5581.0,0.0,Corrective,1.0,1
pytorch,54ea7d33ba39a4a9ecd86cfb4439a49e4143b783,dec5aa2260cef540b622bd9a9504b6f11cb1f607,Gary Miguel,garymiguel@microsoft.com,Fri Jul 09 23:13:27 2021 -0700,1625872407.0,"[JIT] clean up (#60390)

Summary:
* Minor: spelling, grammar.
* Add calls to `GRAPH_DUMP()` where they were missing.
* Add or expand a few comments.
* Move a few comments to seemingly more appropriate spots.
* In canonicalize_graph_fuser_ops.cpp inline `runnableInputs()` since it
  was only called in one place and had a misleading comment and
  confusing name.
* In `PeepholeOptimizeImpl::optimizeBlock()`, set `changed = true;` when
  removing `aten::is_complex`. Pretty sure its absence was a bug.
* Delete unused `_jit_pass_remove_inplace_ops` and and its
  implementation `RemoveInplaceOps()`.
* In `preprocessCaffe2Ops()`, remove redundant check for nested optional
  types. It was already checked in `checkONNXCompatibility()`.
* In `EncoderBase::AddAttribute`, log the unexpected attribute kind.
  I don't remember the repro case now but I did hit this error at some
  point and this additional logging made it easier to understand.
* In `fuseConvBatchNorm()` in eval_peephole.cpp, consistently use
  camelCase instead of snake_case for local variables.
* Add curly braces around the bodies of if and loops.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/60390

Reviewed By: Krovatkin

Differential Revision: D29523283

Pulled By: SplitInfinity

fbshipit-source-id: 4e16c5648616f53da07d68dab7fdf252e06a0752",274.0,302.0,"test/test_jit.py,torch/_C/__init__.pyi.in,torch/csrc/jit/jit_log.cpp,torch/csrc/jit/jit_log.h,torch/csrc/jit/passes/canonicalize_graph_fuser_ops.cpp,torch/csrc/jit/passes/constant_propagation.cpp,torch/csrc/jit/passes/erase_number_types.cpp,torch/csrc/jit/passes/inline_fork_wait.cpp,torch/csrc/jit/passes/lower_tuples.cpp,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx/constant_fold.cpp,torch/csrc/jit/passes/onnx/constant_fold.h,torch/csrc/jit/passes/onnx/eval_peephole.cpp,torch/csrc/jit/passes/onnx/eval_peephole.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp,torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp,torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp,torch/csrc/jit/passes/onnx/shape_type_inference.cpp,torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp,torch/csrc/jit/passes/peephole.cpp,torch/csrc/jit/passes/peephole_alias_sensitive.cpp,torch/csrc/jit/passes/remove_inplace_ops.cpp,torch/csrc/jit/passes/remove_inplace_ops.h,torch/csrc/jit/passes/remove_mutation.cpp,torch/csrc/jit/python/init.cpp,torch/csrc/jit/python/python_arg_flatten.cpp,torch/csrc/jit/serialization/export.cpp",29.0,9,2,3.641782873,14.0,28364.0,22.0,6730841.9655172415,13734.0,31005.0,0.0,Corrective,1.0,1
pytorch,09046713cc02fbfb87705fb74183870320a639de,dece155335eaf808df54474fd4efaa65b694fde8,anjali411,chourdiaanjali123@gmail.com,Fri Feb 28 16:40:20 2020 -0800,1582908020.0,"Modified assertEqual to handle complex tensors (#33773)

Summary:
- Modified assertEqual to handle complex tensors
- added a test in test_torch.py to test torch.zeros
- added dispatch for complex for index_kernel, index_put_kernel
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33773

Differential Revision: D20135553

Pulled By: anjali411

fbshipit-source-id: f716604535c0447ecffa335b0fc843431397c988",20.0,6.0,"aten/src/ATen/native/cpu/IndexKernel.cpp,aten/src/ATen/native/cuda/IndexKernel.cu,c10/core/ScalarType.h,test/test_torch.py,torch/testing/_internal/common_utils.py",5.0,12,4,2.23355643,40.0,17661.0,5.0,1374838.2,15065.0,40558.33333,0.0,Feature Addition,1.0,1
pytorch,d79e45bbba250b6e9e1e7854cb2f2ec45f9d9800,ded6fb0293f45b4f6ab70a831e66516266fdc96f,SsnL,tongzhou.wang.1994@gmail.com,Tue Jan 29 20:23:06 2019 -0800,1548793386.0,"Add stack & cat support for CPU Half (#16389)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/6968

Needed for #14705
Pull Request resolved: https://github.com/pytorch/pytorch/pull/16389

Differential Revision: D13861446

Pulled By: gchanan

fbshipit-source-id: 7b8700b95aaf252d9669693dbddccb2302e58409",201.0,187.0,"aten/src/ATen/Declarations.cwrap,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensor.h,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THTensorMoreMath.cpp,test/common_utils.py,test/test_torch.py",7.0,6,2,1.999497467,41.0,17019.0,4.0,1240412.7142857143,6694.0,20720.33333,0.0,Corrective,1.0,1
pytorch,bada92ddcd8059fad976c38fba9dd8a8f17bbc2e,df0a4474c49af16bae563000d01a853a179977c8,Sam Gross,colesbury@gmail.com,Mon Feb 12 22:50:19 2018 -0500,1518475819.0,"Allow and warn when indexing a zero-dim Variable (#5114)

This better maintains backwards compatibility when Tensors and Variables
are merged. For example:

   >>> loss = var.sum().data[0]

Currently, `var.sum().data` is 1-dim so indexing. Once scalars are
enabled and Variable and Tensor are merged it will be zero-dim. This
change allows that expression to continue working (with a warning). In
the future, the canonical way to compute that expression will be:

   >>> loss = float(var.sum())

Or an equivalent alternative:

   >>> loss = var.sum().item()

Also fixes a few error cases.",57.0,26.0,"test/test_indexing.py,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h",4.0,5,2,1.214409416,6.0,1068.0,4.0,462347.25,524.0,1538.905869,0.0,Corrective,1.0,1
pytorch,1237cf6b6ca86ac6afd5c0a8d3075c9a2d85b6e4,df14650f0b14b80db132b0c1797dc595fbee1054,Driss Guessous,drisspg@fb.com,Mon Jan 23 20:50:46 2023 +0000,1674507046.0,"[SDPA] Update SDPA API and make function Public (#92189)

# Summary
In preparation for pt 2.0 launch this PR updates SDPA's API and makes the function a nn.funcitonal public function.

## Changes
### API
Previously the the function signature was:
`scaled_dot_product_attention(query, key, value, attn_mask=None, need_attn_weights=False, dropout_p=0.0, is_causal=False) -> (Tensor, Tensor)`
Updated signature:
`scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False) -> Tensor`

This PR removes the need_attn_weights optional boolean variable and updates the return type to a singular tensor.

#### Reasoning:
The main goal of this function is to provide an easy interface for users to call into fused attention kernels e.g.  (FlashAttention). The fused kernels do not currently support arbitrary attn_mask or dropout but there is a PR to mem-efficient attention to enable these. We want to have the API surface ready for when the backing kernels get updated.

The fused kernels save on memory usage by not materializing the weights and it is unlikely that a fast fused implementation will enable this feature so we are removing.

Discussed with folks at FAIR/Xformers and +1 this API change.

#### Make function Public
In preparation for the pt 2.0 launch we make the function public to start to generate user feedback

Pull Request resolved: https://github.com/pytorch/pytorch/pull/92189
Approved by: https://github.com/cpuhrsch",428.0,426.0,"aten/src/ATen/autocast_mode.cpp,aten/src/ATen/functorch/BatchRulesDecompositions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp,aten/src/ATen/native/transformers/attention.cpp,aten/src/ATen/native/transformers/attention.h,aten/src/ATen/native/transformers/cuda/attention.cu,aten/src/ATen/native/transformers/cuda/flash_attn/fmha_api.cpp,aten/src/ATen/native/transformers/cuda/flash_attn/fmha_api.h,aten/src/ATen/native/transformers/cuda/sdp_utils.h,benchmarks/transformer/sdp.py,benchmarks/transformer/sdp_backwards.py,test/allowlist_for_publicAPI.json,test/distributed/_tensor/test_dtensor_ops.py,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/inductor/test_torchinductor_opinfo.py,test/test_fx.py,test/test_nestedtensor.py,test/test_transformers.py,torch/_meta_registrations.py,torch/nn/functional.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",26.0,22,4,2.577292078,42.0,69949.0,21.0,1617978.2692307692,11589.0,26615.5,0.0,Feature Addition,0.0,1
pytorch,908b451efb1b35a3ef26154bc959cae7ff29e15e,df1d68d52e15dffabc0ef714e8e064eda7105cd7,Michael Suo,suo@fb.com,Wed Feb 05 21:05:55 2020 -0800,1580936755.0,"[jit] fix parser for one-line functions (#32941)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32941

The Python grammar allows single-statement one-line functions. So we
should allow it in the string parser.

Test Plan: Imported from OSS

Differential Revision: D19704153

Pulled By: suo

fbshipit-source-id: 8c06cc9c600aa2a9567b484a1ecc0360aad443e3",26.0,8.0,"test/test_jit.py,torch/csrc/jit/script/parser.cpp",2.0,5,2,0.522559375,13.0,18934.0,2.0,2089964.0,14609.0,39407.33333,0.0,Corrective,1.0,1
pytorch,0435059ddf76077f108f2d92930bf44127a21e7f,df70e2fde57595e87bff55cb06af75aaa336b8a2,Jeffrey Wan,jw3468@fb.com,Fri Mar 26 18:17:46 2021 -0700,1616782666.0,"Refactor get analytical jacobian (#54049)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54049

The goal of this is to factor out the core logic of getting the analytical jacobian which is effectively doing `f(grad_out) = grad_out^T J = grad_input`. This allows us to test a lot of logic that was not possible before because now we can replace f with whatever we want in order to simulate potential issues that gradcheck is designed to catch.

Edit: I realize a lot of things this PR was originally aiming to allow is actually possible with hooks, hence the tests have already been added in a earlier PR in the stack. But this is still slightly useful for reducing code duplication when adding the new fast gradcheck code (more details below)

After this change, `get_analytical_jacobian` is only responsible for gathering a list of rows that are later combined into a single Jacobian tensor. This means we don't have to perform any checks for correctness of the dtypes/size at this step

We factor out that logic into a separate function, `combine_jacobian_rows`, which handles the list of rows -> single Tensor step for each jacobian, and the error checking it entails. (This allows this code to be shared between the fast/slow versions.)

Test Plan: Imported from OSS

Reviewed By: ailzhang

Differential Revision: D27307240

Pulled By: soulitzer

fbshipit-source-id: 65bb58cda000ed6f3114e5b525ac3cae8da5b878",68.0,42.0,torch/autograd/gradcheck.py,1.0,2,1,0,28.0,702.0,1.0,161019.0,10101.0,22354.0,0.0,Corrective,0.0,1
pytorch,a661e58731776ab65a6ef5e2e3ea49b7c0a41e46,df8bb5a42b3c2b1c57bbec9a1c60db6aeafe0f20,kshitij12345,kshitijkalambarkar@gmail.com,Tue Apr 20 08:01:07 2021 -0700,1618905667.0,"Add OpInfo for polygamma and remove torch_op_tests Infra (#51966)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

* OpInfo entry for Polygamma
* Removes infra of torch_op_tests

Pull Request resolved: https://github.com/pytorch/pytorch/pull/51966

Reviewed By: bdhirsh

Differential Revision: D27851858

Pulled By: mruberry

fbshipit-source-id: 7f1d0273065e1df56a152f95a14513959af29a1b",142.0,270.0,"aten/src/ATen/native/native_functions.yaml,test/test_fx.py,test/test_unary_ufuncs.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",5.0,10,4,1.142496902,16.0,20960.0,5.0,228542.6,11018.0,24335.0,0.0,Feature Addition,0.0,1
pytorch,99dac4dd4822adcc1462ad85d43b2f9201174360,dfb533ca5bc1b37d154e812e63a681283c667117,Kshiteej K,kshitijkalambarkar@gmail.com,Thu Dec 01 14:43:30 2022 +0000,1669905810.0,"add vjp test with non-contig inputs (#89375)

Ref: https://github.com/pytorch/functorch/issues/1029

We update `test_vjp` to do contiguous and non-contiguous sample testing.

Prev Time: ~32s
New Time : ~50s
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89375
Approved by: https://github.com/zou3519",33.0,1.0,test/functorch/test_ops.py,1.0,2,1,0,1.0,1812.0,1.0,163579.0,10073.0,23111.5,0.0,Feature Addition,0.0,1
pytorch,3bb0f1f343c1d3f865b2e6060d65a5b2e2c6ac26,dfc7fa03e5d33f909b9d7853dd001086f5d782a0,Nikita Vedeneev,nik@quansight.com,Thu Mar 25 20:28:50 2021 -0700,1616704130.0,"lu_backward: more numerically stable and with complex support. (#53994)

Summary:
As per title.

Numerical stability increased by replacing inverses with solutions to systems of linear triangular equations.

Unblocks computing `torch.det` for FULL-rank inputs of complex dtypes via the LU decomposition once https://github.com/pytorch/pytorch/pull/48125/files is merged:
```
LU, pivots = input.lu()
P, L, U = torch.lu_unpack(LU, pivots)
det_input = P.det() * torch.prod(U.diagonal(0, -1, -2), dim=-1)  # P is not differentiable, so we are fine even if it is complex.
```

Unfortunately, since `lu_backward` is implemented as `autograd.Function`, we cannot support both autograd and scripting at the moment.
The solution would be to move all the lu-related methods to ATen, see https://github.com/pytorch/pytorch/issues/53364.

Resolves https://github.com/pytorch/pytorch/issues/52891
TODOs:
* extend lu_backward for tall/wide matrices of full rank.
* move lu-related functionality to ATen and make it differentiable.
* handle rank-deficient inputs.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53994

Reviewed By: pbelevich

Differential Revision: D27188529

Pulled By: anjali411

fbshipit-source-id: 8e053b240413dbf074904dce01cd564583d1f064",70.0,38.0,"tools/autograd/gen_variable_type.py,torch/_autograd_functions.py,torch/_tensor.py,torch/functional.py,torch/testing/_internal/common_methods_invocations.py",5.0,5,2,1.479987434,30.0,8149.0,5.0,2999440.6,10073.0,22296.0,0.0,,0.0,1
pytorch,26cdec6ce47cc108c7c94d591e58a0f044e1a942,dfd2edc025b284abc6972bdcfaa9f4f7b8808036,kshitij12345,kshitijkalambarkar@gmail.com,Thu Jun 24 06:59:03 2021 -0700,1624517943.0,"[special] add zeta (#59623)

Summary:
Reference https://github.com/pytorch/pytorch/issues/50345

`zeta` was already present in the codebase to support computation of `polygamma`.

However, `zeta` only had `double(double, double)` signature **for CPU** before the PR (which meant that computation `polygamma` were always upcasted to `double` for zeta part).

With this PR, float computations will take place in float and double in double.

Have also refactored the code and moved the duplicate code from `Math.cuh` to `Math.h`

**Note**: For scipy, q is optional, and if it is `None`, it defaults `1` which corresponds to Reimann-Zeta. However, for `torch.specia.zeta`, I made it mandatory cause for me it feels odd without `q` this is Reimann-Zeta and with `q` it is the general Hurwitz Zeta. I think sticking to just general made more sense as passing `1` for q sounds trivial.

Verify:
* [x] Docs https://14234587-65600975-gh.circle-artifacts.com/0/docs/special.html#torch.special.zeta

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59623

Reviewed By: ngimel

Differential Revision: D29348269

Pulled By: mruberry

fbshipit-source-id: a3f9ebe1f7724dbe66de2b391afb9da1cfc3e4bb",296.0,126.0,"aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/Math.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu,aten/src/ATen/native/cuda/Math.cuh,aten/src/ATen/native/cuda/UnaryGammaKernels.cu,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_binary_ufuncs.py,test/test_fx.py,test/test_fx_experimental.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",18.0,20,5,3.281537204,16.0,36040.0,7.0,581302.3333333334,13314.0,30105.5,0.0,Feature Addition,0.0,1
pytorch,493b3c2c7c2db9b9f3148f227b1e15a82efbc0d2,dfdd797723c7d80634d83f3260e9dfd4b2b431f9,Hong Xu,hong@topbug.net,Tue Aug 18 20:39:23 2020 -0700,1597783163.0,"Replace all AT_ASSERTM under ATen CUDA kernels. (#42989)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/42989

Test Plan: Imported from OSS

Reviewed By: colesbury

Differential Revision: D23190011

Pulled By: ezyang

fbshipit-source-id: 7489598d7d920f32334943c1bf12bba74208a96c",12.0,8.0,"aten/src/ATen/native/cuda/IndexKernel.cu,aten/src/ATen/native/cuda/RNN.cu,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/SummaryOps.cu",4.0,5,1,1.760964047,5.0,2068.0,4.0,5206560.5,4376.0,10241.0,0.0,,0.0,1
pytorch,65412f95f0d17861d79e5031717266d8bdcd6b09,dfe484a3b32aeaa41819f7d1a341ca34920b2559,Aaron Gokaslan,aaronGokaslan@gmail.com,Sun May 14 14:20:56 2023 +0000,1684074056.0,"[BE]: Bugfix functorch and some generic typing improvements (#101337)

Fixes some typing bugs found with newer versions of mypy

Pull Request resolved: https://github.com/pytorch/pytorch/pull/101337
Approved by: https://github.com/ezyang",24.0,13.0,"torch/_dynamo/eval_frame.py,torch/_functorch/pyfunctorch.py,torch/_inductor/codegen/wrapper.py,torch/_logging/_internal.py,torch/distributed/optim/zero_redundancy_optimizer.pyi,torch/distributed/utils.py,torch/nn/utils/rnn.pyi,torch/utils/tensorboard/_onnx_graph.py,torchgen/selective_build/selector.py",9.0,14,2,2.643116788,3.0,4110.0,9.0,7443318.0,15811.0,35503.5,0.0,Corrective,1.0,1
pytorch,c6773c67d6f5a773e1c73949912701941572b2e8,dfeb7898f27b839f1c535de0d4d447ee18c8015e,Richard Zou,zou3519@gmail.com,Fri Apr 30 16:59:02 2021 -0700,1619801942.0,[functorch] pytree output support for vmap,147.0,62.0,"functorch/functorch/_src/eager_transforms.py,functorch/functorch/_src/pytree_hacks.py,functorch/functorch/_src/vmap.py,functorch/test/test_vmap.py",4.0,4,1,1.735324691,1.0,3084.0,3.0,0.6666666666666666,41.0,119.5,0.0,,0.0,1
pytorch,399169fc925efc00ff4b51c5c4170117105abb8b,e011a8e18bf469a6a612fd1e7647159c353730a9,Kulin Seth,kulinseth@gmail.com,Fri May 13 18:28:53 2022 +0000,1652466533.0,"Enable PyTorch operations on MPS Backend. (#77343)

Add PyTorch operations to MPS backend.

- https://github.com/pytorch/pytorch/issues/77394
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77343
Approved by: https://github.com/albanD",17614.0,183.0,"CMakeLists.txt,aten/src/ATen/Context.cpp,aten/src/ATen/mps/MPSDevice.h,aten/src/ATen/mps/MPSDevice.mm,aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/DispatchStub.h,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/Pooling.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/cudnn/ConvShared.cpp,aten/src/ATen/native/miopen/Conv_miopen.cpp,aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/operations/Activation.mm,aten/src/ATen/native/mps/operations/AdaptiveAveragePooling.mm,aten/src/ATen/native/mps/operations/BinaryOps.mm,aten/src/ATen/native/mps/operations/Blas.mm,aten/src/ATen/native/mps/operations/ConstantOps.mm,aten/src/ATen/native/mps/operations/Convolution.mm,aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/mps/operations/Distributions.mm,aten/src/ATen/native/mps/operations/Indexing.mm,aten/src/ATen/native/mps/operations/Linear.mm,aten/src/ATen/native/mps/operations/LinearAlgebra.mm,aten/src/ATen/native/mps/operations/LossOps.mm,aten/src/ATen/native/mps/operations/Normalization.mm,aten/src/ATen/native/mps/operations/PointwiseOps.mm,aten/src/ATen/native/mps/operations/Pooling.mm,aten/src/ATen/native/mps/operations/RangeFactories.mm,aten/src/ATen/native/mps/operations/ReduceOps.mm,aten/src/ATen/native/mps/operations/Repeat.mm,aten/src/ATen/native/mps/operations/RnnOps.mm,aten/src/ATen/native/mps/operations/Scalar.mm,aten/src/ATen/native/mps/operations/ScatterGather.mm,aten/src/ATen/native/mps/operations/Shape.mm,aten/src/ATen/native/mps/operations/SoftMax.mm,aten/src/ATen/native/mps/operations/TensorCompare.mm,aten/src/ATen/native/mps/operations/TriangularOps.mm,aten/src/ATen/native/mps/operations/UnaryOps.mm,aten/src/ATen/native/native_functions.yaml,c10/core/TensorImpl.h,caffe2/CMakeLists.txt,docs/source/notes/mps.rst,test/test_autograd.py,test/test_linalg.py,test/test_modules.py,test/test_mps.py,test/test_nn.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_modules.py,torchgen/dest/register_dispatch_key.py",55.0,23,8,4.266237578,81.0,98570.0,19.0,2453230.785714286,3193.0,7688.5,0.0,Feature Addition,0.0,1
pytorch,9de0b63554e25827b0642d1038ceeefd3eee1cc5,e01fc56ecb7c7437fab5bd00665386e05e077ce4,eellison,elias_ellison@brown.edu,Mon Nov 11 19:24:43 2019 -0800,1573500283.0,"move type inference for arange into c++ (#27629)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/17662

I'm not sure if `arange` needs to be in python_arg_parser at all, given the schemas in native_functions.yaml. In any case this at least fixes the dytpe mismatch.

In follow up PRs I will try to handle some of the other ops that do type inference at the python level, like randint.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27629

Differential Revision: D17885939

Pulled By: eellison

fbshipit-source-id: f97a8bc722b7ab77de1c42a992e49a4a3175ad60",105.0,33.0,"aten/src/ATen/native/TensorFactories.cpp,test/cpp/api/functional.cpp,test/cpp/api/modules.cpp,test/cpp/api/tensor.cpp,test/test_jit.py,test/test_torch.py,tools/autograd/templates/python_torch_functions.cpp,torch/csrc/jit/tracer.cpp,torch/onnx/symbolic_helper.py",9.0,14,4,3.025705489,42.0,39821.0,6.0,272185.5555555556,13008.0,35907.33333,0.0,Corrective,1.0,1
pytorch,afb2d27b24b515f380e889028fe53998d29d4e38,e033db04777cc1e269d39a53ec86a2889cf075f6,Rohan Varma,rvarm1@fb.com,Tue Jun 09 02:06:56 2020 -0700,1591668416.0,"Enable RRef timeout for tensorpipe (#39531)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39531

Enables RRef timeout support in TP agent by having TP agent mark
timeout errors with `makeRPCError` API. Also does some refactoring so TP agent
can print out the timeout for each future that has timed out.
ghstack-source-id: 105461555

Test Plan: CI

Differential Revision: D21881475

fbshipit-source-id: f63300e1f0a80ac7eebc983752070c0ec6ac17a6",17.0,10.0,"torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h,torch/testing/_internal/distributed/rpc/rpc_test.py",3.0,8,1,0.824980257,2.0,4273.0,3.0,299855.6666666667,2669.0,6548.5,0.0,Perfective,0.0,1
pytorch,53f00ae429aa1bd18b407ffd17d06c9e85578edf,e055ffbdc7a09abd58802a116b0d885cd6e31b95,Adam Paszke,adam.paszke@gmail.com,Fri Aug 19 21:22:47 2016 -0700,1471641767.0,Add nn,451.0,0.0,"torch/_thnn/thcunn.py,torch/_thnn/thnn.py,torch/nn/__init__.py,torch/nn/backends/__init__.py,torch/nn/backends/backend.py,torch/nn/backends/thnn.py,torch/nn/cuda.py,torch/nn/functions/__init__.py,torch/nn/functions/linear.py,torch/nn/functions/thnn.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py,torch/nn/modules/container.py,torch/nn/modules/conv_2d.py,torch/nn/modules/criterion.py,torch/nn/modules/linear.py,torch/nn/modules/module.py,torch/nn/modules/relu.py",18.0,6,1,3.197298543,4.0,27.0,1.0,2248.0,123.0,2208.528571,0.0,Feature Addition,0.0,1
pytorch,1c09bfde1b311f836ecc1b0f491895b2ac83a297,e05d689c49f650601243693ed433a23930d31e46,Peter Goldsborough,psag@fb.com,Mon Sep 24 21:28:54 2018 -0700,1537824534.0,"Unify C++ API with C++ extensions (#11510)

Summary:
Currently the C++ API and C++ extensions are effectively two different, entirely orthogonal code paths. This PR unifies the C++ API with the C++ extension API by adding an element of Python binding support to the C++ API. This means the `torch/torch.h` included by C++ extensions, which currently routes to `torch/csrc/torch.h`, can now be rerouted to `torch/csrc/api/include/torch/torch.h` -- i.e. the main C++ API header. This header then includes Python binding support conditioned on a define (`TORCH_WITH_PYTHON_BINDINGS`), *which is only passed when building a C++ extension*.

Currently stacked on top of https://github.com/pytorch/pytorch/pull/11498

Why is this useful?

1. One less codepath. In particular, there has been trouble again and again due to the two `torch/torch.h` header files and ambiguity when both ended up in the include path. This is now fixed.
2. I have found that it is quite common to want to bind a C++ API module back into Python. This could be for simple experimentation, or to have your training loop in Python but your models in C++. This PR makes this easier by adding pybind11 support to the C++ API.
3. The C++ extension API simply becomes richer by gaining access to the C++ API headers.

soumith ezyang apaszke
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11510

Reviewed By: ezyang

Differential Revision: D9998835

Pulled By: goldsborough

fbshipit-source-id: 7a94b44a9d7e0377b7f1cfc99ba2060874d51535",326.0,82.0,"cmake/TorchConfig.cmake.in,setup.py,test/cpp_extensions/complex_registration_extension.cpp,test/cpp_extensions/cpp_api_extension.cpp,test/cpp_extensions/cuda_extension.cpp,test/cpp_extensions/cudnn_extension.cpp,test/cpp_extensions/doubler.h,test/cpp_extensions/extension.cpp,test/cpp_extensions/half_support.cu,test/cpp_extensions/jit_extension.cpp,test/cpp_extensions/jit_extension2.cpp,test/test_cpp_extensions.py,torch/CMakeLists.txt,torch/csrc/api/include/torch/nn/modules/dropout.h,torch/csrc/api/include/torch/nn/modules/sequential.h,torch/csrc/api/include/torch/nn/pimpl-inl.h,torch/csrc/api/include/torch/nn/pimpl.h,torch/csrc/api/include/torch/python.h,torch/csrc/api/include/torch/torch.h,torch/csrc/tensor/python_tensor.cpp,torch/csrc/torch.h,torch/extension.h,torch/utils/cpp_extension.py",23.0,12,3,3.408989086,43.0,4498.0,15.0,4419367.1,4290.0,12323.83333,0.0,Corrective,1.0,1
pytorch,840680bbf33049414f76023cf60d98001b40c425,e0bd7cc821c883e25fd4e16e7689d887aba31b4a,Lu Fang,lufang@fb.com,Fri May 03 18:03:58 2019 -0700,1556906638.0,"Change the export of _dim_arange in ONNX (#20078)

Summary:
Previously using ATen op, now fully switched to pure ONNX/Caffe2 ops.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20078

Reviewed By: zrphercule

Differential Revision: D15188774

Pulled By: houseroad

fbshipit-source-id: 8ae3094369497e2f3ebf478cda222b73de2a995e",11.0,1.0,"test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic.py",2.0,4,2,0.918295834,12.0,3450.0,2.0,163334.5,8457.0,25151.83333,0.0,,0.0,1
pytorch,5b4d110f51937908d9b78ad91dca288247fc3d56,e0c1786587d350c3ab00530734549b6bf7773e06,shubhambhokare1,shubhambhokare@gmail.com,Thu Apr 21 20:35:25 2022 +0000,1650573325.0,"[onnx] Add support for torch.cross and torch.cdist

Add support for following operators:
- torch.cross
- torch.linalg.cross
- torch.cdist
- torch.nn.pairwisedistance
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75278
Approved by: https://github.com/BowenBao",104.0,10.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,test/onnx/test_utility_funs.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",4.0,4,2,1.835354916,4.0,17328.0,3.0,411121.75,2507.0,5953.0,0.0,Feature Addition,0.0,1
pytorch,3a03af2f505494582f1359254431f0c47fd88705,e0d829a266e55d61b7f0e1347d410c3d3c556f7a,Mike Ruberry,mruberry@devfair044.h1.fair,Mon Jan 24 09:28:07 2022 -0800,1643016487.0,"Kill the test_torch.py mixin and creates test_scatter_gather_ops (#71691)

Summary:
Per title.

Also annotates test_torch.py with additional cleanup tasks and adds empty sample inputs to elementwise unary and binary OpInfos.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/71691

Reviewed By: ngimel

Differential Revision: D33735126

Pulled By: mruberry

fbshipit-source-id: 8cc097a7581a8b620540c95b2a5889c1165ecf23
(cherry picked from commit 5c6a245a3f9ba7c064fc77c8cd4045f903e73cfd)",3001.0,3222.0,"test/test_cuda.py,test/test_scatter_gather_ops.py,test/test_tensor_creation_ops.py,test/test_torch.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",6.0,4,2,0.46824509,45.0,35934.0,5.0,1847711.2,132.0,299.5,0.0,Feature Addition,0.0,1
pytorch,cbdb694f158b8471d71822873c3ac130203cc218,e0ff6a21d8bf3935f4332fb18b40b48472f9b648,Yukio Siraichi,yukio.siraichi@gmail.com,Sat May 21 08:05:48 2022 +0900,1653120348.0,"Replace `set_output` by `set_output_raw_strided` in the codebase.

Partially fix #69813

This PR replaces every `set_output` call for `set_output_raw_strided`.

There were some instances that called the 2 or 3-argument `set_output`. Those were
correctly translated to be a 5-argument `set_output_raw_strided`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76097

Approved by: https://github.com/ezyang",138.0,134.0,"aten/src/ATen/native/AdaptiveMaxPooling2d.cpp,aten/src/ATen/native/AdaptiveMaxPooling3d.cpp,aten/src/ATen/native/AveragePool2d.cpp,aten/src/ATen/native/AveragePool3d.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/Blas.cpp,aten/src/ATen/native/Cross.cpp,aten/src/ATen/native/DilatedMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool2d.cpp,aten/src/ATen/native/FractionalMaxPool3d.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LossNLL.cpp,aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/Pow.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOpsUtils.h,aten/src/ATen/native/ReflectionPad.cpp,aten/src/ATen/native/ReplicationPadding.cpp,aten/src/ATen/native/SoftMax.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/TriangularOps.cpp,aten/src/ATen/native/UpSampleBicubic2d.cpp,aten/src/ATen/native/UpSampleBilinear2d.cpp,aten/src/ATen/native/UpSampleLinear1d.cpp,aten/src/ATen/native/UpSampleNearest1d.cpp,aten/src/ATen/native/UpSampleNearest2d.cpp,aten/src/ATen/native/UpSampleNearest3d.cpp,aten/src/ATen/native/UpSampleTrilinear3d.cpp,aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp",33.0,5,1,4.595540279,11.0,27972.0,25.0,5111549.606060606,3524.0,8364.5,0.0,Corrective,1.0,1
pytorch,169541871a7a6663cc86c3ab68501a62a5d8c67c,e0ffe72649cef3a1dfd321545194d1b3574975c7,Jongsoo Park,jongsoo@fb.com,Sat Jan 25 02:45:10 2020 -0800,1579920310.0,"[aten] fix shadowing variable warning (#32573)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/32573

Fix the following warning
```
caffe2/aten/src/ATen/ParallelOpenMP.h:36:9: warning: declaration of ânum_threadsâ shadows a previous local [-Wshadow=compatible-local]
     int64_t num_threads = omp_get_num_threads();
         ^~~~~~~~~~~
caffe2/aten/src/ATen/ParallelOpenMP.h:29:9: note: shadowed declaration is here
   int64_t num_threads = omp_in_parallel() ? 1 : omp_get_max_threads();
         ^~~~~~~~~~~
```

Test Plan: CI

Reviewed By: ilia-cher

Differential Revision: D19552578

fbshipit-source-id: b8388de1aaa2bb7676b777c93b8ba9c25f5a3d51",1.0,2.0,aten/src/ATen/ParallelOpenMP.h,1.0,3,1,0,1.0,100.0,1.0,9749788.0,14392.0,39092.33333,0.0,Corrective,1.0,1
pytorch,05853945a4c918dcc05139263a4f50c337b50332,e1148db7f257c9cec1bd85b725aa647557bf2178,Thomas Viehmann,tv.github@beamnet.de,Tue May 15 02:08:14 2018 +0200,1526350094.0,"Implement logsumexp (fixes #2591) (#7254)

* Implement logsumexp (fixes #2591)

* Add logsumexp_backward, fix _out declaration.

Thank you Simon and Edward for your comments!",82.0,0.0,"aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,tools/autograd/templates/Functions.cpp,torch/_tensor_docs.py,torch/_torch_docs.py",8.0,9,4,2.518974609,40.0,22379.0,5.0,319919.75,654.0,3631.0,0.0,Corrective,1.0,1
pytorch,596a33585146ce8c73b14da0c815624c328e2e71,e11d2b9c9c1cc510350086dd6055fca0ced31dd8,Richard Zou,zou3519@users.noreply.github.com,Fri Nov 03 11:59:05 2017 -0400,1509710345.0,"Better error messages for Aten tensor types (#3449)

* Better error messages for Aten tensor types

* Address comments, add unit test",79.0,1.0,"test/test_torch.py,torch/csrc/Exceptions.cpp,torch/csrc/Exceptions.h",3.0,3,2,1.044795138,39.0,4829.0,2.0,497795.6666666667,2064.0,23984.85823,0.0,Feature Addition,0.0,1
pytorch,8262920b72374b1d9643f35057663ab02ab20330,e1ca7229887cd17ce2199ac42184d3c6a23e9691,yunjey,yunjey47@naver.com,Tue Aug 01 08:57:46 2017 +0900,1501577866.0,"Add comments for default value (#2248)

Added comments for default value in nn.functional",70.0,57.0,torch/nn/functional.py,1.0,2,1,0,24.0,1182.0,1.0,462530.0,1286.0,17173.02883,0.0,Feature Addition,0.0,1
pytorch,6e2da426f00688513559b94526f939b514910277,e1dbd9a288f6d9c487c537349aaf8d5b687486a8,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Tue Nov 29 17:55:43 2022 +0000,1669744543.0,"Revert ""[GHA] Decrease Windows test timeout to 120 minutes (#89694)""

This reverts commit faa032c5e58502de6ea461e531109d2acc22e56a.

Reverted https://github.com/pytorch/pytorch/pull/89694 on behalf of https://github.com/clee2000 due to broke periodic b/c they take ~2.5 hrs, also broke mem leak check b/c its slow, should probably look into having this be a parameter",1.0,1.0,.github/workflows/_win-test.yml,1.0,2,1,0,1.0,202.0,1.0,88250.0,9975.0,22962.0,0.0,,0.0,1
pytorch,dcc6aed52c3347f1146b2fe92022789195130707,e2011b29aa5fc02bd70146ea059dfc53c83341f7,lezcano,lezcano-93@hotmail.com,Thu Jan 27 23:07:54 2022 -0800,1643324874.0,"Add OpInfo test to check that floating point inputs in OpInfos have requires_grad set to True (#69909)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69909

This test detected a number of sampling methods that were not generating
the samples as expected, e.g. `index_put`, `cosine_embedding`, `stft`, but
perhaps most notably the generator for `BinOps`.

It also detected that `reminder` and `fmod` did not have implemented the
backward formula for the second input. I added this in the previous PR.

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D33774422

Pulled By: mruberry

fbshipit-source-id: 76cfc75b1fdfd72ee64aa524665f83a75fe52509
(cherry picked from commit 13ea7b436bc6301be4cf7bb7d559177d895502b3)",122.0,53.0,"test/test_ops.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",3.0,6,3,0.626285948,15.0,19768.0,2.0,5496.333333333333,263.0,538.0,0.0,Feature Addition,0.0,1
pytorch,cac866262558442b1ad49a70854b73fb6c0fe65d,e2361bcc0f406356d0b474fcf7d85409b0e659a5,Samantha Andow,samdow@fb.com,Tue Jul 19 13:58:40 2022 -0400,1658239120.0,"[functorch] Add exhaustive testing of vmap autograd composability (pytorch/functorch#851)

* refactor to make simpler based on comments

* cleanup

* more failing tests

* fix test failures

* more test failures

* update xfails",60.0,40.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1339.0,1.0,0.0,1177.0,1581.0,0.0,Corrective,1.0,1
pytorch,eab3f428833ce9e11e2db97e516fe4b78ea2e4dc,e23cbd633f03af6ef311361b885d5d8e4a9a0f17,kshitij12345,kshitijkalambarkar@gmail.com,Tue Apr 19 16:33:18 2022 +0000,1650385998.0,"[complex32] jiterator support

Reference #74537

Support for jiterating with `c10::complex<Half>`. Note that computation will take place in `complex<float>` by allowing implicit casting in JITerated code (similar to Half and BFloat16 which upcast to float for computation).

We add `complex32` support for `sigmoid` and `sigmoid_backward` in this PR. This is tested with `test_ops.py::test_dtypes and test_ops.py::test_complex_half_reference_testing`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75656
Approved by: https://github.com/ngimel",151.0,24.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/OpMathType.h,aten/src/ATen/cuda/llvm_complex.cpp,aten/src/ATen/cuda/llvm_jit_strings.h,aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu,aten/src/ATen/native/cuda/CUDAJitLoops.cuh,aten/src/ATen/native/cuda/JitLoops.cuh,aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu,aten/src/ATen/native/cuda/jit_utils.cpp,aten/src/ATen/native/cuda/jit_utils.h,test/test_ops.py,torch/testing/_internal/common_methods_invocations.py",12.0,10,3,2.827390169,10.0,22284.0,11.0,2014382.6666666667,2417.0,5694.5,0.0,Feature Addition,0.0,1
pytorch,ae9789fccc442e5baf8b0d1a37be73de8dc9966c,e2458bce9720c621cf1dfddad8511d99ab1ce1d2,Adam Paszke,adam.paszke@gmail.com,Thu Oct 27 20:31:36 2016 +0200,1477600296.0,Add Parameter class to nn,59.0,15.0,"test/test_nn.py,torch/csrc/autograd/variable.cpp,torch/nn/__init__.py,torch/nn/modules/container.py,torch/nn/modules/module.py,torch/nn/modules/rnn.py,torch/nn/parameter.py",7.0,6,2,2.044265543,10.0,2062.0,4.0,440275.5,52.0,64.15263348,0.0,Feature Addition,0.0,1
pytorch,08044527093bf056d65af16c87223486f4cd8a21,e259894e83f599b3b3c0170252dbbf0dc7fb0b11,Hong Xu,hong@topbug.net,Fri Jun 28 06:51:32 2019 -0700,1561704692.0,"Test raising TypeError in torch.from_numpy() (#21607)

Summary:
With some additional cleanup.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21607

Differential Revision: D16046063

Pulled By: li-roy

fbshipit-source-id: 15256a0e94afea39db3cb581c546c2a18a8a7fda",15.0,6.0,"test/test_torch.py,torch/_torch_docs.py,torch/csrc/utils/tensor_numpy.cpp",3.0,4,2,1.409975019,41.0,19322.0,3.0,136105.66666666666,9683.0,28152.83333,0.0,Feature Addition,0.0,1
pytorch,ba08cf336def66d7982c1cc54a0c7043e0c50b0c,e268fc97c3c931cfc303d044969699d50d8383e0,Brennan Vincent,btv@fb.com,Wed Jun 05 00:35:10 2019 -0700,1559694910.0,"Re-add Tensor.T (#21175)

Summary:
Something flaky is going on with `test_inplace_view_saved_output` on Windows.

With my PR #20598 applied, the test fails, even though there is no obvious reason it should be related, so the PR was reverted.

Based on commenting out various parts of my change and re-building, I think the problem is with the name -- renaming everything from `T` to `asdf` seems to make the test stop failing. I can't be sure that this is actually the case though, since I could just be seeing patterns in non-deterministic build output...

I spoke with colesbury offline and we agreed that it is okay to just disable this test on Windows for now and not block landing the main change. He will look into why it is failing.

**Test Plan:** I will wait to make sure the Windows CI suite passes before landing this.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21175

Differential Revision: D15566970

Pulled By: umanwizard

fbshipit-source-id: edf223375d41faaab0a3a14dca50841f08030da3",56.0,0.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,test/test_torch.py,tools/autograd/gen_autograd.py,tools/autograd/gen_python_functions.py,torch/_tensor_docs.py,torch/csrc/autograd/python_variable.cpp",11.0,13,5,2.918488454,42.0,26271.0,4.0,373682.5454545455,9151.0,26747.83333,0.0,Feature Addition,0.0,1
pytorch,a08e8dd70c55e9b62c917227a314384b16403c9e,e26c1726cf9c17bc09d7bc5bb4d4a0111b84ee1d,Bowen Bao,bowbao@microsoft.com,Mon Nov 09 19:40:55 2020 -0800,1604950855.0,"[ONNX] Fix scripting rand/randn/where (#45793)

Summary:
- rand/randn: the type signature of int[] is different in scripting, thus failing the check.
- where: scripting produces dynamic cases which are supported by `unbind` export of higher opsets.
- test_list_pass: this test fails when using new scripting api, should be fixed by https://github.com/pytorch/pytorch/issues/45369

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45793

Reviewed By: mrshenli

Differential Revision: D24566096

Pulled By: bzinodev

fbshipit-source-id: 6fe0925c66dee342106d71c9cbc3c95cabe639f7",16.0,9.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py,torch/onnx/utils.py",4.0,4,2,1.810699333,7.0,9549.0,3.0,1660001.75,6581.0,14966.5,0.0,Corrective,1.0,1
pytorch,d1def931666172413cec91792ab17833b11a6ee4,e27740b38ef97875d74dee8f285a79a89c7363bd,Serhat Yilmaz,serhaty@fb.com,Thu Apr 29 22:40:36 2021 -0700,1619736036.0,"[torch] Add backward support for segment reduce (CPU only)

Summary:
This is to setup boiler plate code for backward and CPU implementation.

Next Steps in order:
- Add backward support for CUDA
- Add support for more aggregation types
- Benchmarking (for cuda mainly)/more testing/documentation
- Support for multi dimension

Test Plan:
Updated unit test to also check correctness of backward.

Wait for CI signal

Reviewed By: ngimel

Differential Revision: D27970340

fbshipit-source-id: 3e608c7fe3628b0a761dd8affc6aad8f65a6ef7f",142.0,10.0,"aten/src/ATen/native/SegmentReduce.cpp,aten/src/ATen/native/SegmentReduce.h,aten/src/ATen/native/native_functions.yaml,test/test_segment_reductions.py,tools/autograd/derivatives.yaml",5.0,7,3,1.397813199,16.0,11195.0,4.0,99259.8,11472.0,25942.0,0.0,Corrective,0.0,1
pytorch,079cd4e1fc1c7c8fed6659945eb00f04bc607c4f,e293c4ea73dad74791bc38282b3dc5308cc59e8e,Geovanni Zhang,850734033@qq.com,Fri Sep 13 16:26:02 2019 -0700,1568391962.0,"Fix 'in' return true incorrectly (#24156)

Summary:
Because of 'return NotImplemented', __contains__ return True when the element is not a number.
bool(NotImplemented) == True
Pull Request resolved: https://github.com/pytorch/pytorch/pull/24156

Differential Revision: D16829895

Pulled By: zou3519

fbshipit-source-id: 9d3d58025b2b78b33a26fdfcfa6029d0d049f11f",14.0,1.0,"test/test_torch.py,torch/tensor.py",2.0,2,2,0.970950594,40.0,13829.0,1.0,38054.0,11387.0,32014.33333,0.0,Corrective,1.0,1
pytorch,ff533b1efa26ed0dc5e3caa332de05f53963e360,e2e71c1f4c924c5e9e02b25eb66296a697f4b3e7,samdow,samdow@fb.com,Thu Aug 18 16:28:41 2022 -0400,1660840121.0,"[functorch] add linalg solve batch rule (#82814)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82814
Approved by: https://github.com/zou3519",107.0,10.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,functorch/functorch/csrc/BatchRulesDecompositions.cpp,functorch/functorch/csrc/BatchRulesLinearAlgebra.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",6.0,8,2,1.787026002,4.0,11471.0,6.0,785460.1666666666,6569.0,15209.5,0.0,Feature Addition,0.0,1
pytorch,30d06218cbb3fd787b15a2194c2cd3fb0e1edf46,e33df2b88a31c08567c81f49c45f1eb530cd7ef4,josecabjim,caballero.jimenez.jose@gmail.com,Sun Nov 12 23:46:49 2017 +0000,1510530409.0,"Add border-padding for grid_sampler (#3599)

* adds border padding to spatial grid sampler

* fixes flake8 * adds docs",320.0,194.0,"aten/src/THCUNN/SpatialGridSamplerBilinear.cu,aten/src/THCUNN/generic/SpatialGridSamplerBilinear.cu,aten/src/THCUNN/generic/THCUNN.h,aten/src/THNN/generic/SpatialGridSamplerBilinear.c,aten/src/THNN/generic/THNN.h,test/test_nn.py,torch/nn/_functions/vision.py,torch/nn/functional.py",8.0,10,3,2.023547074,37.0,10368.0,5.0,711906.0,2115.0,24069.35823,0.0,Corrective,1.0,1
pytorch,e142d703830defec14602f49f6872d4194f30eaa,e358adb42c2ec57b1243bfa841f13b73ab4997d2,Luca Wehrstedt,lcw@fb.com,Tue Jun 02 16:34:41 2020 -0700,1591115681.0,"[TensorPipe] Acquire lock when adding message to timeout map (#39398)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39398

The `timeoutMapMutex_` was only used to guard accesses in the timeout thread, but it should have been used also to guard accesses in the `send` method.

The way I found this bug is rather odd. A test was failing because a timeout of 0.5 seconds was firing when it wasn't supposed to. The test was built with TSAN enabled and the point where we were wasting those 500ms was precisely when accessing the `timeoutMap_` in the `send` method. There is of course no reason it would take so long, so I suspect that either such an access triggered a whole lot of lengthy checks in TSAN or, perhaps, that TSAN was delaying it on purpose because it thought it was smelly and wanted to see whether it could cause a race.
ghstack-source-id: 105088618

Test Plan: The test started passing.

Differential Revision: D21838465

fbshipit-source-id: 02cf2bf1fef2e97da99b9c4e77070fe35d2bcbb0",7.0,5.0,torch/csrc/distributed/rpc/tensorpipe_agent.cpp,1.0,4,1,0,1.0,777.0,1.0,84.0,2507.0,6219.5,0.0,Corrective,1.0,1
pytorch,21d203b5cac9272b2f6d73113cfa5647eb0c5ca3,e358c49a5b093352940979c4a92b8c88ef1dd98f,soulitzer,soulitzer@gmail.com,Fri Nov 19 22:24:01 2021 -0800,1637360641.0,"Add OpInfo test and fix a couple cases (#66294)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66294

In this PR:
- OpInfo for forward AD now checks batched forward grad when `op.check_batched_grad=True`
- Adds setting to disable the test for individual ops `check_batched_forward_grad` and disable for the ops here: https://github.com/pytorch/pytorch/issues/66357

Fixes some more failures:
- Make Forward AD metadata less strict by allowing stride to differ when size is 1
- Fix sum batching rule when logical tensor is a scalar and dim is unspecified
- Batching rule for `_reshape_alias`
- ~Batching rules now preserve storage offset for view operator that return non-zero storage offset~ (moved to previous PR)

Test Plan: Imported from OSS

Reviewed By: zou3519, albanD

Differential Revision: D31842020

Pulled By: soulitzer

fbshipit-source-id: 3517a8fb9d6291fccb53c0b1631eab5bbb24ebd1",102.0,22.0,"aten/src/ATen/BatchingRegistrations.cpp,test/test_autograd.py,test/test_ops.py,test/test_vmap.py,torch/autograd/gradcheck.py,torch/csrc/autograd/autograd_meta.cpp,torch/testing/_internal/common_methods_invocations.py",7.0,10,3,1.825790524,42.0,29272.0,6.0,198896.57142857145,17227.0,40560.0,0.0,Corrective,1.0,1
pytorch,c2afd590ae716021bb1747a47756dc6aa356f50d,e37f02469d448c8f6ae4c1fa81cb94f7fa0e88a1,gchanan,gregchanan@gmail.com,Tue Jan 23 16:49:15 2018 -0500,1516726155.0,"Favor Variables over Tensors for scalar constructors in torch.distribâ¦ (#4791)

* Favor Variables over Tensors for scalar constructors in torch.distributions.

Current behvior:
1) distribution constructors containing only python number elements will have their python numbers upcasted to Tensors.
2) Python number arguments of distribution constructors that also contain tensors and variables will be upcasted
to the first tensor/variable type.

This PR changes the above to favor Variables as follows:
1) The python numbers will now be upcasted to Variables
2) An error will be raised if the first tensor/variable type is not a Variable.

This is done in preparation for the introduction of Scalars (0-dimensional tensors), which are only available on the Variable API.
Note that we are (separately) merging Variable and Tensor, so this PR should have no real long-term effect.

Also note that the above means we don't change the behavior of constructors without python number arguments.

* Fix tests that require numpy.",117.0,113.0,"test/test_distributions.py,torch/distributions/beta.py,torch/distributions/utils.py",3.0,3,2,0.412731186,8.0,2269.0,3.0,458427.6666666667,460.0,1402.905869,0.0,Corrective,1.0,1
pytorch,c20b0080c65887133e5a9f3f7406d2f3d0fbff53,e39991e8387ccd028627fe6031a33ea94f5aeecc,Hongyi Jia,jiayisuse@fb.com,Wed May 13 04:05:33 2020 -0700,1589342733.0,"[TensorPipe Agent] Bind default IP address (#37910)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37910

To resolve the issue: https://github.com/pytorch/pytorch/issues/36715

In tensorpipe rpc agent, we currently hardcoded localhost as pipes handshake IP address. This prevents us from setting up cross-host connections. As the first step, we start binding IP address for a given network device. For now it's defaulted to eth0. Will provide options to let user configure

Test Plan: CI

Reviewed By: lw

Differential Revision: D21421094

fbshipit-source-id: 60f612cbaeddcef7bd285136ad75af20709a7d56",73.0,2.0,"torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h",2.0,4,1,0.353359335,1.0,723.0,2.0,21623.0,1998.0,5113.5,0.0,Feature Addition,0.0,1
pytorch,03a05e791aabe4f2e3938626bff849641dc101ea,e3ac61587aa368c613ef01df1f328a396b64cd5d,Yuanhao Ji,jiyuanhao@apache.org,Mon Apr 15 06:21:52 2024 +0000,1713162112.0,"Enable UFMT on `test/functorch` (#123541)

Partially addresses #123062

Ran lintrunner on:

- `test/functorch`

Co-authored-by: Edward Z. Yang <ezyang@meta.com>
Pull Request resolved: https://github.com/pytorch/pytorch/pull/123541
Approved by: https://github.com/zou3519, https://github.com/ezyang",7779.0,4228.0,".lintrunner.toml,test/functorch/attn_ft.py,test/functorch/attn_positional.py,test/functorch/common_utils.py,test/functorch/discover_coverage.py,test/functorch/functorch_additional_op_db.py,test/functorch/test_aotdispatch.py,test/functorch/test_control_flow.py,test/functorch/test_dims.py,test/functorch/test_eager_transforms.py,test/functorch/test_logging.py,test/functorch/test_memory_efficient_fusion.py,test/functorch/test_minifier.py,test/functorch/test_ops.py,test/functorch/test_parsing.py,test/functorch/test_rearrange.py,test/functorch/test_vmap.py,test/functorch/test_vmap_registrations.py,test/functorch/xfail_suggester.py",19.0,2,1,3.064830311,3.0,26502.0,17.0,13165074.05263158,27403.0,64898.0,0.0,Feature Addition,0.0,1
pytorch,8195a0aaa7305a4e0d9ab6b832086c6061c2c4ab,e3ca7346ce37d756903c06e69850bdff135b6009,Xinya Zhang,Xinya.Zhang@amd.com,Thu Jan 04 22:21:31 2024 +0000,1704406891.0,"Re-add initial Flash Attention support on ROCM (#115981)

Note about the Updates:

This PR:
1. skips more flash attention related UTs on MI200
2. Fix additional ATen compiling errors after hipification
3. Fix the author ""root"" of a specific commit
4. Includes the patch from Nikita in favor of block level static initialization.

CAVEAT: This revised PR has a commit that modifies the CI to force its running on MI200 nodes. That specific commit must be reverted before merge.

Original PR (https://github.com/pytorch/pytorch/pull/114309) Note:

This pull requests add initial Flash Attention support for AMD/ROCM platform. It added a specialized Triton repository/branch as a compile-time dependency for Flash Attention math library on AMD/ROCM. This triton submodule is not used at runtime and will not be shipped to the final pytorch package. We have the plan to release this specialized Triton as a separate project.

Know limitations:

- Only supports MI200 series GPU (i.e., `gcnArchName == gfx90a:sramecc+:xnack-`.
- Only supports power of two sequence lengths.
- No support for varlen APIs.
- Only support head dimension 16,32,64,128.
- Performance is still being optimized.

Fixes #112997

Pull Request resolved: https://github.com/pytorch/pytorch/pull/115981
Approved by: https://github.com/malfet",885.0,38.0,"CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/native/transformers/attention.cpp,aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip,caffe2/CMakeLists.txt,cmake/Dependencies.cmake,cmake/External/oort.cmake,cmake/Summary.cmake,test/dynamo/test_ctx_manager.py,test/functorch/test_ops.py,test/inductor/test_torchinductor.py,test/test_flop_counter.py,test/test_native_mha.py,test/test_nn.py,test/test_transformers.py,tools/amd_build/build_amd.py,torch/testing/_internal/common_cuda.py,torch/testing/_internal/common_methods_invocations.py,torch/utils/hipify/cuda_to_hip_mappings.py",21.0,23,6,1.95380909,84.0,69855.0,14.0,1313340.142857143,23631.0,53636.5,0.0,Corrective,1.0,1
pytorch,581e846d9def4af9682ba84b215a0545410a551b,e3d0a3ca8850dc1ffc9fd0f740206523125c58b5,PyTorch MergeBot,pytorchmergebot@users.noreply.github.com,Wed Jun 22 19:30:02 2022 +0000,1655926202.0,"Revert ""More forward AD formulas""

This reverts commit 6b20ef6b917aaaf332d10d9e65e2854ff79965f0.

Reverted https://github.com/pytorch/pytorch/pull/77975 on behalf of https://github.com/janeyx99 due to I think this is the real culprit of the broken tests in https://hud.pytorch.org/pytorch/pytorch/commit/28a7ee8cec122d6f6c9a9b891b7cc6bb8224c9a7 for the trunk-only slow test job",256.0,202.0,"aten/src/ATen/native/ReduceOps.cpp,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",6.0,11,3,1.703203823,18.0,35484.0,2.0,12299.5,4630.0,10981.0,0.0,,0.0,1
pytorch,2934153f3508ac1710f358677d83de72789bb275,e3da16a99eb4b475a90fe424eaf9156c3f7370f4,Edward Yang,ezyang@fb.com,Fri Mar 22 14:46:50 2019 -0700,1553266010.0,"Add test for #17271 (torch.exp incorrect for 2**31 size tensor) (#18292)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18292
ghimport-source-id: a3e96584db0eef7b6202a1211808f9f6e59dd529

Stack from [ghstack](https://github.com/ezyang/ghstack):
* **#18292 Add test for #17271 (torch.exp incorrect for 2**31 size tensor)**
* #18291 Correctly call superclass setUp in TestCase subclasses.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Differential Revision: D14567642

fbshipit-source-id: c60ee7597a86f5d2c5c0b72cb106f17815950427",9.0,0.0,test/test_torch.py,1.0,1,1,0,40.0,10726.0,1.0,62597.0,7626.0,23194.83333,0.0,Corrective,0.0,1
pytorch,6a604f16cc452424c58ce456da7f46e1619b9e18,e3e15b5d9534f1c10170e169f1423e9298648e86,Tongzhou Wang,SsnL@users.noreply.github.com,Wed May 23 15:03:12 2018 -0400,1527087792.0,"[PyTorch] [gradcheck] change backward() to grad() (#7710)

* Change backward calls to grad to avoid memory leak from #7343; Replace unnecesary create_graph=True with retain_graph=True

* fix gradgradcheck use of make_non_contiguous

* allow non-contguous target

* remove unnecessray .grad.zero_()

* remove contiguous_detach

* fix PReLU double backward always returning ggW as a scalar

* let noncontig gO require grad

* move requires_grad to return",50.0,62.0,"test/common_nn.py,test/test_distributions.py,test/test_legacy_nn.py,tools/autograd/templates/Functions.cpp,torch/autograd/gradcheck.py",5.0,6,3,0.858468777,40.0,7684.0,4.0,2391587.6,1182.0,3205.305292,0.0,Corrective,1.0,1
pytorch,dad02bceb99dc7f214f220ddc0bd8fb5a81392fc,e3e7b76310612f2d372e8340c12c758565e24e8a,Adam Paszke,adam.paszke@gmail.com,Tue Jan 31 21:34:33 2017 +0100,1485898473.0,Rename all normal and log_normal args to std,28.0,28.0,"torch/_tensor_docs.py,torch/_torch_docs.py,torch/autograd/variable.py,torch/csrc/generic/methods/TensorRandom.cwrap",4.0,5,1,1.806357913,22.0,7352.0,3.0,102191.0,446.0,4352.616645,0.0,,0.0,1
pytorch,c2e8b7aafe6f9e718a71475ec153745ddee538b8,e4a3747cd8b613fdc0d0b1f81277f91377c9f0da,Filip Binkiewicz,filip.binkiewicz@gmail.com,Sun Oct 29 19:56:51 2017 +0100,1509307011.0,Add unit tests for casting onto scalars,23.0,0.0,test/test_torch.py,1.0,1,1,0,38.0,4682.0,1.0,60694.0,2031.0,23895.35823,0.0,Feature Addition,0.0,1
pytorch,3dffac91bcdff8a29a6fd1d40b9601ae52dfa929,e4c0bb1809fd9bf9161392bfff7d06092adc224d,Sam Gross,colesbury@gmail.com,Thu Mar 29 22:13:43 2018 -0400,1522361623.0,"Speed up sum over a dimension (#6026)

Perf numbers:
https://gist.github.com/colesbury/9e28dd7b0f27b0b019f68adbd4bd4b88

I've changed the dispatch stub so that it doesn't require every kernel
to be compiled for every instruction set. Kernel implementations are
stored in the stub's table with the REGISTER_DISPATCH macro.

I've also moved vec256 to it's own folder and split up the
specializations before they get too unwieldy.

Change UnaryOpsKernel to use new DisaptchStub

 - Prefer signed integers. Mixing signed and unsigned integers is a
   pain and ATen mostly uses signed integers (int64_t).
 - Use inline lambda instead of struct for UnaryOps
 - Rename partial load overload ""load_partial""",990.0,880.0,"aten/src/ATen/CMakeLists.txt,aten/src/ATen/Parallel.h,aten/src/ATen/cpu/vec256/intrinsics.h,aten/src/ATen/cpu/vec256/vec256.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/CapabilityDispatch.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/ReduceOpsKernel.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h,aten/src/ATen/native/cpu/Vec256.h,test/test_torch.py",17.0,8,2,3.452080707,38.0,7834.0,3.0,129652.81818181818,631.0,2045.905869,0.0,,1.0,1
pytorch,a3bd7b2875a1559cdbea9f6b3d15ac2dfa4655cc,e4eee7c2cf43f4edba7a14687ad59d3ed61d9833,li-roy,8813817+li-roy@users.noreply.github.com,Wed Mar 21 19:40:58 2018 -0400,1521661258.0,"Implement MarginRankingLoss as native function and add reduce=True arg to it (#5346)

* add reduce=True arg to MarginRankingLoss

* make default margin arg match for legacy

* remove accidentally added test

* fix test

* fix native_functions.yaml alphabetical order",72.0,59.0,"aten/src/ATen/native/Loss.cpp,aten/src/ATen/native/native_functions.yaml,test/common_nn.py,test/test_nn.py,torch/legacy/nn/MarginRankingCriterion.py,torch/nn/_functions/loss.py,torch/nn/backends/thnn.py,torch/nn/functional.py,torch/nn/modules/loss.py",9.0,12,3,2.594991874,39.0,11787.0,6.0,706274.3333333334,2492.0,24838.35823,0.0,Corrective,1.0,1
pytorch,d859c3c7cc129dc872871981f28c068c3a4ba494,e519ef53378c15c8d6ba582aba454b2e1ff6169e,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Thu Dec 28 09:56:03 2017 +0500,1514454963.0,Adding torch.expm1() and its inplace function (#4350),149.0,1.0,"aten/doc/Functions.h,aten/doc/Tensor.h,aten/doc/Type.h,aten/src/ATen/Declarations.cwrap,aten/src/TH/THGeneral.c,aten/src/TH/THGeneral.h.in,aten/src/TH/generic/THTensorMath.c,aten/src/TH/generic/THTensorMath.h,aten/src/TH/generic/THVector.h,aten/src/TH/generic/THVectorDefault.c,aten/src/THC/THCGeneral.h.in,aten/src/THC/THCNumerics.cuh,aten/src/THC/generic/THCTensorMathPointwise.cu,aten/src/THC/generic/THCTensorMathPointwise.h,aten/src/THCUNN/THCHalfAutoNumerics.cuh,docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_cuda.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorMath.cwrap,torch/csrc/jit/fusion_compiler.cpp,torch/csrc/jit/interned_strings.h,torch/csrc/jit/passes/graph_fuser.cpp,torch/lib/THD/master_worker/common/Functions.hpp,torch/lib/THD/master_worker/master/generic/THDTensorMath.cpp,torch/lib/THD/master_worker/master/generic/THDTensorMath.h,torch/lib/THD/master_worker/worker/Dispatch.cpp,torch/lib/THD/master_worker/worker/dispatch/TensorMath.cpp",33.0,28,5,3.941527864,40.0,39447.0,22.0,1993815.0909090908,2225.0,24306.85823,0.0,Feature Addition,0.0,1
pytorch,09abaa21899d8f463e174e44a0b77d93213064b4,e53702314783855e3386469d6b754beff6ec4c61,Hugh Perkins,hughperkins@gmail.com,Thu Jul 20 05:53:37 2017 -0400,1500530017.0,add functional embedding (#1987),81.0,0.0,"test/test_nn.py,torch/nn/functional.py",2.0,3,2,0.664159328,32.0,4856.0,1.0,35896.0,1219.0,15029.69861,0.0,Feature Addition,0.0,1
pytorch,dede0bb0659dfa595fea67faadd4a8dcc30350d6,e549ad00467fba138df9540925f3848c6d5d7c1a,Li-Huai (Allan) Lin,qqaatw@gmail.com,Mon Apr 17 08:23:45 2023 +0800,1681719825.0,"Add log_sigmoid_backward forward-AD (#99288)

Fixes #95057
Pull Request resolved: https://github.com/pytorch/pytorch/pull/99288
Approved by: https://github.com/kshitij12345, https://github.com/albanD",5.0,5.0,"test/functorch/test_ops.py,tools/autograd/derivatives.yaml,torch/testing/_internal/common_methods_invocations.py",3.0,7,3,1.370950594,18.0,25599.0,3.0,187505.66666666663,14680.0,33463.0,0.0,Corrective,1.0,1
pytorch,be071d767dc7081bb528f4badd398c24133c9a24,e579ae75b525a084a2032bec7b67d6629e91ef28,Richard Zou,zou3519@users.noreply.github.com,Wed Nov 08 15:02:08 2017 -0500,1510153328.0,"Fix error when default_collate is passed a collection of numpy.str_ (#3404)

* Fix error when default_collate is passed a collection of numpy.str_

* Error if default_collate input is nested nparray containing non-numbers",30.0,4.0,"test/test_dataloader.py,torch/utils/data/dataloader.py",2.0,4,2,0.997502546,35.0,654.0,2.0,1016487.5,2092.0,24031.85823,0.0,Corrective,1.0,1
pytorch,368f5212fa7a45722732e0b75d8288694af3edef,e592a609fdcff4dd5f6443630cfbf5d91425cea6,"Xia, Weiwen",weiwen.xia@intel.com,Fri Apr 26 01:02:55 2024 +0800,1714093375.0,"[Quant][ONEDNN] improve performance of qconv by reducing integration overhead (#123240)

## Description
Framework overhead is found to be big for the onednn qconv op (used for quantization with PT2E X86Inductor backend). This PR reduces the integration overhead by modifying the implementation of qconv.

## performance results
Running quantized Resnet50 on an Intel(R) Xeon(R) Platinum 8490H machine
Before
```
Average latency: 8.378 ms.
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
onednn::qconv2d_pointwise        86.54%       6.954ms        87.42%       7.025ms     132.547us            53
```
After
```
Average latency: 6.255 ms.
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
-------------------------  ------------  ------------  ------------  ------------  ------------  ------------
onednn::qconv2d_pointwise        85.05%       6.381ms        85.98%       6.451ms     121.717us            53
```
Test script:
```python
import torch
import torchvision
import time
import copy
import numpy as np
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.quantize_pt2e import (
    prepare_pt2e,
    convert_pt2e,
)
import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq
from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer

torch._inductor.config.cpp.enable_kernel_profile=True
torch._inductor.config.profiler_mark_wrapper_call = True
torch._inductor.config.freezing = True
torch._inductor.config.cpp_wrapper = True

def bench_model(model, inputs):
    times =[]
    with torch.no_grad():
        for _ in range(5): # warm-up
            output = model(inputs)
        for _ in range(20):
            start_time = time.time()
            output = model(inputs)
            end_time = time.time()
            times.append(end_time - start_time)
        print ('Average latency: %0.3f ms.' % (np.median(times) * 1000.0))

        with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU]) as p:
            out_ipex = model(inputs)
        print(p.key_averages().table(sort_by=""self_cpu_time_total"", row_limit=-1))

def pt2e_ptq(m, example_inputs):

    m = m.eval()

    exported_model = capture_pre_autograd_graph(m, example_inputs)
    quantizer = X86InductorQuantizer()
    quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())
    prepared_model = prepare_pt2e(exported_model, quantizer)

    _ = prepared_model(*example_inputs)

    converted_model = convert_pt2e(prepared_model)
    torch.ao.quantization.move_exported_model_to_eval(converted_model)
    with torch.no_grad():
        optimized_model = torch.compile(converted_model)
        _ = optimized_model(*example_inputs)
        _ = optimized_model(*example_inputs)

    bench_model(optimized_model, *example_inputs)

    return optimized_model

if __name__ == ""__main__"":

    data = torch.randn(16, 3, 224, 224)
    model_fp = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)
    pt2e_ptq(copy.deepcopy(model_fp), (data,))
```

Differential Revision: [D56288440](https://our.internmc.facebook.com/intern/diff/D56288440)
Pull Request resolved: https://github.com/pytorch/pytorch/pull/123240
Approved by: https://github.com/leslie-fang-intel, https://github.com/jgong5, https://github.com/jerryzh168",84.0,35.0,aten/src/ATen/native/quantized/cpu/qconv.cpp,1.0,6,1,0,4.0,1961.0,1.0,20777.0,28000.0,66567.0,0.0,Perfective,0.0,1
pytorch,959b6343b8c8baf35c88c40b461d4b246c37c2db,e59dad9a108a3a7ecd9cdfaefcab575360ffd524,Francisco Massa,fvsmassa@gmail.com,Sun Jan 31 14:32:30 2016 +0100,1454250750.0,"Add THNN conversion for Spatial* modules

Add THNN conversion of SpatialBatchNormalization, SpatialFractionalMaxPooling and SpatialSubSampling

Add THNN convertion of SpatialConvolutionLocal, SpatialFullConvolution and SpatialUpSamplingNearest

THNN conversion of SpatialMaxUnpooling

Remove unfold from generic

Add functional conversion of SpatialCrossMapLRN

Plus fix in the init.c

Fix",432.0,414.0,"generic/SpatialBatchNormalization.c,generic/SpatialConvolutionLocal.c,generic/SpatialFractionalMaxPooling.c,generic/SpatialFullConvolution.c,generic/SpatialMaxUnpooling.c,generic/SpatialSubSampling.c,generic/SpatialUpSamplingNearest.c,generic/THNN.h,init.c",9.0,1,1,2.896709879,6.0,2595.0,2.0,115984.11111111112,5.0,16.83333333,0.0,Corrective,1.0,1
pytorch,2ecc59086a5bc896c82f6f309d25569ec2933467,e5a1a78045e66aad9e763bf69c2455f8136c1eef,George Qi,georgeqi94@gmail.com,Mon Apr 04 23:41:13 2022 +0000,1649115673.0,"masked argmin/argmax

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75212

Approved by: https://github.com/cpuhrsch",221.0,3.0,"torch/_masked/__init__.py,torch/_masked/_docs.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,1,1.394190348,5.0,18162.0,3.0,159626.33333333334,1986.0,4733.0,0.0,,0.0,1
pytorch,41fd51d7d8292a5a4941a33eea7e2ea70467e86a,e5a98c5ab03ba168ccb37edb2039a7da85c71620,BowenBao,bowbao@microsoft.com,Wed Dec 09 19:35:49 2020 -0800,1607542549.0,"[ONNX] Remove usage of isCompleteTensor() in symbolic functions (#48162)

Summary:
`isCompleteTensor()` only returns true when both scalar type and shape is present. All dimensions in the shape must be static. This high requirement is unnecessary for many use cases such as when only rank or scalar type needs to be known.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48162

Reviewed By: malfet

Differential Revision: D25340823

Pulled By: bzinodev

fbshipit-source-id: 1fef61f44918f4339dd6654fb725b18cd58d99cf",262.0,132.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/csrc/jit/python/python_ir.cpp,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",7.0,7,2,1.849122033,3.0,11154.0,5.0,1110702.5714285714,7326.0,16484.5,0.0,,0.0,1
pytorch,18ec4632b335521c8497ca2760a386c6d0d10a64,e5b947a3a807d5359f762743d467799ba52ee8a2,Brian Vaughan,bvaughan@fb.com,Tue Dec 03 14:33:22 2019 -0800,1575383602.0,"Raise an error for is_signed on quantized types (#30527)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30527

When we introduced dtype.is_signed we allowed for support of
quantized types, but we're not sure what the correct result should be.

See discussion at https://github.com/pytorch/pytorch/pull/29511

Test Plan: Imported from OSS

Differential Revision: D18765410

Pulled By: nairbv

fbshipit-source-id: c87cfe999b604cfcbbafa561e04d0d5cdbf41e6d",9.0,4.0,"c10/core/ScalarType.h,test/test_torch.py,torch/csrc/Dtype.cpp",3.0,5,3,1.52623491,40.0,15283.0,2.0,1042814.6666666666,13565.0,37157.83333,0.0,Corrective,0.0,1
pytorch,1da1707568604111b4d5b603d7c0f5b62579284d,e5e0c19882317530d39ca0237851afb4a767d6f9,Kshiteej K,kshitijkalambarkar@gmail.com,Wed Dec 01 14:59:03 2021 -0800,1638370743.0,"OpInfo : embedding_bag (#67252)

Summary:
Adds OpInfo for `embedding_bag`.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67252

Reviewed By: VitalyFedyunin

Differential Revision: D32462157

Pulled By: zou3519

fbshipit-source-id: 70303349a718720c4fa47519fa94ae900e052939",158.0,0.0,"test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.055328485,2.0,15744.0,2.0,290487.5,17411.0,40902.5,0.0,Feature Addition,0.0,1
pytorch,b8e4423278bd52ac7a8d4c5f9221bb6fc67c75ac,e5f46a1d359a91b5b80a749ab8c01450ec6d2961,Adnan Akhundov,aakhundov@meta.com,Sun Feb 11 17:13:47 2024 -0800,1707671627.0,"Check alignment of ReinterpretView args of custom Triton kernels (#119649)

Summary: Currently, when a custom (user-written) Triton kernel has a ReinterpretView argument in IR, we're always skipping the alignment checking for this argument when preparing the `signature_of` for the AOT compilation of the Triton kernel (via setting `TensorArg.check_alignment` to `False`). This is problematic for user-written kernels where, albeit reinterpreted, the argument of the Triton kernel (the data pointer) can still be aligned to 16. When we skip alignment checking, the performance of the AOT-compiled internal Triton kernels can degrade 2x--3x.

In this PR, we replace `TensorArg.check_alignment` by `TensorArg.offset`, in which we specify the offset of the `ReinterpretView.layout` relative to the underlying `ir.Buffer` (corresponding to the data pointer before reinterpretation). As the size and stride of the layout don't change the alignment properties, those can be skipped. Importantly, for `ReinterpretView` arguments of custom Triton kernels, we use `arg.data.get_name()` as the buffer name. That, together with the offset, is used to check the alignment.

Bonus: the namedtuples in `codegen/common.py` are refactored as `dataclass`es, with nicer type hints and default values (for the newly added `TensorArg.offset`).

Test Plan:

```
$ python test/inductor/test_aot_inductor.py -k test_triton_kernel_reinterpret_view
...
----------------------------------------------------------------------
Ran 6 tests in 27.952s

OK (skipped=4)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/119649
Approved by: https://github.com/oulgen",74.0,31.0,"test/inductor/test_aot_inductor.py,torch/_inductor/codegen/common.py,torch/_inductor/codegen/triton.py,torch/_inductor/codegen/triton_utils.py,torch/_inductor/codegen/wrapper.py",5.0,5,2,2.151289491,4.0,11115.0,4.0,134584.8,25042.0,56560.0,0.0,Feature Addition,0.0,1
pytorch,bd2b450b15668017632bf3be1f0375f91e02d902,e5fab4d890af6ee88d5024c2f26af771d37d12d3,Richard Zou,zou3519@users.noreply.github.com,Thu Mar 10 02:12:26 2022 -0500,1646878346.0,"[functorch] Fix unbatched tensor with different randomness behavior for dropout variants (pytorch/functorch#583)

To do this, we override the composite dropout operations to use an
implementation that does not have in-place operations.

Test Plan:
- run existing tests",297.0,69.0,"functorch/functorch/csrc/BatchRulesRandomness.cpp,functorch/functorch/csrc/PyTorchOperatorHacks.cpp,functorch/test/test_vmap.py",3.0,4,1,1.383031061,1.0,4409.0,2.0,0.6666666666666666,871.0,1220.5,0.0,Corrective,1.0,1
pytorch,3e121d968830dd2522da54f66008cf4b4753a8e8,e62bf8927390fb5a9b46e1ac4f85f1c6b4c303b3,Paul Shao,pshao@fb.com,Mon Jul 27 18:15:01 2020 -0700,1595873701.0,"Renaming variables from dX to dY in Learnable Fake Quantize kernels for Better Clarity (#42032)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42032

In this diff, the arguments `dX` within the C++ kernels are named as `dY` for clarity and avoid confusion since it doesn't represent the gradient with respect to the input.

Test Plan:
To test all related fake quantize kernel operators, on a devvm, run the command:

`buck test //caffe2/test:quantization -- learnable`

Reviewed By: z-a-f, jerryzh168

Differential Revision: D22735429

fbshipit-source-id: 9d6d967f08b98a720eca39a4d2280ca8109dcdd6",29.0,29.0,"aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu,aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp",3.0,8,1,1.490783047,2.0,3187.0,2.0,968566.3333333334,3843.0,9066.0,0.0,Preventative,0.0,1
pytorch,eeacb6ae04fc86f4dab7eb18fdf51ee592ceb8eb,e670c261c566a015c30e8c7e9c929f4bc504b267,Peter Bell,peterbell10@live.co.uk,Tue Dec 20 19:19:02 2022 +0000,1671563942.0,"Decompose fill, zero, and zeros_like (#90968)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90968
Approved by: https://github.com/ngimel",29.0,19.0,"test/functorch/test_ops.py,test/functorch/test_vmap.py,torch/_decomp/decompositions.py,torch/_inductor/decomposition.py,torch/testing/_internal/common_methods_invocations.py",5.0,7,2,1.72838011,7.0,29073.0,5.0,179953.8,10783.0,24584.0,0.0,,0.0,1
pytorch,00eed6f36759323c70c6e05ccb1561367921eee0,e68b3ad14f9598334a97c1aff15408e1d8f09002,Shunting Zhang,shunting@fb.com,Mon Aug 28 23:08:07 2023 -0700,1693264087.0,"update triton pin with needed inductor change (#107722)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/107722
Approved by: https://github.com/jansel, https://github.com/cpuhrsch",56.0,15.0,"test/test_sparse_csr.py,torch/_inductor/codegen/triton_utils.py,torch/_inductor/kernel/mm.py,torch/_inductor/kernel/mm_common.py,torch/_inductor/triton_heuristics.py,torch/_inductor/utils.py",6.0,5,2,2.151985841,3.0,6274.0,6.0,555060.8333333334,19151.0,43484.5,0.0,,0.0,1
pytorch,207d6ae60dc5c3c8632eedae056080e473e5b59c,e6953000e862c571e5e604b4b894250218fbae23,Adam Paszke,adam.paszke@gmail.com,Mon Aug 15 13:36:45 2016 -0700,1471268205.0,Add tests for copy and pickle + make CUDA optional in legacy nn tests,49.0,30.0,"test/test_legacy_nn.py,test/test_torch.py",2.0,1,1,0.614571561,3.0,3706.0,2.0,204018.5,109.0,1772.931349,0.0,Feature Addition,0.0,1
pytorch,2299d6a0139263ef28c2a658adf95fd842f3dc92,e6e8745bea8f7e32ff63a13559c710233935cb09,Hui Guo,huiguo@fb.com,Mon Jul 26 06:29:06 2021 -0700,1627280946.0,"[nnc] Add simplifierUnderContext for simplification that needs context info: currently added for-stmt index var bounds info as context (#60687)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/60687

Test Plan: Imported from OSS

Reviewed By: navahgar

Differential Revision: D29373315

Pulled By: huiguoo

fbshipit-source-id: 8729af60dd6d9735187b2118e3e83c75ef21789d",126.0,0.0,"torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",2.0,4,1,0.650022422,1.0,3059.0,1.0,195092.0,14122.0,32294.0,0.0,Feature Addition,0.0,1
pytorch,4af40e347179629171e4c57d648d5effffb2c995,e708de37ccbd15f248d26991443abbd40c8cde48,Adam Paszke,adam.paszke@gmail.com,Sat Jul 15 18:25:56 2017 -0700,1500143156.0,Allow keyword args in long_arg options,12.0,9.0,"test/test_torch.py,tools/cwrap/plugins/KwargsPlugin.py,tools/cwrap/plugins/THPPlugin.py",3.0,4,2,1.35666952,33.0,4926.0,2.0,35780.0,1215.0,15025.69861,0.0,,0.0,1
pytorch,e168dbb90a0c0a6cc6e799386fe410408f6d2cb4,e70ea8d58d8da777c998d820ec3f3c2b5615c8c9,min-jean-cho,min.jean.cho@intel.com,Tue Mar 07 01:19:42 2023 +0000,1678151982.0,"enable taskset core pinning in addition to numactl (#96011)

- port https://github.com/intel-innersource/frameworks.ai.pytorch.ipex-cpu/pull/740 to `run_cpu`
- use-case by https://github.com/pytorch/serve/pull/2166 where `numactl` is unavailable (e.g., requires `privileged` mode)

This PR automatically tries taskset if numactl core binding doesn't work.

Reference:
`taskset` is added to adapt to launcher use-cases such as in docker where `numactl` requires to be ran in  `privileged` mode, where the  `privileged` mode ""wont work for deployments like sagemaker for example"" as raised by TorchServe. Please see [torchserve ipex docker discussion](https://github.com/pytorch/serve/pull/1401#issuecomment-1090817704) for reference. To address such use-cases, `taskset` can be used in place of `numactl` to set core affinity. Note that, unlike `numactl`, `taskset` does not provide memory binding to local memories; however, memory binding may not be needed in these use-cases  that typically do not span multi sockets. Hence we can automatically try taskset if numactl doesn't work.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/96011
Approved by: https://github.com/jgong5, https://github.com/malfet",45.0,7.0,"test/backends/xeon/test_launch.py,torch/backends/xeon/run_cpu.py",2.0,6,2,0.235193382,1.0,742.0,1.0,2178177.0,13079.0,30449.5,0.0,Feature Addition,0.0,1
pytorch,adb4cb2b5b5ae94a1b31e3b803845f04b5ea7a36,e71cf20192a79435805d906279ec3f7f92ba1767,Adam Lerer,adam.lerer@gmail.com,Wed Feb 22 21:24:20 2017 -0500,1487798660.0,improved serialization (no tar copy) (#713),322.0,178.0,"test/common.py,test/test_torch.py,test/test_utils.py,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/serialization.cpp,torch/csrc/generic/serialization.h,torch/multiprocessing/reductions.py,torch/serialization.py,torch/tensor.py",9.0,5,2,2.042243376,24.0,4785.0,2.0,185786.3333333333,480.0,4590.978466,0.0,Perfective,0.0,1
pytorch,a894fff2656cb920ee79cf8cf5c72b7cdb5cdd01,e75fb4356b752097d093c7013ba85c9eb82961ef,David Reiss,dreiss@fb.com,Wed Apr 22 16:20:13 2020 -0700,1587572413.0,"Remove (most) Python 2 support from Python code (#35615)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35615

Python 2 has reached end-of-life and is no longer supported by PyTorch.
Now we can clean up a lot of cruft that we put in place to support it.
These changes were all done manually, and I skipped anything that seemed
like it would take more than a few seconds, so I think it makes sense to
review it manually as well (though using side-by-side view and ignoring
whitespace change might be helpful).

Test Plan: CI

Differential Revision: D20842886

Pulled By: dreiss

fbshipit-source-id: 8cad4e87c45895e7ce3938a88e61157a79504aed",145.0,646.0,"benchmarks/fastrnns/profile.py,caffe2/python/ideep/conv_op_test.py,test/distributed/test_data_parallel.py,test/jit/test_class_type.py,test/jit/test_list_dict.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/run_test.py,test/test_autograd.py,test/test_cpp_extensions_aot.py,test/test_cpp_extensions_jit.py,test/test_cuda.py,test/test_dataloader.py,test/test_jit.py,test/test_jit_legacy.py,test/test_jit_py3.py,test/test_jit_simple.py,test/test_multiprocessing.py,test/test_multiprocessing_spawn.py,test/test_namedtensor.py,test/test_nn.py,test/test_numba_integration.py,test/test_overrides.py,test/test_serialization.py,test/test_sparse.py,test/test_tensorboard.py,test/test_torch.py,test/test_type_hints.py,test/test_utils.py,tools/shared/module_loader.py,torch/_namedtensor_internals.py,torch/distributed/autograd/__init__.py,torch/distributed/rpc/__init__.py,torch/distributed/rpc/api.py,torch/distributed/rpc/internal.py,torch/functional.py,torch/hub.py,torch/jit/__init__.py,torch/jit/_builtins.py,torch/jit/annotations.py,torch/jit/frontend.py,torch/jit/unsupported_tensor_ops.py,torch/multiprocessing/reductions.py,torch/serialization.py,torch/tensor.py,torch/testing/_internal/common_distributed.py,torch/testing/_internal/common_utils.py,torch/testing/_internal/distributed/rpc/dist_autograd_test.py,torch/testing/_internal/distributed/rpc/dist_optimizer_test.py,torch/testing/_internal/distributed/rpc/jit/dist_autograd_test.py,torch/testing/_internal/distributed/rpc/jit/rpc_test.py,torch/testing/_internal/distributed/rpc/rpc_test.py,torch/testing/_internal/jit_metaprogramming_utils.py,torch/testing/_internal/jit_utils.py,torch/utils/collect_env.py,torch/utils/cpp_extension.py,torch/utils/hipify/hipify_python.py",56.0,24,5,4.788007612,49.0,95015.0,44.0,4743781.714285715,1294.0,3456.5,0.0,,0.0,1
pytorch,f1d0d73ed72ed6e7208a5c52aa849dca94772666,e7c1e6a8e39df0d206efe247f5eb0481eb8b8b6c,Luke Yeager,lukeyeager@users.noreply.github.com,Fri Jan 27 20:52:21 2017 -0800,1485550341.0,"[pep8] Fix most lint automatically with autopep8

Here's the command I used to invoke autopep8 (in parallel!):

    git ls-files | grep '\.py$' | xargs -n1 -P`nproc` autopep8 -i

Several rules are ignored in setup.cfg. The goal is to let autopep8
handle everything which it can handle safely, and to disable any rules
which are tricky or controversial to address. We may want to come back
and re-enable some of these rules later, but I'm trying to make this
patch as safe as possible.

Also configures flake8 to match pep8's behavior.

Also configures TravisCI to check the whole project for lint.",3346.0,3028.0,".travis.yml,docs/source/conf.py,setup.cfg,test/common.py,test/common_nn.py,test/data/network1.py,test/data/network2.py,test/error_messages/storage.py,test/optim/test.py,test/test_autograd.py,test/test_cuda.py,test/test_dataloader.py,test/test_legacy_nn.py,test/test_multiprocessing.py,test/test_nn.py,test/test_optim.py,test/test_sparse.py,test/test_torch.py,test/test_utils.py,tools/cwrap/cwrap.py,tools/cwrap/plugins/ArgcountChecker.py,tools/cwrap/plugins/ArgcountSortPlugin.py,tools/cwrap/plugins/ArgumentReferences.py,tools/cwrap/plugins/AutoGPU.py,tools/cwrap/plugins/BeforeAfterCall.py,tools/cwrap/plugins/BoolOption.py,tools/cwrap/plugins/ConstantArguments.py,tools/cwrap/plugins/CuDNNPlugin.py,tools/cwrap/plugins/GILRelease.py,tools/cwrap/plugins/KwargsPlugin.py,tools/cwrap/plugins/NullableArguments.py,tools/cwrap/plugins/OptionalArguments.py,tools/cwrap/plugins/ReturnArguments.py,tools/cwrap/plugins/StandaloneExtension.py,tools/cwrap/plugins/THPPlugin.py,tools/nnwrap/generate_wrappers.py,tools/setup_helpers/env.py,torch/__init__.py,torch/_tensor_docs.py,torch/_tensor_str.py,torch/_thnn/__init__.py,torch/_thnn/utils.py,torch/_torch_docs.py,torch/autograd/__init__.py,torch/autograd/_functions/__init__.py,torch/autograd/_functions/basic_ops.py,torch/autograd/_functions/blas.py,torch/autograd/_functions/linalg.py,torch/autograd/_functions/pointwise.py,torch/autograd/_functions/reduce.py,torch/autograd/_functions/stochastic.py,torch/autograd/_functions/tensor.py,torch/autograd/engine.py,torch/autograd/function.py,torch/autograd/stochastic_function.py,torch/autograd/variable.py,torch/backends/cudnn/__init__.py,torch/backends/cudnn/rnn.py,torch/cuda/__init__.py,torch/cuda/comm.py,torch/cuda/nccl.py,torch/cuda/random.py,torch/cuda/streams.py,torch/functional.py,torch/legacy/nn/Abs.py,torch/legacy/nn/AbsCriterion.py,torch/legacy/nn/Add.py,torch/legacy/nn/AddConstant.py,torch/legacy/nn/BCECriterion.py,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/Bilinear.py,torch/legacy/nn/CAddTable.py,torch/legacy/nn/CDivTable.py,torch/legacy/nn/CMul.py,torch/legacy/nn/CMulTable.py,torch/legacy/nn/CSubTable.py,torch/legacy/nn/Clamp.py,torch/legacy/nn/ClassNLLCriterion.py,torch/legacy/nn/ClassSimplexCriterion.py,torch/legacy/nn/Concat.py,torch/legacy/nn/ConcatTable.py,torch/legacy/nn/Container.py,torch/legacy/nn/Contiguous.py,torch/legacy/nn/Copy.py,torch/legacy/nn/Cosine.py,torch/legacy/nn/CosineDistance.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/Criterion.py,torch/legacy/nn/CriterionTable.py,torch/legacy/nn/CrossEntropyCriterion.py,torch/legacy/nn/DepthConcat.py,torch/legacy/nn/DistKLDivCriterion.py,torch/legacy/nn/DotProduct.py,torch/legacy/nn/Dropout.py,torch/legacy/nn/ELU.py,torch/legacy/nn/Euclidean.py,torch/legacy/nn/Exp.py,torch/legacy/nn/FlattenTable.py,torch/legacy/nn/GradientReversal.py,torch/legacy/nn/HardShrink.py,torch/legacy/nn/HardTanh.py,torch/legacy/nn/HingeEmbeddingCriterion.py,torch/legacy/nn/Identity.py,torch/legacy/nn/Index.py,torch/legacy/nn/JoinTable.py,torch/legacy/nn/L1Cost.py,torch/legacy/nn/L1HingeEmbeddingCriterion.py,torch/legacy/nn/L1Penalty.py,torch/legacy/nn/LeakyReLU.py,torch/legacy/nn/Linear.py,torch/legacy/nn/Log.py,torch/legacy/nn/LogSigmoid.py,torch/legacy/nn/LogSoftMax.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/MM.py,torch/legacy/nn/MSECriterion.py,torch/legacy/nn/MV.py,torch/legacy/nn/MarginCriterion.py,torch/legacy/nn/MarginRankingCriterion.py,torch/legacy/nn/MaskedSelect.py,torch/legacy/nn/Max.py,torch/legacy/nn/Mean.py,torch/legacy/nn/Min.py,torch/legacy/nn/MixtureTable.py,torch/legacy/nn/Module.py,torch/legacy/nn/Mul.py,torch/legacy/nn/MulConstant.py,torch/legacy/nn/MultiCriterion.py,torch/legacy/nn/MultiLabelMarginCriterion.py,torch/legacy/nn/MultiLabelSoftMarginCriterion.py,torch/legacy/nn/MultiMarginCriterion.py,torch/legacy/nn/Narrow.py,torch/legacy/nn/NarrowTable.py,torch/legacy/nn/Normalize.py,torch/legacy/nn/PReLU.py,torch/legacy/nn/Padding.py,torch/legacy/nn/PairwiseDistance.py,torch/legacy/nn/Parallel.py,torch/legacy/nn/ParallelCriterion.py,torch/legacy/nn/ParallelTable.py,torch/legacy/nn/PartialLinear.py,torch/legacy/nn/Power.py,torch/legacy/nn/RReLU.py,torch/legacy/nn/ReLU.py,torch/legacy/nn/ReLU6.py,torch/legacy/nn/Replicate.py,torch/legacy/nn/Reshape.py,torch/legacy/nn/Select.py,torch/legacy/nn/SelectTable.py,torch/legacy/nn/Sequential.py,torch/legacy/nn/Sigmoid.py,torch/legacy/nn/SmoothL1Criterion.py,torch/legacy/nn/SoftMarginCriterion.py,torch/legacy/nn/SoftMax.py,torch/legacy/nn/SoftMin.py,torch/legacy/nn/SoftPlus.py,torch/legacy/nn/SoftShrink.py,torch/legacy/nn/SoftSign.py,torch/legacy/nn/SpatialAdaptiveMaxPooling.py,torch/legacy/nn/SpatialAveragePooling.py,torch/legacy/nn/SpatialBatchNormalization.py,torch/legacy/nn/SpatialClassNLLCriterion.py,torch/legacy/nn/SpatialContrastiveNormalization.py,torch/legacy/nn/SpatialConvolution.py,torch/legacy/nn/SpatialConvolutionLocal.py,torch/legacy/nn/SpatialConvolutionMap.py,torch/legacy/nn/SpatialCrossMapLRN.py,torch/legacy/nn/SpatialDilatedConvolution.py,torch/legacy/nn/SpatialDivisiveNormalization.py,torch/legacy/nn/SpatialDropout.py,torch/legacy/nn/SpatialFractionalMaxPooling.py,torch/legacy/nn/SpatialFullConvolution.py,torch/legacy/nn/SpatialFullConvolutionMap.py,torch/legacy/nn/SpatialLPPooling.py,torch/legacy/nn/SpatialMaxPooling.py,torch/legacy/nn/SpatialMaxUnpooling.py,torch/legacy/nn/SpatialReflectionPadding.py,torch/legacy/nn/SpatialReplicationPadding.py,torch/legacy/nn/SpatialSoftMax.py,torch/legacy/nn/SpatialSubSampling.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/legacy/nn/SpatialUpSamplingNearest.py,torch/legacy/nn/SpatialZeroPadding.py,torch/legacy/nn/SplitTable.py,torch/legacy/nn/Sqrt.py,torch/legacy/nn/Square.py,torch/legacy/nn/Squeeze.py,torch/legacy/nn/Sum.py,torch/legacy/nn/Tanh.py,torch/legacy/nn/TanhShrink.py,torch/legacy/nn/TemporalConvolution.py,torch/legacy/nn/TemporalMaxPooling.py,torch/legacy/nn/TemporalSubSampling.py,torch/legacy/nn/Threshold.py,torch/legacy/nn/Transpose.py,torch/legacy/nn/Unsqueeze.py,torch/legacy/nn/View.py,torch/legacy/nn/VolumetricAveragePooling.py,torch/legacy/nn/VolumetricBatchNormalization.py,torch/legacy/nn/VolumetricConvolution.py,torch/legacy/nn/VolumetricDropout.py,torch/legacy/nn/VolumetricFullConvolution.py,torch/legacy/nn/VolumetricMaxPooling.py,torch/legacy/nn/VolumetricMaxUnpooling.py,torch/legacy/nn/VolumetricReplicationPadding.py,torch/legacy/nn/WeightedEuclidean.py,torch/legacy/nn/WeightedMSECriterion.py,torch/legacy/nn/__init__.py,torch/legacy/nn/utils.py,torch/legacy/optim/adadelta.py,torch/legacy/optim/adagrad.py,torch/legacy/optim/adam.py,torch/legacy/optim/adamax.py,torch/legacy/optim/asgd.py,torch/legacy/optim/cg.py,torch/legacy/optim/lbfgs.py,torch/legacy/optim/nag.py,torch/legacy/optim/rmsprop.py,torch/legacy/optim/rprop.py,torch/legacy/optim/sgd.py,torch/multiprocessing/queue.py,torch/multiprocessing/reductions.py,torch/nn/_functions/activation.py,torch/nn/_functions/batchnorm.py,torch/nn/_functions/conv.py,torch/nn/_functions/dropout.py,torch/nn/_functions/linear.py,torch/nn/_functions/loss.py,torch/nn/_functions/rnn.py,torch/nn/_functions/thnn/activation.py,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/loss.py,torch/nn/_functions/thnn/normalization.py,torch/nn/_functions/thnn/pooling.py,torch/nn/_functions/thnn/sparse.py,torch/nn/_functions/thnn/upsampling.py,torch/nn/backends/__init__.py,torch/nn/backends/backend.py,torch/nn/backends/thnn.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/dropout.py,torch/nn/modules/linear.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/modules/normalization.py,torch/nn/modules/padding.py,torch/nn/modules/pixelshuffle.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/modules/upsampling.py,torch/nn/modules/utils.py,torch/nn/parallel/_functions.py,torch/nn/parallel/parallel_apply.py,torch/optim/adadelta.py,torch/optim/adagrad.py,torch/optim/adam.py,torch/optim/adamax.py,torch/optim/asgd.py,torch/optim/lbfgs.py,torch/optim/optimizer.py,torch/optim/rmsprop.py,torch/optim/rprop.py,torch/serialization.py,torch/sparse/__init__.py,torch/tensor.py,torch/utils/__init__.py,torch/utils/data/dataset.py,torch/utils/ffi/__init__.py,torch/utils/hooks.py,torch/utils/model_zoo.py,torch/utils/serialization/read_lua_file.py,torch/utils/trainer/plugins/__init__.py,torch/utils/trainer/plugins/accuracy.py,torch/utils/trainer/plugins/logger.py,torch/utils/trainer/plugins/loss.py,torch/utils/trainer/plugins/monitor.py,torch/utils/trainer/plugins/plugin.py,torch/utils/trainer/plugins/progress.py,torch/utils/trainer/plugins/time.py,torch/utils/trainer/trainer.py",286.0,36,4,6.055288974,23.0,43950.0,12.0,332457.7132867133,382.0,3806.196975,0.0,Corrective,1.0,1
pytorch,e78e00f4d98c4376e298902db8aae7e7057e86df,e7e1cd945fe218fde228cedbdb1509f1750f70ea,Jane Xu,janeyx@fb.com,Sat Sep 24 03:47:33 2022 +0000,1663991253.0,"Add path optimize kwarg to einsum (#84890)

## This PR seeks to:
- [x] add c++ support for an optimize path
- [x] add python opt_einsum path passthrough
- [x] add opt_einsum to OSS requirements, but a soft one
- [x] show benchmark results here

Additional things I've explored + their conclusions:
- **Delaying the summing over dimensions** => added!
    - The idea here is to not incur kernel calls to `sum` as we try to early sum out in einsum. Thus, we collect all the dimensions that need to be summed together in one contraction + sum at the end instead of summing as we go. While this optimization didn't feel like it made things faster for the random cases we've selected (they all summed 1 dim per contraction), it is a good principle and would help more common use cases that would reduce multiple dimensions at a time (like `bxy,xyi,xyj->bij`).
- **Caching contract_path based on equation and tensor sizes** => dropped :(
    - The benchmarks were strictly worse for all the cases, and, from scanning the use cases, I observed people do not often call einsum on the same equation/tensor order enough for caching to be justified. I do think caching can be effective in the future, but it would require further investigation.

## Not a part of this PR (but are next steps):
- adding opt_einsum package to OSS CI
- adding it to internal CI
- potentially adding a kwarg path argument to the python API -- if the path is given, we wouldn't have to spend time calculating it, but there would be some time lost validating user input.

## Testing:
- Added more tests to CI

## Benchmarking:
**TL;DRs**
- **torch.einsum with opt_einsum is a definite win for the production case**.
- **torch.einsum with opt_einsum installed is consistently fast, but has an overhead** of needing to find the path. If the path is already found/optimal, it will be slightly slower.
- The einsum overhead decreases for bigger dimensions.
- **torch.einsum without opt_einsum installed is comparable to before this commit**, with occasional slowness potentially due to not reshaping/squeezing as we contract until the end.
- For many of the random generated cases, the dimensions were too similar and small where an optimal order wasn't that much more optimal than just going left to right. However, in production, dimensions are commonly quite distinct (batch size will be small, but the data will be huge).
- **torch.einsum opt is comparable (slightly faster overall) compared to numpy.einsum opt for the cpu case**. This is interesting given that torch.einsum currently spends time computing the path, but numpy.einsum takes it as input.
- **torch.einsum opt is significantly faster than numpy.einsum opt for the gpu case**. This is because numpy doesn't take advantage of GPUs.

The following benchmarks were done on an A100 GPU and Linux CPUs. The line in the first chart separates GPU (on top) from CPU, and the line in the second graph separates CPU (on top) and then GPU. Sorry it's flipped ð .

Production example (see [colab benchmark](https://colab.research.google.com/drive/1V2s4v1dOOKwRvp5T_DC-PNUosOV9FFJx?authuser=1#scrollTo=WZoQkC8Mdt6I) for more context):
<img width=""1176"" alt=""image"" src=""https://user-images.githubusercontent.com/31798555/192012636-9a68bfa7-2601-43b1-afeb-b4e0877db6a4.png"">

Randomly generated examples (the same ones as in https://github.com/pytorch/pytorch/pull/60191)
<img width=""1176"" alt=""image"" src=""https://user-images.githubusercontent.com/31798555/192012804-1c639595-b3e6-48c9-a385-ad851c13e1c2.png"">

Open below to see old + not super relevant benchmarking results:
<details>
Benchmark results BEFORE this PR (on Linux -- I will update devices so they are consistent later):
<img width=""776"" alt=""image"" src=""https://user-images.githubusercontent.com/31798555/190807274-18f71fce-556e-47f4-b18c-e0f7d0c0d5aa.png"">

Benchmark results with the code on this PR (on my x86 mac):
For the CPU internal use case --
![image](https://user-images.githubusercontent.com/31798555/190801376-6f591b00-cebd-4ca7-bb23-ae8f17f1634e.png)

For the general use case --
It looks like numpy opt still does better in several of these random cases, but torch einsum opt is consistently faster than torch.einsum.
![image](https://user-images.githubusercontent.com/31798555/190811730-fbb6797d-af59-4f5a-92da-ba4103372014.png)
<details>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84890
Approved by: https://github.com/albanD, https://github.com/soulitzer",189.0,141.0,".github/ci_commit_pins/xla.txt,aten/src/ATen/autocast_mode.cpp,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/native_functions.yaml,setup.py,test/test_linalg.py,torch/functional.py,torch/onnx/symbolic_opset12.py",8.0,9,4,1.102432673,47.0,26019.0,8.0,380730.625,7648.0,17972.5,0.0,Feature Addition,0.0,1
pytorch,6c875f17ca04b3c0eeae599db8992c88bb755c36,e7f28d424125d426084e8a26654e5317c8209b3b,Dhruv Matani,dhruvbird@fb.com,Wed Feb 17 21:38:05 2021 -0800,1613597885.0,"[PyTorch Mobile] Restructure DispatchStub::operator() code to move template independent code into an external method (#51403)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51403

Turns out this isn't a new idea. swolchok posted about this a while ago and this was discussed in the composability group.

Links to posts:
* Template Hoisting: https://fb.workplace.com/groups/llvm.gcc/permalink/2321250667923535/
* C++: Most of the code in a template should depend on the template parameter(s): https://fb.workplace.com/groups/2088132188069398/permalink/2224983771050905/
ghstack-source-id: 121873716

Test Plan: Results in a 10KiB size reduction on fbios. Will re-run BSB for igios.

Reviewed By: swolchok

Differential Revision: D25859327

fbshipit-source-id: 915abebb2643f8ac9a901f3b4d79c63f4bbb5fee",31.0,19.0,"aten/src/ATen/native/DispatchStub.cpp,aten/src/ATen/native/DispatchStub.h",2.0,4,1,0.242292189,2.0,282.0,2.0,3583770.0,8971.0,20144.0,0.0,,1.0,1
pytorch,0bebfe214354bdce8a8d6fde3f7d5607a9b6f05b,e7fe64f6a65cd427e503491f192c14476e18033b,Brian Wignall,brianwignall@gmail.com,Tue Dec 03 04:15:54 2019 -0800,1575346554.0,"Fix typos (#30606)

Summary:
Should be non-semantic.

Uses https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines to find likely typos.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30606

Differential Revision: D18763028

Pulled By: mrshenli

fbshipit-source-id: 896515a2156d062653408852e6c04b429fc5955c",154.0,154.0,".jenkins/caffe2/build.sh,aten/src/ATen/core/MT19937RNGEngine.h,aten/src/ATen/core/PhiloxRNGEngine.h,aten/src/ATen/core/op_registration/op_registration.h,aten/src/ATen/dlpack.h,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/avx_mathfun.h,aten/src/ATen/native/cuda/Loops.cuh,aten/src/ATen/native/cuda/LossCTC.cu,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/test/test_assert.h,aten/src/TH/THTensorApply.h,benchmarks/fastrnns/README.md,binaries/benchmark_helper.cc,binaries/convert_and_benchmark.cc,c10/core/TensorTypeId.h,c10/core/TensorTypeSet.h,c10/test/util/bfloat16_test.cpp,c10/util/Deprecated.h,c10/util/Half.h,c10/util/tempfile.h,caffe2/contrib/tensorrt/tensorrt_op_trt.cc,caffe2/core/blob_serialization.h,caffe2/core/context_gpu.cu,caffe2/core/context_gpu.h,caffe2/core/net_dag_utils.cc,caffe2/core/nomnigraph/include/nomnigraph/Transformations/SubgraphMatcher.h,caffe2/core/operator.h,caffe2/core/plan_executor.cc,caffe2/mobile/contrib/ios/ios_caffe_predictor.h,caffe2/mobile/contrib/ios/mpscnn/mpscnn.mm,caffe2/operators/concat_split_op.h,caffe2/operators/gather_ranges_to_dense_op.cc,caffe2/operators/load_save_op_util.cc,caffe2/operators/quantized/int8_roi_align_op.h,caffe2/operators/reducer_functors.h,caffe2/operators/roi_align_gradient_op.cc,caffe2/operators/roi_align_gradient_op.cu,caffe2/operators/roi_align_op.cc,caffe2/operators/roi_align_op.cu,caffe2/operators/roi_align_rotated_gradient_op.cu,caffe2/operators/roi_align_rotated_op.cc,caffe2/operators/roi_align_rotated_op.cu,caffe2/operators/segment_reduction_op.h,caffe2/operators/stats_put_ops.cc,caffe2/operators/stats_put_ops.h,caffe2/operators/text_file_reader.cc,caffe2/operators/utility_ops.h,caffe2/opt/converter.cc,caffe2/opt/onnxifi_transformer.cc,caffe2/predictor/emulator/data_filler.h,caffe2/proto/caffe2.proto,caffe2/python/crf.py,caffe2/python/dlpack.h,caffe2/python/hypothesis_test.py,caffe2/python/layer_model_helper.py,caffe2/python/layers/batch_lr_loss.py,caffe2/python/layers/feature_sparse_to_dense.py,caffe2/python/lstm_benchmark.py,caffe2/python/memonger.py,caffe2/python/model_helper.py,caffe2/python/modeling/gradient_clipping.py,caffe2/python/models/resnet.py,caffe2/python/modifier_context.py,caffe2/python/normalizer_context.py,caffe2/python/onnx/backend.py,caffe2/python/operator_test/dataset_ops_test.py,caffe2/python/operator_test/gather_ops_test.py,caffe2/python/operator_test/pooling_test.py,caffe2/python/optimizer_context.py,caffe2/python/optimizer_test_util.py,caffe2/python/pipeline.py,caffe2/python/regularizer.py,caffe2/python/regularizer_context.py,caffe2/python/schema.py,caffe2/python/session.py,caffe2/python/task.py,caffe2/quantization/server/dnnlowp_test_utils.py,caffe2/serialize/inline_container.h,caffe2/sgd/clip_tensor_op.cc,caffe2/video/video_decoder.cc,docs/source/community/contribution_guide.rst,docs/source/distributed.rst,docs/source/hub.rst,docs/source/name_inference.rst,modules/detectron/smooth_l1_loss_op.cc,scripts/fbcode-dev-setup/onnx_c2_setup.sh,test/common_utils.py,test/cpp/api/dataloader.cpp,test/cpp/api/nn_utils.cpp,test/cpp/jit/test_utils.h,test/dist_autograd_test.py,test/run_test.py,test/test_cpp_api_parity.py,test/test_nn.py,test/test_quantization.py,test/test_quantized.py,tools/autograd/utils.py,tools/clang_tidy.py,tools/jit/gen_jit_dispatch.py,torch/_jit_internal.py,torch/_torch_docs.py,torch/backends/cudnn/__init__.py,torch/csrc/DataLoader.cpp,torch/csrc/api/include/torch/data/dataloader/stateful.h,torch/csrc/api/include/torch/ordered_dict.h,torch/csrc/autograd/profiler.cpp,torch/csrc/distributed/autograd/context/context.h,torch/csrc/distributed/rpc/process_group_agent.cpp,torch/csrc/distributed/rpc/python_rpc_handler.h,torch/csrc/generic/StorageMethods.cpp,torch/csrc/jit/argument_spec.cpp,torch/csrc/jit/fuser/cpu/temp_file.h,torch/csrc/jit/passes/alias_analysis.h,torch/csrc/jit/passes/create_autodiff_subgraphs.cpp,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/python_ir.cpp,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/register_special_ops.cpp,torch/csrc/jit/script/compiler.cpp,torch/csrc/jit/script/module.h,torch/csrc/jit/script/object.h,torch/csrc/jit/script/parser.cpp,torch/csrc/jit/script/python_sugared_value.cpp,torch/csrc/jit/unpickler.cpp,torch/cuda/__init__.py,torch/distributed/launch.py,torch/jit/__init__.py,torch/lib/libshm/err.h,torch/multiprocessing/reductions.py,torch/nn/functional.py,torch/nn/modules/fold.py,torch/nn/parallel/scatter_gather.pyi,torch/nn/utils/prune.py,torch/onnx/__init__.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/utils/checkpoint.py",142.0,96,12,7.101138651,61.0,96477.0,127.0,12111729.105633805,13563.0,37153.33333,0.0,Corrective,1.0,1
pytorch,726e2ed71579a49afb9a0e44a35a2363c87148a6,e86058559af705f0ffb04cf88b729634d1f612b2,Samantha Andow,samdow@fb.com,Tue Nov 09 20:55:59 2021 -0800,1636491359.0,"Op info for activation functions 2 (softsign, tanh, tanhshrink, threshold, celu, sigmoid, mish, hardsigmoid) (#67492)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67492

Reviewed By: zou3519

Differential Revision: D32282580

Pulled By: samdow

fbshipit-source-id: 115afe790328577357a90117bede3b6502590441",137.0,2.0,"test/test_jit_fuser_te.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.150237232,2.0,14769.0,2.0,48770.0,16965.0,39892.0,0.0,,0.0,1
pytorch,3bf448936cd4dccce56db43eea2c423c10f09705,e86d8ebfb6722a252261c4c82e5d7d2e9bba8ea8,Richard Zou,zou3519@gmail.com,Thu Apr 28 22:09:42 2022 -0700,1651183782.0,[functorch] re-annotated some tests,20.0,20.0,functorch/test/test_vmap.py,1.0,2,1,0,1.0,4159.0,1.0,0.0,1019.0,1403.0,0.0,,0.0,1
pytorch,32462e0ac460c29f58373e83aead482a06655ee2,e876b5d9d00d11e99be70b69448545a948356e98,li-roy,8813817+li-roy@users.noreply.github.com,Sat Mar 17 15:10:48 2018 -0700,1521299448.0,"implement TripletMarginLoss as a native function (#5680)

* implement TripletMarginLoss as a native function

* implement TripletMarginLoss as native function

* fix compile error

* address comments

* address comments

* Add keepdim arg to pairwise distance",119.0,104.0,"aten/src/ATen/native/Distance.cpp,aten/src/ATen/native/Loss.cpp,aten/src/ATen/native/native_functions.yaml,test/common_nn.py,test/test_nn.py,torch/nn/functional.py,torch/nn/modules/distance.py,torch/nn/modules/loss.py",8.0,8,3,2.470700718,37.0,11597.0,7.0,507355.28571428574,2482.0,24814.85823,0.0,Corrective,1.0,1
pytorch,a3855cc611e8be5a76254165a7468503208c7285,e8b950186159247639e6645ba50f57f2a00ac6b0,Sean Ross-Ross,srossross@gmail.com,Fri Sep 09 18:54:47 2022 +0000,1662749687.0,"test: adding uniform (#84292)

Adding OpInfo for uniform

Pull Request resolved: https://github.com/pytorch/pytorch/pull/84292
Approved by: https://github.com/amjames, https://github.com/ngimel",47.0,0.0,"functorch/test/test_vmap.py,torch/testing/_internal/common_methods_invocations.py",2.0,5,2,0.14854949,7.0,21938.0,2.0,134376.5,7216.0,16835.5,0.0,Feature Addition,0.0,1
pytorch,1a164bf30bc82a0476520f39eb0bcbe31a2b9f02,e8bc992b03eb9cebaaae336140dd11c7259f11fd,Ailing Zhang,ailzhang@fb.com,Wed Jun 26 03:21:44 2019 -0700,1561519304.0,"print device when it's not on default device (#22094)

Summary:
we used to not print device when it's on xla. It's sometimes confusing as it looks the same as cpu tensor...
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22094

Differential Revision: D15975405

Pulled By: ailzhang

fbshipit-source-id: f19ceb9e26f5f2f6e7d659de12716f0dfe065f42",27.0,13.0,"test/test_torch.py,torch/_tensor_str.py,torch/csrc/Module.cpp",3.0,3,2,1.542774454,42.0,13226.0,3.0,163663.66666666666,9631.0,28044.83333,0.0,,0.0,1
pytorch,7a3c38ab595ea78f16935df788c4982a0ec56966,e8bdbdaa27606ec944b9f90ab7ef41c0841d9444,Will Feng,yf225@cornell.edu,Mon Apr 23 03:03:54 2018 -0400,1524452634.0,"Terminate dataloader workers properly when parent process is SIGKILL'ed (#6779)

Reopening #6606 with fix for TEST_CUDA import issue on Windows and improvement to how we wait for manager exit in test_manager_unclean_exit. Loop tested on the Windows CI multiple times to make sure this actually fixes the CUDA OOM issue.

* Terminate dataloader workers properly when parent process is SIGKILL'ed

* Wait for worker processes to finish before shutting down manager process

* Add test for checking proper worker exit

* cosmetic change

* Test only if CUDA exists

* Don't call multiprocessing.set_start_method() in Python 2

* import TEST_CUDA only when we are in __main__

* Tune JOIN_TIMEOUT

* handle os.getppid() == 0 case

* Reset to original JOIN_TIMEOUT

* Use WaitForSingleObject() to check parent process status on Windows

* Fix TEST_CUDA import

* clean up

* Check main process only when index_queue.get() times out

* Change index_queues to multiprocessing.Queue

* Move manager checking logic to watchdog class

* Fix bugs in dataloader

* Fix TEST_CUDA import issue

* Don't import TEST_CUDA from common_nn

* Use event to signal manager exit in test

* fix lint

* Add comments",133.0,5.0,"test/test_dataloader.py,torch/utils/data/dataloader.py",2.0,4,2,0.960857691,37.0,1109.0,1.0,290106.0,602.0,3506.0,0.0,Corrective,1.0,1
pytorch,d5ff4326154bad0fe747567a75f49e647d774b94,e97c17afa0c52f32f151729864bec87de32d5a90,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Sun Apr 25 10:43:46 2021 -0700,1619347426.0,"Update internal code for torch.geqrf (#56250)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56250

Moved `apply_geqrf` to `BatchLinearAlgebraKernel.cpp`. Added
`geqrf_stub` dispatch.

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D27907362

Pulled By: mruberry

fbshipit-source-id: 6719464aef29dcf3bbbde060edf79f1e32fc8ad6",89.0,80.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp",3.0,4,1,1.29841576,2.0,4157.0,2.0,352976.3333333333,11245.0,24853.0,0.0,Feature Addition,1.0,1
pytorch,a0695b34cdaed0a0ccf2f1d71ea8bd33729a6048,e995c3d21e996c8c1b22a78e6bf81a4d2b1d2e86,iurii zdebskyi,47012416+izdeby@users.noreply.github.com,Tue Aug 04 21:59:16 2020 -0700,1596578356.0,"Add private API to support tensor lists: _foreach_add(TensorList tensors, Scalar scalar) (#41554)

Summary:
Initial PR for the Tensor List functionality.

**Motivation**
[GitHub issue](https://github.com/pytorch/pytorch/issues/38655)
Current PyTorch optimizer implementations are not efficient in cases when we work with a lot of small feature tensors. Starting a lot of kernels slows down the whole process. We need to reduce the number of kernels that we start.
As an example, we should be looking at [NVIDIAs Apex](https://github.com/NVIDIA/apex).
In order to track progress, we will pick PyTorchs DCGAN model with Adam optimizer and once the optimizer is reimplemented with tensor lists, benchmark the model performance against original model version, Apexs version with original Adam optimizer and itâs FusedAdam optimizer.

**In this PR**
- Adding `multi_tensor_apply` mechanism which will help to efficiently apply passed functor on a given list of tensors on CUDA.
- Adding a first private API - `std::vector<Tensor> _foreach_add(TensorList tensors, Scalar scalar)`

**Tests**
Tested via unit tests

**Plan for the next PRs**

1. Cover these ops with `multi_tensor_apply` support
- exponent
- division
- mul_
- add_
- addcmul_
- addcdiv_
- Sqrt

2. Rewrite PyTorch optimizers to use for-each operators in order to get performance gains.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/41554

Reviewed By: cpuhrsch

Differential Revision: D22829724

Pulled By: izdeby

fbshipit-source-id: 47febdbf7845cf931958a638567b7428a24782b1",365.0,0.0,"aten/src/ATen/native/ForeachOpsKernels.cpp,aten/src/ATen/native/cuda/ForeachTensorAddScalar.cu,aten/src/ATen/native/cuda/ForeachUtils.cuh,aten/src/ATen/native/cuda/MultiTensorApply.cuh,aten/src/ATen/native/native_functions.yaml,test/run_test.py,test/test_foreach.py",7.0,6,2,2.245313231,12.0,7704.0,2.0,317270.0,4047.0,9461.0,0.0,Feature Addition,0.0,1
pytorch,45d5e2b2cb28216b0810cbf90bb202e3118531ab,e9a8e6f74ac037ed3a16b99d0bd48bdaafc73825,Mikayla Gawarecki,mikaylagawarecki@gmail.com,Mon Apr 04 17:39:08 2022 +0000,1649093948.0,"Add include_self flag to scatter_reduce

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74607

Approved by: https://github.com/cpuhrsch",107.0,40.0,"aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/native_functions.yaml,test/test_scatter_gather_ops.py,tools/autograd/derivatives.yaml,torch/_tensor_docs.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",10.0,12,4,2.047428333,35.0,56835.0,6.0,274425.3,1981.0,4711.5,0.0,Feature Addition,0.0,1
pytorch,89ba9dc44f546a0142b6721faa68e1538cc65b00,e9c33e91d94a2ff4739de202b7192918f8748e52,Seth Hendrickson,sethah@users.noreply.github.com,Thu May 31 17:42:49 2018 -0700,1527788569.0,"Remove python bindings for `torch.slice` (#7924)

* skip python bindings for slice

* remove tests

* convert slice test to indexing",11.0,12.0,"test/test_autograd.py,test/test_torch.py,tools/autograd/gen_python_functions.py",3.0,3,2,0.678407091,41.0,11436.0,3.0,373422.6666666667,2681.0,25073.85823,0.0,,0.0,1
pytorch,f3be2816ae8134cf901c183cdf08d5945d10c7d0,e9c8f372c49431907ac525a2abbfe212e549f61e,Wanchao Liang,wanchaol@users.noreply.github.com,Tue Apr 23 18:16:28 2019 -0700,1556043388.0,"dispatch max_pools with no indices, expose max_pools to torch namespace (#19449)

Summary:
in functional interfaces we do boolean dispatch, but all to max_pool\*d_with_indices. This change it to emit max_pool\*d op instead when it's not necessary to expose with_indices ops to different backends (for jit).

It also bind max_pool\*d to the torch namespace, which is the same behavior with avg_pool\*d
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19449

Differential Revision: D15016839

Pulled By: wanchaol

fbshipit-source-id: f77cd5f0bcd6d8534c1296d89b061023a8288a2c",74.0,110.0,"aten/src/ATen/native/native_functions.yaml,test/onnx/expect/TestOperators.test_maxpool.expect,tools/autograd/gen_python_functions.py,torch/nn/functional.py,torch/onnx/symbolic.py",5.0,12,4,0.87349228,35.0,10077.0,5.0,1185059.4,8266.0,24762.83333,0.0,,1.0,1
pytorch,efefd1d7cf3736cfb5c494a5eb3b5b22ab949dc4,e9e47ce8f1860270281cffa2adc07b139fab652e,Christian Puhrsch,cpuhrsch@fb.com,Tue Jul 10 19:30:38 2018 -0700,1531251038.0,"Vectorize sigmoid (#8612)

Summary:
This PR ports the vectorization of sigmoid to also enable better performance for non-contiguous arrays. Detailed timings will follow shortly.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/8612

Reviewed By: ezyang

Differential Revision: D8712298

Pulled By: cpuhrsch

fbshipit-source-id: 01a3d06af8d04513edd024ab1d01a6b753fc6f6a",246.0,88.0,"aten/src/ATen/CPUApplyUtils.h,aten/src/ATen/Declarations.cwrap,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cpu/vec256/vec256_double.h,aten/src/ATen/cpu/vec256/vec256_float.h,aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cpu/vml.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/CapabilityDispatch.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cpu/UnaryOpsKernel.h,aten/src/ATen/native/cuda/CUDAUnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,cmake/Codegen.cmake",14.0,9,2,2.414455283,12.0,7619.0,8.0,2326236.285714286,2876.0,6672.333333,0.0,,0.0,1
pytorch,65748f81c97fdfbca205d77a25f3d5488fdd8689,e9e5588588b650f242eccaeb006bfb3a54ba3564,albanD,desmaison.alban@gmail.com,Tue Jun 01 17:26:33 2021 -0700,1622568393.0,"Improve Tensor traverse to traverse its grad_fn when possible (#58271)

Summary:
There are two main changes here:
- THPVariable will actually visit their grad_fn if there are no other reference to the c++ Tensor and no other reference to the grad_fn. The critical observation compared to the existing comment (thanks Ed!) is that if we also check that the c++ Tensor object is not referenced somewhere else, we're sure that no one can change the grad_fn refcount between the traverse and the clear.
- THPVariable don't need a special clear for this new cases as we're the only owner of the c++ Tensor and so the cdata.reset() will necessarily free the Tensor and all its resources.

The two tests are to ensure:
- That the cycles are indeed collectible by the gc

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58271

Reviewed By: ngimel

Differential Revision: D28796461

Pulled By: albanD

fbshipit-source-id: 62c05930ddd0c48422c79b03118db41a73c1355d",129.0,15.0,"test/test_autograd.py,torch/csrc/autograd/python_variable.cpp,torch/testing/_internal/common_utils.py",3.0,6,2,1.242028338,42.0,12287.0,3.0,724017.6666666666,12589.0,28574.5,0.0,Perfective,0.0,1
pytorch,418aad2c54cf061c9221ede2f69e922e6cc80c66,e9f144b3e84f03d6a507faac40bdfce58b68c1fb,cpuhrsch,cpuhrsch@googlemail.com,Thu Mar 22 05:14:56 2018 -0400,1521695696.0,"parallel_for_2d fix and guarding avx/avx2 compilation (#5926)

Fix for #5921.

I'm adding support compilers that don't support -mavx -mavx2 by revisiting the dispatch code.",102.0,16.0,"aten/src/ATen/CMakeLists.txt,aten/src/ATen/Parallel.h,aten/src/ATen/native/cpu/CapabilityDispatch.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/test/CMakeLists.txt,aten/src/ATen/test/test_parallel.cpp,test/test_torch.py",7.0,7,2,2.430673406,38.0,6727.0,3.0,193770.66666666663,508.0,2414.5,0.0,Corrective,1.0,1
pytorch,652a31b714f39b4dcee93779511339d1f8cfde61,e9f9fd3727bca299111c6b7b23a5fdbc61832484,Adam Paszke,adam.paszke@gmail.com,Wed Aug 10 16:19:13 2016 -0700,1470845953.0,Major refactor,5656.0,6858.0,".gitignore,setup.py,test/legacy/nn.py,test/test.py,tools/cwrap/argument.py,tools/cwrap/config.py,tools/cwrap/cwrap.py,tools/cwrap/functions.py,tools/cwrap/options.py,tools/cwrap/plugins/ArgcountChecker.py,tools/cwrap/plugins/ArgcountSortPlugin.py,tools/cwrap/plugins/ArgumentReferences.py,tools/cwrap/plugins/BeforeCall.py,tools/cwrap/plugins/ConstantArguments.py,tools/cwrap/plugins/NullableArguments.py,tools/cwrap/plugins/OptionalArguments.py,tools/cwrap/plugins/ReturnArguments.py,tools/cwrap/plugins/StandaloneExtension.py,tools/cwrap/plugins/THPLongArgsPlugin.py,tools/cwrap/plugins/THPPlugin.py,tools/cwrap/plugins/__init__.py,tools/cwrap/plugins/templates/module_head.cpp,tools/cwrap/plugins/templates/module_tail.cpp,tools/cwrap/utils.py,tools/nnwrap/__init__.py,tools/nnwrap/generate_wrappers.py,torch/Tensor.py,torch/_thnn/__init__.py,torch/_thnn/thcunn.py,torch/_thnn/thnn.py,torch/_thnn/utils.py,torch/csrc/Module.cpp,torch/csrc/Tensor.h,torch/csrc/cuda/override_macros.h,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/generic/TensorMethods.cwrap.cpp,torch/csrc/generic/utils.cpp,torch/csrc/generic/utils.h,torch/csrc/nn/.gitkeep,torch/csrc/utils.cpp,torch/legacy/cunn/THCUNN.h,torch/legacy/cunn/__init__.py,torch/legacy/cunn/ffi.py,torch/legacy/nn/Add.py,torch/legacy/nn/AddConstant.py,torch/legacy/nn/BCECriterion.py,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/Bilinear.py,torch/legacy/nn/CAddTable.py,torch/legacy/nn/CDivTable.py,torch/legacy/nn/CMul.py,torch/legacy/nn/CMulTable.py,torch/legacy/nn/CSubTable.py,torch/legacy/nn/ClassNLLCriterion.py,torch/legacy/nn/ClassSimplexCriterion.py,torch/legacy/nn/Concat.py,torch/legacy/nn/ConcatTable.py,torch/legacy/nn/Contiguous.py,torch/legacy/nn/Copy.py,torch/legacy/nn/Cosine.py,torch/legacy/nn/CosineDistance.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/Criterion.py,torch/legacy/nn/CrossEntropyCriterion.py,torch/legacy/nn/DepthConcat.py,torch/legacy/nn/DotProduct.py,torch/legacy/nn/Dropout.py,torch/legacy/nn/ELU.py,torch/legacy/nn/Euclidean.py,torch/legacy/nn/Exp.py,torch/legacy/nn/GradientReversal.py,torch/legacy/nn/HardShrink.py,torch/legacy/nn/HingeEmbeddingCriterion.py,torch/legacy/nn/Index.py,torch/legacy/nn/JoinTable.py,torch/legacy/nn/L1HingeEmbeddingCriterion.py,torch/legacy/nn/L1Penalty.py,torch/legacy/nn/Linear.py,torch/legacy/nn/Log.py,torch/legacy/nn/LookupTable.py,torch/legacy/nn/MM.py,torch/legacy/nn/MV.py,torch/legacy/nn/MarginRankingCriterion.py,torch/legacy/nn/MaskedSelect.py,torch/legacy/nn/Max.py,torch/legacy/nn/Min.py,torch/legacy/nn/MixtureTable.py,torch/legacy/nn/Module.py,torch/legacy/nn/Mul.py,torch/legacy/nn/MulConstant.py,torch/legacy/nn/MultiLabelSoftMarginCriterion.py,torch/legacy/nn/Narrow.py,torch/legacy/nn/Normalize.py,torch/legacy/nn/PReLU.py,torch/legacy/nn/Padding.py,torch/legacy/nn/PairwiseDistance.py,torch/legacy/nn/Parallel.py,torch/legacy/nn/PartialLinear.py,torch/legacy/nn/Power.py,torch/legacy/nn/Replicate.py,torch/legacy/nn/Reshape.py,torch/legacy/nn/Select.py,torch/legacy/nn/SelectTable.py,torch/legacy/nn/SoftMin.py,torch/legacy/nn/SoftShrink.py,torch/legacy/nn/SoftSign.py,torch/legacy/nn/SpatialAveragePooling.py,torch/legacy/nn/SpatialClassNLLCriterion.py,torch/legacy/nn/SpatialContrastiveNormalization.py,torch/legacy/nn/SpatialConvolution.py,torch/legacy/nn/SpatialConvolutionLocal.py,torch/legacy/nn/SpatialConvolutionMap.py,torch/legacy/nn/SpatialCrossMapLRN.py,torch/legacy/nn/SpatialDivisiveNormalization.py,torch/legacy/nn/SpatialDropout.py,torch/legacy/nn/SpatialFractionalMaxPooling.py,torch/legacy/nn/SpatialFullConvolution.py,torch/legacy/nn/SpatialFullConvolutionMap.py,torch/legacy/nn/SpatialSubSampling.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/legacy/nn/SpatialUpSamplingNearest.py,torch/legacy/nn/SpatialZeroPadding.py,torch/legacy/nn/SplitTable.py,torch/legacy/nn/Squeeze.py,torch/legacy/nn/Sum.py,torch/legacy/nn/THNN.h,torch/legacy/nn/TanhShrink.py,torch/legacy/nn/TemporalConvolution.py,torch/legacy/nn/TemporalSubSampling.py,torch/legacy/nn/Transpose.py,torch/legacy/nn/Unsqueeze.py,torch/legacy/nn/View.py,torch/legacy/nn/VolumetricConvolution.py,torch/legacy/nn/VolumetricDropout.py,torch/legacy/nn/VolumetricFullConvolution.py,torch/legacy/nn/WeightedEuclidean.py,torch/legacy/nn/WeightedMSECriterion.py,torch/legacy/nn/__init__.py,torch/legacy/nn/ffi.py,torch/legacy/nn/utils.py,torch/lib/build_all.sh",143.0,17,3,4.143266237,5.0,19168.0,2.0,506026.80991735536,84.0,1132.433333,0.0,Perfective,0.0,1
pytorch,17f53bffefa7bd46e134a99bcd4c4e471b81c86b,ea2a568cca71aaf690051782c225ca9dd2e5e1f9,Heitor Schueroff,heitorschueroff@fb.com,Sun Dec 06 16:01:00 2020 -0800,1607270460.0,"Fixed einsum compatibility/performance issues (#46398) (#47860)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/47860

This PR makes torch.einsum compatible with numpy.einsum except for the sublist input option as requested here https://github.com/pytorch/pytorch/issues/21412. It also fixed 2 performance issues linked below and adds a check for reducing to torch.dot instead of torch.bmm which is faster in some cases.

fixes #45854, #37628, #30194, #15671

fixes #41467 with benchmark below
```python
import torch
from torch.utils.benchmark import Timer

a = torch.randn(10000, 100, 101, device='cuda')
b = torch.randn(10000, 101, 3, device='cuda')

c = torch.randn(10000, 100, 1, device='cuda')
d = torch.randn(10000, 100, 1, 3, device='cuda')

print(Timer(
    stmt='torch.einsum(""bij,bjf->bif"", a, b)',
    globals={'a': a, 'b': b}
).blocked_autorange())

print()

print(Timer(
    stmt='torch.einsum(""bic,bicf->bif"", c, d)',
    globals={'c': c, 'd': d}
).blocked_autorange())
```
```
<torch.utils.benchmark.utils.common.Measurement object at 0x7fa37c413850>
torch.einsum(""bij,bjf->bif"", a, b)
  Median: 4.53 ms
  IQR:    0.00 ms (4.53 to 4.53)
  45 measurements, 1 runs per measurement, 1 thread

<torch.utils.benchmark.utils.common.Measurement object at 0x7fa37c413700>
torch.einsum(""bic,bicf->bif"", c, d)
  Median: 63.86 us
  IQR:    1.52 us (63.22 to 64.73)
  4 measurements, 1000 runs per measurement, 1 thread
```

fixes #32591 with benchmark below
```python
import torch
from torch.utils.benchmark import Timer

a = torch.rand(1, 1, 16, 2, 16, 2, 16, 2, 2, 2, 2, device=""cuda"")
b = torch.rand(729, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, device=""cuda"")

print(Timer(
    stmt='(a * b).sum(dim = (-3, -2, -1))',
    globals={'a': a, 'b': b}
).blocked_autorange())

print()

print(Timer(
    stmt='torch.einsum(""...ijk, ...ijk -> ..."", a, b)',
    globals={'a': a, 'b': b}
).blocked_autorange())
```
```
<torch.utils.benchmark.utils.common.Measurement object at 0x7efe0de28850>
(a * b).sum(dim = (-3, -2, -1))
  Median: 17.86 ms
  2 measurements, 10 runs per measurement, 1 thread

<torch.utils.benchmark.utils.common.Measurement object at 0x7efe0de286a0>
torch.einsum(""...ijk, ...ijk -> ..."", a, b)
  Median: 296.11 us
  IQR:    1.38 us (295.42 to 296.81)
  662 measurements, 1 runs per measurement, 1 thread
```

TODO

- [x] add support for ellipsis broadcasting
- [x] fix corner case issues with sumproduct_pair
- [x] update docs and add more comments
- [x] add tests for error cases

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D24923679

Pulled By: heitorschueroff

fbshipit-source-id: 47e48822cd67bbcdadbdfc5ffa25ee8ba4c9620a",543.0,348.0,"aten/src/ATen/native/Linear.cpp,test/test_linalg.py,torch/functional.py",3.0,6,3,1.421691038,30.0,7818.0,3.0,797912.3333333334,7250.0,16372.0,0.0,Corrective,1.0,1
pytorch,c9eab34e637600aab18abf82b6e6b9918685915c,ea3c36b822a7940bad6f0f7bad67f04afa8f5aa2,vishwakftw,cs15btech11043@iith.ac.in,Mon Jul 30 21:32:16 2018 -0700,1532986336.0,"NumPy Scalar to PyTorch Scalar (#9225)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/4985 .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9225

Differential Revision: D8769317

Pulled By: ezyang

fbshipit-source-id: eeaeaf0749c9dc9e372634da68b4bd23e6e3ad28",18.0,2.0,"test/test_torch.py,torch/csrc/utils/tensor_new.cpp",2.0,4,2,0.881290899,40.0,9022.0,2.0,1503287.5,3189.0,8439.333333,0.0,Corrective,1.0,1
pytorch,1d4d6b6f0fe590f44b469d1ee49a967b28ccf81a,ea414e499078c77f4af5047a6e0e5bd5e9389e40,Mike Ruberry,mruberry@devfair044.maas,Tue Oct 01 02:07:28 2019 -0700,1569895648.0,"Adds Device Generic Precision Tests to test_torch.py (#26762)

Summary:
- Lets device generic classes be instantiated for all available device types EXCEPT those specified
- Creates TestDevicePrecision in test_torch.py, letting devices compare their results to the CPU's
- Moves 4 functions from test_cuda.py to TestDevicePrecision
- polygamma and digamma functions were cleaned up

The polygamma and digamma tests always ran with double tensors and will fail when using float tensors, despite former comments and code to the contrary. Notes were added to each function.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26762

Differential Revision: D17677859

Pulled By: mruberry

fbshipit-source-id: 7cbe7d05ee0bc9b622c9127be36ced02f9c4506a",55.0,62.0,"test/common_device_type.py,test/test_cuda.py,test/test_torch.py",3.0,1,1,1.23380436,41.0,16304.0,3.0,245745.3333333333,11925.0,33532.33333,0.0,Feature Addition,0.0,1
pytorch,0853d13f8640d199ac2057ab257fb172e6a71528,ea67a2bd1160666ae238cdae294b74b56a2d7c73,bhushan,bhushan.s.94@gmail.com,Wed Jul 25 16:20:18 2018 -0700,1532535618.0,"Allows negative index to tensor.narrow (Fixes: #9546)

Summary:
Fixes #9546
Test cases added

Reviewed By: ezyang

Differential Revision: D8974842

Pulled By: zou3519

fbshipit-source-id: a7707406c2a21e8e14f9c2a8ad4d64c8b08156df",13.0,3.0,"aten/src/ATen/native/TensorShape.cpp,docs/source/torch.rst,test/test_torch.py",3.0,7,3,1.121640762,41.0,9419.0,3.0,139234.33333333334,3100.0,7546.833333,0.0,Corrective,1.0,1
pytorch,7bcb2a4081757facb7332c09c7604fe8738c2d7c,ea93fb7ac0b47e95a46c701dc6bb79f5a205fc6f,Adam Paszke,adam.paszke@gmail.com,Tue Aug 23 16:22:29 2016 -0700,1471969349.0,Add more nn modules,254.0,81.0,"test/common_nn.py,test/test_legacy_nn.py,test/test_nn.py,torch/autograd/variable.py,torch/nn/backends/thnn.py,torch/nn/functions/thnn.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/container.py,torch/nn/modules/conv.py,torch/nn/modules/conv_2d.py,torch/nn/modules/module.py,torch/nn/modules/pooling.py",14.0,7,2,3.384480434,4.0,2272.0,4.0,140269.9090909091,130.0,2241.528571,0.0,Feature Addition,0.0,1
pytorch,d2517a43db39feb8aaee9cccc6bf6a6c659d8b54,eab3f428833ce9e11e2db97e516fe4b78ea2e4dc,Thiago Crepaldi,thiago.crepaldi@microsoft.com,Tue Apr 19 15:57:50 2022 +0000,1650383870.0,"Update symbolics policy to emit aten::ATen for Caffe2 build only

Currently ONNX exporter symbolics can emit ATen operators when `operator_export_type==ONNX_ATEN_FALLBACK`. However, this is a behavior specific to Caffe2 builds, as the intend use of `ONNX_ATEN_FALLBACK` is to emit ATen operators only when there is no ONNX equivalent.

The reason Caffe2 choses to emit ATen operators when ONNX counterpart exists is for performance on their particular engine implementation, which might not be true for other implementations. e.g. ONNX Runtime can optimize the generated ONNX graph into something more efficient

This PR must be merged only after https://github.com/pytorch/pytorch/pull/73954
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74680
Approved by: https://github.com/garymm, https://github.com/malfet",137.0,56.0,".github/merge_rules.json,test/jit/test_export_modes.py,test/onnx/expect/TestOperators.test_layer_norm_aten.expect,test/onnx/test_operators.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset12.py,torch/onnx/symbolic_opset9.py",7.0,7,3,1.708692314,6.0,6473.0,4.0,1076679.2857142857,2416.0,5688.5,0.0,Perfective,0.0,1
pytorch,a5dbc254f8cc16266f82767bf3dacb44af0e3327,eac0942f6de664ea67e395cc160641b1f9b8eaea,Richard Zou,zou3519@users.noreply.github.com,Mon Oct 30 22:37:36 2017 -0400,1509403056.0,Add more nn docs (#3374),272.0,14.0,"torch/nn/functional.py,torch/nn/modules/container.py,torch/nn/modules/module.py",3.0,3,1,1.0289793,37.0,2309.0,3.0,103336.33333333331,2027.0,23892.85823,0.0,Feature Addition,0.0,1
pytorch,510971f86c94bbaeee9ccfb62c258e1008e4e003,eaca6f32b056decb3b3e4c7fdd7c1cd9d115f479,Luca Wehrstedt,lcw@fb.com,Thu May 28 17:42:29 2020 -0700,1590687749.0,"[TensorPipe] Do not mark future messages as complete after they have timed out (#38931)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38931

When requests time out they are not aborted, so they could in fact still complete successfully but, when they do so, they try to mark an errored future as complete, which causes an error. I don't see any atomic way of doing future->markCompleteIfNeeded, so we implement it on top of it on our side.
ghstack-source-id: 104760689

Test Plan: Hit this error in the RPC test suite, and it disappeared after this fix.

Differential Revision: D21703015

fbshipit-source-id: af92f7819ed907efb9b068a4ca65420739fac8cc",58.0,20.0,"torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h",2.0,4,1,0.904938259,1.0,912.0,2.0,429858.0,2397.0,6016.5,0.0,Corrective,1.0,1
pytorch,46a925795e0a63f4bf81e300175fc4da2cfb7590,eaffd988801a4031fbd7dbd6addc232ee794d9e3,Andres Lugo-Reyes,Andy.LugoReyes@amd.com,Wed May 31 16:53:23 2023 +0000,1685552003.0,"Enable hipSOLVER in ROCm builds (#97370)

Enables the hipSolver backend for ROCm builds
--------------------------------------------------------------------------

- Minimum ROCm version requirement - 5.3
- Introduces new macro USE_LINALG_SOLVER the controls enablement of both cuSOLVER and hipSOLVER
- Adds hipSOLVER API to hipification process
- combines hipSOLVER and hipSPARSE mappings into single SPECIAL map that takes priority among normal mappings
- Torch api to be moved to hipsolver backend (as opposed to magma) include: torch.svd(), torch.geqrf(), torch.orgqr(), torch.ormqr()
- Will enable 100+ linalg unit tests for ROCm

Pull Request resolved: https://github.com/pytorch/pytorch/pull/97370
Approved by: https://github.com/malfet",571.0,254.0,"aten/src/ATen/cuda/CUDAContext.h,aten/src/ATen/cuda/detail/CUDAHooks.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h,aten/src/ATen/native/cuda/linalg/CUDASolver.cpp,aten/src/ATen/native/cuda/linalg/CUDASolver.h,aten/src/ATen/native/cuda/linalg/CusolverDnHandlePool.cpp,cmake/Dependencies.cmake,cmake/public/LoadHIP.cmake,test/functorch/test_ops.py,test/inductor/test_torchinductor_opinfo.py,test/test_linalg.py,torch/_prims/__init__.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_utils.py,torch/utils/hipify/constants.py,torch/utils/hipify/cuda_to_hip_mappings.py,torch/utils/hipify/hipify_python.py",19.0,19,4,2.101748398,20.0,39085.0,19.0,4646382.315789473,16366.0,36966.5,0.0,Feature Addition,0.0,1
pytorch,06d11f04349bdc240ebab988ef6702e1ff6372d5,eb5137a5d137d045ea037173b77ce25be7303ef1,BowenBao,bowbao@microsoft.com,Tue Jul 23 03:20:03 2019 -0700,1563852003.0,"Export torch.arange to ONNX (#22601)

Summary:
Some overlap with https://github.com/pytorch/pytorch/pull/21716 regarding caffe2 nonzero. Will rebase the other one accordingly whichever gets merged first.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22601

Reviewed By: zrphercule

Differential Revision: D16224660

Pulled By: houseroad

fbshipit-source-id: dbfd1b8776cb626601e0bf83b3fcca291806e653",192.0,11.0,"caffe2/onnx/backend.cc,caffe2/onnx/backend.h,test/onnx/test_onnx_opset.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset8.py,torch/onnx/symbolic_opset9.py",7.0,6,3,2.28065173,8.0,6840.0,5.0,2276006.0,10090.0,29149.83333,0.0,,0.0,1
pytorch,c86f8fa746070aa1f40bde6b7a4e425960e4ef66,eb58740651853782351e417b71b2f7af7d6a7f39,Alykhan Tejani,alykhan.tejani@gmail.com,Sat Aug 19 11:34:51 2017 +0100,1503142491.0,add ones_like and zeros_like,114.0,1.0,"docs/source/torch.rst,test/test_torch.py,torch/_torch_docs.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/Tensor.cwrap",5.0,7,3,1.851396349,37.0,11337.0,1.0,395.0,1382.0,14078.50631,0.0,Feature Addition,0.0,1
pytorch,0f04f71b7e8a255da63cb7577e62d08406838001,eb91fc5e5d0869753b05a93b1cb8a5a1a4c282e4,Sam Gross,colesbury@gmail.com,Fri Jan 06 15:59:24 2017 -0500,1483718364.0,Minor fixes to docs (#412),324.0,352.0,torch/docs.py,1.0,1,1,0,8.0,3355.0,1.0,232174.0,119.0,805.6565415,0.0,Corrective,1.0,1
pytorch,27f7d1c2865355c694fb964609df45974748615b,eb9516eaa425f89d8d6729b376dd1d45f1f615f4,kshitij12345,kshitijkalambarkar@gmail.com,Thu Dec 10 08:10:10 2020 -0800,1607587810.0,"[numpy] `torch.exp{2, m1}`: promote integer inputs to float (#48926)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/42515

Pull Request resolved: https://github.com/pytorch/pytorch/pull/48926

Reviewed By: zhangguanheng66

Differential Revision: D25392344

Pulled By: mruberry

fbshipit-source-id: ddbabcfd58cc4c944153b1a224cc232efa022104",31.0,32.0,"aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,test/test_torch.py,test/test_unary_ufuncs.py,torch/csrc/jit/tensorexpr/kernel.cpp,torch/testing/_internal/common_methods_invocations.py",6.0,12,3,1.614710721,44.0,14234.0,3.0,256928.16666666663,7351.0,16547.0,0.0,,0.0,1
pytorch,c90b393c005b01fa6b5f072657b757fe696ee646,ebb008eb68b688c776191b471e261d1e7f9b73a9,Xiaomeng Yang,yangxm@fb.com,Mon Feb 17 22:47:16 2020 -0800,1581979636.0,"Optimize Unfold3dAcc to improve performance of conv3d backward (#33317)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33317

Optimize Unfold3dAcc to improve performance of conv3d backward

Test Plan: buck test mode/dev-nosan //caffe2/test:nn -- ""Conv3d""

Reviewed By: houseroad

Differential Revision: D19892678

fbshipit-source-id: 18873dd1d1409263d9925840db302b21fb3b490d",273.0,167.0,"aten/src/ATen/native/ConvolutionMM3d.cpp,aten/src/ATen/native/Unfold3d.cpp,aten/src/ATen/native/Unfold3d.h",3.0,4,1,0.703919095,1.0,1154.0,1.0,425647.0,14794.0,39720.33333,0.0,Perfective,0.0,1
pytorch,669662cd2f2f1f37fe9c66dee1a9ac7dd0cc201c,ebc216a0765d85f345f9a5cd1dfd2ec360de3a52,Negin Raoof,neginmr@utexas.edu,Mon Nov 04 20:14:18 2019 -0800,1572898458.0,"Opset 11 updates (#28225)

Summary:
This PR contains:
1- pad updates for opset11 symbolic
2- Updated avg_pool for opset11
3- TopK updates for opset 11
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28225

Reviewed By: hl475

Differential Revision: D18282928

Pulled By: houseroad

fbshipit-source-id: aff2cabca9a155a9b475e35fed69a678544d6669",216.0,82.0,"test/onnx/expect/TestOperators.test_topk.expect,test/onnx/expect/TestOperators.test_topk_smallest_unsorted.expect,test/onnx/test_onnx_opset.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_utils.py,torch/autograd/_functions/utils.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset11.py,torch/onnx/symbolic_opset9.py",10.0,7,2,2.923662223,39.0,5981.0,9.0,3128596.5,12786.0,35377.83333,0.0,,0.0,1
pytorch,577bf04872b9c30278c1b4345961703263f9c9cd,ebca80ed08ebaf27e631d33181bb4d02362ece57,atalman,atalman@fb.com,Thu Mar 17 02:07:50 2022 +0000,1647482870.0,"Move test ops gradients and test ops jit to separate files

Fixes #72368

As per reference issue, the test_ops in single file takes around 3:30-4:00Hrs to execute on asan jobs:

Reference : pytorch_test_times.json

```
{
    ""commit"": ""39535fec6c3ff5bf7c2d322d096c59571c3295ed"",
    ""JOB_BASE_NAME"": ""linux-xenial-py3.7-clang7-asan"",
    ""job_times"": {
        ""test_ops"": 14928.355000000636, <- This test group is over 4hrs alone
```
----

Hence separating  test_ops into following parts:
1. TestGradients
2. TestJit
3.  TestCommon and TestMathBits

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74297
Approved by: https://github.com/malfet",519.0,474.0,"test/run_test.py,test/test_ops.py,test/test_ops_gradients.py,test/test_ops_jit.py,torch/autograd/gradcheck.py",5.0,3,2,1.585362468,29.0,3970.0,1.0,21567.0,1490.0,3623.0,0.0,Corrective,1.0,1
pytorch,b37080d97a2188b4e6cf78b255c9399e123994a2,ebcacd5e878f3f8e84c941d6b9567c88f50a2f10,Nikita Shulga,nshulga@fb.com,Tue Apr 28 05:47:09 2020 -0700,1588052829.0,"[Bazel] Build `ATen_CPU_AVX2` lib with AVX2 arch flags enabled (#37381)

Summary:
Make sleef dependency public so that `ATen_CPU_{capability}` libs can depend on it
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37381

Test Plan: CI

Differential Revision: D21273443

Pulled By: malfet

fbshipit-source-id: 7f756c7f3c605e51cf0c27ea37f687913cd48708",11.0,3.0,"BUILD.bazel,aten.bzl,third_party/sleef.BUILD",3.0,1,1,0.946372936,1.0,2962.0,2.0,1285286.0,1469.0,3907.0,0.0,,0.0,1
pytorch,929764ac2ad0d7c26e3defa2d4c4d39f4933a7ef,ebdb32c7497e436c6091b5d68ac2924365eda880,Pieter Noordhuis,pietern@fb.com,Wed Sep 11 13:54:15 2019 -0700,1568210055.0,"Remove global group name tracking for ProcessGroupNCCL (#25905)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25905

Now that we can detect and recover from failures in NCCL we should
allow processes that are started at different times (and perhaps have
had previous NCCL process group instances), to eventually be part of
the same process group. Keeping track of group names in global
variables prevents that, because the processes will be out of sync.

This commit removes the global group name maps and defers
responsibility of isolating access to the same store from multiple
process groups to the store itself. Users can use `c10d::PrefixStore`
to derive new store instances whose keyspace is scoped to some
prefix. Functionally, this is identical to keeping a global map and
using a group name, but also gives more flexibility to the front-end
API to reset state and have processes that have started at different
times to join the same process group.
ghstack-source-id: 89804865

Test Plan: Tests pass.

Differential Revision: D17281416

fbshipit-source-id: eab3b48463a9b0ef24aedeca76e2bb970b9f33ef",48.0,78.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/distributed/distributed_c10d.py,torch/lib/c10d/ProcessGroupNCCL.cpp,torch/lib/c10d/ProcessGroupNCCL.hpp",5.0,8,2,1.462965914,4.0,6401.0,3.0,658037.2,11303.0,31855.83333,0.0,Corrective,1.0,1
pytorch,e06523482a55ae185ef5b555fea242552d272890,ec120fac0c633b719e221ba1efbc8b36fec6947b,Gregory Chanan,gchanan@fb.com,Fri May 26 18:08:43 2017 -0700,1495822123.0,"Add broadcasting support for masked_copy, masked_fill.",175.0,94.0,"test/test_torch.py,tools/cwrap/plugins/Broadcast.py,torch/_tensor_docs.py,torch/csrc/generic/methods/Tensor.cwrap",4.0,8,3,1.302676991,31.0,6750.0,4.0,0.0,895.0,11457.44394,0.0,Feature Addition,0.0,1
pytorch,fef52cc1f84862c31a7c8cfad65b792699e0ca51,ec195129eced539d0466e6baaf6dd372ddb23d4c,Teng Li,tengli@fb.com,Thu Sep 06 19:47:20 2018 -0700,1536263240.0,"Adding setTimeout option in Store (#11265)

Summary:
This will allow users to set customized timeout option for the store.

Tested by my own debug print to make sure that C++ actually used the timeout
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11265

Differential Revision: D9666164

Pulled By: teng-li

fbshipit-source-id: 4eb6441783da106a3fd59b95457e503e83e4640f",70.0,7.0,"test/test_c10d.py,torch/csrc/distributed/c10d/init.cpp,torch/lib/c10d/FileStore.cpp,torch/lib/c10d/FileStore.hpp,torch/lib/c10d/PrefixStore.cpp,torch/lib/c10d/PrefixStore.hpp,torch/lib/c10d/Store.cpp,torch/lib/c10d/Store.hpp,torch/lib/c10d/TCPStore.cpp,torch/lib/c10d/TCPStore.hpp",10.0,7,2,3.137742164,2.0,1859.0,6.0,4665105.8,3898.0,10909.33333,0.0,Corrective,1.0,1
pytorch,774a6f1093b2ed6fe2741dfa7f6400d649343c81,ec2282816956450630327e8f72a19f6deaf9b4cf,Sam Gross,sgross@fb.com,Tue Aug 30 18:55:18 2016 -0700,1472583318.0,Add torch.nn.AvgPool2d,36.0,14.0,"test/common_nn.py,test/test_legacy_nn.py,torch/nn/backends/thnn.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py",5.0,5,2,1.860568863,4.0,1655.0,1.0,525940.0,18.0,325.25,0.0,Feature Addition,0.0,1
pytorch,eb49dabe925b2fa32897dae771f831cb056aef65,ec256ab2f2f7c65c3fa25e16438a467f9959937c,Xiong Wei,xiongw.fnst@cn.fujitsu.com,Fri Nov 20 08:19:27 2020 -0800,1605860367.0,"implement torch.addr using TensorIterator based kernels (#47664)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/47313

This PR implements `torch.addr` function using `TensorIterator` with `cpu_kernel_vec` and `gpu_kernel`.
It helps reduce memory usage, improve performance, and fix the bug when `beta` or `alpha` is a complex number.

Todo
- [x] benchmarking `torch.addr` for the change of this PR, as well as the legacy TH implementation used in PyTorch 1.6.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47664

Reviewed By: zhangguanheng66

Differential Revision: D25059693

Pulled By: ngimel

fbshipit-source-id: 20a90824aa4cb2240e81a9f17a9e2f16ae6e3437",374.0,57.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.h,aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/math_kernel_test.cpp,test/test_linalg.py,torch/_torch_docs.py",8.0,9,3,2.449390536,33.0,24042.0,4.0,197942.0,6952.0,15747.5,0.0,Corrective,1.0,1
pytorch,328b4160688ca9d5611b5a1b3b84eda8184363c0,ec260fe8e9a6adbcb84dd8e933212c7d335db05a,Martin Raison,raison@fb.com,Mon Mar 13 14:25:52 2017 -0700,1489415152.0,add test for dsmm,16.0,0.0,test/test_sparse.py,1.0,1,1,0,16.0,393.0,1.0,0.0,328.0,14999.607,0.0,Feature Addition,0.0,1
pytorch,aabfae0503ebcd3f3ea5fe3403bc0833d3bca158,ec389f512878a2539d8cff35efd72a2a9585443d,Richard Zou,zou3519@users.noreply.github.com,Thu Nov 09 01:20:14 2017 -0500,1510190414.0,"Fix cuda symeig (#3566)

* Fix cuda symeig

* Add symeig test

* Better check for magma",21.0,1.0,"aten/src/THC/generic/THCTensorMathMagma.cu,test/test_cuda.py",2.0,5,2,0.439496987,37.0,1755.0,2.0,276601.0,2095.0,24034.35823,0.0,Corrective,1.0,1
pytorch,950957f8573e3718744add8283f87fb47b6ac42b,ec577300d7d678c044964a8c327e815d8a90ffa2,Peter Bell,peterbell10@live.co.uk,Fri Dec 17 17:56:47 2021 -0800,1639763807.0,"OpInfo: Convert more sample_input_funcs to generators (#69976)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/69976

These are sample functions that already use generators internally, this just moves the `yield` into the sample function itself.

Re-submit of #69257

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D33172953

Pulled By: mruberry

fbshipit-source-id: 7b8bae72df6a225df88a158b7ffa82a71d3c061b",630.0,904.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,15801.0,1.0,58381.0,17840.0,42198.0,0.0,,0.0,1
pytorch,990f4ca76d5bd627a295ce3aa847834d0c33af6a,ec7913afbde30c3b56f2d34d640ad3f06299db29,Mike Ruberry,mruberry@devfair044.maas,Tue Oct 01 00:23:36 2019 -0700,1569889416.0,"Cuts test_torch.py runtime in half by marking four tests as slow (#26789)

Summary:
- Adds slowTest to four tests

On my devfair running test_torch.py takes ~200 seconds with slow tests enabled. Running with the current slowTest annotations takes ~145s. Running with these four additional annotations takes ~64s.

test_sum_dim, for example, takes 30s but was not marked as slow.
test_det_logdet_slogdet takes 17s on CPU and 22s on CUDA for a total of 39s!
test_einsum takes 7s.
test_triu_tril takes 5 seconds on CPU and 9s on CUDA for a total of 14s.

Several of the current slowTests are faster than this. test_cholesky_solve_batched_many_batches, for example, takes a ~3 seconds on CPU and ~4.5 on CUDA, for a total of 7.5s across both devices.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26789

Differential Revision: D17574282

Pulled By: mruberry

fbshipit-source-id: 3e5e505244c09b0ae23bd8c0145828119326719b",6.0,1.0,test/test_torch.py,1.0,1,1,0,40.0,13012.0,1.0,22675.0,11921.0,33527.83333,0.0,Feature Addition,0.0,1
pytorch,fcd567ed1594c18ba072eeaf6917090560176ff9,ec807f2a91845dc4019c926f839ce2a099750c5d,Bram Wasti,bwasti@fb.com,Wed Aug 01 03:43:06 2018 -0700,1533094986.0,"Bail out if netdef has disable_nomnigraph argument

Summary: allow models to override nomnigraph opts

Reviewed By: ajtulloch

Differential Revision: D9035729

fbshipit-source-id: 2b30208263c14ce7039f27c618a3b232bf11ee33",4.0,1.0,caffe2/predictor/predictor.cc,1.0,2,1,0,1.0,189.0,1.0,381487.0,3234.0,8540.333333,0.0,,0.0,1
pytorch,a7eaec6cf232982fdc6548980f5d9a2c84748c2e,ec8e75ea92ae2b5ea73b4aeb3ec7cb39e9f95db9,Sebastian Kaczor,kastianoza@gmail.com,Tue Sep 10 21:59:02 2019 -0700,1568152742.0,"Fix int32 overflow in SummaryOps.cu getBin #25747 (#25748)

Summary:
Fixes issue https://github.com/pytorch/pytorch/issues/25747 by upcasting to int64 before multiplication. Should be good enough for all reasonable nbins
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25748

Differential Revision: D17269111

Pulled By: ezyang

fbshipit-source-id: 484be39080571203264a1bb9898ecf23d1aeafab",9.0,2.0,"aten/src/ATen/native/cuda/SummaryOps.cu,test/test_cuda.py",2.0,6,2,0.945660305,40.0,3413.0,2.0,531319.5,11280.0,31825.33333,0.0,Corrective,1.0,1
pytorch,8f658d537d45c673613c39e88e1801eb36d54a3e,ec9c03c23413589c23adb4e81e0f3d1a26b768e1,Heitor Schueroff,heitorschueroff@fb.com,Tue Jun 29 20:59:46 2021 -0700,1625000386.0,"Implemented torch.cov (#58311)

Summary:
Based from https://github.com/pytorch/pytorch/pull/50466

Adds the initial implementation of `torch.cov` similar to `numpy.cov`. For simplicity, we removed support for many parameters in `numpy.cov` that are either redundant such as `bias`, or have simple workarounds such as `y` and `rowvar`.

cc PandaBoi

closes https://github.com/pytorch/pytorch/issues/19037

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58311

Reviewed By: jbschlosser

Differential Revision: D29431651

Pulled By: heitorschueroff

fbshipit-source-id: 167dea880f534934b145ba94291a9d634c25b01b",263.0,0.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/Correlation.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_torch.py,tools/build_variables.bzl,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",11.0,12,5,2.143140596,45.0,47394.0,11.0,405690.5454545455,13456.0,30471.5,0.0,Feature Addition,0.0,1
pytorch,fcfca9ad62e1881512eaf3298b76dcca3ef8c8bf,eca01eb0a6f7f0f16c7e63a5b2c90ce91b7814a1,Lingyi Liu,lingyiliu@fb.com,Sat Sep 21 18:08:44 2019 -0700,1569089324.0,"quantized average_pool2d and adaptive_avg_pool2d implementation(Revert d17437015) (#26580)

Summary:
In this PR, we tried to fix the windows build issue of  d17437015.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26580

Differential Revision: D17517341

Pulled By: llyfacebook

fbshipit-source-id: db726596aa8f7c992c5a7ddc2781dc3aa0312284",991.0,229.0,"aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py,torch/nn/quantized/functional.py",9.0,13,3,2.173735965,10.0,10480.0,5.0,129455.33333333331,11615.0,32706.33333,0.0,Corrective,1.0,1
pytorch,babd4499783abc699faf36f3a72a9fc491e0e572,eca87f729d071d12ccb31dd2c958a989d8ac17af,Heitor Schueroff,heitorschueroff@fb.com,Fri Aug 27 17:16:02 2021 -0700,1630084562.0,"Added reference tests to ReductionOpInfo (#62900)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/62900

Test Plan: Imported from OSS

Reviewed By: mruberry

Differential Revision: D30408815

Pulled By: heitorschueroff

fbshipit-source-id: 6a1f82ac281920ff7405a42f46ccd796e60af9d6",258.0,61.0,"aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,test/test_reductions.py,torch/testing/_internal/common_methods_invocations.py",3.0,9,3,1.312194325,8.0,12712.0,2.0,760823.0,15004.0,34355.5,0.0,Feature Addition,0.0,1
pytorch,5aa1f769d3614bbe456ce44a1f2ae07275cc6c18,ecd51f8510bb1c593b0613f3dc7caf31dc29e16b,Soumith Chintala,soumith@gmail.com,Tue May 02 19:42:33 2017 -0400,1493754153.0,docs fixes,45.0,32.0,"docs/source/nn.rst,docs/source/sparse.rst,torch/nn/functional.py,torch/nn/modules/instancenorm.py,torch/nn/modules/loss.py",5.0,5,2,1.091888053,27.0,2347.0,4.0,202511.2,700.0,8743.817468,0.0,Corrective,1.0,1
pytorch,6ae0576e1c1a1b987a58ea88547c09e4b4f4425e,ecd5de0f3633f8ecf5e68949183a1c7aaf59284e,Tongzhou Wang,SsnL@users.noreply.github.com,Wed Mar 28 22:44:29 2018 -0400,1522277069.0,"[fft][2 of 3] Forward for fft methods (#5856)

* implement fft ifft rfft irfft

* add tests for fft ifft rfft irfft",958.0,50.0,"aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/SpectralOpsUtils.h,aten/src/ATen/native/cuda/SpectralOps.cu,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/native_functions.yaml,test/test_cuda.py,test/test_torch.py",7.0,7,2,2.235336109,38.0,8556.0,3.0,419845.25,540.0,2473.5,0.0,Feature Addition,0.0,1
pytorch,1cddb27f397168755d45099a7d044e1da0bffa5b,ecf3ca00d85567a81b8bee29982803945a581a4c,Michael Suo,suo@fb.com,Tue Feb 23 21:33:22 2021 -0800,1614116002.0,"[fx] Separate globals assignment from code generation (#51974)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51974

Right now, when an FX `Graph` references an external object, we will emit
code like:

    import foo
    def forward(input: foo.bar.baz):
        ...

This is problematic in a world with `torch.package`, since then name
`foo.bar.baz` may reference a name from any number of packages.

This PR lays the groundwork for FX-package integration by separating the
resolution of external references from the genration of the function
code.

When generating a Graph's Python source, we keep track of all external
references and assign them unique names. At the end, we have a
dictionary mapping names -> actual objects. This becomes the `globals`
namespace we pass to `exec` when installing the forward function in a
`GraphModule`. This is nice because we can always be sure that `exec` is
seeing the same objects that were referenced from the `Graph`, no import
statements needed.

At serialization time, we use a `ModuleEnv` to resolve the globals dict
to a set of import statements that can be run to reprodce the `global`
namespace. This is only used on serialiation/deserialization, and those
functions are expected to check that the import statements are producing
the correct results.

Concretely, the code above will now look like:

    from foo.bar import baz as foo_bar_baz
    def forward(input: foo_bar_baz):
        ...

Test Plan: Imported from OSS

Reviewed By: jamesr66a

Differential Revision: D26340593

Pulled By: suo

fbshipit-source-id: fe247f75205d0a03fd067bdd0f95491e8edf1436",335.0,87.0,"test/quantization/test_numeric_suite_fx.py,test/test_fx.py,test/test_fx_experimental.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/fx/node.py",6.0,4,2,1.306851795,1.0,5356.0,6.0,861717.6666666666,9138.0,20401.5,0.0,Corrective,0.0,1
pytorch,50ae5c9141fc752c80e7fe88a123ea77ee0265f9,ed46b9670ebafa1c6bf7d078dcf5687109fee6ae,samdow,samdow@fb.com,Tue Sep 06 20:41:00 2022 -0400,1662496860.0,"add randomness kwarg to jacfwd (#84220)

From https://github.com/pytorch/functorch/issues/1010, if a user runs jacfwd with a function that uses randomness, it will fail since the default behavior for vmap is error. This lets the user specify the randomness behavior to jacfwd too since it is doing vmap(jvp(forward)). This is less likely to show up in jacrev since that only vmaps over the backwards pass
Pull Request resolved: https://github.com/pytorch/pytorch/pull/84220
Approved by: https://github.com/zou3519",19.0,4.0,"functorch/functorch/_src/eager_transforms.py,functorch/test/test_vmap.py",2.0,4,1,0.886540893,2.0,5989.0,2.0,34105.0,7137.0,16670.5,0.0,Feature Addition,0.0,1
pytorch,e1c799ff828488142a4843f0033a1f0955a6ce5e,ed589dd8e47bbd2ea625b44234ec773d77fb2b63,Richard Zou,zou3519@gmail.com,Fri Dec 16 15:38:57 2022 -0800,1671205137.0,"[functorch] add composition-of-3-transform tests for autograd_function (#90962)

This PR adds the following OpInfo tests:
- vmap x vjp x vmap
- vjp x vmap x vmap
- vjp x vjp x vmap

These OpInfo tests only run for the autograd_function_db. In general,
testing composition of two transforms is sufficient to convince
ourselves that functorch works on a given operator.

The autograd.Function testing (especially the upcoming
generate_vmap_rule) didn't feel rigorous enough to me, so I added these
additional tests to convince myself.

Test Plan:
- new tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/90962
Approved by: https://github.com/samdow, https://github.com/soulitzer",138.0,9.0,"test/functorch/common_utils.py,test/functorch/test_ops.py",2.0,2,1,0.453716339,1.0,2377.0,2.0,11285.0,10661.0,24337.0,0.0,Feature Addition,0.0,1
pytorch,35a5df8c9405f22cc512cd3b49120f14ee03ee45,edc28676ef0322977c61d3114798a15b65b09410,Mike Ruberry,mruberry@devfair044.maas,Thu Oct 17 02:46:32 2019 -0700,1571280392.0,"Adds @overridePrecision decorator (#28131)

Summary:
Adds the overridePrecision decorator, which allows device generic tests to specify per-dtype precision overrides.

Precision is overridden on the test class instance itself, and so is thread-local (so that running multiple tests in parallel will not conflict). It can be accessed directly from a test with self.precision, as before.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28131

Differential Revision: D17969774

Pulled By: mruberry

fbshipit-source-id: c4e0b71afac6bdc7cbf4e799f3054922de764820",83.0,30.0,"test/common_device_type.py,test/test_torch.py",2.0,1,1,0.997230114,40.0,14473.0,2.0,372626.0,12355.0,34492.83333,0.0,Feature Addition,0.0,1
pytorch,a2a4144256613b8f086f71bfe2c216a79e811ea2,edd2507c7399bc890b2d1b3a9a1f76e5dd290c81,Li-Huai (Allan) Lin,qqaatw@gmail.com,Wed Apr 19 12:10:09 2023 +0800,1681906209.0,"[functorch] Prevent using for-loop for out-of-place index_fill batch rule (#99229)

A follow-up PR for https://github.com/pytorch/pytorch/pull/91364#discussion_r1060723192

Pull Request resolved: https://github.com/pytorch/pytorch/pull/99229
Approved by: https://github.com/kshitij12345",72.0,53.0,"aten/src/ATen/functorch/BatchRulesScatterOps.cpp,test/functorch/test_vmap.py",2.0,6,2,0.767404417,1.0,6371.0,2.0,884253.0,14857.0,33733.0,0.0,Preventative,0.0,1
pytorch,76fc690522024d978176b74a73e0222ac4d062de,eddc2370ec33938adbd9a3136852c3ab19e51a78,kshitij12345,kshitijkalambarkar@gmail.com,Thu Sep 08 13:35:19 2022 +0000,1662644119.0,"[functorch] vmapvjpvjp (re-enable test with skips and xfails) (#83999)

Enable `vmapvjpvjp` test and add relevant skips and xfails.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83999
Approved by: https://github.com/zou3519",102.0,8.0,functorch/test/test_ops.py,1.0,2,1,0,2.0,1422.0,1.0,10242.0,7166.0,16712.5,0.0,Feature Addition,0.0,1
pytorch,33f05ca58fcba6b06d4b58033cf14c2ebe3ec709,edf79c4be75ed049d214891b5e0c661917111cb3,Horace He,horacehe2007@yahoo.com,Sat Sep 18 04:20:33 2021 -0700,1631938833.0,[functorch] updated opinfo db,21.0,26.0,"functorch/test/functorch_additional_op_db.py,functorch/test/functorch_lagging_op_db.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",5.0,2,1,1.770329848,1.0,4709.0,2.0,0.0,352.0,528.5,0.0,,0.0,1
pytorch,49f87320babe6fc6f6429a331dd9d28f77db995e,ee00a8049a21a27f126ac820859e0643c70fcc8b,Ethan Steinberg,ethan.steinberg@gmail.com,Sun Apr 29 20:48:11 2018 -0700,1525034891.0,"Add max pooling support to EmbeddingBag (#5725)

* Add max mode support to EmbeddingBag

* Lint fix

* Fix compilation issue on other platforms

* Rebase + don't waste memory when not in max mode

* Oops, missed a spot

* Fix whitespace from merge

* less precision

* Lower precision to avoid spurious failures

* Minor typo

* Switch to size()",341.0,138.0,"aten/src/ATen/native/EmbeddingBag.cpp,aten/src/ATen/native/cuda/EmbeddingBag.cu,aten/src/ATen/native/native_functions.yaml,test/test_nn.py,third_party/onnx,tools/autograd/derivatives.yaml,torch/nn/functional.py,torch/nn/modules/sparse.py",8.0,12,5,1.647389978,42.0,12185.0,7.0,871639.75,2605.0,24984.35823,0.0,Corrective,1.0,1
pytorch,7cd6cc17afaa88b3342dba0814a35c7c6686befd,ee10e7457f492f1d6c61fdde76f1c2b1ef47380d,Varun Agrawal,varagrawal@gmail.com,Tue Jun 13 22:04:06 2017 -0400,1497391446.0,Corrected erroneous docstring for MultiLabelSoftMarginLoss,2.0,2.0,torch/nn/modules/loss.py,1.0,3,1,0,29.0,523.0,1.0,1156.0,1036.0,12557.04911,0.0,Corrective,0.0,1
pytorch,0391bbb376417b29ca6e40467489795991a8fdfe,ee14cf94388735e2552f307cd90775562b14ef28,Sam Gross,colesbury@gmail.com,Sat Oct 15 22:38:26 2016 -0400,1476571106.0,"Add support for pinned memory: (#127)

torch.Storage/Tensor.pin_memory()
 torch.Storage/Tensor.is_pinned()",165.0,12.0,"test/test_cuda.py,test/test_torch.py,torch/__init__.py,torch/csrc/Module.cpp,torch/csrc/cuda/Module.cpp,torch/csrc/generic/Storage.cpp,torch/csrc/generic/StorageMethods.cpp,torch/cuda/__init__.py,torch/storage.py,torch/tensor.py",10.0,6,2,3.086777677,10.0,5510.0,4.0,597164.4,186.0,1106.450488,0.0,Feature Addition,0.0,1
pytorch,9e1cf99a160412396b0d532a50ec5a38a8304a36,ee2c79d699603facd2df415c0209da765bb16472,Igor Fedan,ifedan@fb.com,Sat Sep 28 22:07:12 2019 -0700,1569708432.0,"Migrate le/gt/ge/eq/ne from the TH to Aten. Added support of type promotion. (#27017)

Summary:
https://github.com/pytorch/pytorch/pull/26981
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27017

Differential Revision: D17651454

Pulled By: ifedan

fbshipit-source-id: c6313caa11598a0ef160e1c6d2f3c33d03ce80c5",337.0,1220.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/LegacyDefinitions.cpp,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryOpsKernel.cu,aten/src/ATen/native/cuda/LegacyDefinitions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/CMakeLists.txt,aten/src/THC/THCTensorMath.h,aten/src/THC/THCTensorMathCompare.cuh,aten/src/THC/THCTensorMathCompareT.cuh,aten/src/THC/generated/THCTensorMathCompareBool.cu,aten/src/THC/generated/THCTensorMathCompareByte.cu,aten/src/THC/generated/THCTensorMathCompareChar.cu,aten/src/THC/generated/THCTensorMathCompareDouble.cu,aten/src/THC/generated/THCTensorMathCompareFloat.cu,aten/src/THC/generated/THCTensorMathCompareHalf.cu,aten/src/THC/generated/THCTensorMathCompareInt.cu,aten/src/THC/generated/THCTensorMathCompareLong.cu,aten/src/THC/generated/THCTensorMathCompareShort.cu,aten/src/THC/generated/THCTensorMathCompareTBool.cu,aten/src/THC/generated/THCTensorMathCompareTByte.cu,aten/src/THC/generated/THCTensorMathCompareTChar.cu,aten/src/THC/generated/THCTensorMathCompareTDouble.cu,aten/src/THC/generated/THCTensorMathCompareTFloat.cu,aten/src/THC/generated/THCTensorMathCompareTHalf.cu,aten/src/THC/generated/THCTensorMathCompareTInt.cu,aten/src/THC/generated/THCTensorMathCompareTLong.cu,aten/src/THC/generated/THCTensorMathCompareTShort.cu,aten/src/THC/generic/THCTensorMathCompare.cu,aten/src/THC/generic/THCTensorMathCompare.h,aten/src/THC/generic/THCTensorMathCompareT.cu,aten/src/THC/generic/THCTensorMathCompareT.h,test/test_nn.py,test/test_torch.py,test/test_type_promotion.py",39.0,13,2,3.988570963,44.0,41455.0,1.0,64755.0,11890.0,33482.83333,0.0,Feature Addition,0.0,1
pytorch,52a8031e8c2a6007449699417648d42ac477b16e,ee3ea31f1230e69f79f322cc7cbbb5e769ded758,kshitij12345,kshitijkalambarkar@gmail.com,Sun May 23 22:44:35 2021 -0700,1621809875.0,"OpInfo: split, split_with_sizes (#58184)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58184

Reviewed By: ngimel

Differential Revision: D28627271

Pulled By: mruberry

fbshipit-source-id: e6c0d2b005904ddebc9dab76685403530a6f6519",54.0,12.0,"test/test_torch.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.266764988,43.0,15107.0,2.0,72485.5,12388.0,28055.5,0.0,,0.0,1
pytorch,30ec12fdd5ec6cc1cfc86ddfe0b4864d3d785faa,ee4c77c59f8c110b6843bf268c34f00c472b7791,Adam Paszke,adam.paszke@gmail.com,Thu Jan 19 22:28:49 2017 +0100,1484864929.0,"Docs improvements (#512)

* Always compile .numpy() for all types

* Add torch.nn.functional docs and hidden headers

* Use sphinx to generate torchvision docs

* Remove unused import in ffi utils",506.0,303.0,"docs/source/nn.rst,docs/source/torchvision/transforms.rst,docs/source/torchvision/utils.rst,torch/csrc/generic/methods/TensorSerialization.cwrap,torch/nn/functional.py,torch/utils/ffi/__init__.py",6.0,10,2,1.545613718,19.0,1354.0,5.0,193627.5,367.0,4887.476424,0.0,Feature Addition,0.0,1
pytorch,a4dddef366b8229db4040003971639af7eda46d7,ee6a824a61384a4b1b3df26b0fbcfb2d584d6c7f,Samantha Andow,samdow@fb.com,Tue Apr 05 15:10:10 2022 -0400,1649171410.0,[functorch] add nonzero batch dim tests (pytorch/functorch#646),70.0,40.0,functorch/test/test_vmap.py,1.0,2,1,0,1.0,4048.0,1.0,0.0,930.0,1290.0,0.0,Feature Addition,0.0,1
pytorch,319b08be59b344e19b844ba90ac06489c845e322,ee79413b6a3f895227ff6b5b6e562f2b0bcc130c,kshitij12345,kshitijkalambarkar@gmail.com,Fri May 07 08:19:27 2021 -0700,1620375567.0,"[testing] change unaryufunc default dtypes (#57616)

Summary:
Reference: https://github.com/pytorch/pytorch/pull/56646#pullrequestreview-644839124

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57616

Reviewed By: albanD

Differential Revision: D28249129

Pulled By: mruberry

fbshipit-source-id: 2cfc837fd49100d2b1b2a09d9ca6db93e089e099",26.0,105.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,6515.0,1.0,26844.0,11763.0,26543.5,0.0,,0.0,1
pytorch,d9219d294411fd997dd5e0913753483a0e709159,ee955b8bb9c1e352180a74351148bf1b015acd3a,Edward Z. Yang,ezyang@fb.com,Wed Apr 20 02:56:43 2022 -0700,1650423403.0,"Cannibalize noarch CI job into crossref CI job

crossref is a new strategy for performing tests when you want
to run a normal PyTorch API call, separately run some variation of
the API call (e.g., same thing but all the arguments are meta tensors)
and then cross-reference the results to see that they are consistent.
Any logic you add to CrossRefMode will get run on *every* PyTorch API
call that is called in the course of PyTorch's test suite.  This can
be a good choice for correctness testing if OpInfo testing is not
exhaustive enough.

For now, the crossref test doesn't do anything except verify that
we can validly push a mode onto the torch function mode stack for all
functions.

Signed-off-by: Edward Z. Yang <ezyangfb.com>

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75988

Approved by: https://github.com/seemethere",59.0,29.0,".circleci/cimodel/data/pytorch_build_data.py,.circleci/cimodel/data/pytorch_build_definitions.py,.github/workflows/pull.yml,.jenkins/pytorch/macos-test.sh,.jenkins/pytorch/test.sh,.jenkins/pytorch/win-test.sh,test/jit/test_tracer.py,test/test_jit.py,test/test_overrides.py,test/test_tensorboard.py,test/test_torch.py,torch/autograd/__init__.py,torch/overrides.py,torch/testing/_internal/common_utils.py",14.0,13,5,3.078104239,48.0,36919.0,13.0,1972198.642857143,2460.0,5805.5,0.0,Corrective,0.0,1
pytorch,ccf4dc15250534e6906aaca696f94b430a4da443,ee98e7a82e1c7461561d7d90ab081a993d3f8ee2,Fritz Obermeyer,fritz.obermeyer@gmail.com,Mon Dec 18 18:11:37 2017 -0800,1513620697.0,Implement Dirichlet and Beta distributions (#4117),434.0,5.0,"aten/src/TH/generic/THTensorMath.c,aten/src/TH/generic/THTensorMath.h,docs/source/distributions.rst,test/test_distributions.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorRandom.cwrap,torch/distributions/__init__.py,torch/distributions/beta.py,torch/distributions/dirichlet.py",9.0,12,4,2.293160669,38.0,5572.0,6.0,717216.1428571428,851.0,6607.672317,0.0,,0.0,1
pytorch,e66ea56bb39c502174910d637c340811b3ac2b5a,eec0420eb3c246a8afe2c5c62b718c7770fe9058,Adam Paszke,adam.paszke@gmail.com,Sat Sep 24 00:50:27 2016 -0700,1474678227.0,Initialize nn modules' parameters with a default tensor type,23.0,23.0,"torch/nn/modules/conv.py,torch/nn/modules/linear.py,torch/nn/modules/sparse.py",3.0,3,1,0.807670206,5.0,409.0,1.0,419704.0,188.0,3850.532937,0.0,,0.0,1
pytorch,cf57f73c11f9386496fa3771a36d01239184a60d,eee3e929362fa11f09d2258c16dff46549ebec58,Iurii Zdebskyi,iuriiz@fb.com,Fri Aug 16 22:44:35 2019 -0700,1565995475.0,"Enabled torch.mm and torch.mv for bfloat16

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/24224

Test Plan: Imported from OSS

Differential Revision: D16779996

Pulled By: izdeby

fbshipit-source-id: c859d8945a564edfa3f8a1430f140ae30d484d19",597.0,375.0,"aten/src/ATen/Declarations.cwrap,aten/src/TH/THBlas.cpp,aten/src/TH/THBlas.h,aten/src/TH/THTensorMath.cpp,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/TH/generic/THTensorMath.h,c10/util/BFloat16-inl.h,test/test_torch.py",9.0,8,3,1.576517183,41.0,17872.0,6.0,6152281.666666667,10717.0,30380.83333,0.0,,0.0,1
pytorch,4fa08535ed8c63f05c7e33ca6faa255c0bb5e93b,eee4f1ee4203d0b09f05efe86a0886d94ec857c5,bddppq,bai@in.tum.de,Thu Mar 15 18:50:41 2018 -0700,1521139841.0,"Add symbolic functions for cumsum and embedding_bag (#5786)

* Add symbolic functions for unsqueeze, cumsum and embedding_bag

* unsqueeze already exists",21.0,0.0,torch/onnx/symbolic.py,1.0,2,1,0,8.0,779.0,1.0,153002.0,496.0,2381.5,0.0,Feature Addition,0.0,1
pytorch,100ad48ced6339f644bc260c1192fdd6170a6c02,eee58f8284e4ded2a0d1b5912fef423b36d6bb46,vishwakftw,vishwaks@cs.cmu.edu,Wed Sep 11 21:28:08 2019 -0700,1568237288.0,"Refactor torch.*solve tests (#25733)

Summary:
Changelog:
- De-duplicate the code in tests for torch.solve, torch.cholesky_solve, torch.triangular_solve
- Skip tests explicitly if requirements aren't met for e.g., if NumPy / SciPy aren't available in the environment
- Add generic helpers for these tests in test/common_utils.py
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25733

Test Plan:
- All tests should pass to confirm that the change is not erroneous

Clears one point specified in the discussion in https://github.com/pytorch/pytorch/issues/24333.

Differential Revision: D17315330

Pulled By: zou3519

fbshipit-source-id: c72a793e89af7e2cdb163521816d56747fd70a0e",181.0,336.0,"test/common_utils.py,test/test_cuda.py,test/test_torch.py",3.0,1,1,0.785473838,41.0,17907.0,3.0,32720.33333333333,11328.0,31905.33333,0.0,Feature Addition,0.0,1
pytorch,bf603299b6a61d05744dbee8a4ee642bbac2979f,ef1459020958df42da457650d716545e21f3528a,anderspapitto,anderspapitto@gmail.com,Wed Feb 07 22:11:33 2018 -0800,1518041493.0,"Support calling pack_padedd_sequence with a Variable lengths (#5113)

This was accidentally lost while addressing review comments on
https://github.com/pytorch/pytorch/pull/4695

pack_padded_sequence may be called either with a list or with a
Variable. If called with a list we convert to Variable internally.

I added to test_nn to test the new codepath. The bug was also caught
by the onnx-fb-universe tests (which rely on passing in Variable).",10.0,3.0,"test/test_nn.py,torch/nn/_functions/packing.py,torch/nn/utils/rnn.py",3.0,5,2,1.52623491,37.0,6046.0,2.0,50487.0,431.0,2265.0,0.0,Corrective,1.0,1
pytorch,2aeb16c13a365779faed2d99a8f4fb9f0a01f5f9,ef40757de3616d4f7167d25c4b538a576508bb8b,krshrimali,kushashwaravishrimali@gmail.com,Mon May 31 04:46:11 2021 -0700,1622436371.0,"OpInfo: `zero_` (#58731)

Summary:
See https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58731

Reviewed By: ngimel

Differential Revision: D28784083

Pulled By: mruberry

fbshipit-source-id: f06de8045afd3728b1fedc014c091d8fd1955a9f",23.0,2.0,"test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.48217919,2.0,11750.0,2.0,7796.333333333333,12578.0,28531.0,0.0,,0.0,1
pytorch,10023eeb65d7bc6c71f49bf0ce7eaf5b58d52fd7,ef41201d4acf6b7601577b08b90e36f749db84b8,shubhambhokare1,shubhambhokare@gmail.com,Wed Apr 06 20:13:45 2022 +0000,1649276025.0,"[ONNX] Add bucketize symbolic

Add support for torch.bucketize
Pull Request resolved: https://github.com/pytorch/pytorch/pull/74856
Approved by: https://github.com/garymm",41.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.839004061,4.0,14548.0,2.0,1818310.5,2032.0,4829.5,0.0,Feature Addition,0.0,1
pytorch,ffd39f4c9fa3ffec186bab6ba9cc83257a3a3a45,ef70db09ddf95942650038ba1559cb3dc90921b2,Hugh Perkins,hughperkins@gmail.com,Fri Nov 24 10:14:37 2017 +0000,1511518477.0,fix some more mathjax (#3352),24.0,20.0,"torch/nn/modules/activation.py,torch/nn/modules/rnn.py",2.0,3,1,0.945660305,37.0,1445.0,2.0,913923.5,812.0,6542.172317,0.0,Corrective,1.0,1
pytorch,eed19a0f38a81015ca50dd25e997b1c6e223d46b,ef71046f9c73e3dbcf6df816ddab49500a7cbc5d,George Qi,georgeqi94@gmail.com,Tue Mar 29 17:44:47 2022 +0000,1648575887.0,"masked std

Pull Request resolved: https://github.com/pytorch/pytorch/pull/74523

Approved by: https://github.com/cpuhrsch",184.0,34.0,"test/test_masked.py,torch/_masked/__init__.py,torch/_masked/_docs.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,2,1.639985843,4.0,18353.0,3.0,1110823.5,1814.0,4300.5,0.0,,0.0,1
pytorch,9b97313cc7b1a081d77a8b7e36bac799a1389985,ef9f56eb0b86a2224652f5c5ac1f82dcb4f444c6,Mike Ruberry,mruberry@devfair044.h1.fair,Wed May 04 05:38:33 2022 +0000,1651642713.0,"[primTorch] slice and transpose & etc.

This PR...

Adds the following prims:
- slice
- slice_in_dim
- transpose

Adds the following refs:
- cat
- permute
- transpose
- swap_axes (alias for transpose)
- tensor_split

Makes the following test improvements:
- adds reference inputs for torch.permute
- adds a NumPy reference for torch.permute
- adds reference inputs for torch.cat

Fixes the following bugs:
- adds support for scalars to the min and max prims

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76727
Approved by: https://github.com/ngimel",600.0,58.0,"torch/_prims/__init__.py,torch/_prims/utils.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",4.0,5,1,1.794777934,5.0,20915.0,1.0,10113.0,2839.0,6809.0,0.0,Corrective,1.0,1
pytorch,016af4ebf77ea12164a539080025f6685d7d3e40,efb611a134656aca1b478a08e8bbf186a91ae478,Edward Z. Yang,ezyang@fb.com,Thu Nov 09 08:14:25 2017 +0800,1510215265.0,"Fix misnamed generator argument.

Signed-off-by: Edward Z. Yang <ezyang@fb.com>",1.0,1.0,tools/autograd/derivatives.yaml,1.0,2,1,0,10.0,698.0,1.0,0.0,358.0,2172.5,0.0,Corrective,1.0,1
pytorch,76070fe73c5cce61cb9554990079594f83384629,efc0f6784aa94d75f2b68e2f42a54253c9729d72,Thomas Viehmann,tv@beamnet.de,Wed Sep 12 14:05:06 2018 -0700,1536761106.0,"Move some bmm/baddbmm to ATen (#11292)

Summary:
- Incorporates MKL addition by mingfeima  Thank you! (but all errors are my own)
- Native CPU implementation: defer to matrix multiplication for
  small batches and parallelize over batch dimension for large
  batches.
- Add bmm test for CUDA just to be sure.

This is a partial fix for #10661, getting down to a factor ~5.
Considerable overhead is incurred for the setup in einsum. It might
be more efficient to eventually define an optimized contraction
functions for arbitrary and several dimensions.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/11292

Differential Revision: D9784941

Pulled By: ezyang

fbshipit-source-id: f6dded2c6f5e8f0461fb38f31f9a824992a58358",311.0,20.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/mkl/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,test/test_cuda.py",6.0,7,2,1.967658828,42.0,7930.0,4.0,168722.5,4041.0,11281.83333,0.0,Corrective,1.0,1
pytorch,34075e2c8b0c251b8effc8e09b04587de2c5b2b6,f0054e1a6e04feb2a1a791d5c02da17dadf6d904,BowenBao,bowbao@microsoft.com,Wed Jul 21 22:00:36 2021 -0700,1626904836.0,"[ONNX] Update expand_as for dynamic shape (#61084) (#61559)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/61559

Update expand_as for dynamic shape

Test Plan: Imported from OSS

Reviewed By: nikithamalgifb

Differential Revision: D29767990

Pulled By: SplitInfinity

fbshipit-source-id: 3f1e3f68fd17c5ffbd4a50fccff224fd9d6c84fb

Co-authored-by: Negin Raoof <neginmr@utexas.edu>",36.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.852405179,3.0,12673.0,1.0,2.0,14025.0,31567.0,0.0,,0.0,1
pytorch,"e48db02e10d4632bc650e4a83157d2f5bfc4f2dc,66a20e5c328836c1eb720cf4e2eb916366aae487",f00a5d2f541f4ce30abbd3bf8a2218a638ad526b,soumith,soumith@fb.com,Fri Apr 07 20:47:25 2017 -0700,1491598045.0,Merge commit '66a20e5c328836c1eb720cf4e2eb916366aae487',1.0,0.0,torch/lib/THCUNN/CMakeLists.txt,1.0,3,1,0,28.0,81.0,1.0,15843.0,296.0,26120.66274,0.0,,0.0,1
pytorch,f43e06712829ce24552d73e2442ef31385559729,f02ae65727d40e66839bb0dc0d8de49b2c1a6f9c,avmgithub,mendoza1@us.ibm.com,Wed May 09 15:45:30 2018 -0500,1525880730.0,skip test_utils.TestFFI.test_cpu for ppc64le due to incompatible exception handling (#7422),4.0,1.0,"test/common.py,test/test_utils.py",2.0,1,1,0.970950594,38.0,1195.0,2.0,1302075.0,631.0,3579.5,0.0,,0.0,1
pytorch,8593c6f4f74b9a87b4700deef480f1905b81577f,f033dd60cdf4751cda57e3747d003cc2c6e93e2b,Vishwak Srinivasan,cs15btech11043@iith.ac.in,Sat Jan 20 20:49:09 2018 +0500,1516481349.0,Implementation of the Fisher-Snedecor Distribution (#4706),139.0,9.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/fishersnedecor.py",4.0,5,3,1.276253601,8.0,2170.0,2.0,57796.0,929.0,6752.172317,0.0,,0.0,1
pytorch,e735395fc682cd59a44d04d52625a351b5374e6e,f050b16dd95b2bcce9853882fd3fb07a6fd80378,Pritam Damania,pritam.damania@fb.com,Thu Jan 23 05:05:28 2020 -0800,1579755928.0,"Move pytorch distributed tests to separate folder for contbuild. (#30445)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/30445

Create distributed and rpc directories under caffe/test for better management
of unit tests.

Differential Revision: D18702786

fbshipit-source-id: e9daeed0cfb846ef68806f6decfcb57c0e0e3606",19685.0,19672.0,".jenkins/pytorch/multigpu-test.sh,.jenkins/pytorch/test.sh,.jenkins/pytorch/win-test-helpers/test_python_all_except_nn.bat,.jenkins/pytorch/win-test-helpers/test_python_nn.bat,setup.py,test/common_cuda.py,test/common_device_type.py,test/common_distributed.py,test/common_methods_invocations.py,test/common_nn.py,test/common_quantization.py,test/common_quantized.py,test/common_utils.py,test/data/network1.py,test/data/network2.py,test/dist_autograd_test.py,test/dist_optimizer_test.py,test/dist_utils.py,test/distributed/rpc/test_dist_autograd_spawn.py,test/distributed/rpc/test_dist_optimizer_spawn.py,test/distributed/rpc/test_rpc_spawn.py,test/distributed/test_c10d.py,test/distributed/test_c10d_spawn.py,test/distributed/test_data_parallel.py,test/distributed/test_distributed.py,test/distributed/test_nccl.py,test/expect/__init__.py,test/expecttest.py,test/hypothesis_utils.py,test/jit/test_async.py,test/jit/test_autodiff_subgraph_slicing.py,test/jit/test_builtins.py,test/jit/test_class_type.py,test/jit/test_custom_operators.py,test/jit/test_data_parallel.py,test/jit/test_export_modes.py,test/jit/test_list_dict.py,test/jit/test_logging.py,test/jit/test_models.py,test/jit/test_module_interface.py,test/jit/test_recursive_script.py,test/jit/test_type_sharing.py,test/jit/test_unsupported_ops.py,test/jit_utils.py,test/onnx/export_onnx_tests_generator.py,test/onnx/test_operators.py,test/onnx/test_pytorch_common.py,test/rpc_agent_test_fixture.py,test/rpc_test.py,test/run_test.py,test/test_autograd.py,test/test_c10d.py,test/test_c10d_spawn.py,test/test_cpp_api_parity.py,test/test_cpp_extensions.py,test/test_cuda.py,test/test_cuda_primary_ctx.py,test/test_data_parallel.py,test/test_dataloader.py,test/test_dist_autograd_spawn.py,test/test_dist_optimizer_spawn.py,test/test_distributed.py,test/test_distributions.py,test/test_expecttest.py,test/test_fake_quant.py,test/test_function_schema.py,test/test_indexing.py,test/test_jit.py,test/test_jit_disabled.py,test/test_jit_fuser.py,test/test_jit_py3.py,test/test_logging.py,test/test_mkldnn.py,test/test_module/__init__.py,test/test_module/future_div.py,test/test_module/no_future_div.py,test/test_multiprocessing.py,test/test_multiprocessing_spawn.py,test/test_namedtensor.py,test/test_nccl.py,test/test_nn.py,test/test_numba_integration.py,test/test_optim.py,test/test_overrides.py,test/test_qat.py,test/test_quantization.py,test/test_quantized.py,test/test_quantized_models.py,test/test_quantized_nn_mods.py,test/test_quantized_tensor.py,test/test_rpc_spawn.py,test/test_sparse.py,test/test_tensorboard.py,test/test_throughput_benchmark.py,test/test_torch.py,test/test_type_hints.py,test/test_type_info.py,test/test_type_promotion.py,test/test_utils.py,torch/testing/_internal/__init__.py,torch/testing/_internal/common_cuda.py,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_distributed.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_nn.py,torch/testing/_internal/common_quantization.py,torch/testing/_internal/common_quantized.py,torch/testing/_internal/common_utils.py,torch/testing/_internal/data/__init__.py,torch/testing/_internal/data/network1.py,torch/testing/_internal/data/network2.py,torch/testing/_internal/dist_utils.py,torch/testing/_internal/distributed/__init__.py,torch/testing/_internal/distributed/rpc/__init__.py,torch/testing/_internal/distributed/rpc/dist_autograd_test.py,torch/testing/_internal/distributed/rpc/dist_optimizer_test.py,torch/testing/_internal/distributed/rpc/rpc_agent_test_fixture.py,torch/testing/_internal/distributed/rpc/rpc_test.py,torch/testing/_internal/expecttest.py,torch/testing/_internal/hypothesis_utils.py,torch/testing/_internal/jit_utils.py,torch/testing/_internal/test_module/__init__.py,torch/testing/_internal/test_module/future_div.py,torch/testing/_internal/test_module/no_future_div.py",124.0,18,3,4.714046521,50.0,105101.0,69.0,8021971.211111112,14348.0,39022.33333,0.0,,0.0,1
pytorch,fa72d9a37922b12433870f7608f3aab5a04ff9e3,f05d5bec48d16754913947fa64c31b6fa2684e33,Edward Yang,ezyang@fb.com,Thu Jun 03 17:47:19 2021 -0700,1622742439.0,"Preserve PyObject even when it goes dead (#56017)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/56017

Fixes #55686

This patch is seemingly straightforward but some of the changes are very
subtle.  For the general algorithmic approach, please first read the
quoted issue.  Based on the algorithm, there are some fairly
straightforward changes:

- New boolean on TensorImpl tracking if we own the pyobj or not
- PythonHooks virtual interface for requesting deallocation of pyobj
  when TensorImpl is being released and we own its pyobj, and
  implementation of the hooks in python_tensor.cpp
- Modification of THPVariable to MaybeOwned its C++ tensor, directly
  using swolchok's nice new class

And then, there is python_variable.cpp.  Some of the changes follow the
general algorithmic approach:

- THPVariable_NewWithVar is simply adjusted to handle MaybeOwned and
  initializes as owend (like before)
- THPVariable_Wrap adds the logic for reverting ownership back to
  PyObject when we take out an owning reference to the Python object
- THPVariable_dealloc attempts to resurrect the Python object if
  the C++ tensor is live, and otherwise does the same old implementation
  as before
- THPVariable_tryResurrect implements the resurrection logic.  It is
  modeled after CPython code so read the cited logic and see if
  it is faithfully replicated
- THPVariable_clear is slightly updated for MaybeOwned and also to
  preserve the invariant that if owns_pyobj, then pyobj_ is not null.
  This change is slightly dodgy: the previous implementation has a
  comment mentioning that the pyobj nulling is required to ensure we
  don't try to reuse the dead pyobj.  I don't think, in this new world,
  this is possible, because the invariant says that the pyobj only
  dies if the C++ object is dead too.  But I still unset the field
  for safety.

And then... there is THPVariableMetaType.  colesbury explained in the
issue why this is necessary: when destructing an object in Python, you
start off by running the tp_dealloc of the subclass before moving up
to the parent class (much in the same way C++ destructors work).  The
deallocation process for a vanilla Python-defined class does irreparable
harm to the PyObject instance (e.g., the finalizers get run) making it
no longer valid attempt to resurrect later in the tp_dealloc chain.
(BTW, the fact that objects can resurrect but in an invalid state is
one of the reasons why it's so frickin' hard to write correct __del__
implementations).  So we need to make sure that we actually override
the tp_dealloc of the bottom most *subclass* of Tensor to make sure
we attempt a resurrection before we start finalizing.  To do this,
we need to define a metaclass for Tensor that can override tp_dealloc
whenever we create a new subclass of Tensor.  By the way, it was totally
not documented how to create metaclasses in the C++ API, and it took
a good bit of trial error to figure it out (and the answer is now
immortalized in https://stackoverflow.com/q/67077317/23845 -- the things
that I got wrong in earlier versions of the PR included setting
tp_basicsize incorrectly, incorrectly setting Py_TPFLAGS_HAVE_GC on
the metaclass--you want to leave it unset so that it inherits, and
determining that tp_init is what actually gets called when you construct
a class, not tp_call as another not-to-be-named StackOverflow question
suggests).

Aside: Ordinarily, adding a metaclass to a class is a user visible
change, as it means that it is no longer valid to mixin another class
with a different metaclass.  However, because _C._TensorBase is a C
extension object, it will typically conflict with most other
metaclasses, so this is not BC breaking.

The desired new behavior of a subclass tp_dealloc is to first test if
we should resurrect, and otherwise do the same old behavior.  In an
initial implementation of this patch, I implemented this by saving the
original tp_dealloc (which references subtype_dealloc, the ""standard""
dealloc for all Python defined classes) and invoking it.  However, this
results in an infinite loop, as it attempts to call the dealloc function
of the base type, but incorrectly chooses subclass type (because it is
not a subtype_dealloc, as we have overridden it; see
https://github.com/python/cpython/blob/b38601d49675d90e1ee6faa47f7adaeca992d02d/Objects/typeobject.c#L1261 )
So, with great reluctance, I must duplicate the behavior of
subtype_dealloc in our implementation.  Note that this is not entirely
unheard of in Python binding code; for example, Cython
https://github.com/cython/cython/blob/c25c3ccc4b862592b06e66fd0fc508e4d388437b/Cython/Compiler/ModuleNode.py#L1560
also does similar things.  This logic makes up the bulk of
THPVariable_subclass_dealloc

To review this, you should pull up the CPython copy of subtype_dealloc
https://github.com/python/cpython/blob/b38601d49675d90e1ee6faa47f7adaeca992d02d/Objects/typeobject.c#L1230
and verify that I have specialized the implementation for our case
appropriately.  Among the simplifications I made:

- I assume PyType_IS_GC, because I assume that Tensor subclasses are
  only ever done in Python and those classes are always subject to GC.
  (BTW, yes!  This means I have broken anyone who has extend PyTorch
  tensor from C API directly.  I'm going to guess no one has actually
  done this.)

- I don't bother walking up the type bases to find the parent dealloc;
  I know it is always THPVariable_dealloc.  Similarly, I can get rid
  of some parent type tests based on knowledge of how
  THPVariable_dealloc is defined

- The CPython version calls some private APIs which I can't call, so
  I use the public PyObject_GC_UnTrack APIs.

- I don't allow the finalizer of a Tensor to change its type (but
  more on this shortly)

One alternative I discussed with colesbury was instead of copy pasting
the subtype_dealloc, we could transmute the type of the object that was
dying to turn it into a different object whose tp_dealloc is
subtype_dealloc, so the stock subtype_dealloc would then be applicable.
We decided this would be kind of weird and didn't do it that way.

TODO:

- More code comments

- Figure out how not to increase the size of TensorImpl with the new
  bool field

- Add some torture tests for the THPVariable_subclass_dealloc, e.g.,
  involving subclasses of Tensors that do strange things with finalizers

- Benchmark the impact of taking the GIL to release C++ side tensors
  (e.g., from autograd)

- Benchmark the impact of adding a new metaclass to Tensor (probably
  will be done by separating out the metaclass change into its own
  change)

- Benchmark the impact of changing THPVariable to conditionally own
  Tensor (as opposed to unconditionally owning it, as before)

- Add tests that this actually indeed preserves the Python object

Signed-off-by: Edward Z. Yang <ezyang@fb.com>

Test Plan: Imported from OSS

Reviewed By: albanD

Differential Revision: D27765125

Pulled By: ezyang

fbshipit-source-id: 857f14bdcca2900727412aff4c2e2d7f0af1415a",781.0,171.0,"c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,c10/macros/Macros.h,c10/util/MaybeOwned.h,test/test_torch.py,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/overrides.py",8.0,8,3,1.346586043,44.0,14561.0,8.0,1265672.375,12683.0,28807.0,0.0,Corrective,1.0,1
pytorch,c8166d4b58d877e4b6f9b6160fd83d6775fd1e04,f07ac6a00498f4671b7fc0f74f9750ef3fb803cd,gunandrose4u,52735340+gunandrose4u@users.noreply.github.com,Fri Sep 25 19:35:42 2020 -0700,1601062542.0,"Fix Windows build failure after DDP PR merged (#45335)

Summary:
Fixes #{issue number}
This is resubmit for PR https://github.com/pytorch/pytorch/issues/42897 . Together with fix for Windows build issue introduced by PR https://github.com/pytorch/pytorch/issues/44344 .

Pull Request resolved: https://github.com/pytorch/pytorch/pull/45335

Reviewed By: zou3519

Differential Revision: D23931471

Pulled By: mrshenli

fbshipit-source-id: f49b5a114944c1450b32934b3292170be064f494",464.0,167.0,".jenkins/pytorch/win-test-helpers/installation-helpers/install_miniconda3.bat,CMakeLists.txt,caffe2/CMakeLists.txt,cmake/Dependencies.cmake,test/cpp/dist_autograd/CMakeLists.txt,test/distributed/test_c10d.py,test/distributed/test_c10d_spawn.py,test/run_test.py,tools/build_variables.bzl,torch/CMakeLists.txt,torch/csrc/Module.cpp,torch/csrc/WindowsTorchApiMacro.h,torch/csrc/distributed/c10d/comm.h,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h,torch/csrc/jit/python/pybind_utils.h,torch/csrc/jit/python/python_sugared_value.cpp,torch/csrc/jit/runtime/interpreter.cpp,torch/csrc/jit/serialization/pickler.cpp,torch/csrc/jit/serialization/unpickler.cpp,torch/csrc/utils/future.h,torch/distributed/rendezvous.py,torch/lib/c10d/CMakeLists.txt,torch/lib/c10d/FileStore.cpp,torch/lib/c10d/GlooDeviceFactory.cpp,torch/lib/c10d/ProcessGroupGloo.cpp,torch/lib/c10d/Utils.cpp,torch/lib/c10d/Utils.hpp,torch/lib/c10d/test/CMakeLists.txt,torch/lib/c10d/test/CUDATest.hpp,torch/lib/c10d/test/FileStoreTest.cpp,torch/lib/c10d/test/ProcessGroupGlooTest.cpp,torch/lib/c10d/test/TestUtils.hpp,torch/testing/_internal/common_distributed.py,torch/testing/_internal/common_utils.py,torch/testing/_internal/dist_utils.py,torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py,torch/testing/_internal/distributed/distributed_test.py",39.0,27,6,4.679889581,74.0,31338.0,3.0,47685.94871794872,5453.0,12818.5,0.0,Corrective,1.0,1
pytorch,3799b10c44ccafb467a99c1a203b9c8e892aaa70,f09828ee0e3852d4a80647d812b7f58e7a0428a2,Gregory Chanan,gchanan@fb.com,Fri Jul 13 15:17:56 2018 -0700,1531495076.0,"Support n-dimensional empty tensors in TensorShape methods. (#9362)

Summary:
This includes either bug fixes or NumPy semantics changes for the following methods:
chunk, diagonal, unfold, repeat, flatten, reshape, split, unsqueeze.

The n-dimensional empty tensor feature is still hidden behind a feature flag.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9362

Reviewed By: ezyang

Differential Revision: D8817002

Pulled By: gchanan

fbshipit-source-id: 6ff704ec96375f00b4dd39ebcd976efac0607fb4",152.0,41.0,"aten/src/ATen/native/TensorShape.cpp,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/THC/generic/THCTensor.cpp,aten/src/THC/generic/THCTensorMath.cu,test/test_torch.py",6.0,9,2,1.825199533,40.0,15497.0,4.0,418610.5,2925.0,6860.833333,0.0,Corrective,1.0,1
pytorch,f3374596199b74b90e6551945e5593dacb9ef887,f0b7132b876daf52d588907db93fa3926a616ba8,Lingyi Liu,lingyiliu@fb.com,Fri Sep 20 21:59:59 2019 -0700,1569016799.0,"Revert D17437015: [pytorch][PR] Add the quantized average_pool2d support and adaptive_avg_pool2d support

Test Plan: revert-hammer

Differential Revision:
D17437015

Original commit changeset: 496aed1e4171

fbshipit-source-id: 53e22a85e06bd9d7827579b124b7f136230b6c1d",228.0,963.0,"aten/src/ATen/cpu/vec256/vec256_int.h,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp,aten/src/ATen/native/quantized/cpu/q_avgpool.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py,torch/nn/quantized/functional.py",8.0,13,3,2.162516826,10.0,10325.0,2.0,2205.625,11594.0,32637.33333,0.0,Feature Addition,0.0,1
pytorch,a5b5152d7a432c523fab1b1914ab5bf0c984c213,f0e98dcbd3d2c21ff05d04a16bddc781c141e211,Joel Schlosser,jbschlosser@fb.com,Sun Dec 12 23:52:02 2021 -0800,1639353122.0,"General convolution_backward function (#69044)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69044

Test Plan: Imported from OSS

Reviewed By: zou3519, albanD, H-Huang

Differential Revision: D32708818

Pulled By: jbschlosser

fbshipit-source-id: e563baa3197811d8d51553fc83718ace2f8d1b7a",492.0,97.0,"aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/xnnpack/Convolution.cpp,aten/src/ATen/native/xnnpack/Engine.h,aten/src/ATen/native/xnnpack/Shim.cpp,test/test_nn.py,tools/autograd/derivatives.yaml",8.0,8,3,1.322219961,45.0,35886.0,8.0,3724258.0,17691.0,41800.5,0.0,,0.0,1
pytorch,07f8b61cc60b5fb6d770e482e0aaf7a6b7c78d5f,f0ed927b625068a636d2f2de7d19e505e06a3fb9,Thomas Viehmann,tv.code@beamnet.de,Mon Nov 05 16:53:23 2018 -0800,1541436803.0,"Add diag_embed to ATen and torch (#12447)

Summary:
Fixes: #12160
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12447

Differential Revision: D12916234

Pulled By: SsnL

fbshipit-source-id: 512a04efb0c2e0a54295b857a61be66c3aae13da",117.0,1.0,"aten/src/ATen/core/Tensor.h,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/core/Type.h,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/common_methods_invocations.py,test/test_torch.py,torch/_tensor_docs.py,torch/_torch_docs.py",12.0,9,4,2.069486983,42.0,28138.0,4.0,109104.83333333331,5142.0,15370.33333,0.0,Corrective,1.0,1
pytorch,d9d5a52a146456a0da0971316be7fbe22651d99e,f0f15bc84abda84b1efdea4170df7164d4793d4e,Richard Zou,zou3519@gmail.com,Mon May 10 14:23:14 2021 -0700,1620656594.0,"[functorch] Batching rules for: threshold_backward, clamp_min, clamp_max",98.0,56.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/test/test_vmap.py",2.0,4,1,0.371232327,1.0,2912.0,2.0,0.5,68.0,158.0,0.0,,0.0,1
pytorch,48ea7c808d59d44afd8fbb014781a897fe428d48,f11120967e7c28f5ad9bae261e6a65fb2183927e,Pritam Damania,pritam.damania@fb.com,Thu Jun 10 06:19:55 2021 -0700,1623305995.0,"Support EnumerableShardingSpec in ShardedTensor. (#59061)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/59061

Overall Design: https://github.com/pytorch/pytorch/issues/55207

This PR builds upon https://github.com/pytorch/pytorch/pull/58517 and
https://github.com/pytorch/pytorch/pull/57409 to support creating a
ShardedTensor using EnumerableShardingSpec.
ghstack-source-id: 130780376

Test Plan:
1) unit tests
2) waitforbuildbot

Reviewed By: SciPioneer

Differential Revision: D28734551

fbshipit-source-id: 656f5f2b22041dae071bc475f19fe94c969716e8",357.0,38.0,"test/distributed/_sharded_tensor/test_sharded_tensor.py,test/distributed/_sharding_spec/test_sharding_spec.py,torch/distributed/_sharded_tensor/api.py,torch/distributed/_sharding_spec/api.py,torch/distributed/nn/api/remote_module.py",5.0,10,2,1.193514336,1.0,1685.0,3.0,1056322.4,12907.0,29273.0,0.0,,0.0,1
pytorch,09f2c6a94c7b9f231cf3eee21c8ff501580884f3,f1758305582b28e81c6a079b8fa1cbcb2bb35b0f,Nick Gibson,nickg@fb.com,Fri Sep 18 18:35:46 2020 -0700,1600454146.0,"[NNC] Fuse identical conditions in simplifier (#44886)

Summary:
Adds a pass to the IR Simplifier which fuses together the bodies of Cond statements which have identical conditions. e.g.

```
if (i < 10) {
  do_thing_1;
} else {
  do_thing_2;
}
if (i < 10) {
  do_thing_3;
}
```

is transformed into:

```
if (i < 10) {
  do_thing_1;
  do_thing_3;
} else {
  do_thing_2;
}
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44886

Reviewed By: glaringlee

Differential Revision: D23768565

Pulled By: nickgg

fbshipit-source-id: 3fe40d91e82bdfff8dcb8c56a02a4fd579c070df",517.0,0.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",4.0,7,2,0.753848357,2.0,6412.0,3.0,131583.75,5247.0,12003.0,0.0,Feature Addition,0.0,1
pytorch,c93c884ee2f4136c49a3cdd45a1852fec9d74cd6,f17cfe42936310a2e3fd573e1f4dec8c684d4003,Martin Raison,martinraison@users.noreply.github.com,Fri Mar 03 17:37:03 2017 +0100,1488562623.0,sparse tensor operations (#735),2556.0,271.0,"test/common.py,test/common_nn.py,test/test_autograd.py,test/test_multiprocessing.py,test/test_nn.py,test/test_sparse.py,tools/cwrap/plugins/THPPlugin.py,torch/_utils.py,torch/autograd/_functions/blas.py,torch/autograd/gradcheck.py,torch/autograd/variable.py,torch/csrc/DynamicTypes.cpp,torch/csrc/Module.cpp,torch/csrc/ModuleSparse.cpp,torch/csrc/autograd/grad_buffer.cpp,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/variable.cpp,torch/csrc/cuda/AutoGPU.cpp,torch/csrc/cuda/ModuleSparse.cpp,torch/csrc/generic/SparseTensor.cpp,torch/csrc/generic/methods/SparseTensor.cwrap,torch/csrc/generic/methods/TensorMath.cwrap,torch/cuda/__init__.py,torch/cuda/sparse.py,torch/lib/THCS/generic/THCSTensor.c,torch/lib/THCS/generic/THCSTensor.cu,torch/lib/THCS/generic/THCSTensor.h,torch/lib/THCS/generic/THCSTensorMath.cu,torch/lib/THCS/generic/THCSTensorMath.h,torch/lib/THPP/tensors/THCSTensor.cpp,torch/lib/THPP/tensors/THCSTensor.hpp,torch/lib/THPP/tensors/generic/THCSTensor.cpp,torch/lib/THPP/tensors/generic/THCSTensor.hpp,torch/lib/THPP/tensors/generic/THSTensor.cpp,torch/lib/THS/generic/THSTensor.c,torch/lib/THS/generic/THSTensor.h,torch/lib/THS/generic/THSTensorMath.c,torch/lib/THS/generic/THSTensorMath.h,torch/optim/adadelta.py,torch/optim/adagrad.py,torch/optim/adam.py,torch/optim/adamax.py,torch/optim/asgd.py,torch/optim/lbfgs.py,torch/optim/optimizer.py,torch/optim/rmsprop.py,torch/optim/rprop.py,torch/optim/sgd.py,torch/sparse/__init__.py,torch/storage.py,torch/tensor.py",52.0,23,3,4.006714909,25.0,15816.0,7.0,170802.10638297873,504.0,4675.978466,0.0,,0.0,1
pytorch,b0c86caa1d46a16195682e2afe5456f97265aa53,f1a5044de0639180f667d212800aa43f34026b3c,Khushi Agrawal,khushiagrawal411@gmail.com,Mon Nov 14 18:18:45 2022 +0000,1668449925.0,"[primTorch] _refs & opinfo alpha_dropout (#87989)

Add _refs and OpInfo for `nn.functional.alpha_dropout`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/87989
Approved by: https://github.com/mruberry",123.0,11.0,"test/functorch/test_ops.py,test/functorch/test_vmap.py,torch/_refs/nn/functional/__init__.py,torch/testing/_internal/common_methods_invocations.py",4.0,8,2,1.198901379,7.0,26174.0,2.0,124354.75,9466.0,22082.5,0.0,Feature Addition,0.0,1
pytorch,b7b6b612a7ed28d7f6a794961690b92d60fe15e4,f1adddd1c631a53d997de0bcbe1e35484482239e,Iurii Zdebskyi,iuriiz@fb.com,Thu Jun 06 18:55:01 2019 -0700,1559847301.0,"Updated sum() logic to properly deal with bool tensor (#21421)

Summary:
`torch.tensor([True, False, True], dtype=torch.bool).sum()` should return **2** instead of **True** as it does now.

Tested via unit tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21421

Differential Revision: D15674203

Pulled By: izdeby

fbshipit-source-id: b00e3d0ca809c9b92b750adc05632522dad50c74",2.0,1.0,"aten/src/ATen/native/ReduceOps.cpp,test/test_torch.py",2.0,5,2,0.918295834,40.0,12606.0,2.0,297550.0,9206.0,26836.33333,0.0,,0.0,1
pytorch,460b8715a8dc2f178aafa75df9a9a6c8a928eeb6,f1c57ace1b9dae2941a228b52543952381aafc54,Alykhan Tejani,alykhan.tejani@gmail.com,Fri Jun 02 15:58:19 2017 +0100,1496419099.0,"added input dim checks to convxD and conv_transposedxd (#1695)

* add input dim check for conv2d

* add None check to conv2d

* added input dim checks to convxD and conv_transposedxd

* flake8 fixes",51.0,0.0,"test/test_nn.py,torch/nn/functional.py",2.0,3,2,0.847861745,28.0,3861.0,1.0,360.0,804.0,10496.77615,0.0,Corrective,1.0,1
pytorch,f3e9fa6122b14a5a6dbfc6f776c951a8a87476c4,f1efe5102876958976b648691ffde422d85b6a0e,Vasiliy Kuznetsov,vasiliy@fb.com,Tue Mar 24 22:12:15 2020 -0700,1585087935.0,"add quantized version of hardswish operator (#34820)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/34820

Adds quantized version of hardswish, for common quantized operator coverage.

Note:
* we carry over scale and zero_point from the input to the output, because the
  range of the output is unbounded if x > 0
* we also skip the .out function to not allow the user to specify a custom
  scale+zp (flexible on this).

Test Plan:
```
python test/test_quantized.py

https://gist.github.com/vkuzo/f9b579315ed7f5fdb24839e3218d8465
```

Imported from OSS

Differential Revision: D20472905

fbshipit-source-id: 0f2a83e9f5f7b43485fa46caf30e756dc5d492a9",130.0,0.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qhardswish.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,benchmarks/operator_benchmark/pt/qactivation_test.py,test/test_quantized.py,torch/nn/quantized/functional.py",7.0,14,4,2.27908583,11.0,11622.0,5.0,235554.8333333333,413.0,1169.0,0.0,Feature Addition,0.0,1
pytorch,98e67448fa78bd1bc6f05920ad03efceecc10066,f1f64c8d07b166066f6adc18daa7b9224af7d3cf,Sam Gross,colesbury@gmail.com,Thu Oct 19 19:03:26 2017 -0400,1508439806.0,"Generate autograd functions for NN / more refactors (#3136)

Generate autograd functions for NN and implement more derivatives in derivatives.yaml

A big refactor of gen_variable_type.py",1527.0,528.0,"docs/source/nn.rst,setup.py,test/common_nn.py,test/test_autograd.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/Functions.h,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_functions.cpp,tools/autograd/templates/python_functions.h,tools/autograd/templates/python_nn_functions.cpp,tools/autograd/templates/python_nn_functions.h,tools/autograd/templates/python_nn_functions_dispatch.h,tools/autograd/templates/python_variable_methods.cpp,torch/csrc/Module.cpp,torch/csrc/autograd/function.h,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/utils/wrap_outputs.h,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h",23.0,12,4,2.663223391,39.0,8651.0,4.0,14259.894736842103,298.0,837.9058694,0.0,Perfective,0.0,1
pytorch,09364f4298e45142bf3a3a6a316447d24abb2fdf,f1fdb6efbd09dad3c308b0447682f1f14d2c325e,Jason Ansel,jansel@fb.com,Tue Oct 11 23:01:21 2022 +0000,1665529281.0,"Manual changes for moving dynamo to core (#86621)

This is the subset of the changes in #86461 not auto-generated by `copy_to_core.sh`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/86621
Approved by: https://github.com/albanD",1080.0,1.0,".gitignore,.lintrunner.toml,CODEOWNERS,build_variables.bzl,setup.py,test/test_public_bindings.py,torch/csrc/Module.cpp,torch/csrc/dynamo/eval_frame.c,torch/csrc/dynamo/eval_frame.h,torch/csrc/dynamo/guards.cpp,torch/csrc/dynamo/guards.h,torch/csrc/dynamo/init.cpp,torch/csrc/dynamo/init.h,torch/fx/graph.py",14.0,5,2,1.60091161,50.0,7253.0,6.0,815433.0,8249.0,19581.0,0.0,,0.0,1
pytorch,743cff4a1a3ac0177a6b18575b6de9593a7ae573,f20a04fa2dfcad2c6f3db8fb4879c218a8d6ef59,Alex Suhan,asuhan@fb.com,Fri Aug 21 18:13:59 2020 -0700,1598033639.0,"[TensorExpr] Simplify conditional select (#43350)

Summary:
Fold conditional select when both sides are constant.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43350

Test Plan: test_tensorexpr --gtest_filter=TensorExprTest.ConditionalSelectFold*

Reviewed By: pbelevich

Differential Revision: D23256602

Pulled By: asuhan

fbshipit-source-id: ec04b1e4ae64f59fa574047f2d7af55a717a5262",144.0,7.0,"test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/ir.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h",5.0,7,2,1.14844678,2.0,5698.0,4.0,1847001.4,4450.0,10380.5,0.0,,0.0,1
pytorch,78e991e575c01f781d057d9c8fe2618af227ebab,f21a176c03ea955c53d444ccb52ed9fbe9698e51,Richard Zou,zou3519@gmail.com,Tue Apr 04 19:37:42 2023 -0700,1680637062.0,"Python Dispatcher should respect FuncTorchBatchedDecomposition key (#98328)

Fixes https://github.com/pytorch/pytorch/issues/97425.

Python Dispatcher's resolve_key function should be equivalent to
computeDispatchTableEntryWithDebug. We added a section to
computeDispatchTableEntryWithDebug but forgot to add it to resolve_key.

This PR fixes that discrepancy.

Test Plan:
- new test
Pull Request resolved: https://github.com/pytorch/pytorch/pull/98328
Approved by: https://github.com/Chillee, https://github.com/kshitij12345, https://github.com/Neilblaze",23.0,0.0,"test/functorch/test_vmap.py,torch/_ops.py,torch/csrc/utils/python_dispatch.cpp,torchgen/model.py",4.0,6,3,1.825572921,3.0,9351.0,4.0,1195490.5,14183.0,32486.0,0.0,Corrective,1.0,1
pytorch,fabbb9b391a9addb4edd9beead9944572f8c8c4c,f220892dabe257c680fc3cfb1762a81cc42f8ac1,Richard Zou,zou3519@users.noreply.github.com,Thu Oct 14 01:05:08 2021 -0400,1634173508.0,"[functorch] Make it so that vmap tests generate with bdim=-1 as well as 0 (pytorch/functorch#204)

Fixes pytorch/functorch#62. There are a lot of new xfails.",85.0,29.0,"functorch/functorch/csrc/BatchRulesLinearAlgebra.cpp,functorch/functorch/csrc/BatchRulesReduceOps.cpp,functorch/functorch/csrc/BatchRulesUnaryOps.cpp,functorch/functorch/csrc/BatchRulesViews.cpp,functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_vmap.py",7.0,4,1,2.14943854,1.0,5096.0,7.0,2.4285714285714284,451.0,636.5,0.0,Corrective,1.0,1
pytorch,1f689b6ef9acc4e3ee09b812ec1301a5ca3d8ece,f22aa601ce6d6067cb6ca2ff106cba5b936140ec,Rohan Varma,rvarm1@fb.com,Thu Aug 06 20:28:45 2020 -0700,1596745725.0,"All Gather and gather APIs for Python Objects (#42189)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/42189

Rehash of https://github.com/pytorch/pytorch/pull/28811, which was several months old.

As part of addressing https://github.com/pytorch/pytorch/issues/23232, this PR adds support for the following APIs:

`allgather_object` and `gather_object` to support gather/allgather of generic, pickable Python objects. This has been a long-requested feature so PyTorch should provide these helpers built-in.

The methodology is what is proposed in the original issue:
1) Pickle object to ByteTensor using torch.save
2) Comm. tensor sizes
3) Copy local ByteTensor into a tensor of maximal size
4) Call tensor-based collectives on the result of (3)
5) Unpickle back into object using torch.load

Note that the API is designed to match other than supporting `async_op`. For now, it is a blocking call. If we see demand to support `async_op`, we will have to make more progress on merging work/future to support this.

If this is a suitable approach, we can support `scatter`, `broadcast` in follow up PRs.
ghstack-source-id: 109322433

Reviewed By: mrshenli

Differential Revision: D22785387

fbshipit-source-id: a265a44ec0aa3aaffc3c6966023400495904c7d8",297.0,14.0,"test/distributed/test_distributed.py,torch/distributed/distributed_c10d.py,torch/testing/_internal/common_distributed.py",3.0,6,2,1.141720346,2.0,4882.0,3.0,2222470.0,4130.0,9622.5,0.0,Feature Addition,0.0,1
pytorch,836332e0a1c87fe5a4b06090398239772b4428b8,f273377d197bbd3f1f307cfb06b64510f2ea2195,Trevor Killeen,killeentm@gmail.com,Thu Apr 27 13:24:40 2017 -0700,1493299480.0,add device asserts in scatter/gather kernels,57.0,22.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.511639784,29.0,4142.0,1.0,43.0,710.0,10186.31747,0.0,Feature Addition,0.0,1
pytorch,059c439ed9bd675102a0c6cf1fcf907c88288960,f274558018ff3beefe6282d59bea0350a632484f,Xiang Gao,qasdfgtyuiop@gmail.com,Tue May 17 21:16:42 2022 +0000,1652822202.0,"Bitwise ops improvements (#77621)

- Bitwise shift remove floating point support
- Bitwise and, or, xor add (scalar, tensor) overload
- Use `test_ops.py` to test these ops, including error cases
Pull Request resolved: https://github.com/pytorch/pytorch/pull/77621
Approved by: https://github.com/ngimel",77.0,155.0,"aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/native_functions.yaml,test/onnx/test_pytorch_onnx_onnxruntime.py,test/quantization/core/test_quantized_tensor.py,test/test_binary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",6.0,11,3,1.917365534,16.0,51234.0,6.0,483231.6666666667,3327.0,7965.0,0.0,Feature Addition,0.0,1
pytorch,b1fd7ba019c7002c86762ad9717837e5644f8805,f29110fdf800807432969190317939cea42593d2,Gerard Goossen,ggoossen@fb.com,Tue Mar 03 16:53:04 2020 -0800,1583254384.0,"[pytorch] blas gemm fix for k=0 (#33819)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33819

These conditions are for the specific implementation, the fallback implementation works without these checks. So use that if any of these checks isn't true.

Resubmit of https://github.com/pytorch/pytorch/pull/33419 (which got reverted due to a problem with XLA, but which now has been fixed)
ghstack-source-id: 99333280

Test Plan: Test included

Differential Revision: D20121460

fbshipit-source-id: c1056b8e26751e24078bbe80c7cb4b223bcca7cb",20.0,7.0,"aten/src/TH/generic/THBlas.cpp,test/test_torch.py",2.0,5,2,0.950956048,40.0,16021.0,2.0,327568.5,15148.0,40730.83333,0.0,Corrective,1.0,1
pytorch,cb6d9deec6bb6b666ae5d66cb13aa686903c26f3,f2a35db2d3d5d3d3d360b9545681b10117389e5a,Xinyu Li,lixinyu@fb.com,Tue Nov 05 15:58:02 2019 -0800,1572969482.0,"batch_norm_cpu_inference for channel last (#28982)

Summary:
channels last version for batch_norm_cpu_inference_contiguous

Benchmark:
The benchmark test uses a fixed batch size n=20, channel number in [1,3,10,100,1000], height and width size in [1,4,16,64,256], height and width size are always the same in this test.

We use the following code to do this benchmark.
It tests contiguous, channels last and non-contiguous tensor in each loop and print out the benchmark. It also compare the outputs within each loop to make sure the correctness of the new change.

        for c in [1,3,10,100,1000]:
            for hw in [1,4,16,64,256]:
                print('Benchmark n=20 c={0} h={1} w={2}'.format(c, hw, hw))

                m = nn.BatchNorm2d(c, affine=False)
                m.eval()
                input = torch.randn(20, c, hw, hw)
                output = m(input)
                %timeit m(input)

                for name, param in m.named_parameters():
                    if param.requires_grad:
                        if param.data.dim() == 4:
                            param.data = param.data.contiguous(memory_format=torch.channels_last)
                m.eval()
                input = input.contiguous(memory_format=torch.channels_last)
                output1 = m(input)
                %timeit m(input)

                m = nn.BatchNorm2d(c, affine=False)
                m.eval()
                input = input.permute(0,1,3,2)
                output2 = m(input)
                %timeit m(input)
                output2 = output2.permute(0,1,3,2)

        print(output.equal(output1), output.equal(output2))

Sample output:
Benchmark n=20 c=100 h=256 w=256 -> title line
101 ms Â± 1.57 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) -> contiguous tensor
100 ms Â± 898 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each) -> channels last tensor
1.3 s Â± 10.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each) -> non-contiguous tensor
True True -> 1st output compare with 2nd output, 1st output compare 3rd output, expect True

**Benchmark Before this change:**
Benchmark n=20 c=1 h=1 w=1
10.1 Âµs Â± 158 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.2 Âµs Â± 305 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.7 Âµs Â± 784 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1 h=4 w=4
10.2 Âµs Â± 152 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.1 Âµs Â± 98 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
12.5 Âµs Â± 168 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1 h=16 w=16
11 Âµs Â± 133 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
11 Âµs Â± 148 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
17.3 Âµs Â± 1.32 Âµs per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1 h=64 w=64
24.2 Âµs Â± 536 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
23.9 Âµs Â± 206 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
66 Âµs Â± 409 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=1 h=256 w=256
539 Âµs Â± 7.85 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
539 Âµs Â± 15.9 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.42 ms Â± 33 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=3 h=1 w=1
10 Âµs Â± 108 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
9.97 Âµs Â± 93 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.4 Âµs Â± 625 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=3 h=4 w=4
10.4 Âµs Â± 108 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
16.1 Âµs Â± 601 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
19.1 Âµs Â± 658 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=3 h=16 w=16
13.1 Âµs Â± 163 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
25.3 Âµs Â± 558 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
32.4 Âµs Â± 625 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=3 h=64 w=64
51.1 Âµs Â± 1.81 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
159 Âµs Â± 7.3 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
199 Âµs Â± 1.88 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=3 h=256 w=256
1.25 ms Â± 21.7 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
2.95 ms Â± 203 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
6.14 ms Â± 42.3 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
True True
Benchmark n=20 c=10 h=1 w=1
9.97 Âµs Â± 132 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.5 Âµs Â± 852 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
11.7 Âµs Â± 1.14 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=10 h=4 w=4
11.2 Âµs Â± 84.2 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
29.7 Âµs Â± 343 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
39.4 Âµs Â± 396 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=10 h=16 w=16
19.7 Âµs Â± 632 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
68.3 Âµs Â± 912 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
90.3 Âµs Â± 4.76 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=10 h=64 w=64
325 Âµs Â± 5.01 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
918 Âµs Â± 27.3 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
991 Âµs Â± 44.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=10 h=256 w=256
9.47 ms Â± 73.4 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
34.7 ms Â± 2.12 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
91.5 ms Â± 2.42 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
True True
Benchmark n=20 c=100 h=1 w=1
11.8 Âµs Â± 1.23 Âµs per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
12.1 Âµs Â± 800 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
12 Âµs Â± 533 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=100 h=4 w=4
26.7 Âµs Â± 2.83 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
231 Âµs Â± 8.03 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
335 Âµs Â± 15.1 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=100 h=16 w=16
178 Âµs Â± 20.7 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
1.45 ms Â± 187 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.52 ms Â± 94.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=100 h=64 w=64
6.9 ms Â± 554 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
30.3 ms Â± 1.23 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
27 ms Â± 272 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
True True
Benchmark n=20 c=100 h=256 w=256
98.9 ms Â± 818 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.29 s Â± 12.6 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.32 s Â± 9.4 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
True True
Benchmark n=20 c=1000 h=1 w=1
18.6 Âµs Â± 2.12 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
18.7 Âµs Â± 947 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
15.8 Âµs Â± 261 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1000 h=4 w=4
111 Âµs Â± 2.47 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
2.07 ms Â± 22.3 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
3.19 ms Â± 163 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
True True
Benchmark n=20 c=1000 h=16 w=16
3.87 ms Â± 336 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
25.6 ms Â± 394 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
27 ms Â± 410 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
True True
Benchmark n=20 c=1000 h=64 w=64
70.1 ms Â± 1.9 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
467 ms Â± 26.4 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
444 ms Â± 25.2 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
True True
Benchmark n=20 c=1000 h=256 w=256
2.39 s Â± 19 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
19.2 s Â± 181 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
22.1 s Â± 1.13 s per loop (mean Â± std. dev. of 7 runs, 1 loop each)
True True

**Benchmark After this change:**
Benchmark n=20 c=1 h=1 w=1
10.4 Âµs Â± 247 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.5 Âµs Â± 149 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.7 Âµs Â± 237 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1 h=4 w=4
11.8 Âµs Â± 1.44 Âµs per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
11 Âµs Â± 108 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
13.6 Âµs Â± 142 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1 h=16 w=16
11.9 Âµs Â± 198 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
12.1 Âµs Â± 181 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
18.2 Âµs Â± 205 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1 h=64 w=64
27.6 Âµs Â± 2.4 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
32.2 Âµs Â± 8.69 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
68.9 Âµs Â± 1.5 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=1 h=256 w=256
601 Âµs Â± 49 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
597 Âµs Â± 36.8 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.48 ms Â± 24.1 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=3 h=1 w=1
10.8 Âµs Â± 127 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.6 Âµs Â± 194 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.5 Âµs Â± 137 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=3 h=4 w=4
11.6 Âµs Â± 551 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
11.7 Âµs Â± 266 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
19.9 Âµs Â± 340 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=3 h=16 w=16
13.7 Âµs Â± 223 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
24.7 Âµs Â± 424 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
33.7 Âµs Â± 1.23 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=3 h=64 w=64
53.3 Âµs Â± 1.66 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
212 Âµs Â± 4.68 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
204 Âµs Â± 5.61 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=3 h=256 w=256
1.49 ms Â± 295 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
3.27 ms Â± 136 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
7.08 ms Â± 290 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
True True
Benchmark n=20 c=10 h=1 w=1
10.7 Âµs Â± 166 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.8 Âµs Â± 225 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
10.8 Âµs Â± 192 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=10 h=4 w=4
11.6 Âµs Â± 129 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
12.9 Âµs Â± 503 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
43.7 Âµs Â± 3.5 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=10 h=16 w=16
20.7 Âµs Â± 576 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
37.2 Âµs Â± 795 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
92.5 Âµs Â± 1.21 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
True True
Benchmark n=20 c=10 h=64 w=64
342 Âµs Â± 9.89 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
622 Âµs Â± 37.7 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.03 ms Â± 37.1 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=10 h=256 w=256
9.49 ms Â± 130 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
10.9 ms Â± 408 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
90.5 ms Â± 1.79 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
True True
Benchmark n=20 c=100 h=1 w=1
12 Âµs Â± 575 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
11 Âµs Â± 216 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
11 Âµs Â± 182 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=100 h=4 w=4
22.3 Âµs Â± 451 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
18.7 Âµs Â± 255 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
323 Âµs Â± 6.22 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=100 h=16 w=16
211 Âµs Â± 22 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
222 Âµs Â± 20.9 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
1.5 ms Â± 59.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
True True
Benchmark n=20 c=100 h=64 w=64
7.2 ms Â± 1e+03 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
6.51 ms Â± 121 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
27.4 ms Â± 695 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
True True
Benchmark n=20 c=100 h=256 w=256
101 ms Â± 1.57 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
100 ms Â± 898 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
1.3 s Â± 10.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
True True
Benchmark n=20 c=1000 h=1 w=1
16.9 Âµs Â± 589 ns per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
16.5 Âµs Â± 113 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
16.5 Âµs Â± 168 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
True True
Benchmark n=20 c=1000 h=4 w=4
116 Âµs Â± 6.65 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
67 Âµs Â± 1.18 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
3.23 ms Â± 80 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
True True
Benchmark n=20 c=1000 h=16 w=16
3.53 ms Â± 72.6 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
3.53 ms Â± 125 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
27 ms Â± 129 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
True True
Benchmark n=20 c=1000 h=64 w=64
68.6 ms Â± 1.18 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)
68 ms Â± 288 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)
425 ms Â± 1.25 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
True True
Benchmark n=20 c=1000 h=256 w=256
2.51 s Â± 97.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
2.84 s Â± 471 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
21.5 s Â± 933 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
True True

The channel last batch normalization is getting faster with this change and the previous existing code/logic is not affected based on the benchmark above.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28982

Reviewed By: VitalyFedyunin

Differential Revision: D18253305

Pulled By: glaringlee

fbshipit-source-id: a0fcac65544f10d736141ee70edeab8a3f1b3e02",150.0,21.0,"aten/src/ATen/native/Normalization.cpp,test/test_torch.py",2.0,5,2,0.856405239,40.0,14820.0,2.0,56718.5,12818.0,35460.83333,0.0,Corrective,1.0,1
pytorch,bb71117ecc46d0cb19208fa2e49afc38fd9f689b,f2c1071c335040558b4d64322082e00d2e4c2026,Jason Kuen,xternalz@users.noreply.github.com,Sun Mar 26 15:09:28 2017 +0800,1490540968.0,Adaptive max and average pooling (1D & 2D) (#1084),375.0,3.0,"test/test_nn.py,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/pooling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/pooling.py",6.0,6,2,1.933125642,25.0,4531.0,2.0,131533.33333333334,528.0,3934.495885,0.0,,0.0,1
pytorch,34ede14877e6d64f5e296247d945d4a28b9f60ce,f2d7e949484826d5c1103d4c3be7736039a92b48,Sam Gross,sgross@fb.com,Wed Oct 26 16:51:52 2016 -0700,1477500712.0,"Use torch.Size for Tensor sizes and tuple for strides

See issue #20

The torch.Size class is a tuple subclass which distinguishes sizes from
other tuples so that torch.Tensor(size) is interpreted as size instead
of data.",588.0,472.0,"setup.py,test/common.py,test/test_cuda.py,test/test_legacy_nn.py,test/test_nn.py,test/test_torch.py,tools/cwrap/cwrap.py,tools/cwrap/plugins/THPLongArgsPlugin.py,tools/cwrap/plugins/THPPlugin.py,torch/autograd/functions/pointwise.py,torch/autograd/functions/reduce.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/Size.cpp,torch/csrc/Size.h,torch/csrc/THP.h,torch/csrc/generic/Tensor.cpp,torch/csrc/generic/TensorMethods.cwrap,torch/csrc/utils.cpp,torch/csrc/utils.h,torch/cuda/comm.py,torch/legacy/nn/CMul.py,torch/legacy/nn/Concat.py,torch/legacy/nn/DepthConcat.py,torch/legacy/nn/JoinTable.py,torch/legacy/nn/MixtureTable.py,torch/legacy/nn/Padding.py,torch/legacy/nn/Parallel.py,torch/legacy/nn/Replicate.py,torch/legacy/nn/Reshape.py,torch/legacy/nn/SpatialDivisiveNormalization.py,torch/legacy/nn/SpatialFractionalMaxPooling.py,torch/legacy/nn/SpatialSubtractiveNormalization.py,torch/legacy/nn/SpatialUpSamplingNearest.py,torch/legacy/nn/Sum.py,torch/legacy/nn/View.py,torch/legacy/nn/utils.py,torch/multiprocessing/_tensor.py,torch/tensor.py",39.0,13,3,4.443554258,12.0,15332.0,1.0,74866.0,264.0,2346.64599,0.0,,0.0,1
pytorch,e7b34962326713aa65383c509c7939f2a007211b,f2e41257e4e72060084073308a9ef1b4841a2ed5,Will Constable,whc@fb.com,Fri Jan 29 03:27:29 2021 -0800,1611890849.0,"Back out ""Revert D26077905: Back out ""Revert D25850783: Add torch::deploy, an embedded torch-python interpreter"""" (#51267)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/51267

Original commit changeset: b70185916502

Test Plan: test locally, oss ci-all, fbcode incl deferred

Reviewed By: suo

Differential Revision: D26121251

fbshipit-source-id: 4315b7fd5476914c8e5d6f547e1cfbcf0c227781",1027.0,14.0,".github/workflows/lint.yml,.gitignore,.jenkins/pytorch/build.sh,.jenkins/pytorch/test.sh,CMakeLists.txt,torch/__init__.py,torch/_ops.py,torch/_utils_internal.py,torch/csrc/Module.cpp,torch/csrc/deploy/.gitignore,torch/csrc/deploy/CMakeLists.txt,torch/csrc/deploy/README.md,torch/csrc/deploy/example/trace_simple.py,torch/csrc/deploy/interpreter/CMakeLists.txt,torch/csrc/deploy/interpreter/CMakePythonModules.txt,torch/csrc/deploy/interpreter/freeze.py,torch/csrc/deploy/interpreter/hide_symbols.script,torch/csrc/deploy/interpreter/interpreter.cpp,torch/csrc/deploy/interpreter/interpreter.h,torch/csrc/deploy/interpreter/interpreter_impl.h,torch/csrc/deploy/interpreter/test_main.cpp,torch/csrc/deploy/interpreter/third_party/README.md,torch/cuda/__init__.py,torch/utils/__init__.py",25.0,12,3,3.034946725,75.0,4531.0,2.0,82457.20833333333,8462.0,19090.0,0.0,Feature Addition,0.0,1
pytorch,841995d53b7ea51e8dae64e0d3d4f4d888406d8b,f2ec9fbd03b131fe4f80ad77305271912a687246,Nikita Vedeneev,nik@quansight.com,Tue Oct 18 09:07:35 2022 +0000,1666084055.0,"`torch.ormqr`: backward support (#86800)

Seems good to have, especially when neither `a` nor `tau` requires grads and/or they are pretty small in number.
Fixes https://github.com/pytorch/pytorch/issues/86267

Pull Request resolved: https://github.com/pytorch/pytorch/pull/86800
Approved by: https://github.com/lezcano",141.0,31.0,"test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",8.0,9,3,1.49941538,36.0,47543.0,4.0,191724.125,8470.0,20266.5,0.0,Corrective,1.0,1
pytorch,50049168a655179832aaa98ebdaa72c77b301381,f2f057af99fdffbf51facabcfeb5d6e572a07629,Hao Lu,hlu@fb.com,Mon Oct 23 21:38:12 2017 -0700,1508794692.0,"Support MetaNetDef model specs on mobile

Reviewed By: ajtulloch

Differential Revision: D6010327

fbshipit-source-id: 5f1b81fc9ba92889044b89ae766c45c2d3c090d8",154.0,0.0,"caffe2/core/predictor.cc,caffe2/core/predictor.h,caffe2/core/predictor_test.cc",3.0,2,1,1.121317527,3.0,310.0,2.0,766699.3333333334,2159.0,5350.333333,0.0,,0.0,1
pytorch,a3aa9df59f11a36487fba40f4bb619fae25a8be2,f2f7b02b4ca51a791b1b5a9bdd50d038e2fe0220,soulitzer,soulitzer@gmail.com,Wed Oct 27 15:53:48 2021 -0700,1635350028.0,"Add support for vmap+fwdAD for basic out-of-place op (#66291)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66291

In this PR:
 - Trivial batching rules for `make_dual` and `is_same_size` that enable forward ad + vmap functionality
 - Adds a check in gradcheck that is performed when both `check_batched_grad` and `check_forward_ad` are `True` (an OpInfo using this is added later in the stack).
 - Tests for the gradcheck functionality
 - Tests that basic out-of-place op works

Test Plan: Imported from OSS

Reviewed By: albanD, saketh-are

Differential Revision: D31842018

Pulled By: soulitzer

fbshipit-source-id: 84b18d9a77eeb19897757e37555581f2a9dc43d8",112.0,9.0,"aten/src/ATen/BatchingRegistrations.cpp,test/test_autograd.py,torch/autograd/gradcheck.py",3.0,6,3,1.027327676,42.0,12176.0,3.0,1200435.3333333333,16612.0,38927.5,0.0,Feature Addition,0.0,1
pytorch,379e717a1b35817c7009c3ca717fe53dabab5510,f2f802776019c8f5c51e2c9ef69fe62c2d0a4ed0,Nick Gibson,nickg@fb.com,Fri May 08 00:21:24 2020 -0700,1588897284.0,"[TensorExpr] simplify trivial adds/subs/muls even in Float (#37960)

Summary:
The IR Simplifier early exits when working with dtypes that are not safe to reorder. There are some cases where we still want to simplify ops in these dtypes: x + 0,  x - 0, x * 0 and x * 1.  It's safe to eliminate the op here and it reduces clutter in the expr.

Also added a quick simplification of casts which do nothing (their type is the same as the underlying).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/37960

Differential Revision: D21457736

Pulled By: nickgg

fbshipit-source-id: 40e20a3b55fc1afb2ec50071812238a08bded2ac",174.0,31.0,"test/cpp/tensorexpr/test_simplify.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.cpp",2.0,7,2,0.964956767,1.0,3449.0,2.0,353704.0,1830.0,4729.5,0.0,Feature Addition,0.0,1
pytorch,c15648c6b58b29e5a3c803c3b9b2ccf3635d9c40,f30081a3136640158221cc80e4c53f65a9f5ce49,Sam Gross,sgross@fb.com,Fri Sep 30 23:31:01 2016 -0700,1475278261.0,Use NCCL bcast and reduce functions in comm,337.0,1.0,"test/run_test.sh,test/test_nccl.py,torch/cuda/comm.py,torch/cuda/nccl.py",4.0,3,2,1.136919491,8.0,113.0,1.0,1012683.0,43.0,1906.630952,0.0,,0.0,1
pytorch,864d129bae3c9f1fa41ed1ffce476c8958fd570e,f3218568ada0d13fb5f849fa23460a28ad3e571b,mingfeima,mingfei.ma@intel.com,Fri Jun 11 23:21:38 2021 -0700,1623453698.0,"optimize channels last for BatchNorm2d on CPU (#59286)

Summary:
replacement of https://github.com/pytorch/pytorch/issues/48919
optimize channels last performance for BatchNorm2 on CPU.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59286

Reviewed By: bdhirsh

Differential Revision: D29008198

Pulled By: VitalyFedyunin

fbshipit-source-id: 8a7d020bd6a42ab5c21ffe788b79a22f4ec82ac0",805.0,154.0,"aten/src/ATen/cpu/vec/vec256/functional.h,aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/batch_norm.h,aten/src/ATen/native/cpu/batch_norm_kernel.cpp,aten/src/ATen/test/vec_test_all_types.cpp,aten/src/ATen/test/vec_test_all_types.h,test/test_nn.py",7.0,10,2,1.494567474,43.0,20904.0,4.0,1607477.4285714286,12968.0,29375.5,0.0,Perfective,0.0,1
pytorch,c8ca70e39d0efc9dd008b78fdd5ca089ab965746,f326045b3757236aabe367dfca1894be14ce31ef,Brian Wignall,brianwignall@gmail.com,Sat Jan 18 00:01:29 2020 -0800,1579305689.0,"Fix typos, via a Levenshtein-type corrector (#31523)

Summary:
Should be non-semantic.

Uses https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines to find likely typos, with https://github.com/bwignall/typochecker to help automate the checking.

Uses an updated version of the tool used in https://github.com/pytorch/pytorch/pull/30606 .
Pull Request resolved: https://github.com/pytorch/pytorch/pull/31523

Differential Revision: D19216749

Pulled By: mrshenli

fbshipit-source-id: 7fd489cb9a77cd7e4950c1046f925d57524960ea",284.0,284.0,"CMakeLists.txt,CODEOWNERS,aten/src/ATen/CMakeLists.txt,aten/src/ATen/core/boxing/kernel_lambda.h,aten/src/ATen/core/function_schema.h,aten/src/ATen/core/jit_type.h,aten/src/ATen/cpu/vec256/vec256_base.h,aten/src/ATen/cuda/CUDAGenerator.cpp,aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h,aten/src/ATen/cudnn/Descriptors.h,aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h,aten/src/ATen/miopen/Descriptors.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LossMultiMargin.cpp,aten/src/ATen/native/RNN.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/Unfold3d.cpp,aten/src/ATen/native/UpSample.h,aten/src/ATen/native/cpu/DistanceOpsKernel.cpp,aten/src/ATen/native/cpu/GridSamplerKernel.cpp,aten/src/ATen/native/cuda/Copy.cu,aten/src/ATen/native/cuda/GridSampler.cu,aten/src/ATen/native/cuda/Indexing.cu,aten/src/ATen/native/cuda/Loops.cuh,aten/src/ATen/native/cuda/PersistentSoftmax.cuh,aten/src/ATen/native/cuda/SoftMax.cu,aten/src/ATen/native/cuda/SortingKthValue.cu,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/cuda/UpSample.cuh,aten/src/ATen/native/cudnn/RNN.cpp,aten/src/ATen/native/mkl/SpectralOps.cpp,aten/src/ATen/native/mkldnn/Conv.cpp,aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp,aten/src/ATen/native/quantized/cpu/qclamp.cpp,aten/src/ATen/native/quantized/cpu/qmul.cpp,aten/src/ATen/native/quantized/cpu/qnnpack/src/requantization/fp32-neon.c,aten/src/ATen/native/quantized/cpu/qnnpack/src/requantization/fp32-psimd.c,aten/src/ATen/native/quantized/cpu/qnnpack/src/requantization/precise-scalar.c,aten/src/ATen/native/quantized/cpu/qnnpack_utils.h,aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,aten/src/TH/THGeneral.h.in,aten/src/TH/THStorage.h,aten/src/TH/THTensor.hpp,aten/src/TH/generic/THVectorDispatch.cpp,aten/src/THC/THCIntegerDivider.cuh,aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu,benchmarks/framework_overhead_benchmark/C2Module.py,benchmarks/operator_benchmark/README.md,benchmarks/operator_benchmark/benchmark_caffe2.py,benchmarks/operator_benchmark/benchmark_core.py,benchmarks/operator_benchmark/pt/qrnn_test.py,binaries/convert_and_benchmark.cc,binaries/convert_image_to_tensor.cc,c10/core/StorageImpl.h,c10/test/util/registry_test.cpp,caffe2/contrib/gloo/allreduce_ops.cc,caffe2/contrib/gloo/allreduce_ops.h,caffe2/contrib/gloo/allreduce_ops_gpu.cc,caffe2/contrib/gloo/broadcast_ops.h,caffe2/contrib/gloo/reduce_scatter_ops.h,caffe2/contrib/opencl/OpenCL/cl.hpp,caffe2/contrib/playground/checkpoint.py,caffe2/contrib/tensorrt/tensorrt_tranformer.cc,caffe2/core/blob_serialization.h,caffe2/core/common.h,caffe2/core/context_gpu.cu,caffe2/core/memonger.cc,caffe2/core/net.h,caffe2/core/net_async_tracing.cc,caffe2/core/net_simple_refcount.cc,caffe2/core/nomnigraph/include/nomnigraph/Representations/NeuralNet.h,caffe2/core/operator.h,caffe2/core/operator_schema.h,caffe2/core/scope_guard.h,caffe2/core/static_tracepoint_elfx86.h,caffe2/core/workspace.h,caffe2/experiments/python/net_construct_bench.py,caffe2/image/image_input_op.h,caffe2/mobile/contrib/libopencl-stub/include/CL/cl.hpp,caffe2/mobile/contrib/nnapi/nnapi.cc,caffe2/onnx/backend.cc,caffe2/onnx/onnx_exporter.cc,caffe2/onnx/onnx_exporter.h,caffe2/onnx/torch_ops/defs.cc,caffe2/operators/activation_ops_cudnn.h,caffe2/operators/batch_bucketize_op.cc,caffe2/operators/batch_matmul_op.cc,caffe2/operators/bisect_percentile_op.cc,caffe2/operators/box_with_nms_limit_op.h,caffe2/operators/conv_op_cudnn.cc,caffe2/operators/crf_viterbi_op.cc,caffe2/operators/fused_rowwise_random_quantization_ops.cc,caffe2/operators/gather_op.h,caffe2/operators/generate_proposals_op.h,caffe2/operators/h_softmax_op.cc,caffe2/operators/heatmap_max_keypoint_op.cc,caffe2/operators/lengths_reducer_rowwise_8bit_ops.h,caffe2/operators/load_save_op_util.cc,caffe2/operators/op_utils_cudnn.h,caffe2/operators/pool_op_util.cc,caffe2/operators/reservoir_sampling.cc,caffe2/operators/rnn/recurrent_network_executor_incl.h,caffe2/operators/segment_reduction_op.h,caffe2/operators/sparse_normalize_op.cc,caffe2/operators/string_ops.h,caffe2/operators/stump_func_op.cc,caffe2/operators/summarize_op.cu,caffe2/operators/tile_op.h,caffe2/operators/utility_ops.cu,caffe2/opt/backend_cutting.cc,caffe2/opt/backend_transformer_base.h,caffe2/opt/custom/glow_net_transform.cc,caffe2/opt/onnxifi_transformer.cc,caffe2/opt/onnxifi_transformer.h,caffe2/opt/tvm_transformer.h,caffe2/perfkernels/adagrad.h,caffe2/predictor/emulator/benchmark.cc,caffe2/proto/caffe2.proto,caffe2/python/checkpoint.py,caffe2/python/crf.py,caffe2/python/dataio_test.py,caffe2/python/examples/imagenet_trainer.py,caffe2/python/functional.py,caffe2/python/helpers/fc.py,caffe2/python/layers/batch_normalization.py,caffe2/python/layers/feature_sparse_to_dense.py,caffe2/python/layers/functional.py,caffe2/python/layers/layer_normalization.py,caffe2/python/layers/random_fourier_features.py,caffe2/python/layers/select_record_by_context.py,caffe2/python/layers/tags.py,caffe2/python/mkl/rewrite_graph.py,caffe2/python/modeling/initializers.py,caffe2/python/modeling/parameter_sharing.py,caffe2/python/net_drawer.py,caffe2/python/onnx/ONNXOpCoverage.md,caffe2/python/operator_test/batch_bucketize_op_test.py,caffe2/python/operator_test/box_with_nms_limit_op_test.py,caffe2/python/operator_test/elementwise_op_broadcast_test.py,caffe2/python/operator_test/gather_ops_test.py,caffe2/python/operator_test/heatmap_max_keypoint_op_test.py,caffe2/python/operator_test/one_hot_ops_test.py,caffe2/python/operator_test/pooling_test.py,caffe2/python/operator_test/recurrent_network_test.py,caffe2/python/pipeline.py,caffe2/python/regularizer.py,caffe2/python/rnn_cell.py,caffe2/python/schema.py,caffe2/python/trt/transform.py,caffe2/python/visualize.py,caffe2/python/workspace.py,caffe2/quantization/server/dnnlowp.cc,caffe2/quantization/server/norm_minimization.cc,caffe2/sgd/clip_tensor_op.cc,caffe2/utils/map_utils.h,caffe2/utils/math_cpu.cc,caffe2/utils/proto_utils.h,caffe2/utils/signal_handler.h,caffe2/utils/threadpool/WorkersPool.h,caffe2/utils/zmq_helper.h,caffe2/video/video_input_op.h,cmake/Modules/FindvecLib.cmake,cmake/public/cuda.cmake,docs/source/hub.rst,docs/source/jit.rst,docs/source/notes/extending.rst,docs/source/notes/windows.rst,ios/TestApp/benchmark/setup.rb,modules/detectron/group_spatial_softmax_op.cu,modules/detectron/select_smooth_l1_loss_op.cc,modules/detectron/softmax_focal_loss_op.cu,scripts/xcode_build.rb,test/common_device_type.py,test/common_distributed.py,test/common_utils.py,test/cpp/api/rnn.cpp,test/cpp/jit/test_irparser.cpp,test/dist_autograd_test.py,test/hypothesis_utils.py,test/jit/test_data_parallel.py,test/onnx/debug_embed_params.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_pytorch_onnx_onnxruntime.py,test/test_cpp_extensions.py,test/test_distributions.py,test/test_numba_integration.py,test/test_utils.py,tools/autograd/gen_autograd.py,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,tools/pyi/gen_pyi.py,torch/autograd/__init__.pyi,torch/csrc/api/include/torch/data/datasets/base.h,torch/csrc/api/include/torch/data/datasets/stateful.h,torch/csrc/api/include/torch/data/iterator.h,torch/csrc/api/include/torch/nn/modules/container/sequential.h,torch/csrc/autograd/profiler.cpp,torch/csrc/distributed/autograd/context/container.h,torch/csrc/distributed/autograd/engine/dist_engine.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/rpc/process_group_agent.cpp,torch/csrc/distributed/rpc/python_rpc_handler.h,torch/csrc/distributed/rpc/rpc_agent.h,torch/csrc/distributed/rpc/rref_context.h,torch/csrc/distributed/rpc/utils.h,torch/csrc/jit/constants.cpp,torch/csrc/jit/constants.h,torch/csrc/jit/export.cpp,torch/csrc/jit/export_module.cpp,torch/csrc/jit/fuser/codegen.cpp,torch/csrc/jit/graph_executor.cpp,torch/csrc/jit/import_source.cpp,torch/csrc/jit/init.cpp,torch/csrc/jit/mobile/import.cpp,torch/csrc/jit/operator.cpp,torch/csrc/jit/passes/alias_analysis.cpp,torch/csrc/jit/passes/bailout_graph.cpp,torch/csrc/jit/passes/batch_mm.cpp,torch/csrc/jit/passes/guard_elimination.cpp,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/quantization.cpp,torch/csrc/jit/passes/quantization.h,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/script/concrete_module_type.h,torch/csrc/jit/script/init.cpp,torch/csrc/jit/script/lexer.h,torch/csrc/jit/script/python_sugared_value.h,torch/csrc/jit/script/schema_matching.cpp,torch/csrc/jit/script/schema_matching.h,torch/csrc/jit/script/string_to_type.cpp,torch/csrc/jit/unpickler.h,torch/csrc/tensor/python_tensor.cpp,torch/csrc/utils/throughput_benchmark-inl.h,torch/csrc/utils/variadic.h,torch/distributed/distributed_c10d.py,torch/distributed/launch.py,torch/distributed/rpc/__init__.py,torch/distributions/transforms.py,torch/hub.py,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/multiprocessing/reductions.py,torch/nn/modules/_functions.py,torch/nn/modules/adaptive.py,torch/nn/modules/instancenorm.py,torch/nn/utils/prune.py,torch/onnx/operators.py,torch/utils/checkpoint.py,torch/utils/cpp_extension.py,torch/utils/data/_utils/worker.py,torch/utils/file_baton.py",252.0,146,13,7.897952137,86.0,139422.0,189.0,14890902.087301588,14286.0,38844.83333,0.0,Corrective,1.0,1
pytorch,caa0d0c50a25b6287de58e7012c9c26f727245b6,f32c9bd5e9263c214381cdbf5923ae2331966e02,Pieter Noordhuis,pietern@fb.com,Thu May 09 21:07:36 2019 -0700,1557436056.0,"Refactor core DistributedDataParallel tests (#20235)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/20235

The tests expected to only run for CUDA models. In a future commit we
need to update this to work for CPU models as well. Therefore, we can
no longer rely on only integers being passed for device identifiers.
With this change we pass both the materialized list of devices to use
(as `torch.Device` objects), as well as an optional list of integers.
The latter is specified to exercise the code in the
DistributedDataParallel constructor that turns a list of integers into
CUDA devices, IFF it is used to wrap a single-device CUDA module.

This commit also groups together the 'str' and non-'str' tests. These
used to test passing the list of devices as integers or as
`torch.Device` instances. These are now executed from the same test.

Reviewed By: mrshenli

Differential Revision: D15245429

fbshipit-source-id: 5797ba9db33d2c26db8e7493c91bb52f694285ac",59.0,69.0,test/test_c10d.py,1.0,1,1,0,2.0,2719.0,1.0,6.0,8587.0,25434.33333,0.0,Perfective,0.0,1
pytorch,93953a48b7a1f2ade427b1e536efe72204b30848,f348b1b2b520e880548896e2ae4b596f7262c957,Kulin Seth,kulin_seth@apple.com,Wed May 11 17:19:45 2022 +0000,1652289585.0,"Add the Runtime components for MPS backend. (#76725)

The PR adds the runtime components and few basic operations like copy, as_strided for MPS backend.

Current list of identified TODOs are:

-  https://github.com/pytorch/pytorch/issues/77176
- Unify the logic with CUDACachingAllocator and remove redundant code.
-  https://github.com/pytorch/pytorch/issues/77170
- Look into using C++ smart pointers where possible with ObjC code
- Use empty_strided_generic() to implement the `empty_strided_mps` code
- https://github.com/pytorch/pytorch/issues/77144
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76725
Approved by: https://github.com/albanD",2766.0,28.0,"aten/CMakeLists.txt,aten/src/ATen/CMakeLists.txt,aten/src/ATen/Context.cpp,aten/src/ATen/TensorUtils.cpp,aten/src/ATen/mps/EmptyTensor.cpp,aten/src/ATen/mps/EmptyTensor.h,aten/src/ATen/mps/MPSAllocator.h,aten/src/ATen/mps/MPSAllocator.mm,aten/src/ATen/mps/MPSDevice.h,aten/src/ATen/mps/MPSDevice.mm,aten/src/ATen/mps/MPSGuardImpl.h,aten/src/ATen/mps/MPSGuardImpl.mm,aten/src/ATen/mps/MPSStream.h,aten/src/ATen/mps/MPSStream.mm,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/mps/Copy.h,aten/src/ATen/native/mps/OperationUtils.h,aten/src/ATen/native/mps/OperationUtils.mm,aten/src/ATen/native/mps/TensorFactory.cpp,aten/src/ATen/native/mps/TensorFactory.h,aten/src/ATen/native/mps/operations/Copy.mm,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/scalar_tensor_test.cpp,c10/core/DispatchKey.cpp,caffe2/CMakeLists.txt,cmake/Codegen.cmake,docs/source/backends.rst,torch/_C/__init__.pyi.in,torch/__init__.py,torch/_tensor_str.py,torch/backends/mps/__init__.py,torch/csrc/Module.cpp,torch/testing/_internal/common_device_type.py,torch/testing/_internal/common_utils.py,torchgen/dest/register_dispatch_key.py,torchgen/gen.py,torchgen/model.py",37.0,23,7,3.979895881,57.0,30560.0,20.0,2766203.9,3095.0,7471.0,0.0,Feature Addition,0.0,1
pytorch,cdd9911a12c723e889b84418418ea1ead097806f,f34de6a9f44e373b8f921dd4ef4cde9d4a29fa76,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Thu Apr 01 18:12:47 2021 -0700,1617300767.0,"Modified lstsq_helper to accept rank and singular_values (#54719)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/54719

lstsq_helper now takes rank and singular_values that are modified in-place.
This is required for adding out= variant.

TODO:

- [ ] Fix CI failures

Test Plan: Imported from OSS

Reviewed By: ezyang

Differential Revision: D27439197

Pulled By: mruberry

fbshipit-source-id: f2fe421aa393c2d58f5c50f33e21a9eae57e4f01",51.0,57.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py",4.0,7,2,0.733837351,12.0,14531.0,4.0,121461.5,10327.0,22828.0,0.0,Corrective,1.0,1
pytorch,726bbfffb9aa74ec00753baee6505d8c55645011,f36345eb0b5a7cef8ef4bf01acc134132365a274,Rohan Varma,rvarm1@fb.com,Fri Oct 11 17:58:46 2019 -0700,1570816726.0,"improve error message on incorrect inputs into gather for (#27439)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27439

When users call dist.gather, they have to pass in a `gather_list` to
the function on the destination worker, and this list needs to have the same
size as the number of processes in the group. When the user initializes this
list incorrectly, the current error message is not very helpful:

This changes the error message so that the incorrect gather_list size is
pointed out and the correct one is given.
ghstack-source-id: 91413442

Test Plan: Added a unit test and tested with an incorrect gather_list size.

Differential Revision: D17781370

fbshipit-source-id: b49aad1b1197daf77daa10911296664e6340e2fa",19.0,9.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.985228136,3.0,5459.0,2.0,784765.5,12191.0,34099.83333,0.0,Corrective,0.0,1
pytorch,487624e36927da6c225c188cbd641df8ecedb36b,f363a2e106b8730ce7e5008c9892b2a37db7af74,Nikita Shulga,nshulga@fb.com,Fri Oct 09 04:07:49 2020 -0700,1602216469.0,"Mark top 3 slowest tests as slow (#46068)

Summary:
`TCPStoreTest.test_numkeys_delkeys` takes 5+ min (mostly in idle wait for socket timeout)
`TestDataLoader.test_proper_exit` and `TestDataLoaderPersistentWorkers.test_proper_exit` take 2.5 min each
`TestXNNPACKConv1dTransformPass.test_conv1d_with_relu_fc` takes 2 min to finish

Add option to skip reporting test classes that run for less than a second to `print_test_stats.py` and speed up `TestTorchDeviceTypeCUDA.test_matmul_45724_cuda`

Pull Request resolved: https://github.com/pytorch/pytorch/pull/46068

Reviewed By: mruberry

Differential Revision: D24208660

Pulled By: malfet

fbshipit-source-id: 780e0d8be4f0cf69ea28de79e423291a1f3349b7",28.0,9.0,"test/distributed/test_c10d.py,test/print_test_stats.py,test/test_dataloader.py,test/test_torch.py,test/test_xnnpack_integration.py,torch/testing/_internal/common_methods_invocations.py",6.0,5,2,2.209544087,43.0,30379.0,6.0,205762.0,5860.0,13612.5,0.0,Feature Addition,0.0,1
pytorch,7ad948ffa95af4b19394a182dab477f3de853205,f366e5fc81eb43e518fb1a461f3c42d776a252e0,Zhou Chang,achang.zhou@gmail.com,Thu Mar 02 12:08:05 2017 +0800,1488456485.0,"Support int16 numpy conversions

issue #891",6.0,0.0,"test/test_torch.py,torch/csrc/Module.cpp,torch/csrc/generic/Tensor.cpp",3.0,4,2,1.459147917,24.0,4740.0,2.0,73478.66666666667,502.0,3356.971519,0.0,,0.0,1
pytorch,3ae684266a0a36b0842375fcf8e8476722fd2abe,f36a84b71b6a9629169ae239137c9f67a9cdea8e,Brennan Vincent,btv@fb.com,Wed Dec 12 16:49:04 2018 -0800,1544633344.0,"fix some tests that I accidentally disabled (#15077)

Summary:
While moving these scenarios into `_test_dim_ops` I accidentally left an empty loop in the actual tests, causing them to do nothing.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15077

Differential Revision: D13428759

Pulled By: umanwizard

fbshipit-source-id: 08f53068981d9192c1408878b168e9053f4dc92e",11.0,17.0,test/test_torch.py,1.0,1,1,0,40.0,9589.0,1.0,168557.0,6013.0,18675.33333,0.0,Corrective,1.0,1
pytorch,e26d584445a80a548485097bfbef1f67bba5f771,f393df774b07bd8ae5fdccb9007e68ef38403f5b,Peter Goldsborough,psag@fb.com,Tue Jul 24 20:59:21 2018 -0700,1532465961.0,"Test case for c10d DDP (#9670)

Summary:
Before I can rewrite portions of the c10d DDP in C++ I need proper tests in place to make sure I am not breaking anything as I port code. There were no tests for the c10d DDP in place so I wrote some.

I refactored the c10d tests to derive some tests cases from a general `MultiGPUTestCase` and followed lots of patterns from `test_distributed.py` w.r.t. how tests are skipped (such that the main process doesn't initialize CUDA, which I found is a super important detail!!!).

I am largely unfamiliar with this code so feel free to scrutinize. The DDP test code itself is also largely taken from `test_distributed.py` but more inlined which I find easier to read.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9670

Differential Revision: D8977724

Pulled By: goldsborough

fbshipit-source-id: 186eab38a72384d7992a2ec5c89f304ad42d5944",176.0,28.0,test/test_c10d.py,1.0,1,1,0,2.0,372.0,1.0,2166301.0,3082.0,7489.333333,0.0,Perfective,0.0,1
pytorch,ce8a5876723e5b4a4daeaeeca0df74d6d821cb9a,f3ca8bce1c21f4424e46905d246f285c0e80a989,Samantha Andow,samdow@fb.com,Fri Apr 08 13:32:16 2022 -0400,1649424736.0,"[functorch] Fix normal_ and bernoulli (pytorch/functorch#670)

* normal_fix

* fix binomial test",11.0,8.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/test/test_vmap.py",2.0,4,1,0.949452015,1.0,4630.0,1.0,0.0,948.0,1305.0,0.0,Corrective,1.0,1
pytorch,c524448dd1bf9fcbfced6a6d9c13d8f297c63275,f3ead05d777fa818f7d93da284e9c77781e66e0d,Nick Korovaiko,villedepommes@fb.com,Fri May 14 02:36:38 2021 -0700,1620959798.0,"hardtanh (#57750)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57750

Test Plan: Imported from OSS

Reviewed By: huiguoo

Differential Revision: D28425975

fbshipit-source-id: a5e3dfbd6c77c595528c052e0b4325ef452983eb",21.0,4.0,"torch/csrc/jit/runtime/symbolic_script.cpp,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/jit_metaprogramming_utils.py",3.0,6,1,1.217796812,2.0,8942.0,1.0,92.0,12071.0,27360.5,0.0,,0.0,1
pytorch,a60d712010611738b34296264359c0adc6283c66,f4099af1e99c0715f6ea488619378aadf68ea91f,Pearu Peterson,pearu.peterson@gmail.com,Tue Dec 13 08:16:39 2022 +0200,1670919399.0,"Fix gradcheck for BSR and BSC inputs. (#90719)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/90719
Approved by: https://github.com/soulitzer, https://github.com/cpuhrsch",48.0,0.0,"test/test_autograd.py,torch/autograd/gradcheck.py",2.0,3,2,0.954434003,44.0,11761.0,2.0,315943.0,10525.0,24035.0,0.0,Corrective,1.0,1
pytorch,2213c463ba88fa96abe200843c8ffcd1ccdb9397,f445ed19b2fae12abda723001d1c6c7dd857b266,Peter Bell,peterbell10@live.co.uk,Thu Oct 07 19:47:00 2021 -0700,1633636020.0,"OpInfo for 2d fft functions (#66128)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/66128

cc mruberry peterbell10

Test Plan: Imported from OSS

Reviewed By: dagitses

Differential Revision: D31450217

Pulled By: mruberry

fbshipit-source-id: 1952fc60c5d5f454966c43f5710b8b97a9794d0e",98.0,20.0,"test/test_spectral_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.500426984,2.0,12022.0,2.0,298117.0,16033.0,37105.5,0.0,,0.0,1
pytorch,b03407289fa10c76065feaa4a12dbd53f857e56c,f45d75ed227d4cf7312e31312046d0c023387146,Soumith Chintala,soumith@fb.com,Sat Dec 24 20:36:00 2016 -0500,1482611760.0,make the CUDA-aware tests backoff if CUDA no available,15.0,10.0,"test/run_test.sh,test/test_cuda.py,test/test_nccl.py,test/test_utils.py",4.0,1,1,1.710856298,17.0,1168.0,1.0,5373.0,249.0,2840.784684,0.0,,0.0,1
pytorch,dcc4d11ffa0bcaf02d0f34133247dd5ee8cd6840,f47e00bdc3c89d964c22cda186f80c2b549711f7,Nick Gibson,nickg@fb.com,Sat Aug 01 03:20:28 2020 -0700,1596252028.0,"[NNC] Bounds Inference: make inferred bounds respect gaps (#42185)

Summary:
A heavy refactor of bounds inference to fix some issues and bugs blocking using it to analyze cross thread interactions:
* We were merging all accesses to a Buf into a single bounds info entry, even if they did not overlap. E.g. if we accessed a[0:2] and a[5:6] we would merge that into a bound of a[0:6]. I've changed this behaviour to merge only overlapping bounds.
* We were not separating bounds of different kinds (e.g. Load vs Store) and would merge a Store bounds into a Load bounds, losing the information about what kind of access it was. E.g. this loop would produce bounds: [{Load, 0, 10}] and now produces bounds [{Load, 0, 9}, {Store, 1, 10}]:
```
for i in 1 to 10...
  x[i] = x[i-1]
```
* Both ComputeAt and Rfactor relied on the overzealous merging and only used a single entry in the bounds list to determine the bounds of temporary buffers they created, which could result in temporary buffers allocated smaller than accesses to them. I've fixed Rfactor, but *not* ComputeAt - however all ComputeAt tests still pass (may require loop fusion to trigger this issue) - I will come back to it.

Being more precise about bounds is more complex, rather than taking the minimum of starts and maximum of stops we now need to determine if two bounds overlap or are adjacent. There are many edge cases and so I've added a bunch of test coverage of the merging method.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/42185

Reviewed By: mruberry

Differential Revision: D22870391

Pulled By: nickgg

fbshipit-source-id: 3ee34fcbf0740a47259defeb44cba783b54d0baa",1135.0,394.0,"test/cpp/tensorexpr/test_boundsinference.cpp,test/cpp/tensorexpr/test_loopnest.cpp,test/cpp/tensorexpr/test_simplify.cpp,test/cpp/tensorexpr/tests.h,torch/csrc/jit/tensorexpr/bounds_inference.cpp,torch/csrc/jit/tensorexpr/bounds_inference.h,torch/csrc/jit/tensorexpr/ir_simplifier.cpp,torch/csrc/jit/tensorexpr/ir_simplifier.h,torch/csrc/jit/tensorexpr/loopnest.cpp",9.0,7,2,1.933423377,2.0,8442.0,6.0,4114495.5,3984.0,9348.0,0.0,Corrective,1.0,1
pytorch,b044c95129ca32edee8a9de9e175290caf7343e0,f4a2b0e446fb5260cf78e1f522928585f455b7ec,gchanan,gregchanan@gmail.com,Mon Feb 05 16:09:18 2018 -0500,1517846958.0,"Don't allow scalars where vectors are required in mv, addmv, ger, addr. (#5003)

* Don't allow scalars where vectors are required in mv, addmv, ger, addr.

* Fix scalar_tensor_test for ger.

* Address review comments.

* Fix merge.",125.0,12.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native_parse.py,aten/src/ATen/test/scalar_tensor_test.cpp,test/test_autograd.py,test/test_torch.py,tools/autograd/derivatives.yaml,torch/autograd/gradcheck.py",9.0,10,4,2.44637872,39.0,14881.0,6.0,513024.8888888889,498.0,1480.905869,0.0,Corrective,1.0,1
pytorch,657c97a060897ea30a1c93778c69c0a49576d337,f4a808b582e90cb2be85ffcddcc3fa5b7a2076dc,Richard Zou,zou3519@gmail.com,Wed Aug 03 14:20:57 2022 -0700,1659536457.0,"[functorch] share code between test_vmap_exhaustive and test_op_has_batch_rule (#82659)

test_op_has_batch_rule is technically a superset of
test_vmap_exhaustive, but the signal from both are valuable.

This PR also gets rid of a ""hack"" by adding some more xfails.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82659
Approved by: https://github.com/samdow",29.0,45.0,functorch/test/test_vmap.py,1.0,2,1,0,2.0,4278.0,1.0,14848.0,6070.0,14149.5,0.0,Feature Addition,0.0,1
pytorch,d90cd73aea0bfaed9749b1883567c203580325f6,f4b1e8b3346184bd17de03246dc7bd07bcf25af9,Jiyan Yang,jiyanyang12@gmail.com,Thu Mar 08 21:41:32 2018 -0800,1520545292.0,[Dper2] Add NetModifier abstraction and support for plotting the norm of blobs (#2201),425.0,0.0,"caffe2/python/layer_model_helper.py,caffe2/python/layer_model_instantiator.py,caffe2/python/modeling/compute_norm_for_blobs.py,caffe2/python/modeling/compute_norm_for_blobs_test.py,caffe2/python/modeling/compute_statistics_for_blobs.py,caffe2/python/modeling/compute_statistics_for_blobs_test.py,caffe2/python/modeling/net_modifier.py",7.0,3,1,2.513160464,5.0,657.0,2.0,2659998.0,745.0,1736.305292,0.0,Feature Addition,0.0,1
pytorch,d5a0f97ea7859ae3fd36b4fb5e5716a1c57525f0,f4ce99fd87b50d23359e0997758ab73870b2e6a3,Gregory Chanan,gchanan@fb.com,Wed Jun 07 17:23:12 2017 -0700,1496856192.0,"Add dist, atan2, lerp to fallback functions.

They weren't documented as having those semantics, but tests on
master show they do.",16.0,19.0,"test/test_torch.py,torch/csrc/generic/methods/TensorMath.cwrap",2.0,5,2,0.863120569,31.0,5731.0,2.0,0.0,913.0,11499.94394,0.0,Feature Addition,0.0,1
pytorch,1129b3344a65d65c862b53505ce32677902987c5,f4d9bfaa4dd983288518a310bb900756ee3c6046,Lara Haidar,haidar.lara@gmail.com,Sat May 11 01:31:15 2019 -0700,1557538275.0,"Support Exports to Multiple ONNX Opset (#19294)

Summary:
Support exporting multiple ONNX opsets (more specifically opset 10 for now), following the proposal in https://gist.github.com/spandantiwari/99700e60919c43bd167838038d20f353.
And add support for custom ops (merge with https://github.com/pytorch/pytorch/pull/18297).

This PR will be followed by another PR containing the changes related to testing the ops for different opsets.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/19294

Reviewed By: zrphercule

Differential Revision: D15043951

Pulled By: houseroad

fbshipit-source-id: d336fc35b8827145639137bc348ae07e3c14bb1c",2205.0,1814.0,"aten/src/ATen/core/interned_strings.h,docs/source/onnx.rst,test/onnx/expect/TestOperators.test_maxpool_dilations.expect,test/onnx/test_onnx_opset.py,test/onnx/test_pytorch_onnx_caffe2.py,test/onnx/test_utility_funs.py,torch/csrc/jit/passes/onnx.cpp,torch/onnx/__init__.py,torch/onnx/symbolic.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py,torch/onnx/symbolic_registry.py,torch/onnx/utils.py",14.0,14,4,1.925861469,14.0,5563.0,8.0,3041960.4444444445,8622.0,25516.33333,0.0,Feature Addition,0.0,1
pytorch,4dce051cb00d75035a5e443dd740c8f5c441c83c,f4dd88489a77ff9b300bf6f9b34c233ca82f76d7,lezcano,lezcano-93@hotmail.com,Mon Oct 25 20:20:37 2021 -0700,1635193237.0,"Better and more consistent error messages in torch.linalg (#62734)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/62734

Following https://github.com/pytorch/pytorch/pull/62715#discussion_r682610788
- squareCheckInputs takes a string with the name of the function
- We reuse more functions when checking the inputs

The state of the errors in torch.linalg is far from great though. We
leave a more comprehensive clean-up for the future.

cc jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano

Test Plan: Imported from OSS

Reviewed By: anjali411

Differential Revision: D31823230

Pulled By: mruberry

fbshipit-source-id: eccd531f10d590eb5f9d04a957b7cdcb31c72ea4",83.0,102.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,test/test_linalg.py",4.0,5,2,1.426825138,8.0,15660.0,4.0,496948.75,16547.0,38800.0,0.0,,0.0,1
pytorch,3f2a839dda7c198c16aa8bdb610f2e69831d0663,f51de8b61abe9a8bcf2dd4cef72da8e02693ba03,Alexander Sidorov,salex@fb.com,Wed Jun 26 23:01:58 2019 -0700,1561590118.0,"Back out ""Revert D15435461: [pytorch][PR] PyTorch ThroughputBenchmark"" (#22185)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22185

Original commit changeset: 72a0eac1658b

Differential Revision: D15981928

fbshipit-source-id: d2455d79e81c26ee90d41414cde8ac0f9b703bc3",677.0,0.0,"c10/macros/Export.h,test/test_throughput_benchmark.py,tools/build_variables.py,torch/CMakeLists.txt,torch/csrc/Module.cpp,torch/csrc/jit/init.h,torch/csrc/utils/init.cpp,torch/csrc/utils/init.h,torch/csrc/utils/throughput_benchmark-inl.h,torch/csrc/utils/throughput_benchmark.cpp,torch/csrc/utils/throughput_benchmark.h,torch/utils/__init__.py,torch/utils/throughput_benchmark.py",13.0,9,4,2.69463257,41.0,1637.0,4.0,914189.3076923076,9653.0,28102.83333,0.0,,0.0,1
pytorch,2ecb18881c2b593d368c4195a0fad35e252a3fcb,f536c662bfae11c69a7ff4dd90c67da1e07a6c63,Leonid Vlasenkov,leo.vlasenkov@gmail.com,Tue Jul 11 14:36:19 2017 +0300,1499783779.0,fix op in docs (#2048),2.0,2.0,torch/nn/modules/activation.py,1.0,3,1,0,32.0,714.0,1.0,175.0,1119.0,18470.9346,0.0,Corrective,1.0,1
pytorch,eb22d06e5e411b19116879677cca33dd716155f8,f57c63032e920d7a24e11da4e253d594d8ca6dc2,Gary Miguel,garymiguel@microsoft.com,Mon Nov 08 22:29:12 2021 -0800,1636410552.0,"[ONNX] Fix reciprocal when input is not floating point (#67471) (#67808)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/67808

torch.reciprocal implicitly casts the inputs to float, and ONNX
Reciprocal requires floating point inputs.

Also separate the reciprocal test from other tests, and test different
input types.

Test Plan: Imported from OSS

Reviewed By: msaroufim

Differential Revision: D32181307

Pulled By: malfet

fbshipit-source-id: 3e1109b3c85a49c51dc713656a900b4ee78c8340",15.0,6.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.591672779,3.0,13659.0,2.0,3.0,16931.0,39789.5,0.0,Corrective,1.0,1
pytorch,5c8cbbb0f01b8513c50ef98e84ca2a36056cd46b,f581fba18df2da237222802c6b156bab693ce08e,Sam Gross,sgross@fb.com,Wed Feb 24 22:18:13 2016 -0800,1456352293.0,"Add VolumetricBatchNormalization

The BatchNormalization modules now all extend nn.BatchNormalization and
use the same THNN/THCUNN implementation.",126.0,124.0,"generic/BatchNormalization.c,generic/SpatialBatchNormalization.c,init.c",3.0,1,1,1.059175006,8.0,283.0,2.0,180218.5,5.0,5.166666667,0.0,Feature Addition,0.0,1
pytorch,d28e667159936dee8972d63e3676268832f36507,f595467e5c6569d4a457033d7ee46c7b9b1d28b1,soulitzer,soulitzer@gmail.com,Thu Jul 21 20:42:35 2022 -0400,1658436155.0,"Reenable slow gradcheck and make it pass (#80514)

Context: For a while slow gradcheck CI was skipping nearly all tests and this hid the fact that it should've been failing and timing out (10+h runtime for TestGradients). The CI configuration has since been fixed to correct this, revealing the test failures. This PR reenables slow gradcheck CI and makes it pass again.

This PR:
- makes slow and failing tests run in fast gradcheck mode only
- reduce the input size for slow gradcheck only for unary/binary ufuncs (alternatively, skip the test entirely)
- skip entire test files on slow gradcheck runner if they don't use gradcheck (test_ops, test_meta, test_decomp, test_ops_jit)
- reduces the input size for some ops

Follow ups:
1. Investigate slow mode failures https://github.com/pytorch/pytorch/issues/80411
2. See if we can re-enable slow gradcheck tests for some of the slow tests by reducing the sizes of their inputs

The following are failing in slow mode, they are now running in fast mode only.
```
test_fn_fwgrad_bwgrad___rmod___cuda_float64
test_fn_fwgrad_bwgrad_linalg_householder_product_cuda_complex128
test_fn_fwgrad_bwgrad__masked_prod_cuda_complex128
test_fn_fwgrad_bwgrad__masked_prod_cuda_float64
test_fn_fwgrad_bwgrad_linalg_matrix_power_cuda_complex128
test_fn_fwgrad_bwgrad_cat_cuda_complex128
test_fn_fwgrad_bwgrad_linalg_lu_factor_ex_cuda_float64
test_fn_fwgrad_bwgrad_copysign_cuda_float64
test_fn_fwgrad_bwgrad_cholesky_inverse_cuda_complex128
test_fn_fwgrad_bwgrad_float_power_cuda_complex128
test_fn_fwgrad_bwgrad_fmod_cuda_float64
test_fn_fwgrad_bwgrad_float_power_cuda_float64
test_fn_fwgrad_bwgrad_linalg_lu_cuda_float64
test_fn_fwgrad_bwgrad_remainder_cuda_float64
test_fn_fwgrad_bwgrad_repeat_cuda_complex128
test_fn_fwgrad_bwgrad_prod_cuda_complex128
test_fn_fwgrad_bwgrad_slice_scatter_cuda_float64
test_fn_fwgrad_bwgrad_tile_cuda_complex128
test_fn_fwgrad_bwgrad_pow_cuda_float64
test_fn_fwgrad_bwgrad_pow_cuda_complex128
test_fn_fwgrad_bwgrad_fft_*
test_fn_fwgrad_bwgrad_zero__cuda_complex128
test_fn_gradgrad_linalg_lu_factor_cuda_float64
test_fn_grad_div_trunc_rounding_cuda_float64
test_fn_grad_div_floor_rounding_cuda_float64
```

Marks the OpInfos for the following ops that run slowly in slow gradcheck as `fast_gradcheck` only (the left column represents runtime in seconds):
```
0  918.722  test_fn_fwgrad_bwgrad_nn_functional_conv_transpose3d_cuda_float64
1  795.042  test_fn_fwgrad_bwgrad_nn_functional_unfold_cuda_complex128
2  583.63  test_fn_fwgrad_bwgrad_nn_functional_max_pool3d_cuda_float64
3  516.946  test_fn_fwgrad_bwgrad_svd_cuda_complex128
4  503.179  test_fn_fwgrad_bwgrad_linalg_svd_cuda_complex128
5  460.985  test_fn_fwgrad_bwgrad_linalg_lu_cuda_complex128
6  401.04  test_fn_fwgrad_bwgrad_linalg_lstsq_grad_oriented_cuda_complex128
7  353.671  test_fn_fwgrad_bwgrad_nn_functional_max_pool2d_cuda_float64
8  321.903  test_fn_fwgrad_bwgrad_nn_functional_gaussian_nll_loss_cuda_float64
9  307.951  test_fn_fwgrad_bwgrad_stft_cuda_complex128
10  266.104  test_fn_fwgrad_bwgrad_svd_lowrank_cuda_float64
11  221.032  test_fn_fwgrad_bwgrad_istft_cuda_complex128
12  183.741  test_fn_fwgrad_bwgrad_lu_unpack_cuda_complex128
13  132.019  test_fn_fwgrad_bwgrad_nn_functional_unfold_cuda_float64
14  125.343  test_fn_fwgrad_bwgrad_nn_functional_pad_constant_cuda_complex128
15  124.2  test_fn_fwgrad_bwgrad_kron_cuda_complex128
16  123.721  test_fn_fwgrad_bwgrad_pca_lowrank_cuda_float64
17  121.074  test_fn_fwgrad_bwgrad_nn_functional_max_unpool3d_cuda_float64
18  119.387  test_fn_fwgrad_bwgrad_rot90_cuda_complex128
19  112.889  test_fn_fwgrad_bwgrad__masked_normalize_cuda_complex128
20  107.541  test_fn_fwgrad_bwgrad_dist_cuda_complex128
21  106.727  test_fn_fwgrad_bwgrad_diff_cuda_complex128
22  104.588  test_fn_fwgrad_bwgrad__masked_cumprod_cuda_complex128
23  100.135  test_fn_fwgrad_bwgrad_nn_functional_feature_alpha_dropout_with_train_cuda_float64
24  88.359  test_fn_fwgrad_bwgrad_mH_cuda_complex128
25  86.214  test_fn_fwgrad_bwgrad_nn_functional_max_unpool2d_cuda_float64
26  83.037  test_fn_fwgrad_bwgrad_nn_functional_bilinear_cuda_float64
27  79.987  test_fn_fwgrad_bwgrad__masked_cumsum_cuda_complex128
28  77.822  test_fn_fwgrad_bwgrad_diag_embed_cuda_complex128
29  76.256  test_fn_fwgrad_bwgrad_mT_cuda_complex128
30  74.039  test_fn_fwgrad_bwgrad_linalg_lu_solve_cuda_complex128
```
```
0  334.142  test_fn_fwgrad_bwgrad_unfold_cuda_complex128
1  312.791  test_fn_fwgrad_bwgrad_linalg_lu_factor_cuda_complex128
2  121.963  test_fn_fwgrad_bwgrad_nn_functional_max_unpool3d_cuda_float64
3  108.085  test_fn_fwgrad_bwgrad_diff_cuda_complex128
4  89.418  test_fn_fwgrad_bwgrad_nn_functional_max_unpool2d_cuda_float64
5  72.231  test_fn_fwgrad_bwgrad___rdiv___cuda_complex128
6  69.433  test_fn_fwgrad_bwgrad___getitem___cuda_complex128
7  68.582  test_fn_fwgrad_bwgrad_ldexp_cuda_complex128
8  68.572  test_fn_fwgrad_bwgrad_linalg_pinv_cuda_complex128
9  67.585  test_fn_fwgrad_bwgrad_nn_functional_glu_cuda_float64
10  66.567  test_fn_fwgrad_bwgrad_lu_cuda_float64
```
```
0  630.13  test_fn_gradgrad_nn_functional_conv2d_cuda_complex128
1  81.086  test_fn_gradgrad_linalg_solve_triangular_cuda_complex128
2  71.332  test_fn_gradgrad_norm_cuda_complex128
3  64.308  test_fn_gradgrad__masked_std_cuda_complex128
4  59.519  test_fn_gradgrad_div_no_rounding_mode_cuda_complex128
5  58.836  test_fn_gradgrad_nn_functional_adaptive_avg_pool3
```

Reduces the sizes of the inputs for:
- diff
- diag_embed

Pull Request resolved: https://github.com/pytorch/pytorch/pull/80514
Approved by: https://github.com/albanD",239.0,37.0,".github/workflows/periodic.yml,test/test_decomp.py,test/test_meta.py,test/test_nn.py,test/test_ops.py,test/test_ops_gradients.py,test/test_ops_jit.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",9.0,6,3,1.178295374,47.0,51103.0,8.0,1379950.6666666667,5612.0,13115.0,0.0,Corrective,1.0,1
pytorch,c18af03a412110d6e264fb54f8b74d6693fdb8c7,f5b68e74d75c38d5e2044fc6b62112181080bb3f,Mike Ruberry,mruberry@fb.com,Thu Dec 17 08:58:25 2020 -0800,1608195505.0,"Revert D25574962: [pytorch][PR] Updated derivative rules for complex svd and pinverse

Test Plan: revert-hammer

Differential Revision:
D25574962 (https://github.com/pytorch/pytorch/commit/9955355853a1c189a4a79209f82d39393b4be010)

Original commit changeset: 832b61303e88

fbshipit-source-id: d73f77f3e51b0f535dad6d21c5bebf8d41a6bfbd",81.0,170.0,"test/test_autograd.py,test/test_jit.py,test/test_linalg.py,test/test_ops.py,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py",8.0,8,3,2.125675436,44.0,45946.0,6.0,28771.625,7596.0,16972.0,0.0,,0.0,1
pytorch,01815be1e4e993f2f30dbbbf913f1ffd2ba0ba26,f5bc91f851f7d3b862643b51f06f0281eb225b8c,Nikita Shulga,nshulga@fb.com,Thu May 28 05:42:22 2020 -0700,1590644542.0,"Get rid of multiple inheritence in test_torch (#39110)

Summary:
`_TestTorchMixin` is base class which is instantiated across multiple types.
It was inherited from `object` in order to hide it from unittest test discovery mechanism.
But this approach makes it almost impossible to use static code analyzer on the class.
This PR implements alternative approach by hiding base class into inner class, per https://stackoverflow.com/a/25695512

Change imported class access path in `test_cuda.py`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/39110

Test Plan:
run `test_torch.py --discover-tests` and `test_cuda.py --discover-tests` before and after change:
```
$ python test_torch.py --discover-tests|md5sum
2ca437bb5d65700763ce04cdacf6de3e  -
$ python test_cuda.py --discover-tests|md5sum
b17df916fb0eeb6f0dd7222d7dae392c  -
```

Differential Revision: D21759265

Pulled By: malfet

fbshipit-source-id: b01b06111469e551f7b78387449975e5248f6b9e",4960.0,4953.0,"test/test_cuda.py,test/test_torch.py",2.0,1,1,0.045749523,43.0,21665.0,2.0,33347.0,2384.0,5997.0,0.0,,0.0,1
pytorch,ede08492e15232c33a0b7c10a885fefd0cbcb931,f5df0c9104b0c663db53be50981dd8119707517d,Thomas Viehmann,tv.code@beamnet.de,Tue Jun 25 03:36:14 2019 -0700,1561433774.0,"Don't end on inplace operators in einsum (#22111)

Summary:
Returning the result of an inplace `squeeze_` in `einsum` (which itself is traced) interacts badly with `autograd.Function`.

I must admit that I'm not 100% certain whether it should be necessary to change this, but I consider this a good change overall.

Fixes: https://github.com/pytorch/pytorch/issues/22072
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22111

Differential Revision: D15974990

Pulled By: soumith

fbshipit-source-id: 477e7f23833f02999085f665c175d062e7d32acd",13.0,5.0,aten/src/ATen/native/Linear.cpp,1.0,4,1,0,7.0,523.0,1.0,18394.0,9605.0,27926.83333,0.0,Corrective,1.0,1
pytorch,8b61fbdac9bacbf6e162ce972148abfa5ef3c61c,f5ee619d2a8a2fee8e6c70fc2e19d6b75c305ffe,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Sun Dec 20 22:37:48 2020 -0800,1608503868.0,"Updated derivative rules for complex svd and pinverse (#47761)

Summary:
Updated `svd_backward` to work correctly for complex-valued inputs.
Updated `common_methods_invocations.py` to take dtype, device arguments for input construction.
Removed `test_pinverse` from `test_autograd.py`, it is replaced by entries to `common_methods_invocations.py`.
Added `svd` and `pinverse` to list of complex tests.

References for complex-valued SVD differentiation:

- https://giggleliu.github.io/2019/04/02/einsumbp.html
- https://arxiv.org/abs/1909.02659

The derived rules assume gauge invariance of loss functions, so the result would not be correct for loss functions that are not gauge invariant.
https://re-ra.xyz/Gauge-Problem-in-Automatic-Differentiation/

The same rule is implemented in Tensorflow and [BackwardsLinalg.jl](https://github.com/GiggleLiu/BackwardsLinalg.jl).

Ref. https://github.com/pytorch/pytorch/issues/33152

Pull Request resolved: https://github.com/pytorch/pytorch/pull/47761

Reviewed By: ngimel

Differential Revision: D25658897

Pulled By: mruberry

fbshipit-source-id: ba33ecbbea3f592238c01e62c7f193daf22a9d01",169.0,80.0,"test/test_autograd.py,test/test_linalg.py,test/test_ops.py,tools/autograd/gen_variable_type.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/testing/_internal/common_methods_invocations.py",7.0,8,3,1.993129408,42.0,29935.0,4.0,236257.7142857143,7666.0,17248.5,0.0,Corrective,0.0,1
pytorch,04db1b874ff4fe902af3dd7540159645309490a7,f5f64526da6d01063a8a8a071fe74da712bc1fe0,drisspg,drisspg@fb.com,Tue Apr 12 18:33:29 2022 +0000,1649788409.0,"Adding opinfo tests for binary cross entropy with logits

Adding the first OpInfo Request found here: #74613
Pull Request resolved: https://github.com/pytorch/pytorch/pull/75604
Approved by: https://github.com/albanD",65.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,5.0,16477.0,1.0,90253.0,2206.0,5256.5,0.0,Feature Addition,0.0,1
pytorch,3bd7dbf1196fcdd327ec09993444d5c1f5b8757f,f5fa91ba2e4af4a4782b867ee1fae3d84dbc14b7,Peter Bell,peterbell10@live.co.uk,Mon Nov 29 20:47:19 2021 -0800,1638218839.0,"Sparse: Add additional opinfo tests (#68886)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68886

cc nikitaved pearu cpuhrsch IvanYashchuk

Test Plan: Imported from OSS

Reviewed By: jbschlosser

Differential Revision: D32697933

Pulled By: cpuhrsch

fbshipit-source-id: fffdd1bc663cc1bc49abe8cf3680982d1cb497bc",109.0,16.0,"test/test_sparse.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.456186103,29.0,17369.0,2.0,791881.5,17338.0,40746.5,0.0,Feature Addition,0.0,1
pytorch,bdc306569b3e59df2c29fbd7ce28c94b4b51875a,f61d82546259a922d91cd083f7c6211fba8037af,Samantha Andow,samdow@fb.com,Mon Apr 11 17:17:30 2022 -0400,1649697450.0,"[functorch] Fix CI (pytorch/functorch#686)

* fix ci

* typo

* move from vmapjvpall -> vmapjvpall_has_batch_rule",7.0,23.0,functorch/test/test_ops.py,1.0,2,1,0,1.0,1566.0,1.0,0.0,955.0,1311.0,0.0,Corrective,1.0,1
pytorch,909f31764faf65c80f4d8eaeae8a12c67eb24c17,f61ec2495ee83727098550ddf4caafb6f365f13d,Soumith Chintala,soumith@gmail.com,Mon Jun 12 19:42:37 2017 -0400,1497296557.0,nn.EmbeddingBag to compute a bag of word embeddings (Embedding + Sum/Mean),373.0,2.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/_functions/thnn/auto.py,torch/nn/_functions/thnn/sparse.py,torch/nn/modules/__init__.py,torch/nn/modules/sparse.py",6.0,8,3,1.740281839,30.0,4696.0,2.0,122069.83333333331,955.0,10936.92866,0.0,,0.0,1
pytorch,d4712ee218cd6af3c2096ca7a76fef350173b703,f636dc927687cc50a527c9185f9d95ed65e32996,Michael Suo,suo@fb.com,Wed Dec 26 14:52:25 2018 -0800,1545835945.0,"clang format world (#15524)

Summary:
The PR clang-formats everything in `torch/csrc/jit/` and adds it to the pre-commit hook.

Here is a list of non-mechanical changes:
- I went over each file and fixed up whenever I could tell that clang-format was clobbering comment formatting.
- Made the macros in register_prim_ops a little more clang-format friendly by omitting trailing commas
- Refactored autodiff.cpp to use a helper class with explicit state rather than a bunch of capturing lambdas
- Small improvements to the precommit hook clang-format
Pull Request resolved: https://github.com/pytorch/pytorch/pull/15524

Differential Revision: D13547989

Pulled By: suo

fbshipit-source-id: 3ff1541bb06433ccfe6de6e33f29227a2b5bb493",9396.0,7194.0,"test/cpp/jit/tests.h,tools/clang_format.py,torch/csrc/jit/alias_info.h,torch/csrc/jit/argument_spec.h,torch/csrc/jit/attributes.h,torch/csrc/jit/autodiff.cpp,torch/csrc/jit/autodiff.h,torch/csrc/jit/batched/BatchTensor.cpp,torch/csrc/jit/batched/BatchTensor.h,torch/csrc/jit/catch_utils.hpp,torch/csrc/jit/code_template.h,torch/csrc/jit/constants.cpp,torch/csrc/jit/constants.h,torch/csrc/jit/dynamic_dag.h,torch/csrc/jit/export.cpp,torch/csrc/jit/export.h,torch/csrc/jit/function_schema.h,torch/csrc/jit/fuser/arg_spec.h,torch/csrc/jit/fuser/codegen.cpp,torch/csrc/jit/fuser/codegen.h,torch/csrc/jit/fuser/compiler.cpp,torch/csrc/jit/fuser/compiler.h,torch/csrc/jit/fuser/config.h.in,torch/csrc/jit/fuser/cpu/dynamic_library.h,torch/csrc/jit/fuser/cpu/fused_kernel.cpp,torch/csrc/jit/fuser/cpu/fused_kernel.h,torch/csrc/jit/fuser/cpu/resource_strings.h,torch/csrc/jit/fuser/cpu/temp_file.h,torch/csrc/jit/fuser/cuda/fused_kernel.cpp,torch/csrc/jit/fuser/cuda/fused_kernel.h,torch/csrc/jit/fuser/cuda/resource_strings.h,torch/csrc/jit/fuser/executor.cpp,torch/csrc/jit/fuser/executor.h,torch/csrc/jit/fuser/fallback.cpp,torch/csrc/jit/fuser/fallback.h,torch/csrc/jit/fuser/fused_kernel.h,torch/csrc/jit/fuser/interface.cpp,torch/csrc/jit/fuser/interface.h,torch/csrc/jit/fuser/kernel_cache.cpp,torch/csrc/jit/fuser/kernel_cache.h,torch/csrc/jit/fuser/kernel_spec.h,torch/csrc/jit/fuser/partition_desc.h,torch/csrc/jit/fuser/tensor_desc.h,torch/csrc/jit/fuser/tensor_info.h,torch/csrc/jit/generic_if.h,torch/csrc/jit/graph_executor.cpp,torch/csrc/jit/graph_executor.h,torch/csrc/jit/graph_node_list.h,torch/csrc/jit/hooks_for_testing.cpp,torch/csrc/jit/hooks_for_testing.h,torch/csrc/jit/import.cpp,torch/csrc/jit/import.h,torch/csrc/jit/import_method.cpp,torch/csrc/jit/import_method.h,torch/csrc/jit/init.cpp,torch/csrc/jit/init.h,torch/csrc/jit/interpreter.cpp,torch/csrc/jit/interpreter.h,torch/csrc/jit/ir.cpp,torch/csrc/jit/ir.h,torch/csrc/jit/ivalue.h,torch/csrc/jit/named_value.h,torch/csrc/jit/node_hashing.cpp,torch/csrc/jit/node_hashing.h,torch/csrc/jit/operator.cpp,torch/csrc/jit/operator.h,torch/csrc/jit/passes/batch_mm.cpp,torch/csrc/jit/passes/batch_mm.h,torch/csrc/jit/passes/canonicalize.cpp,torch/csrc/jit/passes/canonicalize.h,torch/csrc/jit/passes/canonicalize_ops.cpp,torch/csrc/jit/passes/canonicalize_ops.h,torch/csrc/jit/passes/common_subexpression_elimination.cpp,torch/csrc/jit/passes/common_subexpression_elimination.h,torch/csrc/jit/passes/constant_pooling.cpp,torch/csrc/jit/passes/constant_pooling.h,torch/csrc/jit/passes/constant_propagation.cpp,torch/csrc/jit/passes/constant_propagation.h,torch/csrc/jit/passes/create_autodiff_subgraphs.h,torch/csrc/jit/passes/dead_code_elimination.cpp,torch/csrc/jit/passes/dead_code_elimination.h,torch/csrc/jit/passes/erase_number_types.cpp,torch/csrc/jit/passes/erase_number_types.h,torch/csrc/jit/passes/graph_fuser.cpp,torch/csrc/jit/passes/graph_fuser.h,torch/csrc/jit/passes/inline_autodiff_subgraphs.h,torch/csrc/jit/passes/inplace_check.cpp,torch/csrc/jit/passes/inplace_check.h,torch/csrc/jit/passes/loop_unrolling.cpp,torch/csrc/jit/passes/loop_unrolling.h,torch/csrc/jit/passes/lower_grad_of.cpp,torch/csrc/jit/passes/lower_grad_of.h,torch/csrc/jit/passes/lower_tuples.cpp,torch/csrc/jit/passes/lower_tuples.h,torch/csrc/jit/passes/onnx.cpp,torch/csrc/jit/passes/onnx.h,torch/csrc/jit/passes/onnx/fixup_onnx_loop.cpp,torch/csrc/jit/passes/onnx/fixup_onnx_loop.h,torch/csrc/jit/passes/onnx/peephole.cpp,torch/csrc/jit/passes/onnx/peephole.h,torch/csrc/jit/passes/onnx/prepare_division_for_onnx.cpp,torch/csrc/jit/passes/onnx/prepare_division_for_onnx.h,torch/csrc/jit/passes/peephole.cpp,torch/csrc/jit/passes/peephole.h,torch/csrc/jit/passes/python_print.cpp,torch/csrc/jit/passes/python_print.h,torch/csrc/jit/passes/remove_expands.cpp,torch/csrc/jit/passes/remove_expands.h,torch/csrc/jit/passes/remove_inplace_ops.cpp,torch/csrc/jit/passes/requires_grad_analysis.cpp,torch/csrc/jit/passes/requires_grad_analysis.h,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/passes/shape_analysis.h,torch/csrc/jit/passes/specialize_undef.cpp,torch/csrc/jit/passes/specialize_undef.h,torch/csrc/jit/passes/to_batch.cpp,torch/csrc/jit/passes/to_batch.h,torch/csrc/jit/passes/utils/check_alias_annotation.cpp,torch/csrc/jit/pybind.h,torch/csrc/jit/pybind_utils.h,torch/csrc/jit/python_arg_flatten.cpp,torch/csrc/jit/python_arg_flatten.h,torch/csrc/jit/python_interpreter.cpp,torch/csrc/jit/python_ir.cpp,torch/csrc/jit/python_ir.h,torch/csrc/jit/python_tracer.cpp,torch/csrc/jit/python_tracer.h,torch/csrc/jit/register_prim_ops.cpp,torch/csrc/jit/register_special_ops.cpp,torch/csrc/jit/resource_guard.h,torch/csrc/jit/scope.cpp,torch/csrc/jit/scope.h,torch/csrc/jit/script/builtin_functions.cpp,torch/csrc/jit/script/builtin_functions.h,torch/csrc/jit/script/compiler.cpp,torch/csrc/jit/script/compiler.h,torch/csrc/jit/script/error_report.h,torch/csrc/jit/script/final_returns.cpp,torch/csrc/jit/script/final_returns.h,torch/csrc/jit/script/init.cpp,torch/csrc/jit/script/jit_exception.h,torch/csrc/jit/script/lexer.cpp,torch/csrc/jit/script/lexer.h,torch/csrc/jit/script/module.cpp,torch/csrc/jit/script/module.h,torch/csrc/jit/script/parse_string_literal.h,torch/csrc/jit/script/parser.cpp,torch/csrc/jit/script/parser.h,torch/csrc/jit/script/python_tree_views.cpp,torch/csrc/jit/script/python_tree_views.h,torch/csrc/jit/script/schema_matching.cpp,torch/csrc/jit/script/schema_matching.h,torch/csrc/jit/script/sugared_value.cpp,torch/csrc/jit/script/sugared_value.h,torch/csrc/jit/script/tree.h,torch/csrc/jit/script/tree_views.h,torch/csrc/jit/script/type_parser.cpp,torch/csrc/jit/script/type_parser.h,torch/csrc/jit/source_location.h,torch/csrc/jit/source_range.h,torch/csrc/jit/stack.h,torch/csrc/jit/symbolic_script.cpp,torch/csrc/jit/symbolic_script.h,torch/csrc/jit/symbolic_variable.h,torch/csrc/jit/tracer.cpp,torch/csrc/jit/tracer.h,torch/csrc/jit/tracing_state.h,torch/csrc/jit/type.h,torch/csrc/jit/variable_tensor_list.h",169.0,15,3,6.113631581,13.0,35400.0,37.0,1716658.349112426,6223.0,19360.83333,0.0,Corrective,1.0,1
pytorch,1703f2abedf09b667170399cb2d7592e70d6f9bf,f646391f26dfdb955548fe94091ae39469cb22f2,Adam Paszke,adam.paszke@gmail.com,Thu Sep 08 19:49:02 2016 -0700,1473364142.0,"Bug fixes and test improvements

Fixed:
* tensor and storage printing
* legacy.nn module printing
* SpatialCrosMapLRN tests

Also, all fixed bugs have regression tests now.",115.0,34.0,"test/common_nn.py,test/test_legacy_nn.py,test/test_torch.py,torch/Tensor.py,torch/TensorPrinting.py,torch/legacy/nn/ConcatTable.py,torch/legacy/nn/Parallel.py,torch/legacy/nn/ParallelTable.py,torch/legacy/nn/SpatialCrossMapLRN.py,torch/legacy/nn/__init__.py",10.0,4,2,2.093428041,7.0,4799.0,2.0,32628.8,158.0,2128.179257,0.0,Corrective,1.0,1
pytorch,e1ff46b6e5b752a889174ba4a995e74898799287,f65ab89eddc5cf449b94fdbddb4715fb8f57d2d6,kshitij12345,kshitijkalambarkar@gmail.com,Mon Oct 05 08:36:53 2020 -0700,1601887013.0,"[numpy] Add torch.nan_to_num (#44592)

Summary:
Reference https://github.com/pytorch/pytorch/issues/42515

TODO:
* [x] Add tests
* [x] Add docs

Pull Request resolved: https://github.com/pytorch/pytorch/pull/44592

Reviewed By: colesbury

Differential Revision: D24079472

Pulled By: mruberry

fbshipit-source-id: 2b67d36cba46eaa7ca16cd72671b57750bd568bc",238.0,4.0,"aten/src/ATen/NumericUtils.h,aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/UnaryOps.h,aten/src/ATen/native/cpu/UnaryOpsKernel.cpp,aten/src/ATen/native/cuda/UnaryOpsKernel.cu,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_unary_ufuncs.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",17.0,15,5,3.424441621,45.0,39931.0,12.0,811333.2352941176,5709.0,13372.0,0.0,Feature Addition,0.0,1
pytorch,aad8738681045c977ddc09cd0507be9a8f95116e,f673def92d24deb4ecf062276c8a3e46f3aca19b,iurii zdebskyi,47012416+izdeby@users.noreply.github.com,Thu Sep 19 19:23:00 2019 -0700,1568920980.0,"Enabled where for bool tensor on CUDA (#26430)

Summary:
Enabled ""where_cuda"" for bool tensors on CUDA
Fixing https://github.com/pytorch/pytorch/issues/26247
Tested via unit tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26430

Differential Revision: D17464181

Pulled By: izdeby

fbshipit-source-id: cbb09925753b2e6f35e7400da3243d4d3fc86b69",7.0,1.0,"aten/src/ATen/native/cuda/TensorCompare.cu,test/test_torch.py",2.0,6,2,0.811278124,40.0,12998.0,2.0,7341950.0,11542.0,32466.83333,0.0,Corrective,1.0,1
pytorch,9b69f21a95fa626522ef371f8557e7286f9db318,f6af76ead7f03b1e75a920d93c3d2d387f5eaef7,Roy Li,royboy@fb.com,Sun Apr 07 08:35:11 2019 -0700,1554626111.0,"Remove tensorFromBlob() from Type (#18779)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/18779
ghimport-source-id: e7453b74fcce0e4f4a9cbce0324992a85272a426

Stack from [ghstack](https://github.com/ezyang/ghstack):
* #18780 Remove tensorWithAllocator() from Type
* **#18779 Remove tensorFromBlob() from Type**

Differential Revision: D14739335

fbshipit-source-id: 8a0619a5b412332efa3b2d60c1edebd53d089d50",116.0,117.0,"aten/src/ATen/DLConvertor.cpp,aten/src/ATen/SparseTensorUtils.h,aten/src/ATen/TensorUtils.cpp,aten/src/ATen/TensorUtils.h,aten/src/ATen/UndefinedType.cpp,aten/src/ATen/UndefinedType.h,aten/src/ATen/core/Type.h,aten/src/ATen/native/Resize.h,aten/src/ATen/native/cuda/TensorTransformations.cu,aten/src/ATen/templates/Functions.h,aten/src/ATen/templates/NativeFunctions.h,aten/src/ATen/templates/Type.h,aten/src/ATen/templates/TypeDefault.cpp,aten/src/ATen/templates/TypeDefault.h,aten/src/ATen/test/atest.cpp,caffe2/contrib/aten/aten_op_template.h,tools/autograd/templates/VariableType.h,torch/csrc/autograd/VariableTypeManual.cpp,torch/csrc/utils/tensor_numpy.cpp",19.0,18,4,3.653487312,13.0,3163.0,10.0,2603489.210526316,7960.0,24028.33333,0.0,,0.0,1
pytorch,33be4c94c04614de2dd164f3366a100dc64260d2,f6bbecf8b503eb6562d52b66124a49b45a57f14a,Mike Ruberry,mruberry@devfair044.h1.fair,Sun May 01 22:42:46 2022 +0000,1651444966.0,"Adds python ref consistency test, elementwise unary reference inputs, and formats test files

Per title.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76626
Approved by: https://github.com/ngimel",3357.0,1706.0,"test/test_binary_ufuncs.py,test/test_ops.py,test/test_prims.py,test/test_tensor_creation_ops.py,test/test_type_promotion.py,test/test_unary_ufuncs.py,test/test_view_ops.py,torch/_prims/__init__.py,torch/_prims/executor.py,torch/_prims/utils.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",12.0,5,2,2.143382408,5.0,35631.0,7.0,711340.3333333334,2737.0,6551.5,0.0,Feature Addition,0.0,1
pytorch,f53d4caa3ae94d6e978d6478bfc423b3be14042a,f6c0cd8cad1053a65518ada507616a2b21e20c5f,vfdev,vfdev.5@gmail.com,Mon Feb 28 21:57:39 2022 +0100,1646085459.0,"[functorch] Explicit error message when calling torch.where(x) (pytorch/functorch#547)

Description:
- Explicit error message when calling torch.where(x)
- Added a test

Related to https://github.com/pytorch/functorch/issues/445#issuecomment-1050946177",21.0,2.0,"functorch/functorch/csrc/BatchRulesDynamic.cpp,functorch/test/test_vmap.py",2.0,4,1,0.755375413,1.0,3667.0,2.0,6.0,834.0,1169.0,0.0,Feature Addition,0.0,1
pytorch,1a23c9901dbfee295bf5b3dad36e4d3ee7e86366,f6c708f86990f3fbae2558bbd6363f74782dfda4,gchanan,gregchanan@gmail.com,Mon Mar 12 20:20:10 2018 -0400,1520886010.0,Ensure torch.tensor and Tensor.new_tensor copy numpy data. (#5713),51.0,25.0,"test/test_torch.py,torch/csrc/autograd/python_variable_indexing.cpp,torch/csrc/utils/tensor_new.cpp,torch/csrc/utils/tensor_new.h",4.0,5,2,1.097116296,38.0,6718.0,4.0,408489.0,2451.0,24777.35823,0.0,,0.0,1
pytorch,a0b7169b7e9ffe9aa4c6d2ce23e50f7c76a1509a,f72d86e0d3f4c71656b1036352c53645cb95d073,Alican Bozkurt,alicanb@gmail.com,Fri Jan 19 20:45:14 2018 -0500,1516394714.0,Implement geometric distribution (#4708),171.0,10.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/constraints.py,torch/distributions/geometric.py,torch/distributions/kl.py",6.0,5,3,1.908669841,8.0,2755.0,4.0,287352.8,926.0,6746.172317,0.0,,0.0,1
pytorch,75b9e4a128496e9b2563078f62a2903f65a6d145,f767cf668395baf29ca7c9f1fa80f0abed8c53c7,Ilqar Ramazanli,iramazanli@fb.com,Tue Sep 07 15:41:09 2021 -0700,1631029269.0,"To change WarmUp Scheduler with ConstantLR and LinearLR (#64395)

Summary:
Partially unblocks https://github.com/pytorch/vision/issues/4281

Previously we have added WarmUp Schedulers to PyTorch Core in the PR : https://github.com/pytorch/pytorch/pull/60836 which had two mode of execution - linear and constant depending on warming up function.

In this PR we are changing this interface to more direct form, as separating linear and constant modes to separate Schedulers. In particular

```Python
scheduler1 = WarmUpLR(optimizer, warmup_factor=0.1, warmup_iters=5, warmup_method=""constant"")
scheduler2 = WarmUpLR(optimizer, warmup_factor=0.1, warmup_iters=5, warmup_method=""linear"")
```

will look like

```Python
scheduler1 = ConstantLR(optimizer, warmup_factor=0.1, warmup_iters=5)
scheduler2 = LinearLR(optimizer, warmup_factor=0.1, warmup_iters=5)
```

correspondingly.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/64395

Reviewed By: datumbox

Differential Revision: D30753688

Pulled By: iramazanli

fbshipit-source-id: e47f86d12033f80982ddf1faf5b46873adb4f324",156.0,100.0,"docs/source/optim.rst,test/test_optim.py,torch/optim/lr_scheduler.py,torch/optim/lr_scheduler.pyi",4.0,5,3,1.23169,35.0,4160.0,2.0,1496488.0,15228.0,34886.5,0.0,Feature Addition,0.0,1
pytorch,6305e572ede4aed71af2f988de2a0f88aaae7dc1,f78e0fc956ce35e874f24b7e441472f1fdd158ca,BowenBao,bowbao@microsoft.com,Wed Apr 13 19:18:43 2022 +0000,1649877523.0,"[ONNX] Support aminmax

Support exporting `torch.aminmax`.
One of the use case is exporting fake quantized models. The observer calls https://github.com/pytorch/pytorch/blob/1601a4dc9f689db3912190dcc8fdc70814896292/torch/ao/quantization/observer.py#L447.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/75714
Approved by: https://github.com/garymm",19.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.998000884,4.0,14702.0,2.0,78985.0,2242.0,5321.0,0.0,,0.0,1
pytorch,cff1ff7fb6744dd6370ccccc5c90230ca3721140,f7986969aff144b3f19a73a5de3595bc1ca1a3f4,James Reed,jamesreed@fb.com,Wed Dec 02 01:19:20 2020 -0800,1606871960.0,"[FX] Delete values after their last use (#48631)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48631

Test Plan: Imported from OSS

Reviewed By: zdevito

Differential Revision: D25235981

Pulled By: jamesr66a

fbshipit-source-id: f79d8873d3ad1ad90b5bd6367fc6119925f116e9",42.0,10.0,torch/fx/graph.py,1.0,2,1,0,1.0,700.0,1.0,1106637.0,7088.0,16013.0,0.0,,0.0,1
pytorch,316f0b89c3aa51329f40a13c00de587da60faa66,f7a8bfd0a15b3c3a07f92157b45869bcc33c9d74,Richard Zou,zou3519@gmail.com,Tue Jan 19 14:46:09 2021 -0800,1611067569.0,"Add batched grad testing to gradcheck, turn it on in test_autograd (#50592)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/50592

This adds a `check_batched_grad=False` option to gradcheck and gradgradcheck.
It defaults to False because gradcheck is a public API and I don't want
to break any existing non-pytorch users of gradcheck.
This:
- runs grad twice with two grad outputs, a & b
- runs a vmapped grad with torch.stack([a, b])
- compares the results of the above against each other.

Furthermore:
- `check_batched_grad=True` is set to be the default for
gradcheck/gradgradcheck inside of test_autograd.py. This is done by
reassigning to the gradcheck object inside test_autograd
- I manually added `check_batched_grad=False` to gradcheck instances
that don't support batched grad.
- I added a denylist for operations that don't support batched grad.

Question:
- Should we have a testing only gradcheck (e.g.,
torch.testing.gradcheck) that has different defaults from our public
API, torch.autograd.gradcheck?

Future:
- The future plan for this is to repeat the above for test_nn.py (the
autogenerated test will require a denylist)
- Finally, we can repeat the above for all pytorch test files that use
gradcheck.

Test Plan: - run tests

Reviewed By: albanD

Differential Revision: D25925942

Pulled By: zou3519

fbshipit-source-id: 4803c389953469d0bacb285774c895009059522f",118.0,26.0,"test/test_autograd.py,torch/autograd/gradcheck.py",2.0,3,2,0.959403754,42.0,8137.0,2.0,223586.0,8168.0,18472.0,0.0,Feature Addition,0.0,1
pytorch,ed6433829795055df5108fc405e9f43381728ffd,f7bcba33a6888383832575626b26ca6006d2d2ae,James Reed,jamesreed@fb.com,Fri Sep 06 04:41:25 2019 -0700,1567744885.0,"Vectorized specialization of max_pool2d for channels-last layout (#25676)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25676

This PR achieves two things:

1) Ensures the channels-last layout is propagated through the operator if we receive an input in that layout. This helps to alleviate unnecessary data movement in, e.g. ResNet inference
2) Applies interleaved vectorization along the channel dimension in the kernel. This allows us to use the functional units on the CPU much more effectively.

Benchmark script

```
import torch, time

for dtype in [torch.qint8, torch.quint8, torch.qint32]:
    print('****', str(dtype), '*****')
    x = torch.rand(1, 56, 56, 256)

    q_x = torch.quantize_linear(x, 0.5, 1, dtype)
    q_x = q_x.permute([0, 3, 1, 2])

    x = x.permute([0, 3, 1, 2])

    NITER = 100

    s = time.time()
    for i in range(NITER):
        float_out = torch.max_pool2d(x, kernel_size=3, stride=None, padding=0, dilation=1)
    time_per_iter_float = (time.time() - s) / NITER

    s = time.time()
    for i in range(NITER):
        quant_out = torch.max_pool2d(q_x, kernel_size=3, stride=None, padding=0, dilation=1)
    time_per_iter_quant = (time.time() - s) / NITER

    ref_quantized = torch.quantize_linear(float_out, 0.5, 1, dtype)
    torch.testing.assert_allclose(ref_quantized.dequantize(), quant_out.dequantize())

    print('time/iter ms (float)', 'time/iter ms (quant)', 'quant/float', sep='\t')
    print(time_per_iter_float * 1000, time_per_iter_quant * 1000, time_per_iter_quant / time_per_iter_float, sep='\t')

    bytes_float = (x.numel() + float_out.numel()) * x.element_size()
    bytes_quant = (q_x.numel() + quant_out.numel()) * q_x.element_size()

    float_bw_gbps = bytes_float / time_per_iter_float / 1e9
    quant_bw_gbps = bytes_quant / time_per_iter_quant / 1e9

    print('GB/s float', 'GB/s quant', sep='\t')
    print(float_bw_gbps, quant_bw_gbps, sep='\t')

```

Before this change (DynDisp to AVX2)

```
**** torch.qint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
5.197856426239014       1.2381434440612793      0.23820270175433766
GB/s float      GB/s quant
0.6816348335661166      0.7153936841878243
**** torch.quint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
5.14232873916626        1.1790156364440918      0.2292765974808621
GB/s float      GB/s quant
0.6889952353715999      0.7512707826941549
**** torch.qint32 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
4.918942451477051       3.401169776916504       0.6914432950715265
GB/s float      GB/s quant
0.7202849057394649      1.041712185038912
```

After this change (DynDisp to AVX2)

```
**** torch.qint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
5.0574493408203125      0.018107891082763672    0.0035804394394243393
GB/s float      GB/s quant
0.700558673203699       48.915690731270566
**** torch.quint8 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
4.984829425811768       0.016908645629882812    0.0033920209069399163
GB/s float      GB/s quant
0.7107645412406512      52.38503540665539
**** torch.qint32 *****
time/iter ms (float)    time/iter ms (quant)    quant/float
4.973354339599609       0.13938188552856445     0.028025729922108406
GB/s float      GB/s quant
0.7124044976624851      25.419658993448625
```

Test Plan: Imported from OSS

Differential Revision: D17196457

Pulled By: jamesr66a

fbshipit-source-id: 614be60ed74bed5d0369c58cc450b430cfabe5fb",318.0,71.0,"aten/src/ATen/cpu/vec256/vec256_qint.h,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qpool.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py",5.0,10,2,2.135327142,1.0,2515.0,3.0,118742.8,11192.0,31552.83333,0.0,Feature Addition,0.0,1
pytorch,800e24616ae0ec36a369900761282bca05169d24,f7bd3f7932dce495a69ba3c28fe2fd61a5564aed,Alykhan Tejani,atejani@twitter.com,Thu Nov 24 11:12:22 2016 +0000,1479985942.0,"added pixel shuffle layer + tests

removed duplicate save_for_backward",108.0,0.0,"test/test_nn.py,torch/nn/functions/pixelshuffle.py,torch/nn/modules/__init__.py,torch/nn/modules/pixelshuffle.py",4.0,5,2,1.605198921,17.0,1261.0,1.0,86577.0,337.0,2440.012243,0.0,Feature Addition,0.0,1
pytorch,83fa3f1c36dccdfe43857087f7588d44bf01e087,f80df4ca796b1073bd17a643d652b425b883fce4,Nik Ved,nik@quansight.com,Thu May 21 02:00:53 2020 -0700,1590026453.0,"port `scatter_add` to ATen (CUDA) (#38262)

Summary:
Fixes [https://github.com/pytorch/pytorch/issues/24622 ](https://github.com/pytorch/pytorch/issues/24622).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38262

Differential Revision: D21656729

Pulled By: ngimel

fbshipit-source-id: 63dcbf8eeaf59d8295bf4e5c8bb9d28ad165d4eb",101.0,155.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/cuda/ScatterGatherKernel.cu,aten/src/ATen/native/native_functions.yaml,aten/src/THC/generic/THCTensorScatterGather.cu,aten/src/THC/generic/THCTensorScatterGather.h,test/test_cuda.py,test/test_torch.py",8.0,8,2,2.038373842,43.0,30504.0,6.0,639089.25,2258.0,5628.0,0.0,Corrective,1.0,1
pytorch,4e6e11c139e4d4bcf948853dbdbf165284a0b3c8,f81db8afb84d0365055d07383bf63f71dbc55ea5,Horace He,chilli@fb.com,Sat Aug 03 01:41:34 2019 -0700,1564796494.0,"Initial torchbind prototype (#21098)

Summary:
I have some test code in there as well, along with a script ""test_libtorch"" to run it. You'll need to modify `test_libtorch` to point to where you have `pytorch` built. I currently require that `pybind11` is included as a subdirectory of the test, but added it to the `.gitignore` to make this reviewable.

Currently, something like this works:
```cpp
struct Foo {
  int x, y;
  Foo(): x(2), y(5){}
  Foo(int x_, int y_) : x(x_), y(y_) {}
  void display() {
    cout<<""x: ""<<x<<' '<<""y: ""<<y<<endl;
  }
  int64_t add(int64_t z) {
    return (x+y)*z;
  }
};
static auto test = torch::jit::class_<Foo>(""Foo"")
                    .def(torch::jit::init<int64_t, int64_t>())
                    .def(""display"", &Foo::display)
                    .def(""add"", &Foo::add)
                    .def(""combine"", &Foo::combine);

```
with
```py
torch.jit.script
def f(x):
    val = torch._C.Foo(5, 3)
    val.display()
    print(val.add(3))
```
results in
```
x: 5 y: 3
24
```

Current issues:
- [x] The python class created by torchscript doesn't interactly properly with the surrounding code.
```
torch.jit.script
def f(x):
    val = torch._C.Foo(5, 3)
    return val
```
- [x] Doesn't properly take in non-pointer classes. Can't define this function signature in cpp (We don't want to support this I believe).
```cpp
  void combine(Foo x) {
```

- [x] Has some issues with memory for blobs when constructing multiple objects (fix constant propagation pass to not treat capsules as the same object).
```py
torch.jit.script
def f(x):
    val = torch._C.Foo(5, 3)
    val2 = torch._C.Foo(100, 0)
    val.display()
    print(val.add(3))
```
- [ ] Can't define multiple constructors (need to define overload string. Currently not possible since we don't support overloaded methods).
- [x] `init` is a little bit different syntax than `pybind`. `.init<...>()` instead of `.def(py::init<>())`
- [x] I couldn't figure out how to add some files into the build so they'd be copied to the `include/` directories, so I symlinked them manually.
- [ ] Currently, the conversion from Python into Torchscript doesn't work.
- [ ] Torchbind also currently requires Python/Pybind dependency. Fixing this would probably involve some kind of macro to bind into Python when possible.
- [ ] We pass back into Python by value, currently. There's no way of passing by reference.
- [x] Currently can only register one method with the same type signature. This is because we create a `static auto opRegistry`, and the function is templated on the type signature.

Somewhat blocked on https://github.com/pytorch/pytorch/pull/21177. We currently use some structures that will be refactored by his PR (namely `return_type_to_ivalue` and `ivalue_to_arg_type`.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/21098

Differential Revision: D16634872

Pulled By: Chillee

fbshipit-source-id: 1408bb89ea649c27d560df59e2cf9920467fe1de",607.0,20.0,".jenkins/pytorch/macos-test.sh,.jenkins/pytorch/test.sh,.jenkins/pytorch/win-test-helpers/test_custom_script_ops.bat,aten/src/ATen/core/ivalue.cpp,aten/src/ATen/core/ivalue.h,aten/src/ATen/core/ivalue_inl.h,aten/src/ATen/core/jit_type.h,aten/src/ATen/core/op_registration/kernel_functor.h,aten/src/ATen/core/type.cpp,c10/util/C++17.h,caffe2/CMakeLists.txt,test/custom_operator/CMakeLists.txt,test/custom_operator/classes.cpp,test/custom_operator/test_custom_classes.py,torch/CMakeLists.txt,torch/__init__.py,torch/_classes.py,torch/_jit_internal.py,torch/_ops.py,torch/csrc/THP_export.h,torch/csrc/jit/node_hashing.cpp,torch/csrc/jit/pybind_utils.h,torch/csrc/jit/script/schema_type_parser.cpp,torch/custom_class.h",24.0,17,6,3.184609964,48.0,8398.0,17.0,5676015.7,10380.0,29667.33333,0.0,Corrective,1.0,1
pytorch,3fa53da61a82a19193619f48c3bfcf91d5eb20c0,f8455ed754196cd554cbaaaaa440dd178383154d,Jane Wang,janewang@fb.com,Wed Dec 12 05:03:13 2018 -0800,1544590993.0,"add gloo support for gather on GPU (#14916)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/14916

as titled

Reviewed By: pietern

Differential Revision: D13267832

fbshipit-source-id: 3b89d08af93f74941f17ff892c33fc2a4a023c19",168.0,7.0,"test/test_c10d.py,torch/lib/c10d/ProcessGroupGloo.cpp",2.0,4,2,0.910487623,3.0,3110.0,1.0,14997.0,6005.0,18666.33333,0.0,Feature Addition,0.0,1
pytorch,f6274a4ef7b34807b116d9516ce62c38efe30a6b,f89569818361b8542efdb24384905163a4d00ca4,Fritz Obermeyer,fritz.obermeyer@gmail.com,Sun Mar 25 11:32:06 2018 -0700,1521977526.0,"Implement MultivariateNormal.mean, .variance properties (#5988)",22.0,2.0,"test/test_distributions.py,torch/distributions/multivariate_normal.py",2.0,3,2,0.979868757,8.0,3487.0,2.0,271730.0,1008.0,6893.172317,0.0,,0.0,1
pytorch,136bb07a93b779acbc84ff341bc397551a8cfcc2,f8b758b141ccfe3f29176cb350606dae51421c43,Dylan Bespalko,dylan.bespalko@gmail.com,Thu Oct 24 16:29:37 2019 -0700,1571934577.0,"CPU-Strided-Complex Support for reduce ops and linpack ops (#27653)

Summary:
In-tree changes to pytorch to support complex numbers are being submitted here.
Out-of-tree support for complex numbers is here: [pytorch-cpu-strided-complex extension](https://gitlab.com/pytorch-complex/pytorch-cpu-strided-complex)

Changes so far:

- [x]  Renamed references to variable ""I"" that may be confused for ""I"" defined in complex.h.  I did this to avoid crazy CI failures messages as complex.h is included by more source files.
     - aten/src/ATen/native/cpu/Loops.h (Renamed I to INDEX)
     - aten/src/ATen/native/cuda/Loops.cuh (Renamed I to INDEX)
     - aten/src/ATen/core/ivalue_inl.h (Renamed I to INDEX)
     - c10/util/Array.h (Renamed I to INDEX)
     - c10/util/C++17.h (Renamed I to INDEX)
    - c10/util/Metaprogramming.h (Renamed I to INDEX)
    - c10/util/SmallVector.h (custom renaming)
- [x]  Added complex support of Linear Algebra Ops.
     - SVD needed to be modified to support mixed data types
     - Example U(std::complex<double)), S(double), V(std::complex<double>)
     - See before and after benchmark below (No observable change in performance).
- [x]  Added complex support of Reduce Ops.
     - var/std computations could have been faster if it was possible to interpret std::complex<double> Tensor as a double Tensor.
- [x]  Added complex derivative support for autograd functionality.
     - derivatives are the same as defined by numpy autograd library for real(), imag(), conj(), angle(). These functions only affect complex numbers.
     - derivative of abs() has not been modified to not interfere with existing code.
     - Autograd defines abs() for complex numbers and fabs() for real numbers. I will look into this further down the road.

 ----------------------------------------
 PyTorch/Caffe2 Operator Micro-benchmarks Before Changes
----------------------------------------
Tag : short

Benchmarking PyTorch: svd
Mode: Eager
Name: svd_M512_N512
Input: M: 512, N: 512
Forward Execution Time (us) : 162339.425
Forward Execution Time (us) : 162517.479
Forward Execution Time (us) : 162847.775

----------------------------------------
PyTorch/Caffe2 Operator Micro-benchmarks After Changes
----------------------------------------
Tag : short

Benchmarking PyTorch: svd
Mode: Eager
Name: svd_M512_N512
Input: M: 512, N: 512
Forward Execution Time (us) : 162032.117
Forward Execution Time (us) : 161943.484
Forward Execution Time (us) : 162513.786
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27653

Differential Revision: D17907886

Pulled By: ezyang

fbshipit-source-id: a88b6d0427591ec1fba09e97c880f535c5d0e513",518.0,219.0,"aten/src/ATen/Dispatch.h,aten/src/ATen/NumericUtils.h,aten/src/ATen/core/ivalue_inl.h,aten/src/ATen/cpu/vec256/vec256_complex_double.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/SharedReduceOps.h,aten/src/ATen/native/cpu/ReduceOpsKernel.cpp,aten/src/ATen/native/cpu/zmath.h,aten/src/ATen/native/cuda/Loops.cuh,c10/core/ScalarType.h,c10/util/Array.h,c10/util/C++17.h,c10/util/Metaprogramming.h,c10/util/SmallVector.h,tools/autograd/derivatives.yaml,torch/_torch_docs.py",19.0,15,4,3.15264053,31.0,16644.0,15.0,3514602.736842105,12504.0,34827.83333,0.0,Feature Addition,0.0,1
pytorch,a5ce0126cced61b1555911b9530e906f81e4e478,f8c18e00d5538fa569e23adb4b9b588a2decfdf8,sunnieshang,sunnieshang@users.noreply.github.com,Tue Jun 05 03:40:43 2018 -0700,1528170043.0,"Fix a corner case for ReShapeOp (#8142)

In my use case, in the backward propogate pass, the reshape need to
change a [0] tensor into [0,0] shaped tensor. The original implementation would
cause out of index issue. This diff fix this problem.",1.0,1.0,caffe2/operators/reshape_op.h,1.0,2,1,0,5.0,135.0,1.0,280774.0,46.0,108.0,0.0,Corrective,1.0,1
pytorch,ffc4a502599dcccb48f9bad5ed44c0d7c4d8f391,f8c408b79a5738289eb13be62c2476cc3ccafd1e,Richard Zou,zou3519@gmail.com,Wed Aug 10 14:43:54 2022 -0700,1660142634.0,"[functorch] vjpvjp inplace testing (#83119)

Test Plan:
- run tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83119
Approved by: https://github.com/Chillee",21.0,17.0,functorch/test/test_ops.py,1.0,2,1,0,2.0,1367.0,1.0,1.0,6342.0,14716.5,0.0,,0.0,1
pytorch,4f5a6c366e001082a292b980308fbaf629aa5ef9,f8d4f980b3df669e2c70993580d9c5b340eca97f,Adam Paszke,adam.paszke@gmail.com,Tue Jan 24 20:37:51 2017 +0100,1485290271.0,Add upsampling modules and functions,212.0,16.0,"test/common_nn.py,test/test_nn.py,torch/nn/_functions/thnn/__init__.py,torch/nn/_functions/thnn/upsampling.py,torch/nn/functional.py,torch/nn/modules/__init__.py,torch/nn/modules/upsampling.py",7.0,6,2,1.940299418,21.0,2731.0,3.0,57388.16666666666,391.0,4920.476424,0.0,Feature Addition,0.0,1
pytorch,1bd291c57cdf854b15cecdf21feb4d3ceab20b8d,f908432eb36138990cf66c681c741fab33fa63e3,Adam Paszke,adam.paszke@gmail.com,Sat Dec 31 15:23:00 2016 +0100,1483197780.0,Ensure that Variable's grad is shared between processes,89.0,0.0,"test/test_multiprocessing.py,torch/multiprocessing/reductions.py",2.0,3,2,0.942309489,18.0,433.0,2.0,29353.0,280.0,6361.724559,0.0,,0.0,1
pytorch,3b6644d1951f1b38481ed3900c2adbab1bcd665f,f91bb960712efcafd06f5ac4a0a5e91c96212305,Adam Paszke,adam.paszke@gmail.com,Mon Jan 16 18:32:22 2017 +0100,1484591542.0,"Remove cmin, cmax and cinv",226.0,269.0,"docs/source/nn.rst,docs/source/tensors.rst,docs/source/torch.rst,test/test_autograd.py,test/test_cuda.py,test/test_torch.py,torch/autograd/_functions/pointwise.py,torch/autograd/_functions/stochastic.py,torch/autograd/variable.py,torch/csrc/Module.cpp,torch/csrc/generic/methods/TensorCompare.cwrap,torch/csrc/generic/methods/TensorMath.cwrap,torch/docs.py,torch/legacy/nn/CosineDistance.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/HingeEmbeddingCriterion.py,torch/legacy/nn/MarginRankingCriterion.py,torch/nn/_functions/loss.py",18.0,13,3,2.628817262,23.0,14116.0,3.0,220114.16666666663,340.0,4847.476424,0.0,,0.0,1
pytorch,2652f2e3346d50da52a35f54a575f0f7489d7ef2,f91fcefc8159c388133daa3b8643bfd37e76b236,Yi Wang,wayi@fb.com,Wed Nov 04 02:29:38 2020 -0800,1604456978.0,"[Gradient Compression] Surface C++ comm hooks to Python API as built-in comm hooks (#47270)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/47270

This is almost same as #46959, except that in caffe2/torch/nn/parallel/distributed.py, BuiltinCommHookType should be imported conditionally, only when dist.is_available(). Otherwise, this Python enum type defined in caffe2/torch/scrc/distributed/c10d/init.cpp cannot be imported. See https://github.com/pytorch/pytorch/issues/47153

I tried to follow another enum type enum type ReduceOp defined in the same file, but did not work, because the C++ enum class is defined torch/lib/c10d library, but BuiltinCommHookType is defined in torch/csrc/distributed library. These two libraries are compiled in two different ways.

To avoid adding typing to distributed package, which can be a new project, I simply removed the arg type of BuiltinCommHookType in this file.

To review the diff on top of #46959, compare V1 vs Latest:
https://www.internalfb.com/diff/D24700959?src_version_fbid=270445741055617

Main Changes in V1 (#46959):
1. Implemented the Pybind part.
2. In the reducer, once the builtin_comm_hook_type is set,  a c++ comm hook instance will be created in Reducer::autograd_hook.
3. Added unit tests for the builit-in comm hooks.

Original PR issue: C++ DDP Communication Hook https://github.com/pytorch/pytorch/issues/46348
ghstack-source-id: 115783237

Test Plan:
buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_builtin_ddp_comm_hooks_nccl

//arvr/projects/eye_tracking/Masquerade:python_test

USE_DISTRIBUTED=0 USE_GLOO=0 BUILD_TEST=0 USE_CUDA=1 USE_MKLDNN=0 DEBUG=0 python setup.py install

Reviewed By: mrshenli

Differential Revision: D24700959

fbshipit-source-id: 69f303a48ae275aa856e6e9b50e12ad8602e1c7a",192.0,21.0,"test/distributed/test_c10d.py,torch/csrc/distributed/c10d/comm.h,torch/csrc/distributed/c10d/default_comm_hooks.h,torch/csrc/distributed/c10d/init.cpp,torch/csrc/distributed/c10d/reducer.cpp,torch/csrc/distributed/c10d/reducer.h,torch/distributed/algorithms/ddp_comm_hooks/__init__.py,torch/lib/c10d/ProcessGroupNCCL.hpp,torch/nn/parallel/distributed.py",9.0,13,2,2.750439701,19.0,9279.0,1.0,78152.0,6452.0,14717.5,0.0,Corrective,1.0,1
pytorch,19ecb5f8adf863ccb1825f8f984b8512245d4c13,f92edf7ef465920e75e86a7b3e7044d49b99f9db,Gregory Chanan,gchanan@fb.com,Tue Jul 10 02:25:01 2018 -0700,1531189501.0,"N-dimensional empty tensors: indexing, factories, reductions. (#9209)

Summary:
This PR implements and tests N-dimensional empty tensors for indexing, factories, and reductions if compiled with -DUSE_TH_SIZE_ZERO_DIM.

Still remaining to add:
1) TensorShape functions
2) Simple linear algebra functions (matrix multiply variants)
3) Other functions that operate over a dimension (but don't reduce).
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9209

Reviewed By: ezyang

Differential Revision: D8751257

Pulled By: gchanan

fbshipit-source-id: 2113374dc7af6caf31a99bf67b3893f130a29e23",567.0,96.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/ExpandUtils.cpp,aten/src/ATen/native/Distance.cpp,aten/src/ATen/native/Embedding.cpp,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/ReduceOpsUtils.h,aten/src/ATen/native/TensorCompare.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/cuda/CUDAReduceOps.cpp,aten/src/ATen/native/cuda/TensorFactories.cu,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/scalar_tensor_test.cpp,aten/src/ATen/test/wrapdim_test.cpp,aten/src/TH/generic/THTensor.cpp,aten/src/TH/generic/THTensorMath.cpp,aten/src/THC/THCTensor.cpp,aten/src/THC/generic/THCTensorMath.cu,aten/src/THC/generic/THCTensorMathReduce.cu,test/test_indexing.py,test/test_torch.py,tools/autograd/gen_python_functions.py,torch/csrc/autograd/python_variable_indexing.cpp",24.0,16,4,3.388257635,42.0,25183.0,17.0,1390522.8260869563,2863.0,6553.833333,0.0,Feature Addition,0.0,1
pytorch,d58f209326dcfec719a740519428dfa635004c24,f95ed474acf442ec1c1fe14aa439bd4deaec88bc,Samantha Andow,samdow@fb.com,Fri Oct 29 15:34:41 2021 -0700,1635521681.0,"Norms Op Info (#67442)

Summary:
Adds op infos for group_norm, instance_norm, and local_response_norm

Pull Request resolved: https://github.com/pytorch/pytorch/pull/67442

Reviewed By: mruberry

Differential Revision: D31992225

Pulled By: samdow

fbshipit-source-id: 5bf3e21cff2a39ca3e47dbe13db7671c617aaad1",164.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,11610.0,1.0,71638.0,16703.0,39141.0,0.0,Feature Addition,0.0,1
pytorch,8983bf13f41ab36e42d0acc7955c156fa441ac29,f98c38497398ac2adb43e8d04b491d6f2e126fdf,lynic,xuanlangjian@gmail.com,Thu Jul 13 13:56:12 2017 +0800,1499954172.0,"Raise error when call from_numpy on 0-dim array (#2075)

* Raise error when call from_numpy on 0-dim array

Fixes: #2055

* reword error message",7.0,3.0,"test/test_torch.py,torch/csrc/generic/Tensor.cpp",2.0,4,2,0.970950594,34.0,5889.0,1.0,32074.0,1154.0,13889.22101,0.0,Corrective,1.0,1
pytorch,9dd8a129de7b3b06e4736a800db3afbd77d2b90b,f99bc714c7bacf9a74c85514ef70b677415d4f0c,Igor Fedan,ifedan@fb.com,Thu Sep 26 23:03:43 2019 -0700,1569539023.0,"Migrate lt and lt_ from the TH to Aten (#25998)

Summary:
https://github.com/pytorch/pytorch/issues/24593
https://github.com/pytorch/pytorch/issues/24727

**torch.lt(Tensor a, Tensor b)**
will compute common dtype (highest) based on inputs and then compare values. The result will be Bool tensor
```
>>> x = torch.tensor([0], dtype=torch.int)
>>> y = torch.tensor([0.5], dtype=torch.double)
>>> x < y
tensor([True])
```
Previously it was impossible to make comparison of two tensors with different dtype.

**torch.lt(Tensor a, Tensor b, out=c)**
will compute common dtype (highest) based on inputs and then compare values. The result can be populated only to Bool tensor
```
>>> x = torch.tensor([0], dtype=torch.int)
>>> y = torch.tensor([0.5], dtype=torch.double)
>>> z = torch.empty([1], dtype=torch.bool)
>>> torch.lt(x, y, out=z)
tensor([True])
```
Previously it was impossible to make comparison of two tensors with different dtype. Also previously the result dtype could be Bool and Byte(deprecated). Currently it will accept only Bool result.

**a.lt_(Tensor b)**
Expects that a and b has same dtype, otherwise it's possible to get an overflow(Example: 'a' is uint8, 'b' is float32. 'a' will be promoted to float32 and the result will be also float32. Then it will be casted back to uint8 so potential for overflow). Will not compute common dtype. Result will have type of a.
```
>>> x = torch.tensor([0], dtype=torch.double)
>>> y = torch.tensor([0.5], dtype=torch.double)
>>> x < y
tensor([True])
```
Works similar to previous implementation.

**torch.lt(Tensor a, Scalar b)**
will check if there is no overflow when converting b to the same type as a. Then will compute common dtype and compare.
```
>>> x = torch.tensor([0], dtype=torch.double)
>>> x < 0.5
tensor([True])

>>> x = torch.tensor([0], dtype=torch.int)
>>> x < 0.5
tensor([True])
```
Fix https://github.com/pytorch/pytorch/issues/22301.

**torch.lt(Tensor a, Scalar b, out=c)**
will check if there is no overflow when converting b to the same type as a. Then will compute common dtype and compare. The result can be populated only to Bool tensor
```
>>> x = torch.tensor([0], dtype=torch.double)
>>> torch.lt(x, 0.5, out=z)
tensor([True])
```
Previously the result dtype could be Bool and Byte(deprecated). Currently it will accept only Bool result. The rest works similar to previous implementation.

**torch.lt_(Tensor a, Scalar b)**
will check if there is no overflow when converting b to the same type as a. Then will compute common dtype and compare. Result will have type of a.
```
>>> x = torch.tensor([0], dtype=torch.int)
>>> x.lt_(1)
tensor([1], dtype=torch.int32)
>>> x = torch.tensor([0], dtype=torch.int)
>>> x.lt_(1.0)
tensor([1], dtype=torch.int32)
```
Works similar to previous implementation.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25998

Differential Revision: D17431853

Pulled By: ifedan

fbshipit-source-id: b5effc6a5d9b32da379395b32abc628b604faaf7",264.0,198.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/core/TensorMethods.h,aten/src/ATen/detail/ScalarTypeConversions.h,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/BinaryOps.h,aten/src/ATen/native/LegacyDefinitions.cpp,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/BinaryOpsKernel.cu,aten/src/ATen/native/cuda/LegacyDefinitions.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/test/tensor_iterator_test.cpp,aten/src/TH/generic/THTensorMoreMath.cpp,aten/src/THC/THCTensorMathCompareT.cuh,aten/src/THC/generic/THCTensorMathCompareT.cu,aten/src/THC/generic/THCTensorMathCompareT.h,test/test_torch.py,test/test_type_promotion.py",19.0,14,2,3.635545801,42.0,32167.0,16.0,2464666.0,11800.0,33087.33333,0.0,Corrective,1.0,1
pytorch,c8914afdfa2414af237ad3d6692a7bb7e40ad9c4,f9a0d0c21e0fd66f003931019c4dcc643b52e291,Hameer Abbasi,hameerabbasi@yahoo.com,Thu Sep 10 16:00:49 2020 -0700,1599753649.0,"Allow Tensor-likes in torch.autograd.gradcheck (#43877)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/42942

Pull Request resolved: https://github.com/pytorch/pytorch/pull/43877

Reviewed By: zou3519

Differential Revision: D23493257

Pulled By: ezyang

fbshipit-source-id: 6cdaabe17157b484e9491189706ccc15420ac239",174.0,24.0,"test/test_overrides.py,torch/autograd/__init__.py,torch/autograd/gradcheck.py,torch/overrides.py",4.0,3,2,1.028250595,41.0,2572.0,4.0,2589555.75,5001.0,11481.5,0.0,Corrective,1.0,1
pytorch,f7a8bfd0a15b3c3a07f92157b45869bcc33c9d74,f9a5ba7398757a33f579d5e1d35d395917efc333,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Tue Jan 19 15:26:02 2021 -0800,1611069962.0,"Added linalg.slogdet (#49194)

Summary:
This PR adds `torch.linalg.slogdet`.

Changes compared to the original torch.slogdet:

- Complex input now works as in NumPy
- Added out= variant (allocates temporary and makes a copy for now)
- Updated `slogdet_backward` to work with complex input

Ref. https://github.com/pytorch/pytorch/issues/42666

Pull Request resolved: https://github.com/pytorch/pytorch/pull/49194

Reviewed By: VitalyFedyunin

Differential Revision: D25916959

Pulled By: mruberry

fbshipit-source-id: cf9be8c5c044870200dcce38be48cd0d10e61a48",274.0,46.0,"aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,test/test_namedtuple_return_api.py,test/test_ops.py,tools/autograd/derivatives.yaml,torch/csrc/api/include/torch/linalg.h,torch/csrc/autograd/FunctionsManual.cpp,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",13.0,18,5,2.848893292,16.0,30489.0,8.0,379201.3076923077,8169.0,18478.5,0.0,Feature Addition,0.0,1
pytorch,f045dab3ddc36405d7159b3fedb1fb76f0d4983b,f9ad5528e0dc7d0af055b2214a14a89c1853fa2c,Brian Stark,brstark@microsoft.com,Wed Feb 12 22:52:54 2020 -0800,1581547974.0,"Fix for rand_like as well. (#33095)

Summary:
This is a followup PR to https://github.com/pytorch/pytorch/issues/32830 This solves the same issue for RandLike which we saw in RandNLike
Pull Request resolved: https://github.com/pytorch/pytorch/pull/33095

Reviewed By: hl475

Differential Revision: D19848625

Pulled By: houseroad

fbshipit-source-id: 147921becf79490027a93606d52c5bc41d9eaf7f",2.0,1.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.918295834,2.0,5076.0,1.0,403673.0,14710.0,39575.33333,0.0,Corrective,1.0,1
pytorch,bc591d76a10c79f179d0bea016e59096add511a3,f9ae296a85c9e3835cd8664d18fea9282c205e58,Brian Hirsh,hirsheybar@fb.com,Thu Sep 24 15:42:44 2020 -0700,1600962164.0,"renaming TestDdpCommHook class so it doesn't get picked up as a test by pytest (#44905)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/44905

Test Plan: Imported from OSS

Reviewed By: mrshenli

Differential Revision: D23825308

Pulled By: bdhirsh

fbshipit-source-id: 17a07b3bd211850d6ecca793fd9ef3f326ca9274",6.0,6.0,test/distributed/test_c10d.py,1.0,2,1,0,2.0,3859.0,1.0,516.0,5395.0,12600.0,0.0,,0.0,1
pytorch,5e088da5ba021735ccb8e562ecd2f39fdd1162a7,f9d02903b7c56360ab8f2e774751b71c19fd7fd9,Sam Gross,colesbury@gmail.com,Tue Aug 15 06:46:25 2017 -0400,1502779585.0,"Always copy indices in Embedding._renorm (#2414)

LookupTable_renorm sorts and de-dupes the passed in indices tensor
in-place.

Fixes #2413",17.0,4.0,"test/test_nn.py,torch/nn/_functions/thnn/sparse.py",2.0,5,2,0.863120569,34.0,4238.0,2.0,313104.0,1317.0,15789.43711,0.0,Corrective,1.0,1
pytorch,345b26ca08030c06c16afe0260a8f0e34c974bbc,f9e7f132fba17d2bdf5282f32798f2692f98f96c,Heitor Schueroff,heitorschueroff@fb.com,Tue Mar 23 22:08:06 2021 -0700,1616537286.0,"Added torch.linalg.matrix_power (#52608)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/52608

**TODO**

- [x] Add OpInfo
- [x] Update documentation
- [x] Add more tests and compare against NumPy

Test Plan: Imported from OSS

Reviewed By: bdhirsh

Differential Revision: D27261532

Pulled By: heitorschueroff

fbshipit-source-id: c1e4ab297da3683f6d5751be8790602f9dc37b6b",229.0,79.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/core/interned_strings.h,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/test_linalg.py,torch/csrc/api/include/torch/linalg.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",10.0,16,4,2.198769272,12.0,28055.0,7.0,304191.5,10010.0,22174.0,0.0,Feature Addition,0.0,1
pytorch,a70020465b8c1cef61b51d51ccaf272a6d0b4149,f9e8dc005aeb5aadcaf397808f50170fe5358d79,kshitij12345,kshitijkalambarkar@gmail.com,Sun May 23 01:24:44 2021 -0700,1621733084.0,"OpInfo: clone, contiguous (#58390)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58390

Reviewed By: soulitzer

Differential Revision: D28567821

Pulled By: mruberry

fbshipit-source-id: bcf42cb4a9a57d8a15a76819b8a9e2df97cf00be",38.0,5.0,"test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.521949079,2.0,11279.0,3.0,222491.66666666663,12382.0,28047.0,0.0,,0.0,1
pytorch,415595ace4040c942d48db482ac61e8b91e778da,f9f135c5d8fc1986e112e62473b3498cdf4d1c43,lixinyu,lixinyu@devgpu175.prn2.facebook.com,Fri Mar 06 13:59:20 2020 -0800,1583503160.0,"ChannelsLast3d support is_contiguous, contiguous, suggest_memory_format, caching (#33033)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33033

Test Plan: Imported from OSS

Differential Revision: D19759661

Pulled By: glaringlee

fbshipit-source-id: 6c4798fa93589338c0c71c5308b9fd1151330245",534.0,255.0,"aten/src/ATen/cudnn/Descriptors.cpp,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/templates/TensorBody.h,c10/core/MemoryFormat.h,c10/core/TensorImpl.cpp,c10/core/TensorImpl.h,test/test_torch.py,torch/csrc/utils/tensor_memoryformats.cpp",9.0,12,4,1.952169005,41.0,20037.0,7.0,1850203.3333333333,15259.0,40922.83333,0.0,,0.0,1
pytorch,0bb341daaadee7b93d1290db0305843508702128,f9f758e3497b357668c2a259ea070bb33c591a0b,Shen Li,cs.shenli@gmail.com,Fri Jan 08 19:45:56 2021 -0800,1610135156.0,"Apply clang-format to rpc cpp files (#50236)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50236

Test Plan: Imported from OSS

Reviewed By: lw

Differential Revision: D25847892

Pulled By: mrshenli

fbshipit-source-id: b4af1221acfcaba8903c629869943abbf877e04e",169.0,155.0,"torch/csrc/distributed/rpc/init.cpp,torch/csrc/distributed/rpc/message.h,torch/csrc/distributed/rpc/process_group_agent.cpp,torch/csrc/distributed/rpc/py_rref.cpp,torch/csrc/distributed/rpc/py_rref.h,torch/csrc/distributed/rpc/request_callback.cpp,torch/csrc/distributed/rpc/request_callback.h,torch/csrc/distributed/rpc/request_callback_impl.cpp,torch/csrc/distributed/rpc/request_callback_no_python.cpp,torch/csrc/distributed/rpc/request_callback_no_python.h,torch/csrc/distributed/rpc/rpc_agent.cpp,torch/csrc/distributed/rpc/rref_context.h,torch/csrc/distributed/rpc/rref_impl.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.cpp,torch/csrc/distributed/rpc/tensorpipe_agent.h",15.0,4,1,2.549558774,2.0,6215.0,7.0,468463.4,7967.0,18018.5,0.0,,0.0,1
pytorch,bd09ab6687cf84b44d6f95d7a501161cea768939,f9fb37ca792f5271777deae4cf2d14d763a68887,Sam Gross,sgross@fb.com,Fri Oct 05 21:48:12 2018 -0700,1538776092.0,"Guard Denormals-Are-Zero with runtime CPU check (#12386)

Summary:
Previously, we were only enabling Flush-To-Zero (FTZ) and
Denormals-Are-Zero (DAZ) when compiling with SSE3 enabled. After,
Christian's patch (https://github.com/pytorch/pytorch/pull/12109) we
won't be compiling core files with SSE3 or SSE4 enabled, to better
support older AMD processors.

This moves the FTZ and DAZ code behind a runtime CPU check in
preparation for that change.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/12386

Differential Revision: D10222237

Pulled By: colesbury

fbshipit-source-id: 7ffe32561ab965e1e5f9eb6e679602bbf4775538",51.0,18.0,"aten/src/ATen/CMakeLists.txt,aten/src/ATen/Context.cpp,aten/src/ATen/cpu/FlushDenormal.cpp,aten/src/ATen/cpu/FlushDenormal.h,setup.py",5.0,4,1,1.81342853,44.0,1793.0,3.0,959961.3333333334,4473.0,13116.83333,0.0,,1.0,1
pytorch,5e72ebeda34742bb33d9461755e5c698305e4305,fa153184c8f70259337777a1fd1d803c7325f758,Paul Shao,pshao@fb.com,Sun Jul 12 19:09:15 2020 -0700,1594580955.0,"Fake Quantization Per Channel Kernel Core Implementation (CPU) (#41037)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/41037

This diff contains the core implementation for the fake quantizer per channel kernel that supports back propagation on the scale and zero point.

Test Plan:
On a devvm, use:
- `buck test //caffe2/test:quantization -- learnable_forward_per_channel`
- `buck test //caffe2/test:quantization -- learnable_backward_per_channel`

Reviewed By: z-a-f

Differential Revision: D22395665

fbshipit-source-id: 280c2405d04adfeda9fb9cfc94d89e8d868e0d41",374.0,1.0,"aten/src/ATen/native/Normalization.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/fake_quant_affine.h,aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp,test/quantization/test_workflow_module.py,tools/autograd/derivatives.yaml",7.0,11,3,1.691503439,16.0,12714.0,3.0,320110.4285714286,3552.0,8402.0,0.0,,0.0,1
pytorch,61012080c848dfb735c1236d86e524696286a9d4,fa189641b5548ee871b5e3f87940b1a2b9631b13,BowenBao,semisqg@gmail.com,Thu May 16 20:45:18 2019 -0700,1558039518.0,"Add export for __and__ & __or__ (#17894)

Summary:
In onnx spec, the supported input/output type for `And` and `Or` is `Bool` only.
Thus in exporting, cast to/from `Bool` is inserted for input/output.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/17894

Reviewed By: zrphercule

Differential Revision: D15103148

Pulled By: houseroad

fbshipit-source-id: 3e1068ea236c743260d42882fb11f0e3a21707e6",51.0,11.0,"test/onnx/test_pytorch_onnx_caffe2.py,torch/onnx/symbolic_helper.py,torch/onnx/symbolic_opset9.py",3.0,4,2,1.058454669,1.0,3560.0,2.0,342070.0,8708.0,25850.33333,0.0,Feature Addition,0.0,1
pytorch,aab30d4ea25686bcd8502aa280ba75d73559c2e4,fa4f363b9355e3bff9216db78fc802d1c8bf9cc8,Dmitry Ulyanov,dmitry.ulyanov.msu@gmail.com,Sun Apr 23 12:49:15 2017 +0300,1492951755.0,"Instance norm (#1283)

* instance norm

* fix whitespaces

* whitespaces

* docs

* ""C"" letter was cyrillic in docs, fixed

* remove force_eval, fix non contiguous case",244.0,2.0,"docs/source/nn.rst,test/test_nn.py,torch/nn/modules/__init__.py,torch/nn/modules/instancenorm.py",4.0,6,3,1.268823775,28.0,3671.0,1.0,102881.0,644.0,6287.335419,0.0,Corrective,1.0,1
pytorch,f8c408b79a5738289eb13be62c2476cc3ccafd1e,fa54021a0c91258c781c5073b672cb542ea609b0,Richard Zou,zou3519@gmail.com,Wed Aug 10 15:00:24 2022 -0700,1660143624.0,"[functorch] Add some more view+inplace grad+vmap tests (#83176)

PyTorch autograd turns view+inplace into some sequence of as_strided and
a special CopySlices node. This PR:
- adds a test for that (`test_inplace_on_view`)
- modifies some other testing that also tests this behavior
(`test_inplace_view`, `test_inplace_manyview`) to also test
non-contiguous inputs.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83176
Approved by: https://github.com/Chillee",51.0,19.0,functorch/test/test_vmap.py,1.0,2,1,0,2.0,4351.0,1.0,4.0,6343.0,14717.0,0.0,Feature Addition,0.0,1
pytorch,3b2cb459fceda571946f31624470873d17336c4a,fa65df37452f77d9e4689a850f60a10cb130fa01,Peter Bell,peterbell10@live.co.uk,Thu Aug 10 16:19:43 2023 +0100,1691684383.0,"[inductor] Type triton size arguments in the kernel index_dtype (#106870)

`JITFunction._key_of` uses the value of the argument to distinguish between
i32 and i64, but this fails if the value is used in indexing calculations where
the value exceeds `INT_MAX`.

Instead, we should use `index_dtype` which means all indexing calculations are
performed in the same dtype.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/106870
Approved by: https://github.com/lezcano
ghstack dependencies: #106626",24.0,9.0,"torch/_inductor/codegen/triton.py,torch/_inductor/codegen/triton_foreach.py,torch/_inductor/codegen/triton_utils.py,torch/_inductor/select_algorithm.py",4.0,3,1,1.828844606,3.0,3830.0,3.0,623786.0,18515.0,41921.0,0.0,,0.0,1
pytorch,ff00cdd728636d8b08071bbf24d4b4f1398b9d7e,fa6e5c5bffde9149bdc5ff66cc6477e4cc972206,Adam Paszke,adam.paszke@gmail.com,Thu Aug 11 20:10:54 2016 -0700,1470946254.0,Update tests and fix CosineEmbeddingCriterion,16.0,14.0,"test/test_cuda.py,test/test_legacy_nn.py,torch/legacy/nn/CosineEmbeddingCriterion.py",3.0,4,2,0.905587262,4.0,1888.0,1.0,15264.0,88.0,1151.433333,0.0,Corrective,1.0,1
pytorch,c488a9e9bf9eddca6d55957304612b88f4638ca7,fa8044d92f8a77b8008ca5e295a341abb9d26f13,Holger Kohr,h.kohr@cwi.nl,Tue Oct 03 09:23:47 2017 +0200,1507022627.0,Add tests for array interface,77.0,0.0,test/test_torch.py,1.0,1,1,0,37.0,4440.0,1.0,66398.0,1892.0,20515.85823,0.0,Feature Addition,0.0,1
pytorch,99068d2e52ab80bd7992c9f79d1d22042b29cf7c,fa8de6b4f3215a0abf8288a4aa1cdc9b8af59d3c,Neeraj Pradhan,npradhan@uber.com,Wed Dec 27 00:49:49 2017 -0800,1514335789.0,Adding the Cauchy distribution to torch.distributions,109.0,3.0,"docs/source/distributions.rst,test/test_distributions.py,torch/distributions/__init__.py,torch/distributions/cauchy.py",4.0,5,3,1.351549727,8.0,998.0,1.0,68546.0,877.0,6653.172317,0.0,Feature Addition,0.0,1
pytorch,a37072170dd7c6b4ca5f047a7221d551c2b2f2cf,faa032c5e58502de6ea461e531109d2acc22e56a,Radek BartoÅ,radek.barton@microsoft.com,Mon Nov 28 17:24:53 2022 +0000,1669656293.0,"[GHA] Decrease Windows test timeout to 120 minutes (#89694)

This PR decreases the Windows tests pipelines timeout to 120 mins per discusison as requested at https://github.com/pytorch/pytorch/issues/73489#issuecomment-1322539593

Closes #73489.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/89694
Approved by: https://github.com/kit1980",1.0,1.0,.github/workflows/_win-test.yml,1.0,2,1,0,1.0,202.0,1.0,1167387.0,9919.0,22834.5,0.0,Non Functional,0.0,1
pytorch,ce5f0d40b676f013d90cc76351235caa2a496b95,faa96c1c47740823c79bd50832be41df52872ecd,Thomas Viehmann,tv.code@beamnet.de,Mon Jul 30 19:46:07 2018 -0700,1532979967.0,"Deal with spaces in einsum equation string (#9994)

Summary:
Fixes #9930
Thank you, vadimkantorov for the report.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/9994

Differential Revision: D9042876

Pulled By: ezyang

fbshipit-source-id: 3bbd1aaaf1b432be40a7652b6a746d80934a216b",5.0,1.0,"aten/src/ATen/native/Linear.cpp,test/test_torch.py",2.0,5,2,0.650022422,40.0,8905.0,2.0,2696278.0,3185.0,8415.833333,0.0,Corrective,1.0,1
pytorch,183aa1534f9e199e1e67e453dfb94dc855dabc0d,faacbfa8bf073988706113c0a3bcfb39207f6201,Gerard Goossen,ggoossen@fb.com,Fri Nov 22 14:21:47 2019 -0800,1574432507.0,"Migrate index_add cpu from TH to ATen (#28421)

Summary:
Migrate index_add cpu from TH to ATen.

I couldn't find replacement for get1d and set1d, so doing pointer arithmetic inplace.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/28421

Test Plan: existing tests

Differential Revision: D18060971

Pulled By: ggoossen

fbshipit-source-id: 413719990cdb2fe578964cde14e93577e48a4342",91.0,68.0,"aten/src/ATen/Declarations.cwrap,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/native_functions.yaml,aten/src/TH/generic/THTensorEvenMoreMath.cpp,aten/src/TH/generic/THTensorMath.h,test/test_torch.py",8.0,7,2,2.054675526,42.0,25765.0,6.0,804643.75,13425.0,36744.33333,0.0,Feature Addition,0.0,1
pytorch,645d57ea0107b6c6608ca2850cfd47fb33b30f01,fab06bfb7512bca972dd833eef19101f6e9b3980,David Reiss,dreiss@fb.com,Wed Apr 08 20:06:51 2020 -0700,1586376411.0,"Add utility for bundling sample inputs with models (#35631)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/35631

Bundling sample inputs with our models with a standardized interface
will make it possible to write benchmarking and code-coverage tools that
call all models in a uniform way.  The intent is to make this a standard
for mobile models within Facebook.  Putting it in torch/utils so tests
can run on GitHub and because it might be useful for others as well.

`augment_model_with_bundled_inputs` is the primary entry point.  See
its docstring for usage information and the test for some example uses.

One design question I had was how much power should be available for
automatic deflating and inflating of inputs.  The current scheme gives
some automatic handling and a reasonable escape hatch
(""_bundled_input_inflate_format"") for top-level tensor arguments, but no
automatic support for (e.g.) tensors in tuples or long strings.  For
more complex cases, we have the ultimate escape hatch of just defining
_generate_bundled_inputs in the model.

Another design question was whether to add the inputs to the model or
wrap the model in a wrapper module that had these methods and delegated
calls to `forward`.  Because models can have other exposed methods and
attributes, the wrapped seemed too onerous.

Test Plan: Unit test.

Differential Revision: D20925013

Pulled By: dreiss

fbshipit-source-id: 4dbbb4cce41e5752133b4ecdb05e1c92bac6b2d5",290.0,0.0,"test/run_test.py,test/test_bundled_inputs.py,torch/utils/bundled_inputs.py",3.0,3,2,1.027235689,8.0,697.0,1.0,171871.0,879.0,2416.0,0.0,Feature Addition,0.0,1
pytorch,57d608d1f93a44472ef44eb87360e5986a6784db,fab48eb200c174eab6fd8f5b9453820c4ab60222,Mike Ruberry,mruberry@devfair044.maas,Sat Oct 12 00:12:11 2019 -0700,1570839131.0,"Makes some CPU-only tests in test_torch generic (#27688)

Summary:
Per title. Also testing putting test_advancedindex back on the default stream.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/27688

Differential Revision: D17888351

Pulled By: mruberry

fbshipit-source-id: af8adeca89f575fc276921b39049b07135ed9776",516.0,482.0,test/test_torch.py,1.0,1,1,0,40.0,13956.0,1.0,176595.0,12207.0,34121.33333,0.0,,0.0,1
pytorch,db446d69ca1f1d20af033df4f437e13d3dc9db88,fac711c23884b2bb8fc7ab54be1e772fbf22a546,Neeraj Pradhan,prad.neeraj@gmail.com,Fri Dec 15 11:41:08 2017 -0800,1513338068.0,Provide full support for distribution shapes (#4193),199.0,86.0,"test/test_distributions.py,torch/distributions/bernoulli.py,torch/distributions/categorical.py,torch/distributions/distribution.py,torch/distributions/gamma.py,torch/distributions/normal.py,torch/distributions/utils.py",7.0,3,2,2.352764633,8.0,698.0,2.0,51745.57142857143,845.0,6598.172317,0.0,,0.0,1
pytorch,6279367297287615ed4e45bb5f117f1e11bfddd9,fae6c6712180d325d2a5c454bfb51eb42d8d3206,Choongwoo Han,cwhan.tunz@gmail.com,Tue Feb 20 00:23:43 2018 +0900,1519086223.0,"Configurable flushing denormal numbers on CPU (#5294)

* Configurable flushing denormal numbers on CPU

* Formatting

* Update docs

* Minor doc changes",87.0,0.0,"aten/CMakeLists.txt,aten/src/ATen/Context.cpp,aten/src/ATen/Context.h,docs/source/torch.rst,test/test_torch.py,torch/_torch_docs.py,torch/csrc/Module.cpp",7.0,8,4,2.149243331,39.0,12627.0,6.0,1616393.2857142857,2383.0,24634.85823,0.0,Non Functional,0.0,1
pytorch,6a448816f5e39f45dac7e77fcf879b256f4aa83d,fae9547cb78add2986447fdb7227ba08a7d0419b,Bin Bao,binbao@meta.com,Wed Sep 06 14:02:13 2023 -0700,1694008933.0,"[inductor] Refactor wrapper.py (#108653)

Summary: Cherry-pick refactoring from https://github.com/pytorch/pytorch/pull/105331 to make the code review easier.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/108653
Approved by: https://github.com/ezyang, https://github.com/khabinov",31.0,30.0,"torch/_inductor/codegen/wrapper.py,torch/_inductor/ir.py",2.0,3,1,0.514091279,2.0,7708.0,2.0,60902.5,19427.0,44122.0,0.0,Perfective,0.0,1
pytorch,6107cf375083ebb6ed538ac3e2c4bacfc5ff9c81,fb00194030ddd3d40bf744da9a82485661a08d31,Jane Xu,janeyx@fb.com,Wed Jul 07 22:41:26 2021 -0700,1625697686.0,"Fix typo in common_utils.py (#61365)

Summary:
Missed this in review of https://github.com/pytorch/pytorch/pull/57953. I don't think this has affected much, though.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61365

Reviewed By: walterddr

Differential Revision: D29593764

Pulled By: janeyx99

fbshipit-source-id: 2c6f6aa961eabca0d8b8a7607aaae979667cca3b",1.0,1.0,torch/testing/_internal/common_utils.py,1.0,3,1,0,2.0,2542.0,1.0,94393.0,13645.0,30876.0,0.0,Corrective,1.0,1
pytorch,8473173c36a171c9ad896804ab36049f80a99073,fb0f285638338da93960d2b654a59c9639671fc0,Michael Suo,suo@fb.com,Tue May 03 20:30:59 2022 -0700,1651609859.0,"[lint] upgrade mypy to latest version

Fixes https://github.com/pytorch/pytorch/issues/75927.

Had to fix some bugs and add some ignores.

To check if clean:
```
lintrunner --paths-cmd='git grep -Il .' --take MYPY,MYPYSTRICT
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/76753

Approved by: https://github.com/malfet",72.0,62.0,".github/scripts/lint_native_functions.py,.lintrunner.toml,benchmarks/instruction_counts/core/expand.py,tools/setup_helpers/cmake.py,tools/shared/module_loader.py,tools/stats/s3_stat_parser.py,tools/test/test_stats.py,torch/_C/__init__.pyi.in,torch/_deploy.py,torch/ao/nn/sparse/quantized/linear.py,torch/autograd/functional.py,torch/distributed/distributed_c10d.py,torch/distributed/elastic/multiprocessing/api.py,torch/fx/experimental/unification/variable.py,torch/fx/graph.py,torch/fx/graph_module.py,torch/hub.py,torch/jit/_script.py,torch/jit/frontend.py,torch/nn/modules/module.py,torch/nn/utils/prune.py,torch/package/_package_pickler.py,torch/package/_package_unpickler.py,torch/package/package_importer.py,torch/utils/bundled_inputs.py,torch/utils/cpp_extension.py,torch/utils/data/dataloader.py,torch/utils/data/datapipes/dataframe/dataframes.py,torch/utils/show_pickle.py,torchgen/gen.py",30.0,33,5,4.275697089,45.0,25512.0,2.0,19568.7,2821.0,6776.5,0.0,Corrective,1.0,1
pytorch,fbf9e379e1061d9be39642a59699bb8ede169ab2,fb1427ea8f921ed0999e1a95d1924b7a13761045,Peter Bell,peterbell10@live.co.uk,Mon Jan 16 15:21:03 2023 +0000,1673882463.0,"squeeze: allow squeezing multiple dimensions at once (#89017)

Ref #70924

This addresses part 1 of the issue, allowing `torch.squeeze` to be
passed a tuple of dimensions. e.g.
```python
x.squeeze(0).squeeze(0)
```
can now be written
```python
x.squeeze((0, 1))
```
(assuming x has at least 2 dimensions)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/89017
Approved by: https://github.com/albanD",347.0,132.0,"aten/src/ATen/FunctionalInverses.cpp,aten/src/ATen/LegacyBatchingRegistrations.cpp,aten/src/ATen/NamedTensorUtils.cpp,aten/src/ATen/NamedTensorUtils.h,aten/src/ATen/functorch/BatchRulesViews.cpp,aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/TensorShape.cpp,aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/nested/NestedTensorMath.cpp,aten/src/ATen/native/ts_native_functions.yaml,test/functorch/test_vmap.py,tools/autograd/derivatives.yaml,torch/_inductor/lowering.py,torch/_prims/__init__.py,torch/_refs/__init__.py,torch/_torch_docs.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/testing/_internal/common_methods_invocations.py",20.0,18,4,3.866795159,38.0,86861.0,10.0,713762.35,11331.0,26006.5,0.0,Feature Addition,0.0,1
pytorch,3a704ff72534f5d3efccef2451eac27028255640,fb2d28f477c76bd94e3e3e9d2f424caa295d75c3,Adam Lerer,alerer@fb.com,Sat Jan 28 17:33:02 2017 -0800,1485624782.0,remove circular references in NestedIOFunction,56.0,36.0,"test/common_nn.py,test/test_nn.py,torch/autograd/function.py",3.0,3,2,0.909789393,22.0,2507.0,1.0,75659.0,390.0,3835.196975,0.0,,0.0,1
pytorch,8dee7b7a16ab76c2a82b4032879ab35790e04a68,fb391a016d08020db808860b06b1971668aa2c12,Michael Lazos,mlazos@fb.com,Wed Jan 31 21:34:18 2024 +0000,1706736858.0,"Test that optimizers are running cudagraphs (#118716)

Updates compiled optimizer tests to ensure that cudagraphs is running when on cuda.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/118716
Approved by: https://github.com/eellison",66.0,43.0,test/inductor/test_compiled_optimizers.py,1.0,2,1,0,1.0,369.0,1.0,406916.0,24611.0,55728.0,0.0,Perfective,0.0,1
pytorch,3ea1da3b2c7a31119e42829abc782067787f8b9d,fb39971464d66a80400f3a40dafba3899f5ca0b0,Adam Paszke,adam.paszke@gmail.com,Tue Sep 13 22:11:56 2016 -0700,1473804716.0,Add more modules to nn,2498.0,477.0,"test/common.py,test/common_nn.py,test/test_legacy_nn.py,test/test_nn.py,torch/autograd/function.py,torch/autograd/variable.py,torch/legacy/nn/SpatialLPPooling.py,torch/nn/backends/thnn.py,torch/nn/functions/activation.py,torch/nn/functions/dropout.py,torch/nn/functions/linear.py,torch/nn/functions/loss.py,torch/nn/functions/thnn.py,torch/nn/functions/thnn/__init__.py,torch/nn/functions/thnn/activation.py,torch/nn/functions/thnn/auto.py,torch/nn/functions/thnn/batchnorm.py,torch/nn/functions/thnn/conv.py,torch/nn/functions/thnn/loss.py,torch/nn/functions/thnn/normalization.py,torch/nn/functions/thnn/pooling.py,torch/nn/functions/thnn/sparse.py,torch/nn/modules/__init__.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/criterion.py,torch/nn/modules/distance.py,torch/nn/modules/dropout.py,torch/nn/modules/linear.py,torch/nn/modules/loss.py,torch/nn/modules/module.py,torch/nn/modules/normalization.py,torch/nn/modules/padding.py,torch/nn/modules/pooling.py,torch/nn/modules/sparse.py,torch/nn/modules/upsampling.py,torch/nn/modules/utils.py",38.0,10,2,4.503106728,7.0,3039.0,1.0,10639.0,110.0,2087.722339,0.0,Feature Addition,0.0,1
pytorch,2fb328eb46a6fa71bfb38d457959887eeb41deda,fb3d9f39cc49071e78fa15684dfdfa7f5478cd58,Sean Ross-Ross,srossross@gmail.com,Fri Jan 20 18:25:20 2023 +0000,1674239120.0,"update vmap to accept nones (#91644)

* Fixes https://github.com/pytorch/functorch/issues/1082
* Fixes https://github.com/pytorch/functorch/issues/439

Pull Request resolved: https://github.com/pytorch/pytorch/pull/91644
Approved by: https://github.com/kshitij12345, https://github.com/Chillee",74.0,19.0,"test/functorch/test_vmap.py,torch/_functorch/vmap.py",2.0,4,2,0.985858726,1.0,5657.0,2.0,1288825.0,11492.0,26315.0,0.0,Corrective,1.0,1
pytorch,f776f30780d12ab0385e4010b07d88327a8abfd5,fb63bb60ecaa9ade8b3907b1723aa3dfe02d3d4c,Pearu Peterson,pearu.peterson@gmail.com,Mon Nov 29 22:21:43 2021 -0800,1638224503.0,"Strided masked norm. (#68584)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68584

Test Plan: Imported from OSS

Reviewed By: pbelevich

Differential Revision: D32581285

Pulled By: cpuhrsch

fbshipit-source-id: 896ee1e58957b46c2f6a16a170adff4cb3b8da62",273.0,32.0,"test/test_masked.py,torch/_masked/__init__.py,torch/testing/_internal/common_methods_invocations.py",3.0,5,2,1.355747953,2.0,14510.0,2.0,562352.3333333334,17341.0,40776.0,0.0,,0.0,1
pytorch,4da9ceb74388bb3df26a3581f281caf1cc554890,fb73cc4dc479121b05485d036ec3a7220aa7ecb0,Peter Bell,peterbell10@live.co.uk,Tue Jan 12 12:40:38 2021 -0800,1610455238.0,"Migrate some torch.fft tests to use OpInfos (#48428)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48428

Test Plan: Imported from OSS

Reviewed By: ngimel

Differential Revision: D25868666

Pulled By: mruberry

fbshipit-source-id: ca6d0c4e44f4c220675dc264a405d960d4b31771",64.0,220.0,"test/test_spectral_ops.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.507935356,2.0,4104.0,2.0,367414.0,8027.0,18212.0,0.0,,0.0,1
pytorch,62c8d30f9f6715d0b60d78fb5f5913a2f3bd185b,fb833aabac9fde268cab4b0569ef068019ec3b51,Slava Kovalevskyi,vsk@fb.com,Wed Aug 10 14:46:25 2022 +0000,1660142785.0,"merge_rules, person_of_interst and CODEOWNERS now better aligned  (#83127)

not 100% alignment just yet
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83127
Approved by: https://github.com/malfet",110.0,74.0,"CODEOWNERS,docs/source/community/persons_of_interest.rst",2.0,3,1,0.347816914,7.0,375.0,2.0,506306.5,6287.0,14556.0,0.0,,0.0,1
pytorch,459090e3ceaff7276a4fbf51161e3a8e627a275f,fb9d8de379e2661aac0d7451170427f46cc463bb,Sebastian Brodehl,foss@s14l.de,Fri Jun 17 16:39:13 2022 +0000,1655483953.0,"Make `LR scheduler` stub complete, including `OneCycleLR` and class attributes. (#59476)

This PR completes the stub file for lr scheduler and includes a previously missing scheduler, namely `OneCycleLR, and adds additional class attributes and methods for all lr scheduler.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/59476
Approved by: https://github.com/jbschlosser",77.0,16.0,torch/optim/lr_scheduler.pyi,1.0,2,1,0,3.0,52.0,1.0,15795673.0,4467.0,10706.5,0.0,Feature Addition,0.0,1
pytorch,ec2461bbd87272631ec3bc4375327b41471c1572,fba13d94a1ad7297af676bb998e9ec7797078b7d,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Tue Jan 31 11:59:08 2023 +0000,1675166348.0,"Remove deprecated torch.symeig (#70988)

The time has come to remove deprecated linear algebra related functions. This PR removes `torch.symeig`.

- [x] XLA PR: https://github.com/pytorch/xla/pull/4498

Pull Request resolved: https://github.com/pytorch/pytorch/pull/70988
Approved by: https://github.com/lezcano, https://github.com/kit1980, https://github.com/malfet",32.0,532.0,".github/ci_commit_pins/xla.txt,aten/src/ATen/autocast_mode.cpp,aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,docs/source/tensors.rst,docs/source/torch.rst,test/cpp/lazy/test_lazy_ops.cpp,test/distributed/_tensor/test_dtensor_ops.py,test/expect/HasDecompTest.test_has_decomposition.expect,test/forward_backward_compatibility/check_forward_backward_compatibility.py,test/functorch/test_aotdispatch.py,test/functorch/test_ops.py,test/functorch/test_vmap.py,test/test_autograd.py,test/test_legacy_vmap.py,test/test_linalg.py,test/test_meta.py,test/test_namedtuple_return_api.py,test/test_proxy_tensor.py,tools/autograd/derivatives.yaml,tools/autograd/gen_python_functions.py,tools/autograd/gen_variable_type.py,torch/__init__.py,torch/_linalg_utils.py,torch/_tensor.py,torch/_tensor_docs.py,torch/_torch_docs.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",33.0,24,6,3.380696765,51.0,123727.0,15.0,379313.6363636364,11849.0,27656.5,0.0,,0.0,1
pytorch,a3909d958a716590418c4bb19c07d7d8b6307746,fbbd036871406a3895607a79dc58dc08551df486,samdow,samdow@fb.com,Tue Aug 02 23:32:11 2022 -0700,1659483131.0,"[Reland] [functorch] Fix linalg batch rules to error on non-matrix inputs (#82176)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/82176
Approved by: https://github.com/zou3519",274.0,72.0,"functorch/functorch/csrc/BatchRulesBinaryOps.cpp,functorch/functorch/csrc/BatchRulesDecompositions.cpp,functorch/functorch/csrc/BatchRulesHelper.cpp,functorch/functorch/csrc/BatchRulesHelper.h,functorch/functorch/csrc/BatchRulesLinearAlgebra.cpp,functorch/test/test_ops.py,functorch/test/test_vmap.py",7.0,4,1,1.712038981,2.0,7214.0,1.0,76348.0,6055.0,14132.0,0.0,Corrective,1.0,1
pytorch,7b65acdf9e8345d2ffbbc3b85ae6667f7260407e,fbe121e395bcfbfbe404c540b25072ad531c9041,Zafar Takhirov,cc.rafaz@zafar.cc,Fri Jan 31 22:38:30 2020 -0800,1580510310.0,"Quantized sigmoid function

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/31851

Test Plan: Imported from OSS

Differential Revision: D19280716

Pulled By: z-a-f

fbshipit-source-id: f47d37e32a675756fcaca293e2c14f90c43891de",259.0,28.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp,aten/src/ATen/native/quantized/cpu/qsigmoid.cpp,aten/src/ATen/native/quantized/cpu/quantized_ops.h,test/test_quantized.py",5.0,8,2,1.573299327,10.0,9714.0,3.0,1140574.75,14525.0,39324.33333,0.0,,0.0,1
pytorch,4134b7abfa60a659de27736704c62277ecf291d2,fbea2ee917b24434b3fb4a01294fb89895279ac2,Rohan Varma,rvarm1@fb.com,Wed Sep 02 01:51:53 2020 -0700,1599011513.0,"broadcast_object API for c10d (#43887)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/43887

As part of addressing #23232, this PR adds support for `broadcast_object_list` which is an API to broadcast arbitrary picklable objects to all the other ranks.  This has been a long-requested feature, so would be good for Pytorch to natively support this.

The implementation approach follows a similar approach as https://github.com/pytorch/pytorch/pull/42189. The input is a list of objects to be broadcasted and it is in place, meaning all ranks part of the group will have their input list modified to contain the broadcasted objects from the src rank.

Note that the API is designed to match the tensor-based collectives other than supporting async_op. For now, it is a blocking call. If we see demand to support async_op, we will have to make more progress on merging work/future to support this.
ghstack-source-id: 111180436

Reviewed By: mrshenli

Differential Revision: D23422577

fbshipit-source-id: fa700abb86eff7128dc29129a0823e83caf4ab0e",100.0,18.0,"docs/source/distributed.rst,test/distributed/test_distributed.py,torch/distributed/distributed_c10d.py",3.0,6,3,1.093466597,16.0,5679.0,3.0,72316.66666666667,4756.0,11064.5,0.0,Feature Addition,0.0,1
pytorch,7203612f852e57b3f1667a614444bbb24beec7fb,fbf28b5458906d08d60239a2cae8c86d890be5c8,Owen Anderson,owen.anderson@oculus.com,Wed Jul 24 17:19:01 2019 -0700,1563988741.0,"Change TensorIterator to be stack allocated, using named return value optimization to elide copies.

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/22519

Differential Revision: D16451460

fbshipit-source-id: 6ca6ae2fdf1af5a2f792b42e55279413971b3c46",116.0,118.0,"aten/src/ATen/native/Activation.cpp,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/Copy.cpp,aten/src/ATen/native/Fill.cpp,aten/src/ATen/native/Indexing.cpp,aten/src/ATen/native/ReduceOps.cpp,aten/src/ATen/native/TensorIterator.cpp,aten/src/ATen/native/TensorIterator.h,aten/src/ATen/native/TensorIteratorReduce.cpp,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/cpu/LerpKernel.cpp,aten/src/ATen/native/cuda/Distributions.cu,aten/src/ATen/native/quantized/README.md,aten/src/ATen/native/quantized/cpu/fake_quantize_per_tensor_affine.cpp,aten/src/ATen/native/quantized/cpu/qadd.cpp,aten/src/ATen/native/quantized/cpu/qconcat.cpp,aten/src/ATen/native/quantized/cpu/qrelu.cpp,aten/src/ATen/test/tensor_iterator_test.cpp",18.0,9,1,3.438699403,7.0,4796.0,17.0,1991117.5555555555,10138.0,29252.33333,0.0,,0.0,1
pytorch,5e3d1ef49f7a89503a3c6ca6dd673ef3d7c12388,fc0d5d2bd3a099367041631c1989b299c3b9f0bc,Peter Bell,peterbell10@live.co.uk,Mon Aug 08 20:05:54 2022 +0100,1659989154.0,"Use UnaryUfuncInfo for type conversion functions (#82349)

This adds varoious comparisons of the vectorized implementation
against the same operation on strided layouts.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/82349
Approved by: https://github.com/ngimel",156.0,145.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,6.0,20077.0,1.0,0.0,6246.0,14472.5,0.0,Feature Addition,0.0,1
pytorch,2d689560050ba68147f7f762217c1d88e594dbc5,fc0d940c5e75d30d916dda5dfdca7c14a02850cf,Hugh Perkins,hughperkins@gmail.com,Thu Jan 04 17:23:21 2018 -0500,1515086601.0,"add gumbel_softmax, based on Eric Jang's implementation (#3341)

* add gumbel_softmax, based on Eric Jang's implementation

* Make gumbel_softmax CUDA friendly

* gumbel_softmax tweaks",120.0,0.0,"test/test_nn.py,torch/nn/functional.py",2.0,3,2,0.996791632,37.0,7082.0,2.0,116130.5,2249.0,24331.35823,0.0,Feature Addition,0.0,1
pytorch,86397f6b2459635dbaf3234803d2d3a9c28f4843,fc19747d648ba58cfd0c79c211cb5d8ff1167c85,kshitij12345,kshitijkalambarkar@gmail.com,Tue May 19 03:47:30 2020 -0700,1589860050.0,"handle grad with `stride=0` on GPU MvBackward (#38321)

Summary:
References : https://github.com/pytorch/pytorch/issues/38315 ,  https://github.com/pytorch/pytorch/issues/29984

cuBlas expects strides to be greater than 0.
Cloning the `grad` allocates a new vector with
non-zero strides.

For CPU, we don't clone and allocate a new vector
as CPU implementation works with stride=0.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/38321

Differential Revision: D21628966

Pulled By: ngimel

fbshipit-source-id: 390caf835af6d1d77ed537b7fcc113a22c3ec301",29.0,4.0,"aten/src/ATen/native/cuda/Blas.cu,test/test_autograd.py,test/test_torch.py",3.0,6,2,1.573355385,43.0,24899.0,2.0,197617.66666666663,2181.0,5424.0,0.0,,0.0,1
pytorch,9ac203cd4eede866616af56eb1043fac29da67b0,fc272929a61681093a8e1fe64e4f2a9792eeae2a,Horace He,chilli@fb.com,Thu Jan 20 02:51:35 2022 +0000,1642647095.0,[functorch] Added support for outputs that don't require gradients + fixed some other tests,61.0,13.0,"functorch/functorch/_src/aot_autograd.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py",3.0,4,1,1.511959961,1.0,2251.0,3.0,0.3333333333333333,726.0,997.0,0.0,Corrective,1.0,1
pytorch,f48a9712b7ec2255a54e459888077b074898e64e,fc58b3f146c630fab1314c0e7ac0f8b09afeb081,Mike Ruberry,mruberry@devfair044.maas,Sun Mar 21 21:43:37 2021 -0700,1616363017.0,"Skips failing pinv and pinverse test (#54392)

Summary:
This will unblock the CI failing due to https://github.com/pytorch/pytorch/issues/54381.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/54392

Reviewed By: ngimel

Differential Revision: D27221925

Pulled By: mruberry

fbshipit-source-id: 5b6e6f21428fd7d97cc75300e3a1aca2a40fbb24",9.0,3.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,4572.0,1.0,151659.0,9944.0,22031.0,0.0,,0.0,1
pytorch,31d03c2f6383e4fde6e81d141e825197c8980293,fc5b4a5a33f1906ca335c26ec4da9357ed196419,lezcano,lezcano-93@hotmail.com,Thu May 05 13:17:24 2022 +0000,1651756644.0,"Add linalg.lu_solve

This PR adds `linalg.lu_solve`. While doing so, I found a bug in MAGMA
when calling the batched MAGMA backend with trans=True. We work around
that by solving the system solving two triangular systems.

We also update the heuristics for this function, as they were fairly
updated. We found that cuSolver is king, so luckily we do not need to
rely on the buggy backend from magma for this function.

We added tests testing this function left and right. We also added tests
for the different backends. We also activated the tests for AMD, as
those should work as well.

Fixes https://github.com/pytorch/pytorch/issues/61657

Pull Request resolved: https://github.com/pytorch/pytorch/pull/72935

Approved by: https://github.com/IvanYashchuk, https://github.com/mruberry",721.0,443.0,"aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/cuda/LinearAlgebra.cu,aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp,aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,docs/source/linalg.rst,test/allowlist_for_publicAPI.json,test/test_linalg.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/linalg/__init__.py,torch/overrides.py,torch/testing/_internal/common_methods_invocations.py",19.0,17,5,3.041553969,20.0,68458.0,5.0,151915.42105263157,2906.0,6994.5,0.0,Corrective,1.0,1
pytorch,1897440e02eeebd893fa131b86d06ad617ecebb8,fc93d1ae6bd00b10be5d1a092b2c80f568634b96,Spandan Tiwari,sptiwari@microsoft.com,Wed Sep 11 01:15:34 2019 -0700,1568164534.0,"Add ONNX export support for torch.log1p. (#25808)

Summary:
`torch.log1p` operator is not supported in ONNX exporter. This PR adds the support.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25808

Reviewed By: zrphercule

Differential Revision: D17298092

Pulled By: houseroad

fbshipit-source-id: 65a919a07797722d7d4df8caf284bd89acd0bb02",20.0,0.0,"test/onnx/test_pytorch_onnx_onnxruntime.py,torch/onnx/symbolic_opset9.py",2.0,4,2,0.721928095,1.0,2983.0,1.0,13394.0,11288.0,31836.33333,0.0,Feature Addition,0.0,1
pytorch,5cbffbbac9a59098637f821e8b6e10f609de30ff,fc99705f22ae6c4165cca705c79f784bb7c7831a,Richard Zou,zou3519@gmail.com,Mon Sep 26 21:48:54 2022 -0700,1664228934.0,"Add functorch M1 shard (#85565)

functorch should have a test wherever regular PyTorch gets tested. This
PR adds an M1 shard to test functorch.

Test Plan:
- wait for CI
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85565
Approved by: https://github.com/malfet, https://github.com/huydhn",24.0,4.0,".github/workflows/trunk.yml,functorch/test/common_utils.py,functorch/test/test_ops.py,functorch/test/test_pythonkey.py,functorch/test/test_vmap.py",5.0,4,2,2.102047566,2.0,7601.0,5.0,282996.2,7729.0,18148.5,0.0,Feature Addition,0.0,1
pytorch,df8a8fbc1b9daf907ea6c45d53f1021cd0112456,fca931d18131d313adbf6d53ccc0a9401e7851e8,Tugsbayasgalan (Tugsuu) Manlaibaatar,tmanlaibaatar@fb.com,Tue Jun 22 18:20:59 2021 -0700,1624386059.0,"List striding with arbitrary step size (#58537)

Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58537

Test Plan: Imported from OSS

Reviewed By: ejguan

Differential Revision: D28531721

Pulled By: tugsbayasgalan

fbshipit-source-id: 8c8ed32ca00366603bfb5086e87dfa62736ff4b2",249.0,58.0,"aten/src/ATen/native/native_functions.yaml,aten/src/ATen/templates/RegisterSchema.cpp,test/backward_compatibility/check_backward_compatibility.py,test/cpp/jit/test_interpreter.cpp,test/cpp/jit/test_utils.cpp,test/jit/test_ignorable_args.py,test/test_jit.py,tools/autograd/derivatives.yaml,torch/csrc/jit/frontend/ir_emitter.cpp,torch/csrc/jit/passes/shape_analysis.cpp,torch/csrc/jit/runtime/register_ops_utils.cpp,torch/csrc/jit/runtime/register_prim_ops.cpp,torch/onnx/symbolic_opset10.py,torch/onnx/symbolic_opset9.py",14.0,19,4,2.147960079,18.0,43157.0,11.0,1288828.0714285714,13227.0,29941.5,0.0,,0.0,1
pytorch,25bcfbf59eeceb476825973afce205228a6de705,fcb0c77c7aac97b6c936c700c67307c6e09f557c,Andreas KÃ¶pf,andreas.koepf@xamla.com,Wed Dec 16 23:38:11 2015 +0100,1450309091.0,Install THCUNN into ${Torch_INSTALL_LUA_CPATH_SUBDIR},8.0,3.0,CMakeLists.txt,1.0,0,0,0,29.0,14.0,1.0,0.0,48.0,324.4047619,0.0,,0.0,1
pytorch,720a7a0d81d4f2ddd3ad90cf3342aad0352ecb70,fcc1f87b6aacdfba9193691568bc405be42ac77e,Kushashwa Ravi Shrimali,kushashwaravishrimali@gmail.com,Fri Aug 13 17:12:01 2021 -0700,1628874721.0,"Fixing user inputs for low, high in `make_tensor` (#61108)

Summary:
**TODOs:**

* [x] Do not clamp inputs for low and high when given and valid.
* [x] Devise rules for modifying `low` and `high` when extremals/invalid values passed.
* [x] Testing with `test_references_numerics_hard` with the revised changes. _(I've tested locally, the changes will take place in a separate PR though after offline discussion with mruberry)_
* [x] Revise comments/documentation for `make_tensor`

See https://github.com/pytorch/pytorch/issues/61758 for tracker issue.

cc: mruberry pmeier

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61108

Reviewed By: VitalyFedyunin

Differential Revision: D30296167

Pulled By: mruberry

fbshipit-source-id: 67e8d15b173209a9c97ca013231494a5fa99f8c7",73.0,43.0,"torch/testing/_internal/common_methods_invocations.py,torch/testing/_internal/common_utils.py",2.0,3,1,0.849751137,2.0,11289.0,2.0,113731.5,14616.0,33549.5,0.0,Corrective,1.0,1
pytorch,cbdbdd3c8c428fb3e2f4edd48ebb1e21b52dc1e0,fcd13549f923342a7474993928d86181b75734d3,Zachary DeVito,zdevito@fb.com,Mon Sep 23 21:21:57 2019 -0700,1569273717.0,"add CondValue to unify refinements and code emission (#26145)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/26145

This is step towards isinstance type refinement.
It primarily does yak shaving in compiler.cpp to unify the handling
of special case behavior that occurs in conditional expressions:

* Handling type refinement as part of emission.
* Handling `is None` static-if specialization.

It introduces a CondValue object that is a Value that also has
additional type refinements that are true when that Value is true,
and potentialy a static-true/false value that, if set, will cause if
statements to be handled statically, omitting typechecking of the other side.

This ends up expanding some behavior, for instance `is None` specialization
used to happen only for single expressions, but now works through
boolean logic.

Test Plan: Imported from OSS

Differential Revision: D17359500

Pulled By: zdevito

fbshipit-source-id: ce93804496c8b4c3197a5966bc28c608465fda64",377.0,349.0,"test/jit_utils.py,test/test_jit.py,torch/csrc/jit/script/compiler.cpp,torch/csrc/jit/script/sugared_value.h",4.0,5,2,0.452193276,12.0,24495.0,4.0,583816.5,11644.0,32782.33333,0.0,Feature Addition,0.0,1
pytorch,9b42e4ef73b265ff6fe8ff231e70ee0744eff587,fd3a7264ae80332ba5ec8f60446e6d7a2c2276c1,Nikita Shulga,nshulga@meta.com,Thu Dec 22 05:35:54 2022 -0800,1671687354.0,"[MPS] Add `group_norm[fwd+backward]` and `mean_var` (take 2) (#91190)

Use Prims to implement group_norm, group_norm_backward and mean_var

Use `torch._ops.ops` instead of `torch.ops` in numerous subpackages in
order to be able to make them importable from `torch/backend/mps/__init__.py` as this alias is defined in
https://github.com/pytorch/pytorch/blob/15af4b1ceea3b689354ad664675b930486804c8e/torch/__init__.py#L1095
is executed last during init process.

Add `__all__` to `torch/backends/mps/__init__.py` as well as alias all imports as private

Add `TestNNMPS.test_group_norm_backward` that validates no NaNs are generated during the backward pass

Fixes https://github.com/pytorch/pytorch/issues/88331
Pull Request resolved: https://github.com/pytorch/pytorch/pull/91190
Approved by: https://github.com/albanD",74.0,26.0,"test/test_mps.py,torch/_decomp/decompositions.py,torch/_prims/__init__.py,torch/_prims/nvfuser_prims.py,torch/_prims_common/__init__.py,torch/_refs/__init__.py,torch/_refs/_conversions.py,torch/_refs/fft.py,torch/_refs/linalg/__init__.py,torch/_refs/nn/functional/__init__.py,torch/_refs/special/__init__.py,torch/_subclasses/fake_tensor.py,torch/_subclasses/fake_utils.py,torch/backends/mps/__init__.py,torch/fx/experimental/symbolic_shapes.py,torch/fx/node.py",16.0,15,2,3.061407393,5.0,26661.0,3.0,46456.0625,10837.0,24708.5,0.0,Corrective,1.0,1
pytorch,2564c0c88974bdfc13321c3d4fdffab454302c81,fd450ff1b93e4c498e7326cb35c7c26760c5ddbf,Brian Hirsh,hirsheybar@fb.com,Thu Apr 08 18:37:35 2021 -0700,1617907055.0,"Revert D27598681: Add OpInfo tests for torch.addbmm

Test Plan: revert-hammer

Differential Revision:
D27598681 (https://github.com/pytorch/pytorch/commit/b5647dd52b48a51fec1387382deb5b59c7651512)

Original commit changeset: 24082f54b12e

fbshipit-source-id: 43d5713829fbaa00353bb7b054b66f537d768cd1",9.0,41.0,"test/test_autograd.py,torch/testing/_internal/common_methods_invocations.py",2.0,4,2,0.242292189,42.0,13270.0,2.0,13038.5,10573.0,23372.5,0.0,Feature Addition,0.0,1
pytorch,2888b7fec56d6f192eb535f61686a27a444f2117,fd8004b42e2a2348ec8837e3fb524b960c1b4cdb,kshitij12345,kshitijkalambarkar@gmail.com,Tue Aug 03 06:15:34 2021 -0700,1627971334.0,"add bfloat16 impl for nextafter (#61829)

Summary:
Add `BFloat16` support for `nextafter`.

* [x] Add OpInfo
* [x] Add Implementation Test (C++ tests)
* [x] Add credit

Pull Request resolved: https://github.com/pytorch/pytorch/pull/61829

Reviewed By: ejguan

Differential Revision: D29932498

Pulled By: mruberry

fbshipit-source-id: 89524531a4800569ba1addd08a4ace330a6f72a4",173.0,3.0,"aten/src/ATen/native/cpu/BinaryOpsKernel.cpp,aten/src/ATen/native/cuda/StepKernel.cu,c10/test/util/bfloat16_test.cpp,c10/util/BFloat16-math.h,test/test_binary_ufuncs.py,torch/testing/_internal/common_methods_invocations.py",6.0,14,4,2.127132613,2.0,12631.0,5.0,3024125.833333333,14354.0,32818.5,0.0,Feature Addition,0.0,1
pytorch,d6a8d28d6529a4f0b80a8c046ca9c36ca6c8b347,fde355f7d4ad95a1f8cba6adc3fff7b4935ecba0,Sam Gross,colesbury@gmail.com,Mon Nov 06 23:19:56 2017 -0500,1510010396.0,"Allow in-place operations on views (#3384)

Allow in-place operations on views

Adds VariableViewImpl, a subclass of VariableImpl which has a pointer to
the base Variable on which it is a view. In-place operations on views
change the grad_fn of the base.

Note that in-place operations only work on views that are the first output of the function that created them. All C++/ATen implemented functions have this behavior, but it's possible to write Python-implemented autograd functions that do not. In-place operations on these view will raise an exception.

Fixes #3313",843.0,345.0,"aten/src/ATen/Local.cwrap,setup.py,test/test_autograd.py,test/test_nn.py,tools/autograd/derivatives.yaml,tools/autograd/gen_variable_type.py,tools/autograd/templates/Functions.cpp,tools/autograd/templates/Functions.h,tools/autograd/templates/VariableType.cpp,tools/autograd/templates/VariableType.h,tools/autograd/templates/python_variable_methods.cpp,torch/autograd/_functions/tensor.py,torch/autograd/variable.py,torch/csrc/autograd/functions/init.cpp,torch/csrc/autograd/functions/special.cpp,torch/csrc/autograd/functions/tensor.cpp,torch/csrc/autograd/functions/tensor.h,torch/csrc/autograd/python_function.cpp,torch/csrc/autograd/python_variable.cpp,torch/csrc/autograd/python_variable.h,torch/csrc/autograd/saved_variable.cpp,torch/csrc/autograd/saved_variable.h,torch/csrc/autograd/variable.cpp,torch/csrc/autograd/variable.h,torch/csrc/autograd/variable_version.h,torch/csrc/utils/tensor_geometry.cpp,torch/csrc/utils/tensor_geometry.h",27.0,14,4,3.809309679,40.0,14782.0,12.0,633480.12,315.0,886.9058694,0.0,Corrective,1.0,1
pytorch,afa1ff8e04dad60dee9f979451a36713903f521d,fe08671756b3ed3747e773b303e7b5849efd3142,Ivan Yashchuk,ivan.yashchuk@aalto.fi,Fri Mar 12 21:36:02 2021 -0800,1615584962.0,"Added cuBLAS path for torch.triangular_solve (#53147)

Summary:
This PR adds the cuBLAS based path for `torch.triangular_solve`
The device dispatching helper function was removed from native_functions.yml, it is replaced with DECLARE/DEFINE_DISPATCH.

`magmaTriangularSolve` is removed and replaced with cuBLAS calls, this is not a BC-breaking change because internally MAGMA just calls the same cuBLAS function and doesn't do anything else.

Batched cuBLAS is faster than batched MAGMA for matrices of size up until 512x512, after that MAGMA is faster. For batches smaller than ~8 and matrix sizes larger than 64x64 a forloop of cuBLAS calls is faster than batched version.

Ref. https://github.com/pytorch/pytorch/issues/47953

Pull Request resolved: https://github.com/pytorch/pytorch/pull/53147

Reviewed By: heitorschueroff

Differential Revision: D27007416

Pulled By: mruberry

fbshipit-source-id: ddfc190346e6a56b84145ed0a9af67ca9cde3506",500.0,258.0,"aten/src/ATen/cuda/CUDABlas.cpp,aten/src/ATen/cuda/CUDABlas.h,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebra.h,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu,aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h,aten/src/ATen/native/native_functions.yaml,test/backward_compatibility/check_backward_compatibility.py,test/test_linalg.py,torch/_torch_docs.py,torch/utils/hipify/cuda_to_hip_mappings.py",14.0,11,3,2.733867274,33.0,43248.0,9.0,979951.1428571428,9754.0,21568.0,0.0,Feature Addition,1.0,1
pytorch,b2865ef389e46f8645d8aed7f589d3900800e418,fe12ac57a4d9a3abf942b8e6343e40dc4719919b,Tongzhou Wang,SsnL@users.noreply.github.com,Fri Dec 01 19:56:48 2017 -0500,1512158208.0,"Improve docs for torch and torch.Tensor (#3969)

* doc overhaul

* update split doc",1156.0,1127.0,"docs/source/tensors.rst,torch/_tensor_docs.py,torch/_torch_docs.py,torch/functional.py,torch/tensor.py",5.0,3,2,1.179132669,36.0,7923.0,4.0,550384.4,2169.0,24176.85823,0.0,Perfective,0.0,1
pytorch,bc1d884061dfb7bec0e1a442567ce9638959ad96,fe190078aa78dad94297aece4d8322e5f4262558,soulitzer,soulitzer@gmail.com,Mon Oct 03 20:18:09 2022 -0400,1664828289.0,"Require bias to be contiguous for depthwise3x3_winograd backend (#85711)

Fixes https://github.com/pytorch/pytorch/issues/85694
Pull Request resolved: https://github.com/pytorch/pytorch/pull/85711
Approved by: https://github.com/malfet, https://github.com/albanD",27.0,9.0,"aten/src/ATen/native/ConvUtils.h,aten/src/ATen/native/Convolution.cpp,functorch/test/test_vmap.py",3.0,6,2,1.389315274,8.0,7033.0,2.0,31431.33333333333,8003.0,18924.5,0.0,Corrective,1.0,1
pytorch,87a05fa8b477bc5f1a46bd24f22015c6cd5c010d,fe1968dea0387f36a5ee479ab030ebeec0e6cb3d,Mike Ruberry,mruberry@devfair044.h1.fair,Fri Apr 29 02:02:25 2022 +0000,1651197745.0,"[primTorch] Prototype nvFuser integration and test_prims.py

This adds prototype nvFuser integration for the following prims:

- broadcast_in_dim
- convert_element_type
- add
- div
- ge
- gt
- le
- lt
- mul

Adding it for additional prims supported by nvFuser's prototype Python frontend should be easy.

This also adds a new sugar to run operations using the ATen or nvFuser trace executors. For example:

```
def foo(a, b):
  return torch.add(a, b)

traced_foo = make_traced(foo)

a = torch.randn((1, 2, 3, 4, 5), device='cuda')
b = torch.randn((1, 2, 3, 4, 5), device='cuda')
result = traced_foo(a, b, executor='nvfuser')
```

Currently only operations with tensor inputs and one tensor output are supported, and the operation must be composed exclusively of reference or prim operations.

Finally, this adds a new test, test_prims.py, that just tests the broadcast_in_dim prim for now. In the future we'll likely have OpInfos for each prim, but we'll need a reference implementation of broadcast_in_dim to make that interesting.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/76560
Approved by: https://github.com/ngimel",328.0,59.0,"test/test_ops.py,test/test_prims.py,torch/_prims/__init__.py,torch/_prims/context.py,torch/_prims/executor.py,torch/_refs/__init__.py,torch/testing/_internal/common_methods_invocations.py",7.0,6,2,2.363154161,5.0,20270.0,2.0,107557.33333333331,2723.0,6511.0,0.0,Feature Addition,0.0,1
pytorch,fc249c79245af10eeb564ab0f1d42b025a8e07f4,fe4170bda8f6b497c0f9ccebbaab6c2130d9c0aa,Pritam Damania,pritam.damania@fb.com,Thu Oct 03 08:05:18 2019 -0700,1570089918.0,"Add send and recv backward functions for builtin operators RPC. (#25527)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/25527

Master GH issue: https://github.com/pytorch/pytorch/issues/23110.

This change builds upon https://github.com/pytorch/pytorch/pull/24876 and
provides all the autograd hooks needed for a forward pass with distributed rpc
for builtin operators. This change does not address distributed rpc for python
UDFs and that will be addressed in follow up PRs.

Summary of changes:
1. Attach send autograd functions when a request is sent from the client and
response is sent from the server.
2. Attach receive autograd functions when a request is received on the server
and a response is received on the client.
3. Generate a globally unique autograd_message_id for each send/recv autograd
function pair to uniquely identify them.
ghstack-source-id: 91240466

Test Plan: unit tests.

Differential Revision: D17148077

fbshipit-source-id: 192d8a3f552ed7cc939f55dcca332965c9bd3233",1792.0,780.0,"caffe2/CMakeLists.txt,test/cpp/dist_autograd/test_dist_autograd.cpp,test/dist_autograd_test.py,test/dist_utils.py,test/rpc_test.py,tools/build_variables.py,torch/CMakeLists.txt,torch/csrc/THP.h,torch/csrc/byte_order.cpp,torch/csrc/byte_order.h,torch/csrc/distributed/autograd/context/dist_autograd_container.cpp,torch/csrc/distributed/autograd/context/dist_autograd_container.h,torch/csrc/distributed/autograd/context/dist_autograd_context.cpp,torch/csrc/distributed/autograd/context/dist_autograd_context.h,torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp,torch/csrc/distributed/autograd/functions/recvrpc_backward.h,torch/csrc/distributed/autograd/init.cpp,torch/csrc/distributed/autograd/utils.cpp,torch/csrc/distributed/autograd/utils.h,torch/csrc/distributed/rpc/functions.cpp,torch/csrc/distributed/rpc/functions.h,torch/csrc/distributed/rpc/future_message.cpp,torch/csrc/distributed/rpc/future_message.h,torch/csrc/distributed/rpc/init.cpp,torch/csrc/distributed/rpc/message.cpp,torch/csrc/distributed/rpc/message.h,torch/csrc/distributed/rpc/process_group_agent.cpp,torch/csrc/distributed/rpc/process_group_agent.h,torch/csrc/distributed/rpc/python_functions.cpp,torch/csrc/distributed/rpc/python_functions.h,torch/csrc/distributed/rpc/python_rpc_handler.cpp,torch/csrc/distributed/rpc/python_rpc_handler.h,torch/csrc/distributed/rpc/python_udf_call.cpp,torch/csrc/distributed/rpc/python_udf_call.h,torch/csrc/distributed/rpc/python_udf_resp.cpp,torch/csrc/distributed/rpc/python_udf_resp.h,torch/csrc/distributed/rpc/request_callback.cpp,torch/csrc/distributed/rpc/request_callback.h,torch/csrc/distributed/rpc/request_callback_impl.cpp,torch/csrc/distributed/rpc/request_callback_impl.h,torch/csrc/distributed/rpc/rpc_agent.cpp,torch/csrc/distributed/rpc/rpc_agent.h,torch/csrc/distributed/rpc/rpc_command_base.h,torch/csrc/distributed/rpc/rpc_with_autograd.cpp,torch/csrc/distributed/rpc/rpc_with_autograd.h,torch/csrc/distributed/rpc/script_call.cpp,torch/csrc/distributed/rpc/script_call.h,torch/csrc/distributed/rpc/script_remote_call.cpp,torch/csrc/distributed/rpc/script_remote_call.h,torch/csrc/distributed/rpc/script_resp.cpp,torch/csrc/distributed/rpc/script_resp.h,torch/csrc/distributed/rpc/script_ret.cpp,torch/csrc/distributed/rpc/script_ret.h,torch/csrc/distributed/rpc/script_rref_proto.cpp,torch/csrc/distributed/rpc/script_rref_proto.h,torch/csrc/distributed/rpc/utils.cpp,torch/csrc/distributed/rpc/utils.h,torch/csrc/generic/StorageMethods.cpp,torch/csrc/generic/serialization.cpp,torch/csrc/utils/byte_order.cpp,torch/csrc/utils/byte_order.h",61.0,14,4,5.225566794,48.0,6201.0,15.0,1846901.951219512,11985.0,33677.83333,0.0,Feature Addition,0.0,1
pytorch,2bdea8b451aa560f0826ffe721cd8fb43083a0cd,fe4e14ed299dbe160d85e8963e5afe65f54d620b,Sam Gross,colesbury@gmail.com,Sat Nov 04 03:00:48 2017 -0400,1509764448.0,Fix fill derivative (#3483),13.0,1.0,"test/test_autograd.py,tools/autograd/derivatives.yaml",2.0,3,2,0.749595257,38.0,2871.0,1.0,54289.0,313.0,871.9058694,0.0,Corrective,1.0,1
pytorch,28630529ac04413667936abbc9e535b5b64af400,fe580e850e688bd7ed823faf8fc977e347f60dee,Vitaly Fedyunin,vitalyf@fb.com,Fri Jun 21 18:34:00 2019 -0700,1561142040.0,"Rewrite lerp operator to use TensorIterator and support compile-time vectorization. (#22038)

Summary:
Get benefit from the compile time vectorization and multi-threading.

Before:

```python
In [1]: import torch
In [2]: x = torch.randn(1000000)
In [3]: y = torch.randn(1000000)
In [4]: w = 0.7
In [5]: timeit torch.lerp(x, y, w)
2.29 ms Â± 23.9 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)
```

After:

```python
In [1]: import torch
In [2]: x = torch.randn(1000000)
In [3]: y = torch.randn(1000000)
In [4]: w = 0.7
In [5]: timeit torch.lerp(x, y, w)
452 Âµs Â± 1.81 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)
```

After with multi-processing:

```python
In [1]: import torch
In [2]: x = torch.randn(1000000)
In [3]: y = torch.randn(1000000)
In [4]: w = 0.7
In [5]: timeit torch.lerp(x, y, w)
167 Âµs Â± 48.8 Âµs per loop (mean Â± std. dev. of 7 runs, 10000 loops each)
```
Pull Request resolved: https://github.com/pytorch/pytorch/pull/22038

Differential Revision: D15941468

Pulled By: VitalyFedyunin

fbshipit-source-id: fa8a5126187df4e6c849452e035b00b22be25739",100.0,68.0,"aten/src/ATen/native/Lerp.cpp,aten/src/ATen/native/Lerp.h,aten/src/ATen/native/cpu/LerpKernel.cpp",3.0,5,1,1.447172029,1.0,112.0,1.0,3267511.0,9549.0,27768.33333,0.0,,0.0,1
pytorch,ad76fc88073d6c252308376a6a6b986dbab010b3,fe68879832f1198c0e2edb24d6b7b415ab9c87ae,Roy Li,royboy@fb.com,Tue Aug 07 16:51:54 2018 -0700,1533660714.0,"Fix dir(torch) for python 3.7 (#10271)

Summary:
fixes #10160.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/10271

Differential Revision: D9188031

Pulled By: li-roy

fbshipit-source-id: a3620553a8ba2b7391acdf78dbe58afcdb6c5f7f",5.0,0.0,"test/test_torch.py,torch/__init__.py",2.0,2,2,0.970950594,41.0,8817.0,2.0,70406.5,3359.0,9057.333333,0.0,Corrective,1.0,1
pytorch,d1d24304ee6ebd6b298e0b0ffb4f42f95bad55ac,fe8e5eb260e3e6af3fb55c2eb2bd7319afc34099,Kurt Mohler,kmohler@quansight.com,Fri May 21 01:14:19 2021 -0700,1621559659.0,"Change native functions to take `c10::string_view` args instead of `std::string` (#57680)

Summary:
Fixes https://github.com/pytorch/pytorch/issues/53546

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57680

Reviewed By: malfet

Differential Revision: D28511799

Pulled By: ezyang

fbshipit-source-id: 43142f994d048b28b3279ccdb7a28cbaa3190973",295.0,183.0,"aten/src/ATen/BatchingRegistrations.cpp,aten/src/ATen/core/Dict_inl.h,aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h,aten/src/ATen/core/ivalue.cpp,aten/src/ATen/core/ivalue.h,aten/src/ATen/core/ivalue_inl.h,aten/src/ATen/core/jit_type.h,aten/src/ATen/core/op_registration/README.md,aten/src/ATen/native/BatchLinearAlgebra.cpp,aten/src/ATen/native/BatchLinearAlgebraKernel.cpp,aten/src/ATen/native/BinaryOps.cpp,aten/src/ATen/native/Convolution.cpp,aten/src/ATen/native/Linear.cpp,aten/src/ATen/native/LinearAlgebra.cpp,aten/src/ATen/native/LinearAlgebraUtils.h,aten/src/ATen/native/SegmentReduce.cpp,aten/src/ATen/native/Sorting.cpp,aten/src/ATen/native/SpectralOps.cpp,aten/src/ATen/native/TensorAdvancedIndexing.cpp,aten/src/ATen/native/TensorFactories.cpp,aten/src/ATen/native/TestOps.cpp,aten/src/ATen/native/cuda/BatchLinearAlgebra.cu,aten/src/ATen/native/sparse/SparseTensorMath.cpp,aten/src/TH/THAllocator.cpp,aten/src/TH/THAllocator.h,c10/util/StringUtil.h,test/test_overrides.py,tools/autograd/gen_autograd_functions.py,tools/autograd/gen_variable_type.py,tools/autograd/load_derivatives.py,tools/codegen/api/python.py,tools/codegen/api/types.py,torch/csrc/Device.cpp,torch/csrc/api/include/torch/fft.h,torch/csrc/api/include/torch/linalg.h,torch/csrc/autograd/FunctionsManual.cpp,torch/csrc/autograd/FunctionsManual.h,torch/csrc/generic/StorageSharing.cpp,torch/csrc/jit/frontend/tracer.cpp,torch/csrc/jit/frontend/tracer.h,torch/csrc/jit/runtime/register_c10_ops.cpp,torch/csrc/jit/runtime/static/ops.cpp,torch/csrc/utils/python_arg_parser.cpp,torch/csrc/utils/python_arg_parser.h,torch/csrc/utils/python_strings.h",45.0,30,5,4.80228397,33.0,45888.0,34.0,2350200.9111111118,12297.0,27835.5,0.0,Corrective,1.0,1
pytorch,6e640a0acfbf9073aae1734db2fceb94c3d8f28d,fea2bb64c85a91b0b4161a54a4cc5c79b42d5398,Peter Bell,peterbell10@live.co.uk,Wed Nov 17 05:08:27 2021 -0800,1637125707.0,"OpInfos for stft, istft, fftshift, ifftshift (#68198)

Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/68198

This unearths some bugs in istft backward, so I've disabled
backward tests but it's fixed in the next PR in the stack.

cc mruberry peterbell10

Test Plan: Imported from OSS

Reviewed By: VitalyFedyunin

Differential Revision: D32467044

Pulled By: mruberry

fbshipit-source-id: 5cf49560cbeb0263a66aafb48ed1bcc8884b75f1",95.0,0.0,torch/testing/_internal/common_methods_invocations.py,1.0,3,1,0,2.0,13275.0,1.0,2227.0,17142.0,40302.5,0.0,Corrective,1.0,1
pytorch,2a78f6376cac3d91aff869fb6d2d4f3afa0aacfc,fea7a79e0bc622628fdcecbd9121af05d6d42249,kshitij12345,kshitijkalambarkar@gmail.com,Mon May 31 04:08:41 2021 -0700,1622434121.0,"[special] Add ndtr (#58126)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/50345

Plot:
![image](https://user-images.githubusercontent.com/19503980/117942099-54efd680-b328-11eb-8948-c3080779ce19.png)
https://colab.research.google.com/drive/1Of67A042rOImj8wrLF_fUTgoy_wVEOZS?usp=sharing

TODO:
* [x] Add docs (https://13385714-65600975-gh.circle-artifacts.com/0/docs/special.html#torch.special.ndtr)

Pull Request resolved: https://github.com/pytorch/pytorch/pull/58126

Reviewed By: anjali411

Differential Revision: D28700957

Pulled By: mruberry

fbshipit-source-id: 5b9991e97ec1e8fd01518cc9d9849108d35fe406",111.0,0.0,"aten/src/ATen/core/aten_interned_strings.h,aten/src/ATen/native/UnaryOps.cpp,aten/src/ATen/native/native_functions.yaml,docs/source/special.rst,test/test_unary_ufuncs.py,torch/csrc/api/include/torch/special.h,torch/overrides.py,torch/special/__init__.py,torch/testing/_internal/common_methods_invocations.py",9.0,16,4,2.567948905,13.0,22591.0,3.0,109119.44444444444,12576.0,28529.0,0.0,Feature Addition,0.0,1
pytorch,4c5192788ba3d875b7ecf72442d22e9f7f0f4178,fef9a66d08377a8db1b05f3dc8898ce167365a33,Peter Goldsborough,psag@fb.com,Wed Jun 27 21:34:06 2018 -0700,1530135246.0,"Use torch:: instead of at:: (#8911)

Summary:
This PR is the final step to making `torch::` the only  namespace users of the C++ API ever see. Basically, I did:

``` cpp

namespace torch {
using namespace at;
}
```

And then changed `torch::` to `at::` almost everywhere. This worked surprisingly well out of the box. So users can now write `torch::relu`  and `torch::log_softmax` and `torch::conv2d` instead of having to know when to use `at::` and when `torch::`. This is happy!

Another thing I did was to have `using Dtype = at::ScalarType`, which will be the eventual name anyway.

ebetica ezyang apaszke zdevito
Closes https://github.com/pytorch/pytorch/pull/8911

Reviewed By: ezyang

Differential Revision: D8668230

Pulled By: goldsborough

fbshipit-source-id: a72ccb70fca763c396c4b0997d3c4767c8cf4fd3",359.0,343.0,"test/cpp/api/integration.cpp,test/cpp/api/misc.cpp,test/cpp/api/module.cpp,test/cpp/api/modules.cpp,test/cpp/api/optim.cpp,test/cpp/api/optim_baseline.h,test/cpp/api/optim_baseline.py,test/cpp/api/rnn.cpp,test/cpp/api/sequential.cpp,test/cpp/api/serialization.cpp,torch/csrc/api/include/torch/nn/module.h,torch/csrc/api/include/torch/nn/modules/rnn.h,torch/csrc/api/include/torch/optim/lbfgs.h,torch/csrc/api/include/torch/serialization.h,torch/csrc/api/include/torch/tensor.h,torch/csrc/api/src/nn/module.cpp,torch/csrc/api/src/nn/modules/batchnorm.cpp,torch/csrc/api/src/nn/modules/conv.cpp,torch/csrc/api/src/nn/modules/dropout.cpp,torch/csrc/api/src/nn/modules/embedding.cpp,torch/csrc/api/src/nn/modules/linear.cpp,torch/csrc/api/src/nn/modules/rnn.cpp,torch/csrc/api/src/optim/adam.cpp,torch/csrc/api/src/optim/lbfgs.cpp",24.0,15,2,2.88733222,7.0,4548.0,5.0,102862.625,2736.0,6219.333333,0.0,,0.0,1
pytorch,5a3997789cc4b4c88ebc1f168bd83e7aea1eadad,ff00cdd728636d8b08071bbf24d4b4f1398b9d7e,Adam Paszke,adam.paszke@gmail.com,Thu Aug 11 15:56:30 2016 -0700,1470930990.0,Add cunn tests,160.0,56.0,"test/common.py,test/test_cuda.py,test/test_legacy_nn.py,torch/Tensor.py,torch/legacy/nn/BatchNormalization.py,torch/legacy/nn/Bilinear.py,torch/legacy/nn/CMul.py,torch/legacy/nn/ClassNLLCriterion.py,torch/legacy/nn/ClassSimplexCriterion.py,torch/legacy/nn/CosineEmbeddingCriterion.py,torch/legacy/nn/Euclidean.py,torch/legacy/nn/Max.py,torch/legacy/nn/Min.py,torch/legacy/nn/MixtureTable.py,torch/legacy/nn/Module.py,torch/legacy/nn/Normalize.py,torch/legacy/nn/PReLU.py,torch/legacy/nn/Reshape.py,torch/legacy/nn/SelectTable.py,torch/legacy/nn/SoftSign.py,torch/legacy/nn/SpatialFullConvolution.py,torch/legacy/nn/SpatialMaxPooling.py,torch/legacy/nn/Sum.py,torch/legacy/nn/VolumetricConvolution.py,torch/legacy/nn/VolumetricFullConvolution.py,torch/legacy/nn/WeightedEuclidean.py,torch/legacy/nn/utils.py",27.0,4,2,3.20864999,5.0,4921.0,3.0,94918.0,87.0,1149.933333,0.0,Feature Addition,0.0,1
pytorch,61a0df5af060bc44ca5cde4303e815ee3b7761ce,ff4f4a0a35de5b31b0ef7a4a3391c651afb8064d,Pieter Noordhuis,pietern@fb.com,Wed Nov 14 23:04:19 2018 -0800,1542236659.0,"Retry test on ""Address already in use"" error (#13911)

Summary:
This fixes #13907.
Pull Request resolved: https://github.com/pytorch/pytorch/pull/13911

Differential Revision: D13046256

Pulled By: pietern

fbshipit-source-id: bab70cd73ef868e23d4857b06e72830ad29ddb4f",23.0,0.0,"test/common_utils.py,test/test_c10d.py",2.0,1,1,0.558629373,2.0,2277.0,2.0,241059.0,5399.0,16365.83333,0.0,Corrective,1.0,1
pytorch,c911c30520ed25e0f0adfde1ce5794f334512c95,ff982ef73d91bf384d8f6df65a0a63dfbc998b7d,kshitij12345,kshitijkalambarkar@gmail.com,Wed May 12 13:03:49 2021 -0700,1620824629.0,"OpInfo: reshape, reshape_as and minor clean-up (#57460)

Summary:
Reference: https://github.com/pytorch/pytorch/issues/54261

Pull Request resolved: https://github.com/pytorch/pytorch/pull/57460

Reviewed By: nairbv

Differential Revision: D28151675

Pulled By: anjali411

fbshipit-source-id: 2b3bcadab3ff5d1761b2922b63afd70a354e785c",23.0,21.0,"test/test_fx.py,test/test_fx_experimental.py,torch/testing/_internal/common_methods_invocations.py",3.0,4,2,0.312219533,2.0,10910.0,3.0,36263.66666666666,11943.0,27138.0,0.0,,0.0,1
pytorch,3dc402fd1eae13839ac5a5aa22aab3017c3fc961,ffc4a502599dcccb48f9bad5ed44c0d7c4d8f391,Richard Zou,zou3519@gmail.com,Tue Aug 09 21:54:22 2022 -0700,1660082062.0,"[functorch] in-place testing for test_vjp (#83114)

This is relatively simple; we just test that `input.clone().inplace_(...)`
gives us the correct gradients while ignoring incompatible sample
inputs.

Test Plan:
- wait for tests
Pull Request resolved: https://github.com/pytorch/pytorch/pull/83114
Approved by: https://github.com/Chillee",8.0,8.0,functorch/test/test_ops.py,1.0,2,1,0,2.0,1367.0,1.0,3.0,6341.0,14716.0,0.0,Corrective,0.0,1
pytorch,cc24b68584408b3edff50a23c8502bfcd6b90dad,ffcc38cf05d65e7f3e06343d69ec446af38193bd,Sam Gross,colesbury@gmail.com,Fri Dec 16 19:45:56 2016 -0500,1481917556.0,"Deterministic ordering of parameters and buffers. (#317)

Uses the assignment syntax to get deterministic ordering of parameters.
The ordering of parameters using the constructor syntax is
non-deterministic because kwargs use dict() in Python 3.5 and earlier.",123.0,138.0,"test/test_nn.py,torch/cuda/streams.py,torch/nn/modules/activation.py,torch/nn/modules/batchnorm.py,torch/nn/modules/conv.py,torch/nn/modules/linear.py,torch/nn/modules/module.py,torch/nn/modules/normalization.py,torch/nn/modules/pooling.py,torch/nn/modules/rnn.py,torch/nn/modules/sparse.py,torch/nn/parameter.py",12.0,5,2,2.655340537,17.0,3721.0,1.0,22600.0,91.0,55.92564749,0.0,,0.0,1
pytorch,754f3d3fe82dcb8f7fe5749dc98f1de08f1616e2,ffd39f4c9fa3ffec186bab6ba9cc83257a3a3a45,Jon Crall,erotemic@gmail.com,Fri Nov 24 10:11:35 2017 -0500,1511518295.0,added missing arg and improved example clarity (#3444),8.0,4.0,torch/nn/modules/loss.py,1.0,3,1,0,35.0,769.0,1.0,144204.0,811.0,6541.172317,0.0,Feature Addition,0.0,1
pytorch,701e05dcbb5d5fa3e92ddf5495ea54406751142f,ffe0c1ae4d8de75bc4d6cf5c43123df1e538d8a4,Xiang Gao,qasdfgtyuiop@gmail.com,Sun Dec 15 00:01:40 2019 -0800,1576368100.0,"Make test_torch.py pass cuda-memcheck (#29243)

Summary:
Make the following changes:
- When there are more than 10k errors, cuda-memcheck only shows 10k errors, in this case we shouldn't raise an Exception
- Add UNDER_CUDA_MEMCHECK environment to allow disabling `pin_memory` tests when running cuda-memcheck.
- Add a `--ci` command option, when turned on, then this script would run output to stdout instead of writing a file, and exit with an error if cuda-memcheck fails
- Add a `--nohang` command option. When turned on, then hang would be treated as pass instead of error
- Do simple filtering on the test to run: if `'cpu'` in the test name but not `'cuda'` is not in the test name
- Add `--split` and `--rank` to allowing splitting the work (NVIDIA CI has a limitation of 3 hours, we have to split the work to satisfy this limitation)
- The error summary could be `ERROR SUMMARY: 1 error`, or `ERROR SUMMARY: 2 errors`, the tail could be `error` or `errors`, it is not of the same length. The script is fixed to handle this case.
- Ignore errors from `cufft`
Pull Request resolved: https://github.com/pytorch/pytorch/pull/29243

Differential Revision: D18941701

Pulled By: mruberry

fbshipit-source-id: 2048428f32b66ef50c67444c03ce4dd9491179d2",60.0,11.0,"test/common_device_type.py,test/scripts/cuda_memcheck_common.py,test/scripts/run_cuda_memcheck.py,test/test_torch.py",4.0,2,1,1.269760798,40.0,15684.0,3.0,2188730.25,13863.0,37769.33333,0.0,Corrective,1.0,1
pytorch,712a3fad27899dcd9b68b4f14235e0f3d32508db,ffffee6aa99331fb6bed453009350805615499a0,Will Feng,yf225@cornell.edu,Tue Jun 12 21:00:49 2018 -0400,1528837249.0,Skip test_multinomial_invalid_probs on Windows (#8360),1.0,0.0,test/test_torch.py,1.0,1,1,0,39.0,7464.0,1.0,3801.0,703.0,3788.5,0.0,,0.0,1
